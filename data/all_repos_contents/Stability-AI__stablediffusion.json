{"setup.py": "from setuptools import setup, find_packages\n\nsetup(\n    name='stable-diffusion',\n    version='0.0.1',\n    description='',\n    packages=find_packages(),\n    install_requires=[\n        'torch',\n        'numpy',\n        'tqdm',\n    ],\n)", "ldm/util.py": "import importlib\n\nimport torch\nfrom torch import optim\nimport numpy as np\n\nfrom inspect import isfunction\nfrom PIL import Image, ImageDraw, ImageFont\n\n\ndef autocast(f):\n    def do_autocast(*args, **kwargs):\n        with torch.cuda.amp.autocast(enabled=True,\n                                     dtype=torch.get_autocast_gpu_dtype(),\n                                     cache_enabled=torch.is_autocast_cache_enabled()):\n            return f(*args, **kwargs)\n\n    return do_autocast\n\n\ndef log_txt_as_img(wh, xc, size=10):\n    # wh a tuple of (width, height)\n    # xc a list of captions to plot\n    b = len(xc)\n    txts = list()\n    for bi in range(b):\n        txt = Image.new(\"RGB\", wh, color=\"white\")\n        draw = ImageDraw.Draw(txt)\n        font = ImageFont.truetype('data/DejaVuSans.ttf', size=size)\n        nc = int(40 * (wh[0] / 256))\n        lines = \"\\n\".join(xc[bi][start:start + nc] for start in range(0, len(xc[bi]), nc))\n\n        try:\n            draw.text((0, 0), lines, fill=\"black\", font=font)\n        except UnicodeEncodeError:\n            print(\"Cant encode string for logging. Skipping.\")\n\n        txt = np.array(txt).transpose(2, 0, 1) / 127.5 - 1.0\n        txts.append(txt)\n    txts = np.stack(txts)\n    txts = torch.tensor(txts)\n    return txts\n\n\ndef ismap(x):\n    if not isinstance(x, torch.Tensor):\n        return False\n    return (len(x.shape) == 4) and (x.shape[1] > 3)\n\n\ndef isimage(x):\n    if not isinstance(x,torch.Tensor):\n        return False\n    return (len(x.shape) == 4) and (x.shape[1] == 3 or x.shape[1] == 1)\n\n\ndef exists(x):\n    return x is not None\n\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if isfunction(d) else d\n\n\ndef mean_flat(tensor):\n    \"\"\"\n    https://github.com/openai/guided-diffusion/blob/27c20a8fab9cb472df5d6bdd6c8d11c8f430b924/guided_diffusion/nn.py#L86\n    Take the mean over all non-batch dimensions.\n    \"\"\"\n    return tensor.mean(dim=list(range(1, len(tensor.shape))))\n\n\ndef count_params(model, verbose=False):\n    total_params = sum(p.numel() for p in model.parameters())\n    if verbose:\n        print(f\"{model.__class__.__name__} has {total_params*1.e-6:.2f} M params.\")\n    return total_params\n\n\ndef instantiate_from_config(config):\n    if not \"target\" in config:\n        if config == '__is_first_stage__':\n            return None\n        elif config == \"__is_unconditional__\":\n            return None\n        raise KeyError(\"Expected key `target` to instantiate.\")\n    return get_obj_from_str(config[\"target\"])(**config.get(\"params\", dict()))\n\n\ndef get_obj_from_str(string, reload=False):\n    module, cls = string.rsplit(\".\", 1)\n    if reload:\n        module_imp = importlib.import_module(module)\n        importlib.reload(module_imp)\n    return getattr(importlib.import_module(module, package=None), cls)\n\n\nclass AdamWwithEMAandWings(optim.Optimizer):\n    # credit to https://gist.github.com/crowsonkb/65f7265353f403714fce3b2595e0b298\n    def __init__(self, params, lr=1.e-3, betas=(0.9, 0.999), eps=1.e-8,  # TODO: check hyperparameters before using\n                 weight_decay=1.e-2, amsgrad=False, ema_decay=0.9999,   # ema decay to match previous code\n                 ema_power=1., param_names=()):\n        \"\"\"AdamW that saves EMA versions of the parameters.\"\"\"\n        if not 0.0 <= lr:\n            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n        if not 0.0 <= eps:\n            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n        if not 0.0 <= betas[0] < 1.0:\n            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n        if not 0.0 <= betas[1] < 1.0:\n            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n        if not 0.0 <= weight_decay:\n            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n        if not 0.0 <= ema_decay <= 1.0:\n            raise ValueError(\"Invalid ema_decay value: {}\".format(ema_decay))\n        defaults = dict(lr=lr, betas=betas, eps=eps,\n                        weight_decay=weight_decay, amsgrad=amsgrad, ema_decay=ema_decay,\n                        ema_power=ema_power, param_names=param_names)\n        super().__init__(params, defaults)\n\n    def __setstate__(self, state):\n        super().__setstate__(state)\n        for group in self.param_groups:\n            group.setdefault('amsgrad', False)\n\n    @torch.no_grad()\n    def step(self, closure=None):\n        \"\"\"Performs a single optimization step.\n        Args:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        \"\"\"\n        loss = None\n        if closure is not None:\n            with torch.enable_grad():\n                loss = closure()\n\n        for group in self.param_groups:\n            params_with_grad = []\n            grads = []\n            exp_avgs = []\n            exp_avg_sqs = []\n            ema_params_with_grad = []\n            state_sums = []\n            max_exp_avg_sqs = []\n            state_steps = []\n            amsgrad = group['amsgrad']\n            beta1, beta2 = group['betas']\n            ema_decay = group['ema_decay']\n            ema_power = group['ema_power']\n\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n                params_with_grad.append(p)\n                if p.grad.is_sparse:\n                    raise RuntimeError('AdamW does not support sparse gradients')\n                grads.append(p.grad)\n\n                state = self.state[p]\n\n                # State initialization\n                if len(state) == 0:\n                    state['step'] = 0\n                    # Exponential moving average of gradient values\n                    state['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n                    # Exponential moving average of squared gradient values\n                    state['exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n                    if amsgrad:\n                        # Maintains max of all exp. moving avg. of sq. grad. values\n                        state['max_exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n                    # Exponential moving average of parameter values\n                    state['param_exp_avg'] = p.detach().float().clone()\n\n                exp_avgs.append(state['exp_avg'])\n                exp_avg_sqs.append(state['exp_avg_sq'])\n                ema_params_with_grad.append(state['param_exp_avg'])\n\n                if amsgrad:\n                    max_exp_avg_sqs.append(state['max_exp_avg_sq'])\n\n                # update the steps for each param group update\n                state['step'] += 1\n                # record the step after step update\n                state_steps.append(state['step'])\n\n            optim._functional.adamw(params_with_grad,\n                    grads,\n                    exp_avgs,\n                    exp_avg_sqs,\n                    max_exp_avg_sqs,\n                    state_steps,\n                    amsgrad=amsgrad,\n                    beta1=beta1,\n                    beta2=beta2,\n                    lr=group['lr'],\n                    weight_decay=group['weight_decay'],\n                    eps=group['eps'],\n                    maximize=False)\n\n            cur_ema_decay = min(ema_decay, 1 - state['step'] ** -ema_power)\n            for param, ema_param in zip(params_with_grad, ema_params_with_grad):\n                ema_param.mul_(cur_ema_decay).add_(param.float(), alpha=1 - cur_ema_decay)\n\n        return loss", "ldm/models/autoencoder.py": "import torch\nimport pytorch_lightning as pl\nimport torch.nn.functional as F\nfrom contextlib import contextmanager\n\nfrom ldm.modules.diffusionmodules.model import Encoder, Decoder\nfrom ldm.modules.distributions.distributions import DiagonalGaussianDistribution\n\nfrom ldm.util import instantiate_from_config\nfrom ldm.modules.ema import LitEma\n\n\nclass AutoencoderKL(pl.LightningModule):\n    def __init__(self,\n                 ddconfig,\n                 lossconfig,\n                 embed_dim,\n                 ckpt_path=None,\n                 ignore_keys=[],\n                 image_key=\"image\",\n                 colorize_nlabels=None,\n                 monitor=None,\n                 ema_decay=None,\n                 learn_logvar=False\n                 ):\n        super().__init__()\n        self.learn_logvar = learn_logvar\n        self.image_key = image_key\n        self.encoder = Encoder(**ddconfig)\n        self.decoder = Decoder(**ddconfig)\n        self.loss = instantiate_from_config(lossconfig)\n        assert ddconfig[\"double_z\"]\n        self.quant_conv = torch.nn.Conv2d(2*ddconfig[\"z_channels\"], 2*embed_dim, 1)\n        self.post_quant_conv = torch.nn.Conv2d(embed_dim, ddconfig[\"z_channels\"], 1)\n        self.embed_dim = embed_dim\n        if colorize_nlabels is not None:\n            assert type(colorize_nlabels)==int\n            self.register_buffer(\"colorize\", torch.randn(3, colorize_nlabels, 1, 1))\n        if monitor is not None:\n            self.monitor = monitor\n\n        self.use_ema = ema_decay is not None\n        if self.use_ema:\n            self.ema_decay = ema_decay\n            assert 0. < ema_decay < 1.\n            self.model_ema = LitEma(self, decay=ema_decay)\n            print(f\"Keeping EMAs of {len(list(self.model_ema.buffers()))}.\")\n\n        if ckpt_path is not None:\n            self.init_from_ckpt(ckpt_path, ignore_keys=ignore_keys)\n\n    def init_from_ckpt(self, path, ignore_keys=list()):\n        sd = torch.load(path, map_location=\"cpu\")[\"state_dict\"]\n        keys = list(sd.keys())\n        for k in keys:\n            for ik in ignore_keys:\n                if k.startswith(ik):\n                    print(\"Deleting key {} from state_dict.\".format(k))\n                    del sd[k]\n        self.load_state_dict(sd, strict=False)\n        print(f\"Restored from {path}\")\n\n    @contextmanager\n    def ema_scope(self, context=None):\n        if self.use_ema:\n            self.model_ema.store(self.parameters())\n            self.model_ema.copy_to(self)\n            if context is not None:\n                print(f\"{context}: Switched to EMA weights\")\n        try:\n            yield None\n        finally:\n            if self.use_ema:\n                self.model_ema.restore(self.parameters())\n                if context is not None:\n                    print(f\"{context}: Restored training weights\")\n\n    def on_train_batch_end(self, *args, **kwargs):\n        if self.use_ema:\n            self.model_ema(self)\n\n    def encode(self, x):\n        h = self.encoder(x)\n        moments = self.quant_conv(h)\n        posterior = DiagonalGaussianDistribution(moments)\n        return posterior\n\n    def decode(self, z):\n        z = self.post_quant_conv(z)\n        dec = self.decoder(z)\n        return dec\n\n    def forward(self, input, sample_posterior=True):\n        posterior = self.encode(input)\n        if sample_posterior:\n            z = posterior.sample()\n        else:\n            z = posterior.mode()\n        dec = self.decode(z)\n        return dec, posterior\n\n    def get_input(self, batch, k):\n        x = batch[k]\n        if len(x.shape) == 3:\n            x = x[..., None]\n        x = x.permute(0, 3, 1, 2).to(memory_format=torch.contiguous_format).float()\n        return x\n\n    def training_step(self, batch, batch_idx, optimizer_idx):\n        inputs = self.get_input(batch, self.image_key)\n        reconstructions, posterior = self(inputs)\n\n        if optimizer_idx == 0:\n            # train encoder+decoder+logvar\n            aeloss, log_dict_ae = self.loss(inputs, reconstructions, posterior, optimizer_idx, self.global_step,\n                                            last_layer=self.get_last_layer(), split=\"train\")\n            self.log(\"aeloss\", aeloss, prog_bar=True, logger=True, on_step=True, on_epoch=True)\n            self.log_dict(log_dict_ae, prog_bar=False, logger=True, on_step=True, on_epoch=False)\n            return aeloss\n\n        if optimizer_idx == 1:\n            # train the discriminator\n            discloss, log_dict_disc = self.loss(inputs, reconstructions, posterior, optimizer_idx, self.global_step,\n                                                last_layer=self.get_last_layer(), split=\"train\")\n\n            self.log(\"discloss\", discloss, prog_bar=True, logger=True, on_step=True, on_epoch=True)\n            self.log_dict(log_dict_disc, prog_bar=False, logger=True, on_step=True, on_epoch=False)\n            return discloss\n\n    def validation_step(self, batch, batch_idx):\n        log_dict = self._validation_step(batch, batch_idx)\n        with self.ema_scope():\n            log_dict_ema = self._validation_step(batch, batch_idx, postfix=\"_ema\")\n        return log_dict\n\n    def _validation_step(self, batch, batch_idx, postfix=\"\"):\n        inputs = self.get_input(batch, self.image_key)\n        reconstructions, posterior = self(inputs)\n        aeloss, log_dict_ae = self.loss(inputs, reconstructions, posterior, 0, self.global_step,\n                                        last_layer=self.get_last_layer(), split=\"val\"+postfix)\n\n        discloss, log_dict_disc = self.loss(inputs, reconstructions, posterior, 1, self.global_step,\n                                            last_layer=self.get_last_layer(), split=\"val\"+postfix)\n\n        self.log(f\"val{postfix}/rec_loss\", log_dict_ae[f\"val{postfix}/rec_loss\"])\n        self.log_dict(log_dict_ae)\n        self.log_dict(log_dict_disc)\n        return self.log_dict\n\n    def configure_optimizers(self):\n        lr = self.learning_rate\n        ae_params_list = list(self.encoder.parameters()) + list(self.decoder.parameters()) + list(\n            self.quant_conv.parameters()) + list(self.post_quant_conv.parameters())\n        if self.learn_logvar:\n            print(f\"{self.__class__.__name__}: Learning logvar\")\n            ae_params_list.append(self.loss.logvar)\n        opt_ae = torch.optim.Adam(ae_params_list,\n                                  lr=lr, betas=(0.5, 0.9))\n        opt_disc = torch.optim.Adam(self.loss.discriminator.parameters(),\n                                    lr=lr, betas=(0.5, 0.9))\n        return [opt_ae, opt_disc], []\n\n    def get_last_layer(self):\n        return self.decoder.conv_out.weight\n\n    @torch.no_grad()\n    def log_images(self, batch, only_inputs=False, log_ema=False, **kwargs):\n        log = dict()\n        x = self.get_input(batch, self.image_key)\n        x = x.to(self.device)\n        if not only_inputs:\n            xrec, posterior = self(x)\n            if x.shape[1] > 3:\n                # colorize with random projection\n                assert xrec.shape[1] > 3\n                x = self.to_rgb(x)\n                xrec = self.to_rgb(xrec)\n            log[\"samples\"] = self.decode(torch.randn_like(posterior.sample()))\n            log[\"reconstructions\"] = xrec\n            if log_ema or self.use_ema:\n                with self.ema_scope():\n                    xrec_ema, posterior_ema = self(x)\n                    if x.shape[1] > 3:\n                        # colorize with random projection\n                        assert xrec_ema.shape[1] > 3\n                        xrec_ema = self.to_rgb(xrec_ema)\n                    log[\"samples_ema\"] = self.decode(torch.randn_like(posterior_ema.sample()))\n                    log[\"reconstructions_ema\"] = xrec_ema\n        log[\"inputs\"] = x\n        return log\n\n    def to_rgb(self, x):\n        assert self.image_key == \"segmentation\"\n        if not hasattr(self, \"colorize\"):\n            self.register_buffer(\"colorize\", torch.randn(3, x.shape[1], 1, 1).to(x))\n        x = F.conv2d(x, weight=self.colorize)\n        x = 2.*(x-x.min())/(x.max()-x.min()) - 1.\n        return x\n\n\nclass IdentityFirstStage(torch.nn.Module):\n    def __init__(self, *args, vq_interface=False, **kwargs):\n        self.vq_interface = vq_interface\n        super().__init__()\n\n    def encode(self, x, *args, **kwargs):\n        return x\n\n    def decode(self, x, *args, **kwargs):\n        return x\n\n    def quantize(self, x, *args, **kwargs):\n        if self.vq_interface:\n            return x, None, [None, None, None]\n        return x\n\n    def forward(self, x, *args, **kwargs):\n        return x\n\n", "ldm/models/diffusion/ddim.py": "\"\"\"SAMPLING ONLY.\"\"\"\n\nimport torch\nimport numpy as np\nfrom tqdm import tqdm\n\nfrom ldm.modules.diffusionmodules.util import make_ddim_sampling_parameters, make_ddim_timesteps, noise_like, extract_into_tensor\n\n\nclass DDIMSampler(object):\n    def __init__(self, model, schedule=\"linear\", device=torch.device(\"cuda\"), **kwargs):\n        super().__init__()\n        self.model = model\n        self.ddpm_num_timesteps = model.num_timesteps\n        self.schedule = schedule\n        self.device = device\n\n    def register_buffer(self, name, attr):\n        if type(attr) == torch.Tensor:\n            if attr.device != self.device:\n                attr = attr.to(self.device)\n        setattr(self, name, attr)\n\n    def make_schedule(self, ddim_num_steps, ddim_discretize=\"uniform\", ddim_eta=0., verbose=True):\n        self.ddim_timesteps = make_ddim_timesteps(ddim_discr_method=ddim_discretize, num_ddim_timesteps=ddim_num_steps,\n                                                  num_ddpm_timesteps=self.ddpm_num_timesteps,verbose=verbose)\n        alphas_cumprod = self.model.alphas_cumprod\n        assert alphas_cumprod.shape[0] == self.ddpm_num_timesteps, 'alphas have to be defined for each timestep'\n        to_torch = lambda x: x.clone().detach().to(torch.float32).to(self.model.device)\n\n        self.register_buffer('betas', to_torch(self.model.betas))\n        self.register_buffer('alphas_cumprod', to_torch(alphas_cumprod))\n        self.register_buffer('alphas_cumprod_prev', to_torch(self.model.alphas_cumprod_prev))\n\n        # calculations for diffusion q(x_t | x_{t-1}) and others\n        self.register_buffer('sqrt_alphas_cumprod', to_torch(np.sqrt(alphas_cumprod.cpu())))\n        self.register_buffer('sqrt_one_minus_alphas_cumprod', to_torch(np.sqrt(1. - alphas_cumprod.cpu())))\n        self.register_buffer('log_one_minus_alphas_cumprod', to_torch(np.log(1. - alphas_cumprod.cpu())))\n        self.register_buffer('sqrt_recip_alphas_cumprod', to_torch(np.sqrt(1. / alphas_cumprod.cpu())))\n        self.register_buffer('sqrt_recipm1_alphas_cumprod', to_torch(np.sqrt(1. / alphas_cumprod.cpu() - 1)))\n\n        # ddim sampling parameters\n        ddim_sigmas, ddim_alphas, ddim_alphas_prev = make_ddim_sampling_parameters(alphacums=alphas_cumprod.cpu(),\n                                                                                   ddim_timesteps=self.ddim_timesteps,\n                                                                                   eta=ddim_eta,verbose=verbose)\n        self.register_buffer('ddim_sigmas', ddim_sigmas)\n        self.register_buffer('ddim_alphas', ddim_alphas)\n        self.register_buffer('ddim_alphas_prev', ddim_alphas_prev)\n        self.register_buffer('ddim_sqrt_one_minus_alphas', np.sqrt(1. - ddim_alphas))\n        sigmas_for_original_sampling_steps = ddim_eta * torch.sqrt(\n            (1 - self.alphas_cumprod_prev) / (1 - self.alphas_cumprod) * (\n                        1 - self.alphas_cumprod / self.alphas_cumprod_prev))\n        self.register_buffer('ddim_sigmas_for_original_num_steps', sigmas_for_original_sampling_steps)\n\n    @torch.no_grad()\n    def sample(self,\n               S,\n               batch_size,\n               shape,\n               conditioning=None,\n               callback=None,\n               normals_sequence=None,\n               img_callback=None,\n               quantize_x0=False,\n               eta=0.,\n               mask=None,\n               x0=None,\n               temperature=1.,\n               noise_dropout=0.,\n               score_corrector=None,\n               corrector_kwargs=None,\n               verbose=True,\n               x_T=None,\n               log_every_t=100,\n               unconditional_guidance_scale=1.,\n               unconditional_conditioning=None, # this has to come in the same format as the conditioning, # e.g. as encoded tokens, ...\n               dynamic_threshold=None,\n               ucg_schedule=None,\n               **kwargs\n               ):\n        if conditioning is not None:\n            if isinstance(conditioning, dict):\n                ctmp = conditioning[list(conditioning.keys())[0]]\n                while isinstance(ctmp, list): ctmp = ctmp[0]\n                cbs = ctmp.shape[0]\n                if cbs != batch_size:\n                    print(f\"Warning: Got {cbs} conditionings but batch-size is {batch_size}\")\n\n            elif isinstance(conditioning, list):\n                for ctmp in conditioning:\n                    if ctmp.shape[0] != batch_size:\n                        print(f\"Warning: Got {cbs} conditionings but batch-size is {batch_size}\")\n\n            else:\n                if conditioning.shape[0] != batch_size:\n                    print(f\"Warning: Got {conditioning.shape[0]} conditionings but batch-size is {batch_size}\")\n\n        self.make_schedule(ddim_num_steps=S, ddim_eta=eta, verbose=verbose)\n        # sampling\n        C, H, W = shape\n        size = (batch_size, C, H, W)\n        print(f'Data shape for DDIM sampling is {size}, eta {eta}')\n\n        samples, intermediates = self.ddim_sampling(conditioning, size,\n                                                    callback=callback,\n                                                    img_callback=img_callback,\n                                                    quantize_denoised=quantize_x0,\n                                                    mask=mask, x0=x0,\n                                                    ddim_use_original_steps=False,\n                                                    noise_dropout=noise_dropout,\n                                                    temperature=temperature,\n                                                    score_corrector=score_corrector,\n                                                    corrector_kwargs=corrector_kwargs,\n                                                    x_T=x_T,\n                                                    log_every_t=log_every_t,\n                                                    unconditional_guidance_scale=unconditional_guidance_scale,\n                                                    unconditional_conditioning=unconditional_conditioning,\n                                                    dynamic_threshold=dynamic_threshold,\n                                                    ucg_schedule=ucg_schedule\n                                                    )\n        return samples, intermediates\n\n    @torch.no_grad()\n    def ddim_sampling(self, cond, shape,\n                      x_T=None, ddim_use_original_steps=False,\n                      callback=None, timesteps=None, quantize_denoised=False,\n                      mask=None, x0=None, img_callback=None, log_every_t=100,\n                      temperature=1., noise_dropout=0., score_corrector=None, corrector_kwargs=None,\n                      unconditional_guidance_scale=1., unconditional_conditioning=None, dynamic_threshold=None,\n                      ucg_schedule=None):\n        device = self.model.betas.device\n        b = shape[0]\n        if x_T is None:\n            img = torch.randn(shape, device=device)\n        else:\n            img = x_T\n\n        if timesteps is None:\n            timesteps = self.ddpm_num_timesteps if ddim_use_original_steps else self.ddim_timesteps\n        elif timesteps is not None and not ddim_use_original_steps:\n            subset_end = int(min(timesteps / self.ddim_timesteps.shape[0], 1) * self.ddim_timesteps.shape[0]) - 1\n            timesteps = self.ddim_timesteps[:subset_end]\n\n        intermediates = {'x_inter': [img], 'pred_x0': [img]}\n        time_range = reversed(range(0,timesteps)) if ddim_use_original_steps else np.flip(timesteps)\n        total_steps = timesteps if ddim_use_original_steps else timesteps.shape[0]\n        print(f\"Running DDIM Sampling with {total_steps} timesteps\")\n\n        iterator = tqdm(time_range, desc='DDIM Sampler', total=total_steps)\n\n        for i, step in enumerate(iterator):\n            index = total_steps - i - 1\n            ts = torch.full((b,), step, device=device, dtype=torch.long)\n\n            if mask is not None:\n                assert x0 is not None\n                img_orig = self.model.q_sample(x0, ts)  # TODO: deterministic forward pass?\n                img = img_orig * mask + (1. - mask) * img\n\n            if ucg_schedule is not None:\n                assert len(ucg_schedule) == len(time_range)\n                unconditional_guidance_scale = ucg_schedule[i]\n\n            outs = self.p_sample_ddim(img, cond, ts, index=index, use_original_steps=ddim_use_original_steps,\n                                      quantize_denoised=quantize_denoised, temperature=temperature,\n                                      noise_dropout=noise_dropout, score_corrector=score_corrector,\n                                      corrector_kwargs=corrector_kwargs,\n                                      unconditional_guidance_scale=unconditional_guidance_scale,\n                                      unconditional_conditioning=unconditional_conditioning,\n                                      dynamic_threshold=dynamic_threshold)\n            img, pred_x0 = outs\n            if callback: callback(i)\n            if img_callback: img_callback(pred_x0, i)\n\n            if index % log_every_t == 0 or index == total_steps - 1:\n                intermediates['x_inter'].append(img)\n                intermediates['pred_x0'].append(pred_x0)\n\n        return img, intermediates\n\n    @torch.no_grad()\n    def p_sample_ddim(self, x, c, t, index, repeat_noise=False, use_original_steps=False, quantize_denoised=False,\n                      temperature=1., noise_dropout=0., score_corrector=None, corrector_kwargs=None,\n                      unconditional_guidance_scale=1., unconditional_conditioning=None,\n                      dynamic_threshold=None):\n        b, *_, device = *x.shape, x.device\n\n        if unconditional_conditioning is None or unconditional_guidance_scale == 1.:\n            model_output = self.model.apply_model(x, t, c)\n        else:\n            x_in = torch.cat([x] * 2)\n            t_in = torch.cat([t] * 2)\n            if isinstance(c, dict):\n                assert isinstance(unconditional_conditioning, dict)\n                c_in = dict()\n                for k in c:\n                    if isinstance(c[k], list):\n                        c_in[k] = [torch.cat([\n                            unconditional_conditioning[k][i],\n                            c[k][i]]) for i in range(len(c[k]))]\n                    else:\n                        c_in[k] = torch.cat([\n                                unconditional_conditioning[k],\n                                c[k]])\n            elif isinstance(c, list):\n                c_in = list()\n                assert isinstance(unconditional_conditioning, list)\n                for i in range(len(c)):\n                    c_in.append(torch.cat([unconditional_conditioning[i], c[i]]))\n            else:\n                c_in = torch.cat([unconditional_conditioning, c])\n            model_uncond, model_t = self.model.apply_model(x_in, t_in, c_in).chunk(2)\n            model_output = model_uncond + unconditional_guidance_scale * (model_t - model_uncond)\n\n        if self.model.parameterization == \"v\":\n            e_t = self.model.predict_eps_from_z_and_v(x, t, model_output)\n        else:\n            e_t = model_output\n\n        if score_corrector is not None:\n            assert self.model.parameterization == \"eps\", 'not implemented'\n            e_t = score_corrector.modify_score(self.model, e_t, x, t, c, **corrector_kwargs)\n\n        alphas = self.model.alphas_cumprod if use_original_steps else self.ddim_alphas\n        alphas_prev = self.model.alphas_cumprod_prev if use_original_steps else self.ddim_alphas_prev\n        sqrt_one_minus_alphas = self.model.sqrt_one_minus_alphas_cumprod if use_original_steps else self.ddim_sqrt_one_minus_alphas\n        sigmas = self.model.ddim_sigmas_for_original_num_steps if use_original_steps else self.ddim_sigmas\n        # select parameters corresponding to the currently considered timestep\n        a_t = torch.full((b, 1, 1, 1), alphas[index], device=device)\n        a_prev = torch.full((b, 1, 1, 1), alphas_prev[index], device=device)\n        sigma_t = torch.full((b, 1, 1, 1), sigmas[index], device=device)\n        sqrt_one_minus_at = torch.full((b, 1, 1, 1), sqrt_one_minus_alphas[index],device=device)\n\n        # current prediction for x_0\n        if self.model.parameterization != \"v\":\n            pred_x0 = (x - sqrt_one_minus_at * e_t) / a_t.sqrt()\n        else:\n            pred_x0 = self.model.predict_start_from_z_and_v(x, t, model_output)\n\n        if quantize_denoised:\n            pred_x0, _, *_ = self.model.first_stage_model.quantize(pred_x0)\n\n        if dynamic_threshold is not None:\n            raise NotImplementedError()\n\n        # direction pointing to x_t\n        dir_xt = (1. - a_prev - sigma_t**2).sqrt() * e_t\n        noise = sigma_t * noise_like(x.shape, device, repeat_noise) * temperature\n        if noise_dropout > 0.:\n            noise = torch.nn.functional.dropout(noise, p=noise_dropout)\n        x_prev = a_prev.sqrt() * pred_x0 + dir_xt + noise\n        return x_prev, pred_x0\n\n    @torch.no_grad()\n    def encode(self, x0, c, t_enc, use_original_steps=False, return_intermediates=None,\n               unconditional_guidance_scale=1.0, unconditional_conditioning=None, callback=None):\n        num_reference_steps = self.ddpm_num_timesteps if use_original_steps else self.ddim_timesteps.shape[0]\n\n        assert t_enc <= num_reference_steps\n        num_steps = t_enc\n\n        if use_original_steps:\n            alphas_next = self.alphas_cumprod[:num_steps]\n            alphas = self.alphas_cumprod_prev[:num_steps]\n        else:\n            alphas_next = self.ddim_alphas[:num_steps]\n            alphas = torch.tensor(self.ddim_alphas_prev[:num_steps])\n\n        x_next = x0\n        intermediates = []\n        inter_steps = []\n        for i in tqdm(range(num_steps), desc='Encoding Image'):\n            t = torch.full((x0.shape[0],), i, device=self.model.device, dtype=torch.long)\n            if unconditional_guidance_scale == 1.:\n                noise_pred = self.model.apply_model(x_next, t, c)\n            else:\n                assert unconditional_conditioning is not None\n                e_t_uncond, noise_pred = torch.chunk(\n                    self.model.apply_model(torch.cat((x_next, x_next)), torch.cat((t, t)),\n                                           torch.cat((unconditional_conditioning, c))), 2)\n                noise_pred = e_t_uncond + unconditional_guidance_scale * (noise_pred - e_t_uncond)\n\n            xt_weighted = (alphas_next[i] / alphas[i]).sqrt() * x_next\n            weighted_noise_pred = alphas_next[i].sqrt() * (\n                    (1 / alphas_next[i] - 1).sqrt() - (1 / alphas[i] - 1).sqrt()) * noise_pred\n            x_next = xt_weighted + weighted_noise_pred\n            if return_intermediates and i % (\n                    num_steps // return_intermediates) == 0 and i < num_steps - 1:\n                intermediates.append(x_next)\n                inter_steps.append(i)\n            elif return_intermediates and i >= num_steps - 2:\n                intermediates.append(x_next)\n                inter_steps.append(i)\n            if callback: callback(i)\n\n        out = {'x_encoded': x_next, 'intermediate_steps': inter_steps}\n        if return_intermediates:\n            out.update({'intermediates': intermediates})\n        return x_next, out\n\n    @torch.no_grad()\n    def stochastic_encode(self, x0, t, use_original_steps=False, noise=None):\n        # fast, but does not allow for exact reconstruction\n        # t serves as an index to gather the correct alphas\n        if use_original_steps:\n            sqrt_alphas_cumprod = self.sqrt_alphas_cumprod\n            sqrt_one_minus_alphas_cumprod = self.sqrt_one_minus_alphas_cumprod\n        else:\n            sqrt_alphas_cumprod = torch.sqrt(self.ddim_alphas)\n            sqrt_one_minus_alphas_cumprod = self.ddim_sqrt_one_minus_alphas\n\n        if noise is None:\n            noise = torch.randn_like(x0)\n        return (extract_into_tensor(sqrt_alphas_cumprod, t, x0.shape) * x0 +\n                extract_into_tensor(sqrt_one_minus_alphas_cumprod, t, x0.shape) * noise)\n\n    @torch.no_grad()\n    def decode(self, x_latent, cond, t_start, unconditional_guidance_scale=1.0, unconditional_conditioning=None,\n               use_original_steps=False, callback=None):\n\n        timesteps = np.arange(self.ddpm_num_timesteps) if use_original_steps else self.ddim_timesteps\n        timesteps = timesteps[:t_start]\n\n        time_range = np.flip(timesteps)\n        total_steps = timesteps.shape[0]\n        print(f\"Running DDIM Sampling with {total_steps} timesteps\")\n\n        iterator = tqdm(time_range, desc='Decoding image', total=total_steps)\n        x_dec = x_latent\n        for i, step in enumerate(iterator):\n            index = total_steps - i - 1\n            ts = torch.full((x_latent.shape[0],), step, device=x_latent.device, dtype=torch.long)\n            x_dec, _ = self.p_sample_ddim(x_dec, cond, ts, index=index, use_original_steps=use_original_steps,\n                                          unconditional_guidance_scale=unconditional_guidance_scale,\n                                          unconditional_conditioning=unconditional_conditioning)\n            if callback: callback(i)\n        return x_dec", "ldm/models/diffusion/sampling_util.py": "import torch\nimport numpy as np\n\n\ndef append_dims(x, target_dims):\n    \"\"\"Appends dimensions to the end of a tensor until it has target_dims dimensions.\n    From https://github.com/crowsonkb/k-diffusion/blob/master/k_diffusion/utils.py\"\"\"\n    dims_to_append = target_dims - x.ndim\n    if dims_to_append < 0:\n        raise ValueError(f'input has {x.ndim} dims but target_dims is {target_dims}, which is less')\n    return x[(...,) + (None,) * dims_to_append]\n\n\ndef norm_thresholding(x0, value):\n    s = append_dims(x0.pow(2).flatten(1).mean(1).sqrt().clamp(min=value), x0.ndim)\n    return x0 * (value / s)\n\n\ndef spatial_norm_thresholding(x0, value):\n    # b c h w\n    s = x0.pow(2).mean(1, keepdim=True).sqrt().clamp(min=value)\n    return x0 * (value / s)", "ldm/models/diffusion/__init__.py": "", "ldm/models/diffusion/plms.py": "\"\"\"SAMPLING ONLY.\"\"\"\n\nimport torch\nimport numpy as np\nfrom tqdm import tqdm\nfrom functools import partial\n\nfrom ldm.modules.diffusionmodules.util import make_ddim_sampling_parameters, make_ddim_timesteps, noise_like\nfrom ldm.models.diffusion.sampling_util import norm_thresholding\n\n\nclass PLMSSampler(object):\n    def __init__(self, model, schedule=\"linear\", device=torch.device(\"cuda\"), **kwargs):\n        super().__init__()\n        self.model = model\n        self.ddpm_num_timesteps = model.num_timesteps\n        self.schedule = schedule\n        self.device = device\n\n    def register_buffer(self, name, attr):\n        if type(attr) == torch.Tensor:\n            if attr.device != self.device:\n                attr = attr.to(self.device)\n        setattr(self, name, attr)\n\n    def make_schedule(self, ddim_num_steps, ddim_discretize=\"uniform\", ddim_eta=0., verbose=True):\n        if ddim_eta != 0:\n            raise ValueError('ddim_eta must be 0 for PLMS')\n        self.ddim_timesteps = make_ddim_timesteps(ddim_discr_method=ddim_discretize, num_ddim_timesteps=ddim_num_steps,\n                                                  num_ddpm_timesteps=self.ddpm_num_timesteps,verbose=verbose)\n        alphas_cumprod = self.model.alphas_cumprod\n        assert alphas_cumprod.shape[0] == self.ddpm_num_timesteps, 'alphas have to be defined for each timestep'\n        to_torch = lambda x: x.clone().detach().to(torch.float32).to(self.model.device)\n\n        self.register_buffer('betas', to_torch(self.model.betas))\n        self.register_buffer('alphas_cumprod', to_torch(alphas_cumprod))\n        self.register_buffer('alphas_cumprod_prev', to_torch(self.model.alphas_cumprod_prev))\n\n        # calculations for diffusion q(x_t | x_{t-1}) and others\n        self.register_buffer('sqrt_alphas_cumprod', to_torch(np.sqrt(alphas_cumprod.cpu())))\n        self.register_buffer('sqrt_one_minus_alphas_cumprod', to_torch(np.sqrt(1. - alphas_cumprod.cpu())))\n        self.register_buffer('log_one_minus_alphas_cumprod', to_torch(np.log(1. - alphas_cumprod.cpu())))\n        self.register_buffer('sqrt_recip_alphas_cumprod', to_torch(np.sqrt(1. / alphas_cumprod.cpu())))\n        self.register_buffer('sqrt_recipm1_alphas_cumprod', to_torch(np.sqrt(1. / alphas_cumprod.cpu() - 1)))\n\n        # ddim sampling parameters\n        ddim_sigmas, ddim_alphas, ddim_alphas_prev = make_ddim_sampling_parameters(alphacums=alphas_cumprod.cpu(),\n                                                                                   ddim_timesteps=self.ddim_timesteps,\n                                                                                   eta=ddim_eta,verbose=verbose)\n        self.register_buffer('ddim_sigmas', ddim_sigmas)\n        self.register_buffer('ddim_alphas', ddim_alphas)\n        self.register_buffer('ddim_alphas_prev', ddim_alphas_prev)\n        self.register_buffer('ddim_sqrt_one_minus_alphas', np.sqrt(1. - ddim_alphas))\n        sigmas_for_original_sampling_steps = ddim_eta * torch.sqrt(\n            (1 - self.alphas_cumprod_prev) / (1 - self.alphas_cumprod) * (\n                        1 - self.alphas_cumprod / self.alphas_cumprod_prev))\n        self.register_buffer('ddim_sigmas_for_original_num_steps', sigmas_for_original_sampling_steps)\n\n    @torch.no_grad()\n    def sample(self,\n               S,\n               batch_size,\n               shape,\n               conditioning=None,\n               callback=None,\n               normals_sequence=None,\n               img_callback=None,\n               quantize_x0=False,\n               eta=0.,\n               mask=None,\n               x0=None,\n               temperature=1.,\n               noise_dropout=0.,\n               score_corrector=None,\n               corrector_kwargs=None,\n               verbose=True,\n               x_T=None,\n               log_every_t=100,\n               unconditional_guidance_scale=1.,\n               unconditional_conditioning=None,\n               # this has to come in the same format as the conditioning, # e.g. as encoded tokens, ...\n               dynamic_threshold=None,\n               **kwargs\n               ):\n        if conditioning is not None:\n            if isinstance(conditioning, dict):\n                cbs = conditioning[list(conditioning.keys())[0]].shape[0]\n                if cbs != batch_size:\n                    print(f\"Warning: Got {cbs} conditionings but batch-size is {batch_size}\")\n            else:\n                if conditioning.shape[0] != batch_size:\n                    print(f\"Warning: Got {conditioning.shape[0]} conditionings but batch-size is {batch_size}\")\n\n        self.make_schedule(ddim_num_steps=S, ddim_eta=eta, verbose=verbose)\n        # sampling\n        C, H, W = shape\n        size = (batch_size, C, H, W)\n        print(f'Data shape for PLMS sampling is {size}')\n\n        samples, intermediates = self.plms_sampling(conditioning, size,\n                                                    callback=callback,\n                                                    img_callback=img_callback,\n                                                    quantize_denoised=quantize_x0,\n                                                    mask=mask, x0=x0,\n                                                    ddim_use_original_steps=False,\n                                                    noise_dropout=noise_dropout,\n                                                    temperature=temperature,\n                                                    score_corrector=score_corrector,\n                                                    corrector_kwargs=corrector_kwargs,\n                                                    x_T=x_T,\n                                                    log_every_t=log_every_t,\n                                                    unconditional_guidance_scale=unconditional_guidance_scale,\n                                                    unconditional_conditioning=unconditional_conditioning,\n                                                    dynamic_threshold=dynamic_threshold,\n                                                    )\n        return samples, intermediates\n\n    @torch.no_grad()\n    def plms_sampling(self, cond, shape,\n                      x_T=None, ddim_use_original_steps=False,\n                      callback=None, timesteps=None, quantize_denoised=False,\n                      mask=None, x0=None, img_callback=None, log_every_t=100,\n                      temperature=1., noise_dropout=0., score_corrector=None, corrector_kwargs=None,\n                      unconditional_guidance_scale=1., unconditional_conditioning=None,\n                      dynamic_threshold=None):\n        device = self.model.betas.device\n        b = shape[0]\n        if x_T is None:\n            img = torch.randn(shape, device=device)\n        else:\n            img = x_T\n\n        if timesteps is None:\n            timesteps = self.ddpm_num_timesteps if ddim_use_original_steps else self.ddim_timesteps\n        elif timesteps is not None and not ddim_use_original_steps:\n            subset_end = int(min(timesteps / self.ddim_timesteps.shape[0], 1) * self.ddim_timesteps.shape[0]) - 1\n            timesteps = self.ddim_timesteps[:subset_end]\n\n        intermediates = {'x_inter': [img], 'pred_x0': [img]}\n        time_range = list(reversed(range(0,timesteps))) if ddim_use_original_steps else np.flip(timesteps)\n        total_steps = timesteps if ddim_use_original_steps else timesteps.shape[0]\n        print(f\"Running PLMS Sampling with {total_steps} timesteps\")\n\n        iterator = tqdm(time_range, desc='PLMS Sampler', total=total_steps)\n        old_eps = []\n\n        for i, step in enumerate(iterator):\n            index = total_steps - i - 1\n            ts = torch.full((b,), step, device=device, dtype=torch.long)\n            ts_next = torch.full((b,), time_range[min(i + 1, len(time_range) - 1)], device=device, dtype=torch.long)\n\n            if mask is not None:\n                assert x0 is not None\n                img_orig = self.model.q_sample(x0, ts)  # TODO: deterministic forward pass?\n                img = img_orig * mask + (1. - mask) * img\n\n            outs = self.p_sample_plms(img, cond, ts, index=index, use_original_steps=ddim_use_original_steps,\n                                      quantize_denoised=quantize_denoised, temperature=temperature,\n                                      noise_dropout=noise_dropout, score_corrector=score_corrector,\n                                      corrector_kwargs=corrector_kwargs,\n                                      unconditional_guidance_scale=unconditional_guidance_scale,\n                                      unconditional_conditioning=unconditional_conditioning,\n                                      old_eps=old_eps, t_next=ts_next,\n                                      dynamic_threshold=dynamic_threshold)\n            img, pred_x0, e_t = outs\n            old_eps.append(e_t)\n            if len(old_eps) >= 4:\n                old_eps.pop(0)\n            if callback: callback(i)\n            if img_callback: img_callback(pred_x0, i)\n\n            if index % log_every_t == 0 or index == total_steps - 1:\n                intermediates['x_inter'].append(img)\n                intermediates['pred_x0'].append(pred_x0)\n\n        return img, intermediates\n\n    @torch.no_grad()\n    def p_sample_plms(self, x, c, t, index, repeat_noise=False, use_original_steps=False, quantize_denoised=False,\n                      temperature=1., noise_dropout=0., score_corrector=None, corrector_kwargs=None,\n                      unconditional_guidance_scale=1., unconditional_conditioning=None, old_eps=None, t_next=None,\n                      dynamic_threshold=None):\n        b, *_, device = *x.shape, x.device\n\n        def get_model_output(x, t):\n            if unconditional_conditioning is None or unconditional_guidance_scale == 1.:\n                e_t = self.model.apply_model(x, t, c)\n            else:\n                x_in = torch.cat([x] * 2)\n                t_in = torch.cat([t] * 2)\n                c_in = torch.cat([unconditional_conditioning, c])\n                e_t_uncond, e_t = self.model.apply_model(x_in, t_in, c_in).chunk(2)\n                e_t = e_t_uncond + unconditional_guidance_scale * (e_t - e_t_uncond)\n\n            if score_corrector is not None:\n                assert self.model.parameterization == \"eps\"\n                e_t = score_corrector.modify_score(self.model, e_t, x, t, c, **corrector_kwargs)\n\n            return e_t\n\n        alphas = self.model.alphas_cumprod if use_original_steps else self.ddim_alphas\n        alphas_prev = self.model.alphas_cumprod_prev if use_original_steps else self.ddim_alphas_prev\n        sqrt_one_minus_alphas = self.model.sqrt_one_minus_alphas_cumprod if use_original_steps else self.ddim_sqrt_one_minus_alphas\n        sigmas = self.model.ddim_sigmas_for_original_num_steps if use_original_steps else self.ddim_sigmas\n\n        def get_x_prev_and_pred_x0(e_t, index):\n            # select parameters corresponding to the currently considered timestep\n            a_t = torch.full((b, 1, 1, 1), alphas[index], device=device)\n            a_prev = torch.full((b, 1, 1, 1), alphas_prev[index], device=device)\n            sigma_t = torch.full((b, 1, 1, 1), sigmas[index], device=device)\n            sqrt_one_minus_at = torch.full((b, 1, 1, 1), sqrt_one_minus_alphas[index],device=device)\n\n            # current prediction for x_0\n            pred_x0 = (x - sqrt_one_minus_at * e_t) / a_t.sqrt()\n            if quantize_denoised:\n                pred_x0, _, *_ = self.model.first_stage_model.quantize(pred_x0)\n            if dynamic_threshold is not None:\n                pred_x0 = norm_thresholding(pred_x0, dynamic_threshold)\n            # direction pointing to x_t\n            dir_xt = (1. - a_prev - sigma_t**2).sqrt() * e_t\n            noise = sigma_t * noise_like(x.shape, device, repeat_noise) * temperature\n            if noise_dropout > 0.:\n                noise = torch.nn.functional.dropout(noise, p=noise_dropout)\n            x_prev = a_prev.sqrt() * pred_x0 + dir_xt + noise\n            return x_prev, pred_x0\n\n        e_t = get_model_output(x, t)\n        if len(old_eps) == 0:\n            # Pseudo Improved Euler (2nd order)\n            x_prev, pred_x0 = get_x_prev_and_pred_x0(e_t, index)\n            e_t_next = get_model_output(x_prev, t_next)\n            e_t_prime = (e_t + e_t_next) / 2\n        elif len(old_eps) == 1:\n            # 2nd order Pseudo Linear Multistep (Adams-Bashforth)\n            e_t_prime = (3 * e_t - old_eps[-1]) / 2\n        elif len(old_eps) == 2:\n            # 3nd order Pseudo Linear Multistep (Adams-Bashforth)\n            e_t_prime = (23 * e_t - 16 * old_eps[-1] + 5 * old_eps[-2]) / 12\n        elif len(old_eps) >= 3:\n            # 4nd order Pseudo Linear Multistep (Adams-Bashforth)\n            e_t_prime = (55 * e_t - 59 * old_eps[-1] + 37 * old_eps[-2] - 9 * old_eps[-3]) / 24\n\n        x_prev, pred_x0 = get_x_prev_and_pred_x0(e_t_prime, index)\n\n        return x_prev, pred_x0, e_t\n", "ldm/models/diffusion/ddpm.py": "\"\"\"\nwild mixture of\nhttps://github.com/lucidrains/denoising-diffusion-pytorch/blob/7706bdfc6f527f58d33f84b7b522e61e6e3164b3/denoising_diffusion_pytorch/denoising_diffusion_pytorch.py\nhttps://github.com/openai/improved-diffusion/blob/e94489283bb876ac1477d5dd7709bbbd2d9902ce/improved_diffusion/gaussian_diffusion.py\nhttps://github.com/CompVis/taming-transformers\n-- merci\n\"\"\"\n\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport pytorch_lightning as pl\nfrom torch.optim.lr_scheduler import LambdaLR\nfrom einops import rearrange, repeat\nfrom contextlib import contextmanager, nullcontext\nfrom functools import partial\nimport itertools\nfrom tqdm import tqdm\nfrom torchvision.utils import make_grid\nfrom pytorch_lightning.utilities.distributed import rank_zero_only\nfrom omegaconf import ListConfig\n\nfrom ldm.util import log_txt_as_img, exists, default, ismap, isimage, mean_flat, count_params, instantiate_from_config\nfrom ldm.modules.ema import LitEma\nfrom ldm.modules.distributions.distributions import normal_kl, DiagonalGaussianDistribution\nfrom ldm.models.autoencoder import IdentityFirstStage, AutoencoderKL\nfrom ldm.modules.diffusionmodules.util import make_beta_schedule, extract_into_tensor, noise_like\nfrom ldm.models.diffusion.ddim import DDIMSampler\n\n\n__conditioning_keys__ = {'concat': 'c_concat',\n                         'crossattn': 'c_crossattn',\n                         'adm': 'y'}\n\n\ndef disabled_train(self, mode=True):\n    \"\"\"Overwrite model.train with this function to make sure train/eval mode\n    does not change anymore.\"\"\"\n    return self\n\n\ndef uniform_on_device(r1, r2, shape, device):\n    return (r1 - r2) * torch.rand(*shape, device=device) + r2\n\n\nclass DDPM(pl.LightningModule):\n    # classic DDPM with Gaussian diffusion, in image space\n    def __init__(self,\n                 unet_config,\n                 timesteps=1000,\n                 beta_schedule=\"linear\",\n                 loss_type=\"l2\",\n                 ckpt_path=None,\n                 ignore_keys=[],\n                 load_only_unet=False,\n                 monitor=\"val/loss\",\n                 use_ema=True,\n                 first_stage_key=\"image\",\n                 image_size=256,\n                 channels=3,\n                 log_every_t=100,\n                 clip_denoised=True,\n                 linear_start=1e-4,\n                 linear_end=2e-2,\n                 cosine_s=8e-3,\n                 given_betas=None,\n                 original_elbo_weight=0.,\n                 v_posterior=0.,  # weight for choosing posterior variance as sigma = (1-v) * beta_tilde + v * beta\n                 l_simple_weight=1.,\n                 conditioning_key=None,\n                 parameterization=\"eps\",  # all assuming fixed variance schedules\n                 scheduler_config=None,\n                 use_positional_encodings=False,\n                 learn_logvar=False,\n                 logvar_init=0.,\n                 make_it_fit=False,\n                 ucg_training=None,\n                 reset_ema=False,\n                 reset_num_ema_updates=False,\n                 ):\n        super().__init__()\n        assert parameterization in [\"eps\", \"x0\", \"v\"], 'currently only supporting \"eps\" and \"x0\" and \"v\"'\n        self.parameterization = parameterization\n        print(f\"{self.__class__.__name__}: Running in {self.parameterization}-prediction mode\")\n        self.cond_stage_model = None\n        self.clip_denoised = clip_denoised\n        self.log_every_t = log_every_t\n        self.first_stage_key = first_stage_key\n        self.image_size = image_size  # try conv?\n        self.channels = channels\n        self.use_positional_encodings = use_positional_encodings\n        self.model = DiffusionWrapper(unet_config, conditioning_key)\n        count_params(self.model, verbose=True)\n        self.use_ema = use_ema\n        if self.use_ema:\n            self.model_ema = LitEma(self.model)\n            print(f\"Keeping EMAs of {len(list(self.model_ema.buffers()))}.\")\n\n        self.use_scheduler = scheduler_config is not None\n        if self.use_scheduler:\n            self.scheduler_config = scheduler_config\n\n        self.v_posterior = v_posterior\n        self.original_elbo_weight = original_elbo_weight\n        self.l_simple_weight = l_simple_weight\n\n        if monitor is not None:\n            self.monitor = monitor\n        self.make_it_fit = make_it_fit\n        if reset_ema: assert exists(ckpt_path)\n        if ckpt_path is not None:\n            self.init_from_ckpt(ckpt_path, ignore_keys=ignore_keys, only_model=load_only_unet)\n            if reset_ema:\n                assert self.use_ema\n                print(f\"Resetting ema to pure model weights. This is useful when restoring from an ema-only checkpoint.\")\n                self.model_ema = LitEma(self.model)\n        if reset_num_ema_updates:\n            print(\" +++++++++++ WARNING: RESETTING NUM_EMA UPDATES TO ZERO +++++++++++ \")\n            assert self.use_ema\n            self.model_ema.reset_num_updates()\n\n        self.register_schedule(given_betas=given_betas, beta_schedule=beta_schedule, timesteps=timesteps,\n                               linear_start=linear_start, linear_end=linear_end, cosine_s=cosine_s)\n\n        self.loss_type = loss_type\n\n        self.learn_logvar = learn_logvar\n        self.logvar = torch.full(fill_value=logvar_init, size=(self.num_timesteps,))\n        if self.learn_logvar:\n            self.logvar = nn.Parameter(self.logvar, requires_grad=True)\n\n        self.ucg_training = ucg_training or dict()\n        if self.ucg_training:\n            self.ucg_prng = np.random.RandomState()\n\n    def register_schedule(self, given_betas=None, beta_schedule=\"linear\", timesteps=1000,\n                          linear_start=1e-4, linear_end=2e-2, cosine_s=8e-3):\n        if exists(given_betas):\n            betas = given_betas\n        else:\n            betas = make_beta_schedule(beta_schedule, timesteps, linear_start=linear_start, linear_end=linear_end,\n                                       cosine_s=cosine_s)\n        alphas = 1. - betas\n        alphas_cumprod = np.cumprod(alphas, axis=0)\n        alphas_cumprod_prev = np.append(1., alphas_cumprod[:-1])\n\n        timesteps, = betas.shape\n        self.num_timesteps = int(timesteps)\n        self.linear_start = linear_start\n        self.linear_end = linear_end\n        assert alphas_cumprod.shape[0] == self.num_timesteps, 'alphas have to be defined for each timestep'\n\n        to_torch = partial(torch.tensor, dtype=torch.float32)\n\n        self.register_buffer('betas', to_torch(betas))\n        self.register_buffer('alphas_cumprod', to_torch(alphas_cumprod))\n        self.register_buffer('alphas_cumprod_prev', to_torch(alphas_cumprod_prev))\n\n        # calculations for diffusion q(x_t | x_{t-1}) and others\n        self.register_buffer('sqrt_alphas_cumprod', to_torch(np.sqrt(alphas_cumprod)))\n        self.register_buffer('sqrt_one_minus_alphas_cumprod', to_torch(np.sqrt(1. - alphas_cumprod)))\n        self.register_buffer('log_one_minus_alphas_cumprod', to_torch(np.log(1. - alphas_cumprod)))\n        self.register_buffer('sqrt_recip_alphas_cumprod', to_torch(np.sqrt(1. / alphas_cumprod)))\n        self.register_buffer('sqrt_recipm1_alphas_cumprod', to_torch(np.sqrt(1. / alphas_cumprod - 1)))\n\n        # calculations for posterior q(x_{t-1} | x_t, x_0)\n        posterior_variance = (1 - self.v_posterior) * betas * (1. - alphas_cumprod_prev) / (\n                1. - alphas_cumprod) + self.v_posterior * betas\n        # above: equal to 1. / (1. / (1. - alpha_cumprod_tm1) + alpha_t / beta_t)\n        self.register_buffer('posterior_variance', to_torch(posterior_variance))\n        # below: log calculation clipped because the posterior variance is 0 at the beginning of the diffusion chain\n        self.register_buffer('posterior_log_variance_clipped', to_torch(np.log(np.maximum(posterior_variance, 1e-20))))\n        self.register_buffer('posterior_mean_coef1', to_torch(\n            betas * np.sqrt(alphas_cumprod_prev) / (1. - alphas_cumprod)))\n        self.register_buffer('posterior_mean_coef2', to_torch(\n            (1. - alphas_cumprod_prev) * np.sqrt(alphas) / (1. - alphas_cumprod)))\n\n        if self.parameterization == \"eps\":\n            lvlb_weights = self.betas ** 2 / (\n                    2 * self.posterior_variance * to_torch(alphas) * (1 - self.alphas_cumprod))\n        elif self.parameterization == \"x0\":\n            lvlb_weights = 0.5 * np.sqrt(torch.Tensor(alphas_cumprod)) / (2. * 1 - torch.Tensor(alphas_cumprod))\n        elif self.parameterization == \"v\":\n            lvlb_weights = torch.ones_like(self.betas ** 2 / (\n                    2 * self.posterior_variance * to_torch(alphas) * (1 - self.alphas_cumprod)))\n        else:\n            raise NotImplementedError(\"mu not supported\")\n        lvlb_weights[0] = lvlb_weights[1]\n        self.register_buffer('lvlb_weights', lvlb_weights, persistent=False)\n        assert not torch.isnan(self.lvlb_weights).all()\n\n    @contextmanager\n    def ema_scope(self, context=None):\n        if self.use_ema:\n            self.model_ema.store(self.model.parameters())\n            self.model_ema.copy_to(self.model)\n            if context is not None:\n                print(f\"{context}: Switched to EMA weights\")\n        try:\n            yield None\n        finally:\n            if self.use_ema:\n                self.model_ema.restore(self.model.parameters())\n                if context is not None:\n                    print(f\"{context}: Restored training weights\")\n\n    @torch.no_grad()\n    def init_from_ckpt(self, path, ignore_keys=list(), only_model=False):\n        sd = torch.load(path, map_location=\"cpu\")\n        if \"state_dict\" in list(sd.keys()):\n            sd = sd[\"state_dict\"]\n        keys = list(sd.keys())\n        for k in keys:\n            for ik in ignore_keys:\n                if k.startswith(ik):\n                    print(\"Deleting key {} from state_dict.\".format(k))\n                    del sd[k]\n        if self.make_it_fit:\n            n_params = len([name for name, _ in\n                            itertools.chain(self.named_parameters(),\n                                            self.named_buffers())])\n            for name, param in tqdm(\n                    itertools.chain(self.named_parameters(),\n                                    self.named_buffers()),\n                    desc=\"Fitting old weights to new weights\",\n                    total=n_params\n            ):\n                if not name in sd:\n                    continue\n                old_shape = sd[name].shape\n                new_shape = param.shape\n                assert len(old_shape) == len(new_shape)\n                if len(new_shape) > 2:\n                    # we only modify first two axes\n                    assert new_shape[2:] == old_shape[2:]\n                # assumes first axis corresponds to output dim\n                if not new_shape == old_shape:\n                    new_param = param.clone()\n                    old_param = sd[name]\n                    if len(new_shape) == 1:\n                        for i in range(new_param.shape[0]):\n                            new_param[i] = old_param[i % old_shape[0]]\n                    elif len(new_shape) >= 2:\n                        for i in range(new_param.shape[0]):\n                            for j in range(new_param.shape[1]):\n                                new_param[i, j] = old_param[i % old_shape[0], j % old_shape[1]]\n\n                        n_used_old = torch.ones(old_shape[1])\n                        for j in range(new_param.shape[1]):\n                            n_used_old[j % old_shape[1]] += 1\n                        n_used_new = torch.zeros(new_shape[1])\n                        for j in range(new_param.shape[1]):\n                            n_used_new[j] = n_used_old[j % old_shape[1]]\n\n                        n_used_new = n_used_new[None, :]\n                        while len(n_used_new.shape) < len(new_shape):\n                            n_used_new = n_used_new.unsqueeze(-1)\n                        new_param /= n_used_new\n\n                    sd[name] = new_param\n\n        missing, unexpected = self.load_state_dict(sd, strict=False) if not only_model else self.model.load_state_dict(\n            sd, strict=False)\n        print(f\"Restored from {path} with {len(missing)} missing and {len(unexpected)} unexpected keys\")\n        if len(missing) > 0:\n            print(f\"Missing Keys:\\n {missing}\")\n        if len(unexpected) > 0:\n            print(f\"\\nUnexpected Keys:\\n {unexpected}\")\n\n    def q_mean_variance(self, x_start, t):\n        \"\"\"\n        Get the distribution q(x_t | x_0).\n        :param x_start: the [N x C x ...] tensor of noiseless inputs.\n        :param t: the number of diffusion steps (minus 1). Here, 0 means one step.\n        :return: A tuple (mean, variance, log_variance), all of x_start's shape.\n        \"\"\"\n        mean = (extract_into_tensor(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start)\n        variance = extract_into_tensor(1.0 - self.alphas_cumprod, t, x_start.shape)\n        log_variance = extract_into_tensor(self.log_one_minus_alphas_cumprod, t, x_start.shape)\n        return mean, variance, log_variance\n\n    def predict_start_from_noise(self, x_t, t, noise):\n        return (\n                extract_into_tensor(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t -\n                extract_into_tensor(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape) * noise\n        )\n\n    def predict_start_from_z_and_v(self, x_t, t, v):\n        # self.register_buffer('sqrt_alphas_cumprod', to_torch(np.sqrt(alphas_cumprod)))\n        # self.register_buffer('sqrt_one_minus_alphas_cumprod', to_torch(np.sqrt(1. - alphas_cumprod)))\n        return (\n                extract_into_tensor(self.sqrt_alphas_cumprod, t, x_t.shape) * x_t -\n                extract_into_tensor(self.sqrt_one_minus_alphas_cumprod, t, x_t.shape) * v\n        )\n\n    def predict_eps_from_z_and_v(self, x_t, t, v):\n        return (\n                extract_into_tensor(self.sqrt_alphas_cumprod, t, x_t.shape) * v +\n                extract_into_tensor(self.sqrt_one_minus_alphas_cumprod, t, x_t.shape) * x_t\n        )\n\n    def q_posterior(self, x_start, x_t, t):\n        posterior_mean = (\n                extract_into_tensor(self.posterior_mean_coef1, t, x_t.shape) * x_start +\n                extract_into_tensor(self.posterior_mean_coef2, t, x_t.shape) * x_t\n        )\n        posterior_variance = extract_into_tensor(self.posterior_variance, t, x_t.shape)\n        posterior_log_variance_clipped = extract_into_tensor(self.posterior_log_variance_clipped, t, x_t.shape)\n        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n\n    def p_mean_variance(self, x, t, clip_denoised: bool):\n        model_out = self.model(x, t)\n        if self.parameterization == \"eps\":\n            x_recon = self.predict_start_from_noise(x, t=t, noise=model_out)\n        elif self.parameterization == \"x0\":\n            x_recon = model_out\n        if clip_denoised:\n            x_recon.clamp_(-1., 1.)\n\n        model_mean, posterior_variance, posterior_log_variance = self.q_posterior(x_start=x_recon, x_t=x, t=t)\n        return model_mean, posterior_variance, posterior_log_variance\n\n    @torch.no_grad()\n    def p_sample(self, x, t, clip_denoised=True, repeat_noise=False):\n        b, *_, device = *x.shape, x.device\n        model_mean, _, model_log_variance = self.p_mean_variance(x=x, t=t, clip_denoised=clip_denoised)\n        noise = noise_like(x.shape, device, repeat_noise)\n        # no noise when t == 0\n        nonzero_mask = (1 - (t == 0).float()).reshape(b, *((1,) * (len(x.shape) - 1)))\n        return model_mean + nonzero_mask * (0.5 * model_log_variance).exp() * noise\n\n    @torch.no_grad()\n    def p_sample_loop(self, shape, return_intermediates=False):\n        device = self.betas.device\n        b = shape[0]\n        img = torch.randn(shape, device=device)\n        intermediates = [img]\n        for i in tqdm(reversed(range(0, self.num_timesteps)), desc='Sampling t', total=self.num_timesteps):\n            img = self.p_sample(img, torch.full((b,), i, device=device, dtype=torch.long),\n                                clip_denoised=self.clip_denoised)\n            if i % self.log_every_t == 0 or i == self.num_timesteps - 1:\n                intermediates.append(img)\n        if return_intermediates:\n            return img, intermediates\n        return img\n\n    @torch.no_grad()\n    def sample(self, batch_size=16, return_intermediates=False):\n        image_size = self.image_size\n        channels = self.channels\n        return self.p_sample_loop((batch_size, channels, image_size, image_size),\n                                  return_intermediates=return_intermediates)\n\n    def q_sample(self, x_start, t, noise=None):\n        noise = default(noise, lambda: torch.randn_like(x_start))\n        return (extract_into_tensor(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start +\n                extract_into_tensor(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape) * noise)\n\n    def get_v(self, x, noise, t):\n        return (\n                extract_into_tensor(self.sqrt_alphas_cumprod, t, x.shape) * noise -\n                extract_into_tensor(self.sqrt_one_minus_alphas_cumprod, t, x.shape) * x\n        )\n\n    def get_loss(self, pred, target, mean=True):\n        if self.loss_type == 'l1':\n            loss = (target - pred).abs()\n            if mean:\n                loss = loss.mean()\n        elif self.loss_type == 'l2':\n            if mean:\n                loss = torch.nn.functional.mse_loss(target, pred)\n            else:\n                loss = torch.nn.functional.mse_loss(target, pred, reduction='none')\n        else:\n            raise NotImplementedError(\"unknown loss type '{loss_type}'\")\n\n        return loss\n\n    def p_losses(self, x_start, t, noise=None):\n        noise = default(noise, lambda: torch.randn_like(x_start))\n        x_noisy = self.q_sample(x_start=x_start, t=t, noise=noise)\n        model_out = self.model(x_noisy, t)\n\n        loss_dict = {}\n        if self.parameterization == \"eps\":\n            target = noise\n        elif self.parameterization == \"x0\":\n            target = x_start\n        elif self.parameterization == \"v\":\n            target = self.get_v(x_start, noise, t)\n        else:\n            raise NotImplementedError(f\"Parameterization {self.parameterization} not yet supported\")\n\n        loss = self.get_loss(model_out, target, mean=False).mean(dim=[1, 2, 3])\n\n        log_prefix = 'train' if self.training else 'val'\n\n        loss_dict.update({f'{log_prefix}/loss_simple': loss.mean()})\n        loss_simple = loss.mean() * self.l_simple_weight\n\n        loss_vlb = (self.lvlb_weights[t] * loss).mean()\n        loss_dict.update({f'{log_prefix}/loss_vlb': loss_vlb})\n\n        loss = loss_simple + self.original_elbo_weight * loss_vlb\n\n        loss_dict.update({f'{log_prefix}/loss': loss})\n\n        return loss, loss_dict\n\n    def forward(self, x, *args, **kwargs):\n        # b, c, h, w, device, img_size, = *x.shape, x.device, self.image_size\n        # assert h == img_size and w == img_size, f'height and width of image must be {img_size}'\n        t = torch.randint(0, self.num_timesteps, (x.shape[0],), device=self.device).long()\n        return self.p_losses(x, t, *args, **kwargs)\n\n    def get_input(self, batch, k):\n        x = batch[k]\n        if len(x.shape) == 3:\n            x = x[..., None]\n        x = rearrange(x, 'b h w c -> b c h w')\n        x = x.to(memory_format=torch.contiguous_format).float()\n        return x\n\n    def shared_step(self, batch):\n        x = self.get_input(batch, self.first_stage_key)\n        loss, loss_dict = self(x)\n        return loss, loss_dict\n\n    def training_step(self, batch, batch_idx):\n        for k in self.ucg_training:\n            p = self.ucg_training[k][\"p\"]\n            val = self.ucg_training[k][\"val\"]\n            if val is None:\n                val = \"\"\n            for i in range(len(batch[k])):\n                if self.ucg_prng.choice(2, p=[1 - p, p]):\n                    batch[k][i] = val\n\n        loss, loss_dict = self.shared_step(batch)\n\n        self.log_dict(loss_dict, prog_bar=True,\n                      logger=True, on_step=True, on_epoch=True)\n\n        self.log(\"global_step\", self.global_step,\n                 prog_bar=True, logger=True, on_step=True, on_epoch=False)\n\n        if self.use_scheduler:\n            lr = self.optimizers().param_groups[0]['lr']\n            self.log('lr_abs', lr, prog_bar=True, logger=True, on_step=True, on_epoch=False)\n\n        return loss\n\n    @torch.no_grad()\n    def validation_step(self, batch, batch_idx):\n        _, loss_dict_no_ema = self.shared_step(batch)\n        with self.ema_scope():\n            _, loss_dict_ema = self.shared_step(batch)\n            loss_dict_ema = {key + '_ema': loss_dict_ema[key] for key in loss_dict_ema}\n        self.log_dict(loss_dict_no_ema, prog_bar=False, logger=True, on_step=False, on_epoch=True)\n        self.log_dict(loss_dict_ema, prog_bar=False, logger=True, on_step=False, on_epoch=True)\n\n    def on_train_batch_end(self, *args, **kwargs):\n        if self.use_ema:\n            self.model_ema(self.model)\n\n    def _get_rows_from_list(self, samples):\n        n_imgs_per_row = len(samples)\n        denoise_grid = rearrange(samples, 'n b c h w -> b n c h w')\n        denoise_grid = rearrange(denoise_grid, 'b n c h w -> (b n) c h w')\n        denoise_grid = make_grid(denoise_grid, nrow=n_imgs_per_row)\n        return denoise_grid\n\n    @torch.no_grad()\n    def log_images(self, batch, N=8, n_row=2, sample=True, return_keys=None, **kwargs):\n        log = dict()\n        x = self.get_input(batch, self.first_stage_key)\n        N = min(x.shape[0], N)\n        n_row = min(x.shape[0], n_row)\n        x = x.to(self.device)[:N]\n        log[\"inputs\"] = x\n\n        # get diffusion row\n        diffusion_row = list()\n        x_start = x[:n_row]\n\n        for t in range(self.num_timesteps):\n            if t % self.log_every_t == 0 or t == self.num_timesteps - 1:\n                t = repeat(torch.tensor([t]), '1 -> b', b=n_row)\n                t = t.to(self.device).long()\n                noise = torch.randn_like(x_start)\n                x_noisy = self.q_sample(x_start=x_start, t=t, noise=noise)\n                diffusion_row.append(x_noisy)\n\n        log[\"diffusion_row\"] = self._get_rows_from_list(diffusion_row)\n\n        if sample:\n            # get denoise row\n            with self.ema_scope(\"Plotting\"):\n                samples, denoise_row = self.sample(batch_size=N, return_intermediates=True)\n\n            log[\"samples\"] = samples\n            log[\"denoise_row\"] = self._get_rows_from_list(denoise_row)\n\n        if return_keys:\n            if np.intersect1d(list(log.keys()), return_keys).shape[0] == 0:\n                return log\n            else:\n                return {key: log[key] for key in return_keys}\n        return log\n\n    def configure_optimizers(self):\n        lr = self.learning_rate\n        params = list(self.model.parameters())\n        if self.learn_logvar:\n            params = params + [self.logvar]\n        opt = torch.optim.AdamW(params, lr=lr)\n        return opt\n\n\nclass LatentDiffusion(DDPM):\n    \"\"\"main class\"\"\"\n\n    def __init__(self,\n                 first_stage_config,\n                 cond_stage_config,\n                 num_timesteps_cond=None,\n                 cond_stage_key=\"image\",\n                 cond_stage_trainable=False,\n                 concat_mode=True,\n                 cond_stage_forward=None,\n                 conditioning_key=None,\n                 scale_factor=1.0,\n                 scale_by_std=False,\n                 force_null_conditioning=False,\n                 *args, **kwargs):\n        self.force_null_conditioning = force_null_conditioning\n        self.num_timesteps_cond = default(num_timesteps_cond, 1)\n        self.scale_by_std = scale_by_std\n        assert self.num_timesteps_cond <= kwargs['timesteps']\n        # for backwards compatibility after implementation of DiffusionWrapper\n        if conditioning_key is None:\n            conditioning_key = 'concat' if concat_mode else 'crossattn'\n        if cond_stage_config == '__is_unconditional__' and not self.force_null_conditioning:\n            conditioning_key = None\n        ckpt_path = kwargs.pop(\"ckpt_path\", None)\n        reset_ema = kwargs.pop(\"reset_ema\", False)\n        reset_num_ema_updates = kwargs.pop(\"reset_num_ema_updates\", False)\n        ignore_keys = kwargs.pop(\"ignore_keys\", [])\n        super().__init__(conditioning_key=conditioning_key, *args, **kwargs)\n        self.concat_mode = concat_mode\n        self.cond_stage_trainable = cond_stage_trainable\n        self.cond_stage_key = cond_stage_key\n        try:\n            self.num_downs = len(first_stage_config.params.ddconfig.ch_mult) - 1\n        except:\n            self.num_downs = 0\n        if not scale_by_std:\n            self.scale_factor = scale_factor\n        else:\n            self.register_buffer('scale_factor', torch.tensor(scale_factor))\n        self.instantiate_first_stage(first_stage_config)\n        self.instantiate_cond_stage(cond_stage_config)\n        self.cond_stage_forward = cond_stage_forward\n        self.clip_denoised = False\n        self.bbox_tokenizer = None\n\n        self.restarted_from_ckpt = False\n        if ckpt_path is not None:\n            self.init_from_ckpt(ckpt_path, ignore_keys)\n            self.restarted_from_ckpt = True\n            if reset_ema:\n                assert self.use_ema\n                print(\n                    f\"Resetting ema to pure model weights. This is useful when restoring from an ema-only checkpoint.\")\n                self.model_ema = LitEma(self.model)\n        if reset_num_ema_updates:\n            print(\" +++++++++++ WARNING: RESETTING NUM_EMA UPDATES TO ZERO +++++++++++ \")\n            assert self.use_ema\n            self.model_ema.reset_num_updates()\n\n    def make_cond_schedule(self, ):\n        self.cond_ids = torch.full(size=(self.num_timesteps,), fill_value=self.num_timesteps - 1, dtype=torch.long)\n        ids = torch.round(torch.linspace(0, self.num_timesteps - 1, self.num_timesteps_cond)).long()\n        self.cond_ids[:self.num_timesteps_cond] = ids\n\n    @rank_zero_only\n    @torch.no_grad()\n    def on_train_batch_start(self, batch, batch_idx, dataloader_idx):\n        # only for very first batch\n        if self.scale_by_std and self.current_epoch == 0 and self.global_step == 0 and batch_idx == 0 and not self.restarted_from_ckpt:\n            assert self.scale_factor == 1., 'rather not use custom rescaling and std-rescaling simultaneously'\n            # set rescale weight to 1./std of encodings\n            print(\"### USING STD-RESCALING ###\")\n            x = super().get_input(batch, self.first_stage_key)\n            x = x.to(self.device)\n            encoder_posterior = self.encode_first_stage(x)\n            z = self.get_first_stage_encoding(encoder_posterior).detach()\n            del self.scale_factor\n            self.register_buffer('scale_factor', 1. / z.flatten().std())\n            print(f\"setting self.scale_factor to {self.scale_factor}\")\n            print(\"### USING STD-RESCALING ###\")\n\n    def register_schedule(self,\n                          given_betas=None, beta_schedule=\"linear\", timesteps=1000,\n                          linear_start=1e-4, linear_end=2e-2, cosine_s=8e-3):\n        super().register_schedule(given_betas, beta_schedule, timesteps, linear_start, linear_end, cosine_s)\n\n        self.shorten_cond_schedule = self.num_timesteps_cond > 1\n        if self.shorten_cond_schedule:\n            self.make_cond_schedule()\n\n    def instantiate_first_stage(self, config):\n        model = instantiate_from_config(config)\n        self.first_stage_model = model.eval()\n        self.first_stage_model.train = disabled_train\n        for param in self.first_stage_model.parameters():\n            param.requires_grad = False\n\n    def instantiate_cond_stage(self, config):\n        if not self.cond_stage_trainable:\n            if config == \"__is_first_stage__\":\n                print(\"Using first stage also as cond stage.\")\n                self.cond_stage_model = self.first_stage_model\n            elif config == \"__is_unconditional__\":\n                print(f\"Training {self.__class__.__name__} as an unconditional model.\")\n                self.cond_stage_model = None\n                # self.be_unconditional = True\n            else:\n                model = instantiate_from_config(config)\n                self.cond_stage_model = model.eval()\n                self.cond_stage_model.train = disabled_train\n                for param in self.cond_stage_model.parameters():\n                    param.requires_grad = False\n        else:\n            assert config != '__is_first_stage__'\n            assert config != '__is_unconditional__'\n            model = instantiate_from_config(config)\n            self.cond_stage_model = model\n\n    def _get_denoise_row_from_list(self, samples, desc='', force_no_decoder_quantization=False):\n        denoise_row = []\n        for zd in tqdm(samples, desc=desc):\n            denoise_row.append(self.decode_first_stage(zd.to(self.device),\n                                                       force_not_quantize=force_no_decoder_quantization))\n        n_imgs_per_row = len(denoise_row)\n        denoise_row = torch.stack(denoise_row)  # n_log_step, n_row, C, H, W\n        denoise_grid = rearrange(denoise_row, 'n b c h w -> b n c h w')\n        denoise_grid = rearrange(denoise_grid, 'b n c h w -> (b n) c h w')\n        denoise_grid = make_grid(denoise_grid, nrow=n_imgs_per_row)\n        return denoise_grid\n\n    def get_first_stage_encoding(self, encoder_posterior):\n        if isinstance(encoder_posterior, DiagonalGaussianDistribution):\n            z = encoder_posterior.sample()\n        elif isinstance(encoder_posterior, torch.Tensor):\n            z = encoder_posterior\n        else:\n            raise NotImplementedError(f\"encoder_posterior of type '{type(encoder_posterior)}' not yet implemented\")\n        return self.scale_factor * z\n\n    def get_learned_conditioning(self, c):\n        if self.cond_stage_forward is None:\n            if hasattr(self.cond_stage_model, 'encode') and callable(self.cond_stage_model.encode):\n                c = self.cond_stage_model.encode(c)\n                if isinstance(c, DiagonalGaussianDistribution):\n                    c = c.mode()\n            else:\n                c = self.cond_stage_model(c)\n        else:\n            assert hasattr(self.cond_stage_model, self.cond_stage_forward)\n            c = getattr(self.cond_stage_model, self.cond_stage_forward)(c)\n        return c\n\n    def meshgrid(self, h, w):\n        y = torch.arange(0, h).view(h, 1, 1).repeat(1, w, 1)\n        x = torch.arange(0, w).view(1, w, 1).repeat(h, 1, 1)\n\n        arr = torch.cat([y, x], dim=-1)\n        return arr\n\n    def delta_border(self, h, w):\n        \"\"\"\n        :param h: height\n        :param w: width\n        :return: normalized distance to image border,\n         wtith min distance = 0 at border and max dist = 0.5 at image center\n        \"\"\"\n        lower_right_corner = torch.tensor([h - 1, w - 1]).view(1, 1, 2)\n        arr = self.meshgrid(h, w) / lower_right_corner\n        dist_left_up = torch.min(arr, dim=-1, keepdims=True)[0]\n        dist_right_down = torch.min(1 - arr, dim=-1, keepdims=True)[0]\n        edge_dist = torch.min(torch.cat([dist_left_up, dist_right_down], dim=-1), dim=-1)[0]\n        return edge_dist\n\n    def get_weighting(self, h, w, Ly, Lx, device):\n        weighting = self.delta_border(h, w)\n        weighting = torch.clip(weighting, self.split_input_params[\"clip_min_weight\"],\n                               self.split_input_params[\"clip_max_weight\"], )\n        weighting = weighting.view(1, h * w, 1).repeat(1, 1, Ly * Lx).to(device)\n\n        if self.split_input_params[\"tie_braker\"]:\n            L_weighting = self.delta_border(Ly, Lx)\n            L_weighting = torch.clip(L_weighting,\n                                     self.split_input_params[\"clip_min_tie_weight\"],\n                                     self.split_input_params[\"clip_max_tie_weight\"])\n\n            L_weighting = L_weighting.view(1, 1, Ly * Lx).to(device)\n            weighting = weighting * L_weighting\n        return weighting\n\n    def get_fold_unfold(self, x, kernel_size, stride, uf=1, df=1):  # todo load once not every time, shorten code\n        \"\"\"\n        :param x: img of size (bs, c, h, w)\n        :return: n img crops of size (n, bs, c, kernel_size[0], kernel_size[1])\n        \"\"\"\n        bs, nc, h, w = x.shape\n\n        # number of crops in image\n        Ly = (h - kernel_size[0]) // stride[0] + 1\n        Lx = (w - kernel_size[1]) // stride[1] + 1\n\n        if uf == 1 and df == 1:\n            fold_params = dict(kernel_size=kernel_size, dilation=1, padding=0, stride=stride)\n            unfold = torch.nn.Unfold(**fold_params)\n\n            fold = torch.nn.Fold(output_size=x.shape[2:], **fold_params)\n\n            weighting = self.get_weighting(kernel_size[0], kernel_size[1], Ly, Lx, x.device).to(x.dtype)\n            normalization = fold(weighting).view(1, 1, h, w)  # normalizes the overlap\n            weighting = weighting.view((1, 1, kernel_size[0], kernel_size[1], Ly * Lx))\n\n        elif uf > 1 and df == 1:\n            fold_params = dict(kernel_size=kernel_size, dilation=1, padding=0, stride=stride)\n            unfold = torch.nn.Unfold(**fold_params)\n\n            fold_params2 = dict(kernel_size=(kernel_size[0] * uf, kernel_size[0] * uf),\n                                dilation=1, padding=0,\n                                stride=(stride[0] * uf, stride[1] * uf))\n            fold = torch.nn.Fold(output_size=(x.shape[2] * uf, x.shape[3] * uf), **fold_params2)\n\n            weighting = self.get_weighting(kernel_size[0] * uf, kernel_size[1] * uf, Ly, Lx, x.device).to(x.dtype)\n            normalization = fold(weighting).view(1, 1, h * uf, w * uf)  # normalizes the overlap\n            weighting = weighting.view((1, 1, kernel_size[0] * uf, kernel_size[1] * uf, Ly * Lx))\n\n        elif df > 1 and uf == 1:\n            fold_params = dict(kernel_size=kernel_size, dilation=1, padding=0, stride=stride)\n            unfold = torch.nn.Unfold(**fold_params)\n\n            fold_params2 = dict(kernel_size=(kernel_size[0] // df, kernel_size[0] // df),\n                                dilation=1, padding=0,\n                                stride=(stride[0] // df, stride[1] // df))\n            fold = torch.nn.Fold(output_size=(x.shape[2] // df, x.shape[3] // df), **fold_params2)\n\n            weighting = self.get_weighting(kernel_size[0] // df, kernel_size[1] // df, Ly, Lx, x.device).to(x.dtype)\n            normalization = fold(weighting).view(1, 1, h // df, w // df)  # normalizes the overlap\n            weighting = weighting.view((1, 1, kernel_size[0] // df, kernel_size[1] // df, Ly * Lx))\n\n        else:\n            raise NotImplementedError\n\n        return fold, unfold, normalization, weighting\n\n    @torch.no_grad()\n    def get_input(self, batch, k, return_first_stage_outputs=False, force_c_encode=False,\n                  cond_key=None, return_original_cond=False, bs=None, return_x=False):\n        x = super().get_input(batch, k)\n        if bs is not None:\n            x = x[:bs]\n        x = x.to(self.device)\n        encoder_posterior = self.encode_first_stage(x)\n        z = self.get_first_stage_encoding(encoder_posterior).detach()\n\n        if self.model.conditioning_key is not None and not self.force_null_conditioning:\n            if cond_key is None:\n                cond_key = self.cond_stage_key\n            if cond_key != self.first_stage_key:\n                if cond_key in ['caption', 'coordinates_bbox', \"txt\"]:\n                    xc = batch[cond_key]\n                elif cond_key in ['class_label', 'cls']:\n                    xc = batch\n                else:\n                    xc = super().get_input(batch, cond_key).to(self.device)\n            else:\n                xc = x\n            if not self.cond_stage_trainable or force_c_encode:\n                if isinstance(xc, dict) or isinstance(xc, list):\n                    c = self.get_learned_conditioning(xc)\n                else:\n                    c = self.get_learned_conditioning(xc.to(self.device))\n            else:\n                c = xc\n            if bs is not None:\n                c = c[:bs]\n\n            if self.use_positional_encodings:\n                pos_x, pos_y = self.compute_latent_shifts(batch)\n                ckey = __conditioning_keys__[self.model.conditioning_key]\n                c = {ckey: c, 'pos_x': pos_x, 'pos_y': pos_y}\n\n        else:\n            c = None\n            xc = None\n            if self.use_positional_encodings:\n                pos_x, pos_y = self.compute_latent_shifts(batch)\n                c = {'pos_x': pos_x, 'pos_y': pos_y}\n        out = [z, c]\n        if return_first_stage_outputs:\n            xrec = self.decode_first_stage(z)\n            out.extend([x, xrec])\n        if return_x:\n            out.extend([x])\n        if return_original_cond:\n            out.append(xc)\n        return out\n\n    @torch.no_grad()\n    def decode_first_stage(self, z, predict_cids=False, force_not_quantize=False):\n        if predict_cids:\n            if z.dim() == 4:\n                z = torch.argmax(z.exp(), dim=1).long()\n            z = self.first_stage_model.quantize.get_codebook_entry(z, shape=None)\n            z = rearrange(z, 'b h w c -> b c h w').contiguous()\n\n        z = 1. / self.scale_factor * z\n        return self.first_stage_model.decode(z)\n\n    @torch.no_grad()\n    def encode_first_stage(self, x):\n        return self.first_stage_model.encode(x)\n\n    def shared_step(self, batch, **kwargs):\n        x, c = self.get_input(batch, self.first_stage_key)\n        loss = self(x, c)\n        return loss\n\n    def forward(self, x, c, *args, **kwargs):\n        t = torch.randint(0, self.num_timesteps, (x.shape[0],), device=self.device).long()\n        if self.model.conditioning_key is not None:\n            assert c is not None\n            if self.cond_stage_trainable:\n                c = self.get_learned_conditioning(c)\n            if self.shorten_cond_schedule:  # TODO: drop this option\n                tc = self.cond_ids[t].to(self.device)\n                c = self.q_sample(x_start=c, t=tc, noise=torch.randn_like(c.float()))\n        return self.p_losses(x, c, t, *args, **kwargs)\n\n    def apply_model(self, x_noisy, t, cond, return_ids=False):\n        if isinstance(cond, dict):\n            # hybrid case, cond is expected to be a dict\n            pass\n        else:\n            if not isinstance(cond, list):\n                cond = [cond]\n            key = 'c_concat' if self.model.conditioning_key == 'concat' else 'c_crossattn'\n            cond = {key: cond}\n\n        x_recon = self.model(x_noisy, t, **cond)\n\n        if isinstance(x_recon, tuple) and not return_ids:\n            return x_recon[0]\n        else:\n            return x_recon\n\n    def _predict_eps_from_xstart(self, x_t, t, pred_xstart):\n        return (extract_into_tensor(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t - pred_xstart) / \\\n               extract_into_tensor(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape)\n\n    def _prior_bpd(self, x_start):\n        \"\"\"\n        Get the prior KL term for the variational lower-bound, measured in\n        bits-per-dim.\n        This term can't be optimized, as it only depends on the encoder.\n        :param x_start: the [N x C x ...] tensor of inputs.\n        :return: a batch of [N] KL values (in bits), one per batch element.\n        \"\"\"\n        batch_size = x_start.shape[0]\n        t = torch.tensor([self.num_timesteps - 1] * batch_size, device=x_start.device)\n        qt_mean, _, qt_log_variance = self.q_mean_variance(x_start, t)\n        kl_prior = normal_kl(mean1=qt_mean, logvar1=qt_log_variance, mean2=0.0, logvar2=0.0)\n        return mean_flat(kl_prior) / np.log(2.0)\n\n    def p_losses(self, x_start, cond, t, noise=None):\n        noise = default(noise, lambda: torch.randn_like(x_start))\n        x_noisy = self.q_sample(x_start=x_start, t=t, noise=noise)\n        model_output = self.apply_model(x_noisy, t, cond)\n\n        loss_dict = {}\n        prefix = 'train' if self.training else 'val'\n\n        if self.parameterization == \"x0\":\n            target = x_start\n        elif self.parameterization == \"eps\":\n            target = noise\n        elif self.parameterization == \"v\":\n            target = self.get_v(x_start, noise, t)\n        else:\n            raise NotImplementedError()\n\n        loss_simple = self.get_loss(model_output, target, mean=False).mean([1, 2, 3])\n        loss_dict.update({f'{prefix}/loss_simple': loss_simple.mean()})\n\n        logvar_t = self.logvar[t].to(self.device)\n        loss = loss_simple / torch.exp(logvar_t) + logvar_t\n        # loss = loss_simple / torch.exp(self.logvar) + self.logvar\n        if self.learn_logvar:\n            loss_dict.update({f'{prefix}/loss_gamma': loss.mean()})\n            loss_dict.update({'logvar': self.logvar.data.mean()})\n\n        loss = self.l_simple_weight * loss.mean()\n\n        loss_vlb = self.get_loss(model_output, target, mean=False).mean(dim=(1, 2, 3))\n        loss_vlb = (self.lvlb_weights[t] * loss_vlb).mean()\n        loss_dict.update({f'{prefix}/loss_vlb': loss_vlb})\n        loss += (self.original_elbo_weight * loss_vlb)\n        loss_dict.update({f'{prefix}/loss': loss})\n\n        return loss, loss_dict\n\n    def p_mean_variance(self, x, c, t, clip_denoised: bool, return_codebook_ids=False, quantize_denoised=False,\n                        return_x0=False, score_corrector=None, corrector_kwargs=None):\n        t_in = t\n        model_out = self.apply_model(x, t_in, c, return_ids=return_codebook_ids)\n\n        if score_corrector is not None:\n            assert self.parameterization == \"eps\"\n            model_out = score_corrector.modify_score(self, model_out, x, t, c, **corrector_kwargs)\n\n        if return_codebook_ids:\n            model_out, logits = model_out\n\n        if self.parameterization == \"eps\":\n            x_recon = self.predict_start_from_noise(x, t=t, noise=model_out)\n        elif self.parameterization == \"x0\":\n            x_recon = model_out\n        else:\n            raise NotImplementedError()\n\n        if clip_denoised:\n            x_recon.clamp_(-1., 1.)\n        if quantize_denoised:\n            x_recon, _, [_, _, indices] = self.first_stage_model.quantize(x_recon)\n        model_mean, posterior_variance, posterior_log_variance = self.q_posterior(x_start=x_recon, x_t=x, t=t)\n        if return_codebook_ids:\n            return model_mean, posterior_variance, posterior_log_variance, logits\n        elif return_x0:\n            return model_mean, posterior_variance, posterior_log_variance, x_recon\n        else:\n            return model_mean, posterior_variance, posterior_log_variance\n\n    @torch.no_grad()\n    def p_sample(self, x, c, t, clip_denoised=False, repeat_noise=False,\n                 return_codebook_ids=False, quantize_denoised=False, return_x0=False,\n                 temperature=1., noise_dropout=0., score_corrector=None, corrector_kwargs=None):\n        b, *_, device = *x.shape, x.device\n        outputs = self.p_mean_variance(x=x, c=c, t=t, clip_denoised=clip_denoised,\n                                       return_codebook_ids=return_codebook_ids,\n                                       quantize_denoised=quantize_denoised,\n                                       return_x0=return_x0,\n                                       score_corrector=score_corrector, corrector_kwargs=corrector_kwargs)\n        if return_codebook_ids:\n            raise DeprecationWarning(\"Support dropped.\")\n            model_mean, _, model_log_variance, logits = outputs\n        elif return_x0:\n            model_mean, _, model_log_variance, x0 = outputs\n        else:\n            model_mean, _, model_log_variance = outputs\n\n        noise = noise_like(x.shape, device, repeat_noise) * temperature\n        if noise_dropout > 0.:\n            noise = torch.nn.functional.dropout(noise, p=noise_dropout)\n        # no noise when t == 0\n        nonzero_mask = (1 - (t == 0).float()).reshape(b, *((1,) * (len(x.shape) - 1)))\n\n        if return_codebook_ids:\n            return model_mean + nonzero_mask * (0.5 * model_log_variance).exp() * noise, logits.argmax(dim=1)\n        if return_x0:\n            return model_mean + nonzero_mask * (0.5 * model_log_variance).exp() * noise, x0\n        else:\n            return model_mean + nonzero_mask * (0.5 * model_log_variance).exp() * noise\n\n    @torch.no_grad()\n    def progressive_denoising(self, cond, shape, verbose=True, callback=None, quantize_denoised=False,\n                              img_callback=None, mask=None, x0=None, temperature=1., noise_dropout=0.,\n                              score_corrector=None, corrector_kwargs=None, batch_size=None, x_T=None, start_T=None,\n                              log_every_t=None):\n        if not log_every_t:\n            log_every_t = self.log_every_t\n        timesteps = self.num_timesteps\n        if batch_size is not None:\n            b = batch_size if batch_size is not None else shape[0]\n            shape = [batch_size] + list(shape)\n        else:\n            b = batch_size = shape[0]\n        if x_T is None:\n            img = torch.randn(shape, device=self.device)\n        else:\n            img = x_T\n        intermediates = []\n        if cond is not None:\n            if isinstance(cond, dict):\n                cond = {key: cond[key][:batch_size] if not isinstance(cond[key], list) else\n                list(map(lambda x: x[:batch_size], cond[key])) for key in cond}\n            else:\n                cond = [c[:batch_size] for c in cond] if isinstance(cond, list) else cond[:batch_size]\n\n        if start_T is not None:\n            timesteps = min(timesteps, start_T)\n        iterator = tqdm(reversed(range(0, timesteps)), desc='Progressive Generation',\n                        total=timesteps) if verbose else reversed(\n            range(0, timesteps))\n        if type(temperature) == float:\n            temperature = [temperature] * timesteps\n\n        for i in iterator:\n            ts = torch.full((b,), i, device=self.device, dtype=torch.long)\n            if self.shorten_cond_schedule:\n                assert self.model.conditioning_key != 'hybrid'\n                tc = self.cond_ids[ts].to(cond.device)\n                cond = self.q_sample(x_start=cond, t=tc, noise=torch.randn_like(cond))\n\n            img, x0_partial = self.p_sample(img, cond, ts,\n                                            clip_denoised=self.clip_denoised,\n                                            quantize_denoised=quantize_denoised, return_x0=True,\n                                            temperature=temperature[i], noise_dropout=noise_dropout,\n                                            score_corrector=score_corrector, corrector_kwargs=corrector_kwargs)\n            if mask is not None:\n                assert x0 is not None\n                img_orig = self.q_sample(x0, ts)\n                img = img_orig * mask + (1. - mask) * img\n\n            if i % log_every_t == 0 or i == timesteps - 1:\n                intermediates.append(x0_partial)\n            if callback: callback(i)\n            if img_callback: img_callback(img, i)\n        return img, intermediates\n\n    @torch.no_grad()\n    def p_sample_loop(self, cond, shape, return_intermediates=False,\n                      x_T=None, verbose=True, callback=None, timesteps=None, quantize_denoised=False,\n                      mask=None, x0=None, img_callback=None, start_T=None,\n                      log_every_t=None):\n\n        if not log_every_t:\n            log_every_t = self.log_every_t\n        device = self.betas.device\n        b = shape[0]\n        if x_T is None:\n            img = torch.randn(shape, device=device)\n        else:\n            img = x_T\n\n        intermediates = [img]\n        if timesteps is None:\n            timesteps = self.num_timesteps\n\n        if start_T is not None:\n            timesteps = min(timesteps, start_T)\n        iterator = tqdm(reversed(range(0, timesteps)), desc='Sampling t', total=timesteps) if verbose else reversed(\n            range(0, timesteps))\n\n        if mask is not None:\n            assert x0 is not None\n            assert x0.shape[2:3] == mask.shape[2:3]  # spatial size has to match\n\n        for i in iterator:\n            ts = torch.full((b,), i, device=device, dtype=torch.long)\n            if self.shorten_cond_schedule:\n                assert self.model.conditioning_key != 'hybrid'\n                tc = self.cond_ids[ts].to(cond.device)\n                cond = self.q_sample(x_start=cond, t=tc, noise=torch.randn_like(cond))\n\n            img = self.p_sample(img, cond, ts,\n                                clip_denoised=self.clip_denoised,\n                                quantize_denoised=quantize_denoised)\n            if mask is not None:\n                img_orig = self.q_sample(x0, ts)\n                img = img_orig * mask + (1. - mask) * img\n\n            if i % log_every_t == 0 or i == timesteps - 1:\n                intermediates.append(img)\n            if callback: callback(i)\n            if img_callback: img_callback(img, i)\n\n        if return_intermediates:\n            return img, intermediates\n        return img\n\n    @torch.no_grad()\n    def sample(self, cond, batch_size=16, return_intermediates=False, x_T=None,\n               verbose=True, timesteps=None, quantize_denoised=False,\n               mask=None, x0=None, shape=None, **kwargs):\n        if shape is None:\n            shape = (batch_size, self.channels, self.image_size, self.image_size)\n        if cond is not None:\n            if isinstance(cond, dict):\n                cond = {key: cond[key][:batch_size] if not isinstance(cond[key], list) else\n                list(map(lambda x: x[:batch_size], cond[key])) for key in cond}\n            else:\n                cond = [c[:batch_size] for c in cond] if isinstance(cond, list) else cond[:batch_size]\n        return self.p_sample_loop(cond,\n                                  shape,\n                                  return_intermediates=return_intermediates, x_T=x_T,\n                                  verbose=verbose, timesteps=timesteps, quantize_denoised=quantize_denoised,\n                                  mask=mask, x0=x0)\n\n    @torch.no_grad()\n    def sample_log(self, cond, batch_size, ddim, ddim_steps, **kwargs):\n        if ddim:\n            ddim_sampler = DDIMSampler(self)\n            shape = (self.channels, self.image_size, self.image_size)\n            samples, intermediates = ddim_sampler.sample(ddim_steps, batch_size,\n                                                         shape, cond, verbose=False, **kwargs)\n\n        else:\n            samples, intermediates = self.sample(cond=cond, batch_size=batch_size,\n                                                 return_intermediates=True, **kwargs)\n\n        return samples, intermediates\n\n    @torch.no_grad()\n    def get_unconditional_conditioning(self, batch_size, null_label=None):\n        if null_label is not None:\n            xc = null_label\n            if isinstance(xc, ListConfig):\n                xc = list(xc)\n            if isinstance(xc, dict) or isinstance(xc, list):\n                c = self.get_learned_conditioning(xc)\n            else:\n                if hasattr(xc, \"to\"):\n                    xc = xc.to(self.device)\n                c = self.get_learned_conditioning(xc)\n        else:\n            if self.cond_stage_key in [\"class_label\", \"cls\"]:\n                xc = self.cond_stage_model.get_unconditional_conditioning(batch_size, device=self.device)\n                return self.get_learned_conditioning(xc)\n            else:\n                raise NotImplementedError(\"todo\")\n        if isinstance(c, list):  # in case the encoder gives us a list\n            for i in range(len(c)):\n                c[i] = repeat(c[i], '1 ... -> b ...', b=batch_size).to(self.device)\n        else:\n            c = repeat(c, '1 ... -> b ...', b=batch_size).to(self.device)\n        return c\n\n    @torch.no_grad()\n    def log_images(self, batch, N=8, n_row=4, sample=True, ddim_steps=50, ddim_eta=0., return_keys=None,\n                   quantize_denoised=True, inpaint=True, plot_denoise_rows=False, plot_progressive_rows=True,\n                   plot_diffusion_rows=True, unconditional_guidance_scale=1., unconditional_guidance_label=None,\n                   use_ema_scope=True,\n                   **kwargs):\n        ema_scope = self.ema_scope if use_ema_scope else nullcontext\n        use_ddim = ddim_steps is not None\n\n        log = dict()\n        z, c, x, xrec, xc = self.get_input(batch, self.first_stage_key,\n                                           return_first_stage_outputs=True,\n                                           force_c_encode=True,\n                                           return_original_cond=True,\n                                           bs=N)\n        N = min(x.shape[0], N)\n        n_row = min(x.shape[0], n_row)\n        log[\"inputs\"] = x\n        log[\"reconstruction\"] = xrec\n        if self.model.conditioning_key is not None:\n            if hasattr(self.cond_stage_model, \"decode\"):\n                xc = self.cond_stage_model.decode(c)\n                log[\"conditioning\"] = xc\n            elif self.cond_stage_key in [\"caption\", \"txt\"]:\n                xc = log_txt_as_img((x.shape[2], x.shape[3]), batch[self.cond_stage_key], size=x.shape[2] // 25)\n                log[\"conditioning\"] = xc\n            elif self.cond_stage_key in ['class_label', \"cls\"]:\n                try:\n                    xc = log_txt_as_img((x.shape[2], x.shape[3]), batch[\"human_label\"], size=x.shape[2] // 25)\n                    log['conditioning'] = xc\n                except KeyError:\n                    # probably no \"human_label\" in batch\n                    pass\n            elif isimage(xc):\n                log[\"conditioning\"] = xc\n            if ismap(xc):\n                log[\"original_conditioning\"] = self.to_rgb(xc)\n\n        if plot_diffusion_rows:\n            # get diffusion row\n            diffusion_row = list()\n            z_start = z[:n_row]\n            for t in range(self.num_timesteps):\n                if t % self.log_every_t == 0 or t == self.num_timesteps - 1:\n                    t = repeat(torch.tensor([t]), '1 -> b', b=n_row)\n                    t = t.to(self.device).long()\n                    noise = torch.randn_like(z_start)\n                    z_noisy = self.q_sample(x_start=z_start, t=t, noise=noise)\n                    diffusion_row.append(self.decode_first_stage(z_noisy))\n\n            diffusion_row = torch.stack(diffusion_row)  # n_log_step, n_row, C, H, W\n            diffusion_grid = rearrange(diffusion_row, 'n b c h w -> b n c h w')\n            diffusion_grid = rearrange(diffusion_grid, 'b n c h w -> (b n) c h w')\n            diffusion_grid = make_grid(diffusion_grid, nrow=diffusion_row.shape[0])\n            log[\"diffusion_row\"] = diffusion_grid\n\n        if sample:\n            # get denoise row\n            with ema_scope(\"Sampling\"):\n                samples, z_denoise_row = self.sample_log(cond=c, batch_size=N, ddim=use_ddim,\n                                                         ddim_steps=ddim_steps, eta=ddim_eta)\n                # samples, z_denoise_row = self.sample(cond=c, batch_size=N, return_intermediates=True)\n            x_samples = self.decode_first_stage(samples)\n            log[\"samples\"] = x_samples\n            if plot_denoise_rows:\n                denoise_grid = self._get_denoise_row_from_list(z_denoise_row)\n                log[\"denoise_row\"] = denoise_grid\n\n            if quantize_denoised and not isinstance(self.first_stage_model, AutoencoderKL) and not isinstance(\n                    self.first_stage_model, IdentityFirstStage):\n                # also display when quantizing x0 while sampling\n                with ema_scope(\"Plotting Quantized Denoised\"):\n                    samples, z_denoise_row = self.sample_log(cond=c, batch_size=N, ddim=use_ddim,\n                                                             ddim_steps=ddim_steps, eta=ddim_eta,\n                                                             quantize_denoised=True)\n                    # samples, z_denoise_row = self.sample(cond=c, batch_size=N, return_intermediates=True,\n                    #                                      quantize_denoised=True)\n                x_samples = self.decode_first_stage(samples.to(self.device))\n                log[\"samples_x0_quantized\"] = x_samples\n\n        if unconditional_guidance_scale > 1.0:\n            uc = self.get_unconditional_conditioning(N, unconditional_guidance_label)\n            if self.model.conditioning_key == \"crossattn-adm\":\n                uc = {\"c_crossattn\": [uc], \"c_adm\": c[\"c_adm\"]}\n            with ema_scope(\"Sampling with classifier-free guidance\"):\n                samples_cfg, _ = self.sample_log(cond=c, batch_size=N, ddim=use_ddim,\n                                                 ddim_steps=ddim_steps, eta=ddim_eta,\n                                                 unconditional_guidance_scale=unconditional_guidance_scale,\n                                                 unconditional_conditioning=uc,\n                                                 )\n                x_samples_cfg = self.decode_first_stage(samples_cfg)\n                log[f\"samples_cfg_scale_{unconditional_guidance_scale:.2f}\"] = x_samples_cfg\n\n        if inpaint:\n            # make a simple center square\n            b, h, w = z.shape[0], z.shape[2], z.shape[3]\n            mask = torch.ones(N, h, w).to(self.device)\n            # zeros will be filled in\n            mask[:, h // 4:3 * h // 4, w // 4:3 * w // 4] = 0.\n            mask = mask[:, None, ...]\n            with ema_scope(\"Plotting Inpaint\"):\n                samples, _ = self.sample_log(cond=c, batch_size=N, ddim=use_ddim, eta=ddim_eta,\n                                             ddim_steps=ddim_steps, x0=z[:N], mask=mask)\n            x_samples = self.decode_first_stage(samples.to(self.device))\n            log[\"samples_inpainting\"] = x_samples\n            log[\"mask\"] = mask\n\n            # outpaint\n            mask = 1. - mask\n            with ema_scope(\"Plotting Outpaint\"):\n                samples, _ = self.sample_log(cond=c, batch_size=N, ddim=use_ddim, eta=ddim_eta,\n                                             ddim_steps=ddim_steps, x0=z[:N], mask=mask)\n            x_samples = self.decode_first_stage(samples.to(self.device))\n            log[\"samples_outpainting\"] = x_samples\n\n        if plot_progressive_rows:\n            with ema_scope(\"Plotting Progressives\"):\n                img, progressives = self.progressive_denoising(c,\n                                                               shape=(self.channels, self.image_size, self.image_size),\n                                                               batch_size=N)\n            prog_row = self._get_denoise_row_from_list(progressives, desc=\"Progressive Generation\")\n            log[\"progressive_row\"] = prog_row\n\n        if return_keys:\n            if np.intersect1d(list(log.keys()), return_keys).shape[0] == 0:\n                return log\n            else:\n                return {key: log[key] for key in return_keys}\n        return log\n\n    def configure_optimizers(self):\n        lr = self.learning_rate\n        params = list(self.model.parameters())\n        if self.cond_stage_trainable:\n            print(f\"{self.__class__.__name__}: Also optimizing conditioner params!\")\n            params = params + list(self.cond_stage_model.parameters())\n        if self.learn_logvar:\n            print('Diffusion model optimizing logvar')\n            params.append(self.logvar)\n        opt = torch.optim.AdamW(params, lr=lr)\n        if self.use_scheduler:\n            assert 'target' in self.scheduler_config\n            scheduler = instantiate_from_config(self.scheduler_config)\n\n            print(\"Setting up LambdaLR scheduler...\")\n            scheduler = [\n                {\n                    'scheduler': LambdaLR(opt, lr_lambda=scheduler.schedule),\n                    'interval': 'step',\n                    'frequency': 1\n                }]\n            return [opt], scheduler\n        return opt\n\n    @torch.no_grad()\n    def to_rgb(self, x):\n        x = x.float()\n        if not hasattr(self, \"colorize\"):\n            self.colorize = torch.randn(3, x.shape[1], 1, 1).to(x)\n        x = nn.functional.conv2d(x, weight=self.colorize)\n        x = 2. * (x - x.min()) / (x.max() - x.min()) - 1.\n        return x\n\n\nclass DiffusionWrapper(pl.LightningModule):\n    def __init__(self, diff_model_config, conditioning_key):\n        super().__init__()\n        self.sequential_cross_attn = diff_model_config.pop(\"sequential_crossattn\", False)\n        self.diffusion_model = instantiate_from_config(diff_model_config)\n        self.conditioning_key = conditioning_key\n        assert self.conditioning_key in [None, 'concat', 'crossattn', 'hybrid', 'adm', 'hybrid-adm', 'crossattn-adm']\n\n    def forward(self, x, t, c_concat: list = None, c_crossattn: list = None, c_adm=None):\n        if self.conditioning_key is None:\n            out = self.diffusion_model(x, t)\n        elif self.conditioning_key == 'concat':\n            xc = torch.cat([x] + c_concat, dim=1)\n            out = self.diffusion_model(xc, t)\n        elif self.conditioning_key == 'crossattn':\n            if not self.sequential_cross_attn:\n                cc = torch.cat(c_crossattn, 1)\n            else:\n                cc = c_crossattn\n            if hasattr(self, \"scripted_diffusion_model\"):\n                # TorchScript changes names of the arguments\n                # with argument cc defined as context=cc scripted model will produce\n                # an error: RuntimeError: forward() is missing value for argument 'argument_3'.\n                out = self.scripted_diffusion_model(x, t, cc)\n            else:\n                out = self.diffusion_model(x, t, context=cc)\n        elif self.conditioning_key == 'hybrid':\n            xc = torch.cat([x] + c_concat, dim=1)\n            cc = torch.cat(c_crossattn, 1)\n            out = self.diffusion_model(xc, t, context=cc)\n        elif self.conditioning_key == 'hybrid-adm':\n            assert c_adm is not None\n            xc = torch.cat([x] + c_concat, dim=1)\n            cc = torch.cat(c_crossattn, 1)\n            out = self.diffusion_model(xc, t, context=cc, y=c_adm)\n        elif self.conditioning_key == 'crossattn-adm':\n            assert c_adm is not None\n            cc = torch.cat(c_crossattn, 1)\n            out = self.diffusion_model(x, t, context=cc, y=c_adm)\n        elif self.conditioning_key == 'adm':\n            cc = c_crossattn[0]\n            out = self.diffusion_model(x, t, y=cc)\n        else:\n            raise NotImplementedError()\n\n        return out\n\n\nclass LatentUpscaleDiffusion(LatentDiffusion):\n    def __init__(self, *args, low_scale_config, low_scale_key=\"LR\", noise_level_key=None, **kwargs):\n        super().__init__(*args, **kwargs)\n        # assumes that neither the cond_stage nor the low_scale_model contain trainable params\n        assert not self.cond_stage_trainable\n        self.instantiate_low_stage(low_scale_config)\n        self.low_scale_key = low_scale_key\n        self.noise_level_key = noise_level_key\n\n    def instantiate_low_stage(self, config):\n        model = instantiate_from_config(config)\n        self.low_scale_model = model.eval()\n        self.low_scale_model.train = disabled_train\n        for param in self.low_scale_model.parameters():\n            param.requires_grad = False\n\n    @torch.no_grad()\n    def get_input(self, batch, k, cond_key=None, bs=None, log_mode=False):\n        if not log_mode:\n            z, c = super().get_input(batch, k, force_c_encode=True, bs=bs)\n        else:\n            z, c, x, xrec, xc = super().get_input(batch, self.first_stage_key, return_first_stage_outputs=True,\n                                                  force_c_encode=True, return_original_cond=True, bs=bs)\n        x_low = batch[self.low_scale_key][:bs]\n        x_low = rearrange(x_low, 'b h w c -> b c h w')\n        x_low = x_low.to(memory_format=torch.contiguous_format).float()\n        zx, noise_level = self.low_scale_model(x_low)\n        if self.noise_level_key is not None:\n            # get noise level from batch instead, e.g. when extracting a custom noise level for bsr\n            raise NotImplementedError('TODO')\n\n        all_conds = {\"c_concat\": [zx], \"c_crossattn\": [c], \"c_adm\": noise_level}\n        if log_mode:\n            # TODO: maybe disable if too expensive\n            x_low_rec = self.low_scale_model.decode(zx)\n            return z, all_conds, x, xrec, xc, x_low, x_low_rec, noise_level\n        return z, all_conds\n\n    @torch.no_grad()\n    def log_images(self, batch, N=8, n_row=4, sample=True, ddim_steps=200, ddim_eta=1., return_keys=None,\n                   plot_denoise_rows=False, plot_progressive_rows=True, plot_diffusion_rows=True,\n                   unconditional_guidance_scale=1., unconditional_guidance_label=None, use_ema_scope=True,\n                   **kwargs):\n        ema_scope = self.ema_scope if use_ema_scope else nullcontext\n        use_ddim = ddim_steps is not None\n\n        log = dict()\n        z, c, x, xrec, xc, x_low, x_low_rec, noise_level = self.get_input(batch, self.first_stage_key, bs=N,\n                                                                          log_mode=True)\n        N = min(x.shape[0], N)\n        n_row = min(x.shape[0], n_row)\n        log[\"inputs\"] = x\n        log[\"reconstruction\"] = xrec\n        log[\"x_lr\"] = x_low\n        log[f\"x_lr_rec_@noise_levels{'-'.join(map(lambda x: str(x), list(noise_level.cpu().numpy())))}\"] = x_low_rec\n        if self.model.conditioning_key is not None:\n            if hasattr(self.cond_stage_model, \"decode\"):\n                xc = self.cond_stage_model.decode(c)\n                log[\"conditioning\"] = xc\n            elif self.cond_stage_key in [\"caption\", \"txt\"]:\n                xc = log_txt_as_img((x.shape[2], x.shape[3]), batch[self.cond_stage_key], size=x.shape[2] // 25)\n                log[\"conditioning\"] = xc\n            elif self.cond_stage_key in ['class_label', 'cls']:\n                xc = log_txt_as_img((x.shape[2], x.shape[3]), batch[\"human_label\"], size=x.shape[2] // 25)\n                log['conditioning'] = xc\n            elif isimage(xc):\n                log[\"conditioning\"] = xc\n            if ismap(xc):\n                log[\"original_conditioning\"] = self.to_rgb(xc)\n\n        if plot_diffusion_rows:\n            # get diffusion row\n            diffusion_row = list()\n            z_start = z[:n_row]\n            for t in range(self.num_timesteps):\n                if t % self.log_every_t == 0 or t == self.num_timesteps - 1:\n                    t = repeat(torch.tensor([t]), '1 -> b', b=n_row)\n                    t = t.to(self.device).long()\n                    noise = torch.randn_like(z_start)\n                    z_noisy = self.q_sample(x_start=z_start, t=t, noise=noise)\n                    diffusion_row.append(self.decode_first_stage(z_noisy))\n\n            diffusion_row = torch.stack(diffusion_row)  # n_log_step, n_row, C, H, W\n            diffusion_grid = rearrange(diffusion_row, 'n b c h w -> b n c h w')\n            diffusion_grid = rearrange(diffusion_grid, 'b n c h w -> (b n) c h w')\n            diffusion_grid = make_grid(diffusion_grid, nrow=diffusion_row.shape[0])\n            log[\"diffusion_row\"] = diffusion_grid\n\n        if sample:\n            # get denoise row\n            with ema_scope(\"Sampling\"):\n                samples, z_denoise_row = self.sample_log(cond=c, batch_size=N, ddim=use_ddim,\n                                                         ddim_steps=ddim_steps, eta=ddim_eta)\n                # samples, z_denoise_row = self.sample(cond=c, batch_size=N, return_intermediates=True)\n            x_samples = self.decode_first_stage(samples)\n            log[\"samples\"] = x_samples\n            if plot_denoise_rows:\n                denoise_grid = self._get_denoise_row_from_list(z_denoise_row)\n                log[\"denoise_row\"] = denoise_grid\n\n        if unconditional_guidance_scale > 1.0:\n            uc_tmp = self.get_unconditional_conditioning(N, unconditional_guidance_label)\n            # TODO explore better \"unconditional\" choices for the other keys\n            # maybe guide away from empty text label and highest noise level and maximally degraded zx?\n            uc = dict()\n            for k in c:\n                if k == \"c_crossattn\":\n                    assert isinstance(c[k], list) and len(c[k]) == 1\n                    uc[k] = [uc_tmp]\n                elif k == \"c_adm\":  # todo: only run with text-based guidance?\n                    assert isinstance(c[k], torch.Tensor)\n                    #uc[k] = torch.ones_like(c[k]) * self.low_scale_model.max_noise_level\n                    uc[k] = c[k]\n                elif isinstance(c[k], list):\n                    uc[k] = [c[k][i] for i in range(len(c[k]))]\n                else:\n                    uc[k] = c[k]\n\n            with ema_scope(\"Sampling with classifier-free guidance\"):\n                samples_cfg, _ = self.sample_log(cond=c, batch_size=N, ddim=use_ddim,\n                                                 ddim_steps=ddim_steps, eta=ddim_eta,\n                                                 unconditional_guidance_scale=unconditional_guidance_scale,\n                                                 unconditional_conditioning=uc,\n                                                 )\n                x_samples_cfg = self.decode_first_stage(samples_cfg)\n                log[f\"samples_cfg_scale_{unconditional_guidance_scale:.2f}\"] = x_samples_cfg\n\n        if plot_progressive_rows:\n            with ema_scope(\"Plotting Progressives\"):\n                img, progressives = self.progressive_denoising(c,\n                                                               shape=(self.channels, self.image_size, self.image_size),\n                                                               batch_size=N)\n            prog_row = self._get_denoise_row_from_list(progressives, desc=\"Progressive Generation\")\n            log[\"progressive_row\"] = prog_row\n\n        return log\n\n\nclass LatentFinetuneDiffusion(LatentDiffusion):\n    \"\"\"\n         Basis for different finetunas, such as inpainting or depth2image\n         To disable finetuning mode, set finetune_keys to None\n    \"\"\"\n\n    def __init__(self,\n                 concat_keys: tuple,\n                 finetune_keys=(\"model.diffusion_model.input_blocks.0.0.weight\",\n                                \"model_ema.diffusion_modelinput_blocks00weight\"\n                                ),\n                 keep_finetune_dims=4,\n                 # if model was trained without concat mode before and we would like to keep these channels\n                 c_concat_log_start=None,  # to log reconstruction of c_concat codes\n                 c_concat_log_end=None,\n                 *args, **kwargs\n                 ):\n        ckpt_path = kwargs.pop(\"ckpt_path\", None)\n        ignore_keys = kwargs.pop(\"ignore_keys\", list())\n        super().__init__(*args, **kwargs)\n        self.finetune_keys = finetune_keys\n        self.concat_keys = concat_keys\n        self.keep_dims = keep_finetune_dims\n        self.c_concat_log_start = c_concat_log_start\n        self.c_concat_log_end = c_concat_log_end\n        if exists(self.finetune_keys): assert exists(ckpt_path), 'can only finetune from a given checkpoint'\n        if exists(ckpt_path):\n            self.init_from_ckpt(ckpt_path, ignore_keys)\n\n    def init_from_ckpt(self, path, ignore_keys=list(), only_model=False):\n        sd = torch.load(path, map_location=\"cpu\")\n        if \"state_dict\" in list(sd.keys()):\n            sd = sd[\"state_dict\"]\n        keys = list(sd.keys())\n        for k in keys:\n            for ik in ignore_keys:\n                if k.startswith(ik):\n                    print(\"Deleting key {} from state_dict.\".format(k))\n                    del sd[k]\n\n            # make it explicit, finetune by including extra input channels\n            if exists(self.finetune_keys) and k in self.finetune_keys:\n                new_entry = None\n                for name, param in self.named_parameters():\n                    if name in self.finetune_keys:\n                        print(\n                            f\"modifying key '{name}' and keeping its original {self.keep_dims} (channels) dimensions only\")\n                        new_entry = torch.zeros_like(param)  # zero init\n                assert exists(new_entry), 'did not find matching parameter to modify'\n                new_entry[:, :self.keep_dims, ...] = sd[k]\n                sd[k] = new_entry\n\n        missing, unexpected = self.load_state_dict(sd, strict=False) if not only_model else self.model.load_state_dict(\n            sd, strict=False)\n        print(f\"Restored from {path} with {len(missing)} missing and {len(unexpected)} unexpected keys\")\n        if len(missing) > 0:\n            print(f\"Missing Keys: {missing}\")\n        if len(unexpected) > 0:\n            print(f\"Unexpected Keys: {unexpected}\")\n\n    @torch.no_grad()\n    def log_images(self, batch, N=8, n_row=4, sample=True, ddim_steps=200, ddim_eta=1., return_keys=None,\n                   quantize_denoised=True, inpaint=True, plot_denoise_rows=False, plot_progressive_rows=True,\n                   plot_diffusion_rows=True, unconditional_guidance_scale=1., unconditional_guidance_label=None,\n                   use_ema_scope=True,\n                   **kwargs):\n        ema_scope = self.ema_scope if use_ema_scope else nullcontext\n        use_ddim = ddim_steps is not None\n\n        log = dict()\n        z, c, x, xrec, xc = self.get_input(batch, self.first_stage_key, bs=N, return_first_stage_outputs=True)\n        c_cat, c = c[\"c_concat\"][0], c[\"c_crossattn\"][0]\n        N = min(x.shape[0], N)\n        n_row = min(x.shape[0], n_row)\n        log[\"inputs\"] = x\n        log[\"reconstruction\"] = xrec\n        if self.model.conditioning_key is not None:\n            if hasattr(self.cond_stage_model, \"decode\"):\n                xc = self.cond_stage_model.decode(c)\n                log[\"conditioning\"] = xc\n            elif self.cond_stage_key in [\"caption\", \"txt\"]:\n                xc = log_txt_as_img((x.shape[2], x.shape[3]), batch[self.cond_stage_key], size=x.shape[2] // 25)\n                log[\"conditioning\"] = xc\n            elif self.cond_stage_key in ['class_label', 'cls']:\n                xc = log_txt_as_img((x.shape[2], x.shape[3]), batch[\"human_label\"], size=x.shape[2] // 25)\n                log['conditioning'] = xc\n            elif isimage(xc):\n                log[\"conditioning\"] = xc\n            if ismap(xc):\n                log[\"original_conditioning\"] = self.to_rgb(xc)\n\n        if not (self.c_concat_log_start is None and self.c_concat_log_end is None):\n            log[\"c_concat_decoded\"] = self.decode_first_stage(c_cat[:, self.c_concat_log_start:self.c_concat_log_end])\n\n        if plot_diffusion_rows:\n            # get diffusion row\n            diffusion_row = list()\n            z_start = z[:n_row]\n            for t in range(self.num_timesteps):\n                if t % self.log_every_t == 0 or t == self.num_timesteps - 1:\n                    t = repeat(torch.tensor([t]), '1 -> b', b=n_row)\n                    t = t.to(self.device).long()\n                    noise = torch.randn_like(z_start)\n                    z_noisy = self.q_sample(x_start=z_start, t=t, noise=noise)\n                    diffusion_row.append(self.decode_first_stage(z_noisy))\n\n            diffusion_row = torch.stack(diffusion_row)  # n_log_step, n_row, C, H, W\n            diffusion_grid = rearrange(diffusion_row, 'n b c h w -> b n c h w')\n            diffusion_grid = rearrange(diffusion_grid, 'b n c h w -> (b n) c h w')\n            diffusion_grid = make_grid(diffusion_grid, nrow=diffusion_row.shape[0])\n            log[\"diffusion_row\"] = diffusion_grid\n\n        if sample:\n            # get denoise row\n            with ema_scope(\"Sampling\"):\n                samples, z_denoise_row = self.sample_log(cond={\"c_concat\": [c_cat], \"c_crossattn\": [c]},\n                                                         batch_size=N, ddim=use_ddim,\n                                                         ddim_steps=ddim_steps, eta=ddim_eta)\n                # samples, z_denoise_row = self.sample(cond=c, batch_size=N, return_intermediates=True)\n            x_samples = self.decode_first_stage(samples)\n            log[\"samples\"] = x_samples\n            if plot_denoise_rows:\n                denoise_grid = self._get_denoise_row_from_list(z_denoise_row)\n                log[\"denoise_row\"] = denoise_grid\n\n        if unconditional_guidance_scale > 1.0:\n            uc_cross = self.get_unconditional_conditioning(N, unconditional_guidance_label)\n            uc_cat = c_cat\n            uc_full = {\"c_concat\": [uc_cat], \"c_crossattn\": [uc_cross]}\n            with ema_scope(\"Sampling with classifier-free guidance\"):\n                samples_cfg, _ = self.sample_log(cond={\"c_concat\": [c_cat], \"c_crossattn\": [c]},\n                                                 batch_size=N, ddim=use_ddim,\n                                                 ddim_steps=ddim_steps, eta=ddim_eta,\n                                                 unconditional_guidance_scale=unconditional_guidance_scale,\n                                                 unconditional_conditioning=uc_full,\n                                                 )\n                x_samples_cfg = self.decode_first_stage(samples_cfg)\n                log[f\"samples_cfg_scale_{unconditional_guidance_scale:.2f}\"] = x_samples_cfg\n\n        return log\n\n\nclass LatentInpaintDiffusion(LatentFinetuneDiffusion):\n    \"\"\"\n    can either run as pure inpainting model (only concat mode) or with mixed conditionings,\n    e.g. mask as concat and text via cross-attn.\n    To disable finetuning mode, set finetune_keys to None\n     \"\"\"\n\n    def __init__(self,\n                 concat_keys=(\"mask\", \"masked_image\"),\n                 masked_image_key=\"masked_image\",\n                 *args, **kwargs\n                 ):\n        super().__init__(concat_keys, *args, **kwargs)\n        self.masked_image_key = masked_image_key\n        assert self.masked_image_key in concat_keys\n\n    @torch.no_grad()\n    def get_input(self, batch, k, cond_key=None, bs=None, return_first_stage_outputs=False):\n        # note: restricted to non-trainable encoders currently\n        assert not self.cond_stage_trainable, 'trainable cond stages not yet supported for inpainting'\n        z, c, x, xrec, xc = super().get_input(batch, self.first_stage_key, return_first_stage_outputs=True,\n                                              force_c_encode=True, return_original_cond=True, bs=bs)\n\n        assert exists(self.concat_keys)\n        c_cat = list()\n        for ck in self.concat_keys:\n            cc = rearrange(batch[ck], 'b h w c -> b c h w').to(memory_format=torch.contiguous_format).float()\n            if bs is not None:\n                cc = cc[:bs]\n                cc = cc.to(self.device)\n            bchw = z.shape\n            if ck != self.masked_image_key:\n                cc = torch.nn.functional.interpolate(cc, size=bchw[-2:])\n            else:\n                cc = self.get_first_stage_encoding(self.encode_first_stage(cc))\n            c_cat.append(cc)\n        c_cat = torch.cat(c_cat, dim=1)\n        all_conds = {\"c_concat\": [c_cat], \"c_crossattn\": [c]}\n        if return_first_stage_outputs:\n            return z, all_conds, x, xrec, xc\n        return z, all_conds\n\n    @torch.no_grad()\n    def log_images(self, *args, **kwargs):\n        log = super(LatentInpaintDiffusion, self).log_images(*args, **kwargs)\n        log[\"masked_image\"] = rearrange(args[0][\"masked_image\"],\n                                        'b h w c -> b c h w').to(memory_format=torch.contiguous_format).float()\n        return log\n\n\nclass LatentDepth2ImageDiffusion(LatentFinetuneDiffusion):\n    \"\"\"\n    condition on monocular depth estimation\n    \"\"\"\n\n    def __init__(self, depth_stage_config, concat_keys=(\"midas_in\",), *args, **kwargs):\n        super().__init__(concat_keys=concat_keys, *args, **kwargs)\n        self.depth_model = instantiate_from_config(depth_stage_config)\n        self.depth_stage_key = concat_keys[0]\n\n    @torch.no_grad()\n    def get_input(self, batch, k, cond_key=None, bs=None, return_first_stage_outputs=False):\n        # note: restricted to non-trainable encoders currently\n        assert not self.cond_stage_trainable, 'trainable cond stages not yet supported for depth2img'\n        z, c, x, xrec, xc = super().get_input(batch, self.first_stage_key, return_first_stage_outputs=True,\n                                              force_c_encode=True, return_original_cond=True, bs=bs)\n\n        assert exists(self.concat_keys)\n        assert len(self.concat_keys) == 1\n        c_cat = list()\n        for ck in self.concat_keys:\n            cc = batch[ck]\n            if bs is not None:\n                cc = cc[:bs]\n                cc = cc.to(self.device)\n            cc = self.depth_model(cc)\n            cc = torch.nn.functional.interpolate(\n                cc,\n                size=z.shape[2:],\n                mode=\"bicubic\",\n                align_corners=False,\n            )\n\n            depth_min, depth_max = torch.amin(cc, dim=[1, 2, 3], keepdim=True), torch.amax(cc, dim=[1, 2, 3],\n                                                                                           keepdim=True)\n            cc = 2. * (cc - depth_min) / (depth_max - depth_min + 0.001) - 1.\n            c_cat.append(cc)\n        c_cat = torch.cat(c_cat, dim=1)\n        all_conds = {\"c_concat\": [c_cat], \"c_crossattn\": [c]}\n        if return_first_stage_outputs:\n            return z, all_conds, x, xrec, xc\n        return z, all_conds\n\n    @torch.no_grad()\n    def log_images(self, *args, **kwargs):\n        log = super().log_images(*args, **kwargs)\n        depth = self.depth_model(args[0][self.depth_stage_key])\n        depth_min, depth_max = torch.amin(depth, dim=[1, 2, 3], keepdim=True), \\\n                               torch.amax(depth, dim=[1, 2, 3], keepdim=True)\n        log[\"depth\"] = 2. * (depth - depth_min) / (depth_max - depth_min) - 1.\n        return log\n\n\nclass LatentUpscaleFinetuneDiffusion(LatentFinetuneDiffusion):\n    \"\"\"\n        condition on low-res image (and optionally on some spatial noise augmentation)\n    \"\"\"\n    def __init__(self, concat_keys=(\"lr\",), reshuffle_patch_size=None,\n                 low_scale_config=None, low_scale_key=None, *args, **kwargs):\n        super().__init__(concat_keys=concat_keys, *args, **kwargs)\n        self.reshuffle_patch_size = reshuffle_patch_size\n        self.low_scale_model = None\n        if low_scale_config is not None:\n            print(\"Initializing a low-scale model\")\n            assert exists(low_scale_key)\n            self.instantiate_low_stage(low_scale_config)\n            self.low_scale_key = low_scale_key\n\n    def instantiate_low_stage(self, config):\n        model = instantiate_from_config(config)\n        self.low_scale_model = model.eval()\n        self.low_scale_model.train = disabled_train\n        for param in self.low_scale_model.parameters():\n            param.requires_grad = False\n\n    @torch.no_grad()\n    def get_input(self, batch, k, cond_key=None, bs=None, return_first_stage_outputs=False):\n        # note: restricted to non-trainable encoders currently\n        assert not self.cond_stage_trainable, 'trainable cond stages not yet supported for upscaling-ft'\n        z, c, x, xrec, xc = super().get_input(batch, self.first_stage_key, return_first_stage_outputs=True,\n                                              force_c_encode=True, return_original_cond=True, bs=bs)\n\n        assert exists(self.concat_keys)\n        assert len(self.concat_keys) == 1\n        # optionally make spatial noise_level here\n        c_cat = list()\n        noise_level = None\n        for ck in self.concat_keys:\n            cc = batch[ck]\n            cc = rearrange(cc, 'b h w c -> b c h w')\n            if exists(self.reshuffle_patch_size):\n                assert isinstance(self.reshuffle_patch_size, int)\n                cc = rearrange(cc, 'b c (p1 h) (p2 w) -> b (p1 p2 c) h w',\n                               p1=self.reshuffle_patch_size, p2=self.reshuffle_patch_size)\n            if bs is not None:\n                cc = cc[:bs]\n                cc = cc.to(self.device)\n            if exists(self.low_scale_model) and ck == self.low_scale_key:\n                cc, noise_level = self.low_scale_model(cc)\n            c_cat.append(cc)\n        c_cat = torch.cat(c_cat, dim=1)\n        if exists(noise_level):\n            all_conds = {\"c_concat\": [c_cat], \"c_crossattn\": [c], \"c_adm\": noise_level}\n        else:\n            all_conds = {\"c_concat\": [c_cat], \"c_crossattn\": [c]}\n        if return_first_stage_outputs:\n            return z, all_conds, x, xrec, xc\n        return z, all_conds\n\n    @torch.no_grad()\n    def log_images(self, *args, **kwargs):\n        log = super().log_images(*args, **kwargs)\n        log[\"lr\"] = rearrange(args[0][\"lr\"], 'b h w c -> b c h w')\n        return log\n\n\nclass ImageEmbeddingConditionedLatentDiffusion(LatentDiffusion):\n    def __init__(self, embedder_config, embedding_key=\"jpg\", embedding_dropout=0.5,\n                 freeze_embedder=True, noise_aug_config=None, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.embed_key = embedding_key\n        self.embedding_dropout = embedding_dropout\n        self._init_embedder(embedder_config, freeze_embedder)\n        self._init_noise_aug(noise_aug_config)\n\n    def _init_embedder(self, config, freeze=True):\n        embedder = instantiate_from_config(config)\n        if freeze:\n            self.embedder = embedder.eval()\n            self.embedder.train = disabled_train\n            for param in self.embedder.parameters():\n                param.requires_grad = False\n\n    def _init_noise_aug(self, config):\n        if config is not None:\n            # use the KARLO schedule for noise augmentation on CLIP image embeddings\n            noise_augmentor = instantiate_from_config(config)\n            assert isinstance(noise_augmentor, nn.Module)\n            noise_augmentor = noise_augmentor.eval()\n            noise_augmentor.train = disabled_train\n            self.noise_augmentor = noise_augmentor\n        else:\n            self.noise_augmentor = None\n\n    def get_input(self, batch, k, cond_key=None, bs=None, **kwargs):\n        outputs = LatentDiffusion.get_input(self, batch, k, bs=bs, **kwargs)\n        z, c = outputs[0], outputs[1]\n        img = batch[self.embed_key][:bs]\n        img = rearrange(img, 'b h w c -> b c h w')\n        c_adm = self.embedder(img)\n        if self.noise_augmentor is not None:\n            c_adm, noise_level_emb = self.noise_augmentor(c_adm)\n            # assume this gives embeddings of noise levels\n            c_adm = torch.cat((c_adm, noise_level_emb), 1)\n        if self.training:\n            c_adm = torch.bernoulli((1. - self.embedding_dropout) * torch.ones(c_adm.shape[0],\n                                                                               device=c_adm.device)[:, None]) * c_adm\n        all_conds = {\"c_crossattn\": [c], \"c_adm\": c_adm}\n        noutputs = [z, all_conds]\n        noutputs.extend(outputs[2:])\n        return noutputs\n\n    @torch.no_grad()\n    def log_images(self, batch, N=8, n_row=4, **kwargs):\n        log = dict()\n        z, c, x, xrec, xc = self.get_input(batch, self.first_stage_key, bs=N, return_first_stage_outputs=True,\n                                           return_original_cond=True)\n        log[\"inputs\"] = x\n        log[\"reconstruction\"] = xrec\n        assert self.model.conditioning_key is not None\n        assert self.cond_stage_key in [\"caption\", \"txt\"]\n        xc = log_txt_as_img((x.shape[2], x.shape[3]), batch[self.cond_stage_key], size=x.shape[2] // 25)\n        log[\"conditioning\"] = xc\n        uc = self.get_unconditional_conditioning(N, kwargs.get('unconditional_guidance_label', ''))\n        unconditional_guidance_scale = kwargs.get('unconditional_guidance_scale', 5.)\n\n        uc_ = {\"c_crossattn\": [uc], \"c_adm\": c[\"c_adm\"]}\n        ema_scope = self.ema_scope if kwargs.get('use_ema_scope', True) else nullcontext\n        with ema_scope(f\"Sampling\"):\n            samples_cfg, _ = self.sample_log(cond=c, batch_size=N, ddim=True,\n                                             ddim_steps=kwargs.get('ddim_steps', 50), eta=kwargs.get('ddim_eta', 0.),\n                                             unconditional_guidance_scale=unconditional_guidance_scale,\n                                             unconditional_conditioning=uc_, )\n            x_samples_cfg = self.decode_first_stage(samples_cfg)\n            log[f\"samplescfg_scale_{unconditional_guidance_scale:.2f}\"] = x_samples_cfg\n        return log\n", "ldm/models/diffusion/dpm_solver/sampler.py": "\"\"\"SAMPLING ONLY.\"\"\"\nimport torch\n\nfrom .dpm_solver import NoiseScheduleVP, model_wrapper, DPM_Solver\n\nMODEL_TYPES = {\n    \"eps\": \"noise\",\n    \"v\": \"v\"\n}\n\n\nclass DPMSolverSampler(object):\n    def __init__(self, model, device=torch.device(\"cuda\"), **kwargs):\n        super().__init__()\n        self.model = model\n        self.device = device\n        to_torch = lambda x: x.clone().detach().to(torch.float32).to(model.device)\n        self.register_buffer('alphas_cumprod', to_torch(model.alphas_cumprod))\n\n    def register_buffer(self, name, attr):\n        if type(attr) == torch.Tensor:\n            if attr.device != self.device:\n                attr = attr.to(self.device)\n        setattr(self, name, attr)\n\n    @torch.no_grad()\n    def sample(self,\n               S,\n               batch_size,\n               shape,\n               conditioning=None,\n               callback=None,\n               normals_sequence=None,\n               img_callback=None,\n               quantize_x0=False,\n               eta=0.,\n               mask=None,\n               x0=None,\n               temperature=1.,\n               noise_dropout=0.,\n               score_corrector=None,\n               corrector_kwargs=None,\n               verbose=True,\n               x_T=None,\n               log_every_t=100,\n               unconditional_guidance_scale=1.,\n               unconditional_conditioning=None,\n               # this has to come in the same format as the conditioning, # e.g. as encoded tokens, ...\n               **kwargs\n               ):\n        if conditioning is not None:\n            if isinstance(conditioning, dict):\n                ctmp = conditioning[list(conditioning.keys())[0]]\n                while isinstance(ctmp, list): ctmp = ctmp[0]\n                if isinstance(ctmp, torch.Tensor):\n                    cbs = ctmp.shape[0]\n                    if cbs != batch_size:\n                        print(f\"Warning: Got {cbs} conditionings but batch-size is {batch_size}\")\n            elif isinstance(conditioning, list):\n                for ctmp in conditioning:\n                    if ctmp.shape[0] != batch_size:\n                        print(f\"Warning: Got {ctmp.shape[0]} conditionings but batch-size is {batch_size}\")\n            else:\n                if isinstance(conditioning, torch.Tensor):\n                    if conditioning.shape[0] != batch_size:\n                        print(f\"Warning: Got {conditioning.shape[0]} conditionings but batch-size is {batch_size}\")\n\n        # sampling\n        C, H, W = shape\n        size = (batch_size, C, H, W)\n\n        print(f'Data shape for DPM-Solver sampling is {size}, sampling steps {S}')\n\n        device = self.model.betas.device\n        if x_T is None:\n            img = torch.randn(size, device=device)\n        else:\n            img = x_T\n\n        ns = NoiseScheduleVP('discrete', alphas_cumprod=self.alphas_cumprod)\n\n        model_fn = model_wrapper(\n            lambda x, t, c: self.model.apply_model(x, t, c),\n            ns,\n            model_type=MODEL_TYPES[self.model.parameterization],\n            guidance_type=\"classifier-free\",\n            condition=conditioning,\n            unconditional_condition=unconditional_conditioning,\n            guidance_scale=unconditional_guidance_scale,\n        )\n\n        dpm_solver = DPM_Solver(model_fn, ns, predict_x0=True, thresholding=False)\n        x = dpm_solver.sample(img, steps=S, skip_type=\"time_uniform\", method=\"multistep\", order=2,\n                              lower_order_final=True)\n\n        return x.to(device), None\n", "ldm/models/diffusion/dpm_solver/dpm_solver.py": "import torch\nimport torch.nn.functional as F\nimport math\nfrom tqdm import tqdm\n\n\nclass NoiseScheduleVP:\n    def __init__(\n            self,\n            schedule='discrete',\n            betas=None,\n            alphas_cumprod=None,\n            continuous_beta_0=0.1,\n            continuous_beta_1=20.,\n    ):\n        \"\"\"Create a wrapper class for the forward SDE (VP type).\n        ***\n        Update: We support discrete-time diffusion models by implementing a picewise linear interpolation for log_alpha_t.\n                We recommend to use schedule='discrete' for the discrete-time diffusion models, especially for high-resolution images.\n        ***\n        The forward SDE ensures that the condition distribution q_{t|0}(x_t | x_0) = N ( alpha_t * x_0, sigma_t^2 * I ).\n        We further define lambda_t = log(alpha_t) - log(sigma_t), which is the half-logSNR (described in the DPM-Solver paper).\n        Therefore, we implement the functions for computing alpha_t, sigma_t and lambda_t. For t in [0, T], we have:\n            log_alpha_t = self.marginal_log_mean_coeff(t)\n            sigma_t = self.marginal_std(t)\n            lambda_t = self.marginal_lambda(t)\n        Moreover, as lambda(t) is an invertible function, we also support its inverse function:\n            t = self.inverse_lambda(lambda_t)\n        ===============================================================\n        We support both discrete-time DPMs (trained on n = 0, 1, ..., N-1) and continuous-time DPMs (trained on t in [t_0, T]).\n        1. For discrete-time DPMs:\n            For discrete-time DPMs trained on n = 0, 1, ..., N-1, we convert the discrete steps to continuous time steps by:\n                t_i = (i + 1) / N\n            e.g. for N = 1000, we have t_0 = 1e-3 and T = t_{N-1} = 1.\n            We solve the corresponding diffusion ODE from time T = 1 to time t_0 = 1e-3.\n            Args:\n                betas: A `torch.Tensor`. The beta array for the discrete-time DPM. (See the original DDPM paper for details)\n                alphas_cumprod: A `torch.Tensor`. The cumprod alphas for the discrete-time DPM. (See the original DDPM paper for details)\n            Note that we always have alphas_cumprod = cumprod(betas). Therefore, we only need to set one of `betas` and `alphas_cumprod`.\n            **Important**:  Please pay special attention for the args for `alphas_cumprod`:\n                The `alphas_cumprod` is the \\hat{alpha_n} arrays in the notations of DDPM. Specifically, DDPMs assume that\n                    q_{t_n | 0}(x_{t_n} | x_0) = N ( \\sqrt{\\hat{alpha_n}} * x_0, (1 - \\hat{alpha_n}) * I ).\n                Therefore, the notation \\hat{alpha_n} is different from the notation alpha_t in DPM-Solver. In fact, we have\n                    alpha_{t_n} = \\sqrt{\\hat{alpha_n}},\n                and\n                    log(alpha_{t_n}) = 0.5 * log(\\hat{alpha_n}).\n        2. For continuous-time DPMs:\n            We support two types of VPSDEs: linear (DDPM) and cosine (improved-DDPM). The hyperparameters for the noise\n            schedule are the default settings in DDPM and improved-DDPM:\n            Args:\n                beta_min: A `float` number. The smallest beta for the linear schedule.\n                beta_max: A `float` number. The largest beta for the linear schedule.\n                cosine_s: A `float` number. The hyperparameter in the cosine schedule.\n                cosine_beta_max: A `float` number. The hyperparameter in the cosine schedule.\n                T: A `float` number. The ending time of the forward process.\n        ===============================================================\n        Args:\n            schedule: A `str`. The noise schedule of the forward SDE. 'discrete' for discrete-time DPMs,\n                    'linear' or 'cosine' for continuous-time DPMs.\n        Returns:\n            A wrapper object of the forward SDE (VP type).\n\n        ===============================================================\n        Example:\n        # For discrete-time DPMs, given betas (the beta array for n = 0, 1, ..., N - 1):\n        >>> ns = NoiseScheduleVP('discrete', betas=betas)\n        # For discrete-time DPMs, given alphas_cumprod (the \\hat{alpha_n} array for n = 0, 1, ..., N - 1):\n        >>> ns = NoiseScheduleVP('discrete', alphas_cumprod=alphas_cumprod)\n        # For continuous-time DPMs (VPSDE), linear schedule:\n        >>> ns = NoiseScheduleVP('linear', continuous_beta_0=0.1, continuous_beta_1=20.)\n        \"\"\"\n\n        if schedule not in ['discrete', 'linear', 'cosine']:\n            raise ValueError(\n                \"Unsupported noise schedule {}. The schedule needs to be 'discrete' or 'linear' or 'cosine'\".format(\n                    schedule))\n\n        self.schedule = schedule\n        if schedule == 'discrete':\n            if betas is not None:\n                log_alphas = 0.5 * torch.log(1 - betas).cumsum(dim=0)\n            else:\n                assert alphas_cumprod is not None\n                log_alphas = 0.5 * torch.log(alphas_cumprod)\n            self.total_N = len(log_alphas)\n            self.T = 1.\n            self.t_array = torch.linspace(0., 1., self.total_N + 1)[1:].reshape((1, -1))\n            self.log_alpha_array = log_alphas.reshape((1, -1,))\n        else:\n            self.total_N = 1000\n            self.beta_0 = continuous_beta_0\n            self.beta_1 = continuous_beta_1\n            self.cosine_s = 0.008\n            self.cosine_beta_max = 999.\n            self.cosine_t_max = math.atan(self.cosine_beta_max * (1. + self.cosine_s) / math.pi) * 2. * (\n                        1. + self.cosine_s) / math.pi - self.cosine_s\n            self.cosine_log_alpha_0 = math.log(math.cos(self.cosine_s / (1. + self.cosine_s) * math.pi / 2.))\n            self.schedule = schedule\n            if schedule == 'cosine':\n                # For the cosine schedule, T = 1 will have numerical issues. So we manually set the ending time T.\n                # Note that T = 0.9946 may be not the optimal setting. However, we find it works well.\n                self.T = 0.9946\n            else:\n                self.T = 1.\n\n    def marginal_log_mean_coeff(self, t):\n        \"\"\"\n        Compute log(alpha_t) of a given continuous-time label t in [0, T].\n        \"\"\"\n        if self.schedule == 'discrete':\n            return interpolate_fn(t.reshape((-1, 1)), self.t_array.to(t.device),\n                                  self.log_alpha_array.to(t.device)).reshape((-1))\n        elif self.schedule == 'linear':\n            return -0.25 * t ** 2 * (self.beta_1 - self.beta_0) - 0.5 * t * self.beta_0\n        elif self.schedule == 'cosine':\n            log_alpha_fn = lambda s: torch.log(torch.cos((s + self.cosine_s) / (1. + self.cosine_s) * math.pi / 2.))\n            log_alpha_t = log_alpha_fn(t) - self.cosine_log_alpha_0\n            return log_alpha_t\n\n    def marginal_alpha(self, t):\n        \"\"\"\n        Compute alpha_t of a given continuous-time label t in [0, T].\n        \"\"\"\n        return torch.exp(self.marginal_log_mean_coeff(t))\n\n    def marginal_std(self, t):\n        \"\"\"\n        Compute sigma_t of a given continuous-time label t in [0, T].\n        \"\"\"\n        return torch.sqrt(1. - torch.exp(2. * self.marginal_log_mean_coeff(t)))\n\n    def marginal_lambda(self, t):\n        \"\"\"\n        Compute lambda_t = log(alpha_t) - log(sigma_t) of a given continuous-time label t in [0, T].\n        \"\"\"\n        log_mean_coeff = self.marginal_log_mean_coeff(t)\n        log_std = 0.5 * torch.log(1. - torch.exp(2. * log_mean_coeff))\n        return log_mean_coeff - log_std\n\n    def inverse_lambda(self, lamb):\n        \"\"\"\n        Compute the continuous-time label t in [0, T] of a given half-logSNR lambda_t.\n        \"\"\"\n        if self.schedule == 'linear':\n            tmp = 2. * (self.beta_1 - self.beta_0) * torch.logaddexp(-2. * lamb, torch.zeros((1,)).to(lamb))\n            Delta = self.beta_0 ** 2 + tmp\n            return tmp / (torch.sqrt(Delta) + self.beta_0) / (self.beta_1 - self.beta_0)\n        elif self.schedule == 'discrete':\n            log_alpha = -0.5 * torch.logaddexp(torch.zeros((1,)).to(lamb.device), -2. * lamb)\n            t = interpolate_fn(log_alpha.reshape((-1, 1)), torch.flip(self.log_alpha_array.to(lamb.device), [1]),\n                               torch.flip(self.t_array.to(lamb.device), [1]))\n            return t.reshape((-1,))\n        else:\n            log_alpha = -0.5 * torch.logaddexp(-2. * lamb, torch.zeros((1,)).to(lamb))\n            t_fn = lambda log_alpha_t: torch.arccos(torch.exp(log_alpha_t + self.cosine_log_alpha_0)) * 2. * (\n                        1. + self.cosine_s) / math.pi - self.cosine_s\n            t = t_fn(log_alpha)\n            return t\n\n\ndef model_wrapper(\n        model,\n        noise_schedule,\n        model_type=\"noise\",\n        model_kwargs={},\n        guidance_type=\"uncond\",\n        condition=None,\n        unconditional_condition=None,\n        guidance_scale=1.,\n        classifier_fn=None,\n        classifier_kwargs={},\n):\n    \"\"\"Create a wrapper function for the noise prediction model.\n    DPM-Solver needs to solve the continuous-time diffusion ODEs. For DPMs trained on discrete-time labels, we need to\n    firstly wrap the model function to a noise prediction model that accepts the continuous time as the input.\n    We support four types of the diffusion model by setting `model_type`:\n        1. \"noise\": noise prediction model. (Trained by predicting noise).\n        2. \"x_start\": data prediction model. (Trained by predicting the data x_0 at time 0).\n        3. \"v\": velocity prediction model. (Trained by predicting the velocity).\n            The \"v\" prediction is derivation detailed in Appendix D of [1], and is used in Imagen-Video [2].\n            [1] Salimans, Tim, and Jonathan Ho. \"Progressive distillation for fast sampling of diffusion models.\"\n                arXiv preprint arXiv:2202.00512 (2022).\n            [2] Ho, Jonathan, et al. \"Imagen Video: High Definition Video Generation with Diffusion Models.\"\n                arXiv preprint arXiv:2210.02303 (2022).\n\n        4. \"score\": marginal score function. (Trained by denoising score matching).\n            Note that the score function and the noise prediction model follows a simple relationship:\n            ```\n                noise(x_t, t) = -sigma_t * score(x_t, t)\n            ```\n    We support three types of guided sampling by DPMs by setting `guidance_type`:\n        1. \"uncond\": unconditional sampling by DPMs.\n            The input `model` has the following format:\n            ``\n                model(x, t_input, **model_kwargs) -> noise | x_start | v | score\n            ``\n        2. \"classifier\": classifier guidance sampling [3] by DPMs and another classifier.\n            The input `model` has the following format:\n            ``\n                model(x, t_input, **model_kwargs) -> noise | x_start | v | score\n            ``\n            The input `classifier_fn` has the following format:\n            ``\n                classifier_fn(x, t_input, cond, **classifier_kwargs) -> logits(x, t_input, cond)\n            ``\n            [3] P. Dhariwal and A. Q. Nichol, \"Diffusion models beat GANs on image synthesis,\"\n                in Advances in Neural Information Processing Systems, vol. 34, 2021, pp. 8780-8794.\n        3. \"classifier-free\": classifier-free guidance sampling by conditional DPMs.\n            The input `model` has the following format:\n            ``\n                model(x, t_input, cond, **model_kwargs) -> noise | x_start | v | score\n            ``\n            And if cond == `unconditional_condition`, the model output is the unconditional DPM output.\n            [4] Ho, Jonathan, and Tim Salimans. \"Classifier-free diffusion guidance.\"\n                arXiv preprint arXiv:2207.12598 (2022).\n\n    The `t_input` is the time label of the model, which may be discrete-time labels (i.e. 0 to 999)\n    or continuous-time labels (i.e. epsilon to T).\n    We wrap the model function to accept only `x` and `t_continuous` as inputs, and outputs the predicted noise:\n    ``\n        def model_fn(x, t_continuous) -> noise:\n            t_input = get_model_input_time(t_continuous)\n            return noise_pred(model, x, t_input, **model_kwargs)\n    ``\n    where `t_continuous` is the continuous time labels (i.e. epsilon to T). And we use `model_fn` for DPM-Solver.\n    ===============================================================\n    Args:\n        model: A diffusion model with the corresponding format described above.\n        noise_schedule: A noise schedule object, such as NoiseScheduleVP.\n        model_type: A `str`. The parameterization type of the diffusion model.\n                    \"noise\" or \"x_start\" or \"v\" or \"score\".\n        model_kwargs: A `dict`. A dict for the other inputs of the model function.\n        guidance_type: A `str`. The type of the guidance for sampling.\n                    \"uncond\" or \"classifier\" or \"classifier-free\".\n        condition: A pytorch tensor. The condition for the guided sampling.\n                    Only used for \"classifier\" or \"classifier-free\" guidance type.\n        unconditional_condition: A pytorch tensor. The condition for the unconditional sampling.\n                    Only used for \"classifier-free\" guidance type.\n        guidance_scale: A `float`. The scale for the guided sampling.\n        classifier_fn: A classifier function. Only used for the classifier guidance.\n        classifier_kwargs: A `dict`. A dict for the other inputs of the classifier function.\n    Returns:\n        A noise prediction model that accepts the noised data and the continuous time as the inputs.\n    \"\"\"\n\n    def get_model_input_time(t_continuous):\n        \"\"\"\n        Convert the continuous-time `t_continuous` (in [epsilon, T]) to the model input time.\n        For discrete-time DPMs, we convert `t_continuous` in [1 / N, 1] to `t_input` in [0, 1000 * (N - 1) / N].\n        For continuous-time DPMs, we just use `t_continuous`.\n        \"\"\"\n        if noise_schedule.schedule == 'discrete':\n            return (t_continuous - 1. / noise_schedule.total_N) * 1000.\n        else:\n            return t_continuous\n\n    def noise_pred_fn(x, t_continuous, cond=None):\n        if t_continuous.reshape((-1,)).shape[0] == 1:\n            t_continuous = t_continuous.expand((x.shape[0]))\n        t_input = get_model_input_time(t_continuous)\n        if cond is None:\n            output = model(x, t_input, **model_kwargs)\n        else:\n            output = model(x, t_input, cond, **model_kwargs)\n        if model_type == \"noise\":\n            return output\n        elif model_type == \"x_start\":\n            alpha_t, sigma_t = noise_schedule.marginal_alpha(t_continuous), noise_schedule.marginal_std(t_continuous)\n            dims = x.dim()\n            return (x - expand_dims(alpha_t, dims) * output) / expand_dims(sigma_t, dims)\n        elif model_type == \"v\":\n            alpha_t, sigma_t = noise_schedule.marginal_alpha(t_continuous), noise_schedule.marginal_std(t_continuous)\n            dims = x.dim()\n            return expand_dims(alpha_t, dims) * output + expand_dims(sigma_t, dims) * x\n        elif model_type == \"score\":\n            sigma_t = noise_schedule.marginal_std(t_continuous)\n            dims = x.dim()\n            return -expand_dims(sigma_t, dims) * output\n\n    def cond_grad_fn(x, t_input):\n        \"\"\"\n        Compute the gradient of the classifier, i.e. nabla_{x} log p_t(cond | x_t).\n        \"\"\"\n        with torch.enable_grad():\n            x_in = x.detach().requires_grad_(True)\n            log_prob = classifier_fn(x_in, t_input, condition, **classifier_kwargs)\n            return torch.autograd.grad(log_prob.sum(), x_in)[0]\n\n    def model_fn(x, t_continuous):\n        \"\"\"\n        The noise predicition model function that is used for DPM-Solver.\n        \"\"\"\n        if t_continuous.reshape((-1,)).shape[0] == 1:\n            t_continuous = t_continuous.expand((x.shape[0]))\n        if guidance_type == \"uncond\":\n            return noise_pred_fn(x, t_continuous)\n        elif guidance_type == \"classifier\":\n            assert classifier_fn is not None\n            t_input = get_model_input_time(t_continuous)\n            cond_grad = cond_grad_fn(x, t_input)\n            sigma_t = noise_schedule.marginal_std(t_continuous)\n            noise = noise_pred_fn(x, t_continuous)\n            return noise - guidance_scale * expand_dims(sigma_t, dims=cond_grad.dim()) * cond_grad\n        elif guidance_type == \"classifier-free\":\n            if guidance_scale == 1. or unconditional_condition is None:\n                return noise_pred_fn(x, t_continuous, cond=condition)\n            else:\n                x_in = torch.cat([x] * 2)\n                t_in = torch.cat([t_continuous] * 2)\n                if isinstance(condition, dict):\n                    assert isinstance(unconditional_condition, dict)\n                    c_in = dict()\n                    for k in condition:\n                        if isinstance(condition[k], list):\n                            c_in[k] = [torch.cat([unconditional_condition[k][i], condition[k][i]]) for i in range(len(condition[k]))]\n                        else:\n                            c_in[k] = torch.cat([unconditional_condition[k], condition[k]])\n                else:\n                    c_in = torch.cat([unconditional_condition, condition])\n                noise_uncond, noise = noise_pred_fn(x_in, t_in, cond=c_in).chunk(2)\n                return noise_uncond + guidance_scale * (noise - noise_uncond)\n\n    assert model_type in [\"noise\", \"x_start\", \"v\"]\n    assert guidance_type in [\"uncond\", \"classifier\", \"classifier-free\"]\n    return model_fn\n\n\nclass DPM_Solver:\n    def __init__(self, model_fn, noise_schedule, predict_x0=False, thresholding=False, max_val=1.):\n        \"\"\"Construct a DPM-Solver.\n        We support both the noise prediction model (\"predicting epsilon\") and the data prediction model (\"predicting x0\").\n        If `predict_x0` is False, we use the solver for the noise prediction model (DPM-Solver).\n        If `predict_x0` is True, we use the solver for the data prediction model (DPM-Solver++).\n            In such case, we further support the \"dynamic thresholding\" in [1] when `thresholding` is True.\n            The \"dynamic thresholding\" can greatly improve the sample quality for pixel-space DPMs with large guidance scales.\n        Args:\n            model_fn: A noise prediction model function which accepts the continuous-time input (t in [epsilon, T]):\n                ``\n                def model_fn(x, t_continuous):\n                    return noise\n                ``\n            noise_schedule: A noise schedule object, such as NoiseScheduleVP.\n            predict_x0: A `bool`. If true, use the data prediction model; else, use the noise prediction model.\n            thresholding: A `bool`. Valid when `predict_x0` is True. Whether to use the \"dynamic thresholding\" in [1].\n            max_val: A `float`. Valid when both `predict_x0` and `thresholding` are True. The max value for thresholding.\n\n        [1] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S Sara Mahdavi, Rapha Gontijo Lopes, et al. Photorealistic text-to-image diffusion models with deep language understanding. arXiv preprint arXiv:2205.11487, 2022b.\n        \"\"\"\n        self.model = model_fn\n        self.noise_schedule = noise_schedule\n        self.predict_x0 = predict_x0\n        self.thresholding = thresholding\n        self.max_val = max_val\n\n    def noise_prediction_fn(self, x, t):\n        \"\"\"\n        Return the noise prediction model.\n        \"\"\"\n        return self.model(x, t)\n\n    def data_prediction_fn(self, x, t):\n        \"\"\"\n        Return the data prediction model (with thresholding).\n        \"\"\"\n        noise = self.noise_prediction_fn(x, t)\n        dims = x.dim()\n        alpha_t, sigma_t = self.noise_schedule.marginal_alpha(t), self.noise_schedule.marginal_std(t)\n        x0 = (x - expand_dims(sigma_t, dims) * noise) / expand_dims(alpha_t, dims)\n        if self.thresholding:\n            p = 0.995  # A hyperparameter in the paper of \"Imagen\" [1].\n            s = torch.quantile(torch.abs(x0).reshape((x0.shape[0], -1)), p, dim=1)\n            s = expand_dims(torch.maximum(s, self.max_val * torch.ones_like(s).to(s.device)), dims)\n            x0 = torch.clamp(x0, -s, s) / s\n        return x0\n\n    def model_fn(self, x, t):\n        \"\"\"\n        Convert the model to the noise prediction model or the data prediction model.\n        \"\"\"\n        if self.predict_x0:\n            return self.data_prediction_fn(x, t)\n        else:\n            return self.noise_prediction_fn(x, t)\n\n    def get_time_steps(self, skip_type, t_T, t_0, N, device):\n        \"\"\"Compute the intermediate time steps for sampling.\n        Args:\n            skip_type: A `str`. The type for the spacing of the time steps. We support three types:\n                - 'logSNR': uniform logSNR for the time steps.\n                - 'time_uniform': uniform time for the time steps. (**Recommended for high-resolutional data**.)\n                - 'time_quadratic': quadratic time for the time steps. (Used in DDIM for low-resolutional data.)\n            t_T: A `float`. The starting time of the sampling (default is T).\n            t_0: A `float`. The ending time of the sampling (default is epsilon).\n            N: A `int`. The total number of the spacing of the time steps.\n            device: A torch device.\n        Returns:\n            A pytorch tensor of the time steps, with the shape (N + 1,).\n        \"\"\"\n        if skip_type == 'logSNR':\n            lambda_T = self.noise_schedule.marginal_lambda(torch.tensor(t_T).to(device))\n            lambda_0 = self.noise_schedule.marginal_lambda(torch.tensor(t_0).to(device))\n            logSNR_steps = torch.linspace(lambda_T.cpu().item(), lambda_0.cpu().item(), N + 1).to(device)\n            return self.noise_schedule.inverse_lambda(logSNR_steps)\n        elif skip_type == 'time_uniform':\n            return torch.linspace(t_T, t_0, N + 1).to(device)\n        elif skip_type == 'time_quadratic':\n            t_order = 2\n            t = torch.linspace(t_T ** (1. / t_order), t_0 ** (1. / t_order), N + 1).pow(t_order).to(device)\n            return t\n        else:\n            raise ValueError(\n                \"Unsupported skip_type {}, need to be 'logSNR' or 'time_uniform' or 'time_quadratic'\".format(skip_type))\n\n    def get_orders_and_timesteps_for_singlestep_solver(self, steps, order, skip_type, t_T, t_0, device):\n        \"\"\"\n        Get the order of each step for sampling by the singlestep DPM-Solver.\n        We combine both DPM-Solver-1,2,3 to use all the function evaluations, which is named as \"DPM-Solver-fast\".\n        Given a fixed number of function evaluations by `steps`, the sampling procedure by DPM-Solver-fast is:\n            - If order == 1:\n                We take `steps` of DPM-Solver-1 (i.e. DDIM).\n            - If order == 2:\n                - Denote K = (steps // 2). We take K or (K + 1) intermediate time steps for sampling.\n                - If steps % 2 == 0, we use K steps of DPM-Solver-2.\n                - If steps % 2 == 1, we use K steps of DPM-Solver-2 and 1 step of DPM-Solver-1.\n            - If order == 3:\n                - Denote K = (steps // 3 + 1). We take K intermediate time steps for sampling.\n                - If steps % 3 == 0, we use (K - 2) steps of DPM-Solver-3, and 1 step of DPM-Solver-2 and 1 step of DPM-Solver-1.\n                - If steps % 3 == 1, we use (K - 1) steps of DPM-Solver-3 and 1 step of DPM-Solver-1.\n                - If steps % 3 == 2, we use (K - 1) steps of DPM-Solver-3 and 1 step of DPM-Solver-2.\n        ============================================\n        Args:\n            order: A `int`. The max order for the solver (2 or 3).\n            steps: A `int`. The total number of function evaluations (NFE).\n            skip_type: A `str`. The type for the spacing of the time steps. We support three types:\n                - 'logSNR': uniform logSNR for the time steps.\n                - 'time_uniform': uniform time for the time steps. (**Recommended for high-resolutional data**.)\n                - 'time_quadratic': quadratic time for the time steps. (Used in DDIM for low-resolutional data.)\n            t_T: A `float`. The starting time of the sampling (default is T).\n            t_0: A `float`. The ending time of the sampling (default is epsilon).\n            device: A torch device.\n        Returns:\n            orders: A list of the solver order of each step.\n        \"\"\"\n        if order == 3:\n            K = steps // 3 + 1\n            if steps % 3 == 0:\n                orders = [3, ] * (K - 2) + [2, 1]\n            elif steps % 3 == 1:\n                orders = [3, ] * (K - 1) + [1]\n            else:\n                orders = [3, ] * (K - 1) + [2]\n        elif order == 2:\n            if steps % 2 == 0:\n                K = steps // 2\n                orders = [2, ] * K\n            else:\n                K = steps // 2 + 1\n                orders = [2, ] * (K - 1) + [1]\n        elif order == 1:\n            K = 1\n            orders = [1, ] * steps\n        else:\n            raise ValueError(\"'order' must be '1' or '2' or '3'.\")\n        if skip_type == 'logSNR':\n            # To reproduce the results in DPM-Solver paper\n            timesteps_outer = self.get_time_steps(skip_type, t_T, t_0, K, device)\n        else:\n            timesteps_outer = self.get_time_steps(skip_type, t_T, t_0, steps, device)[\n                torch.cumsum(torch.tensor([0, ] + orders)).to(device)]\n        return timesteps_outer, orders\n\n    def denoise_to_zero_fn(self, x, s):\n        \"\"\"\n        Denoise at the final step, which is equivalent to solve the ODE from lambda_s to infty by first-order discretization.\n        \"\"\"\n        return self.data_prediction_fn(x, s)\n\n    def dpm_solver_first_update(self, x, s, t, model_s=None, return_intermediate=False):\n        \"\"\"\n        DPM-Solver-1 (equivalent to DDIM) from time `s` to time `t`.\n        Args:\n            x: A pytorch tensor. The initial value at time `s`.\n            s: A pytorch tensor. The starting time, with the shape (x.shape[0],).\n            t: A pytorch tensor. The ending time, with the shape (x.shape[0],).\n            model_s: A pytorch tensor. The model function evaluated at time `s`.\n                If `model_s` is None, we evaluate the model by `x` and `s`; otherwise we directly use it.\n            return_intermediate: A `bool`. If true, also return the model value at time `s`.\n        Returns:\n            x_t: A pytorch tensor. The approximated solution at time `t`.\n        \"\"\"\n        ns = self.noise_schedule\n        dims = x.dim()\n        lambda_s, lambda_t = ns.marginal_lambda(s), ns.marginal_lambda(t)\n        h = lambda_t - lambda_s\n        log_alpha_s, log_alpha_t = ns.marginal_log_mean_coeff(s), ns.marginal_log_mean_coeff(t)\n        sigma_s, sigma_t = ns.marginal_std(s), ns.marginal_std(t)\n        alpha_t = torch.exp(log_alpha_t)\n\n        if self.predict_x0:\n            phi_1 = torch.expm1(-h)\n            if model_s is None:\n                model_s = self.model_fn(x, s)\n            x_t = (\n                    expand_dims(sigma_t / sigma_s, dims) * x\n                    - expand_dims(alpha_t * phi_1, dims) * model_s\n            )\n            if return_intermediate:\n                return x_t, {'model_s': model_s}\n            else:\n                return x_t\n        else:\n            phi_1 = torch.expm1(h)\n            if model_s is None:\n                model_s = self.model_fn(x, s)\n            x_t = (\n                    expand_dims(torch.exp(log_alpha_t - log_alpha_s), dims) * x\n                    - expand_dims(sigma_t * phi_1, dims) * model_s\n            )\n            if return_intermediate:\n                return x_t, {'model_s': model_s}\n            else:\n                return x_t\n\n    def singlestep_dpm_solver_second_update(self, x, s, t, r1=0.5, model_s=None, return_intermediate=False,\n                                            solver_type='dpm_solver'):\n        \"\"\"\n        Singlestep solver DPM-Solver-2 from time `s` to time `t`.\n        Args:\n            x: A pytorch tensor. The initial value at time `s`.\n            s: A pytorch tensor. The starting time, with the shape (x.shape[0],).\n            t: A pytorch tensor. The ending time, with the shape (x.shape[0],).\n            r1: A `float`. The hyperparameter of the second-order solver.\n            model_s: A pytorch tensor. The model function evaluated at time `s`.\n                If `model_s` is None, we evaluate the model by `x` and `s`; otherwise we directly use it.\n            return_intermediate: A `bool`. If true, also return the model value at time `s` and `s1` (the intermediate time).\n            solver_type: either 'dpm_solver' or 'taylor'. The type for the high-order solvers.\n                The type slightly impacts the performance. We recommend to use 'dpm_solver' type.\n        Returns:\n            x_t: A pytorch tensor. The approximated solution at time `t`.\n        \"\"\"\n        if solver_type not in ['dpm_solver', 'taylor']:\n            raise ValueError(\"'solver_type' must be either 'dpm_solver' or 'taylor', got {}\".format(solver_type))\n        if r1 is None:\n            r1 = 0.5\n        ns = self.noise_schedule\n        dims = x.dim()\n        lambda_s, lambda_t = ns.marginal_lambda(s), ns.marginal_lambda(t)\n        h = lambda_t - lambda_s\n        lambda_s1 = lambda_s + r1 * h\n        s1 = ns.inverse_lambda(lambda_s1)\n        log_alpha_s, log_alpha_s1, log_alpha_t = ns.marginal_log_mean_coeff(s), ns.marginal_log_mean_coeff(\n            s1), ns.marginal_log_mean_coeff(t)\n        sigma_s, sigma_s1, sigma_t = ns.marginal_std(s), ns.marginal_std(s1), ns.marginal_std(t)\n        alpha_s1, alpha_t = torch.exp(log_alpha_s1), torch.exp(log_alpha_t)\n\n        if self.predict_x0:\n            phi_11 = torch.expm1(-r1 * h)\n            phi_1 = torch.expm1(-h)\n\n            if model_s is None:\n                model_s = self.model_fn(x, s)\n            x_s1 = (\n                    expand_dims(sigma_s1 / sigma_s, dims) * x\n                    - expand_dims(alpha_s1 * phi_11, dims) * model_s\n            )\n            model_s1 = self.model_fn(x_s1, s1)\n            if solver_type == 'dpm_solver':\n                x_t = (\n                        expand_dims(sigma_t / sigma_s, dims) * x\n                        - expand_dims(alpha_t * phi_1, dims) * model_s\n                        - (0.5 / r1) * expand_dims(alpha_t * phi_1, dims) * (model_s1 - model_s)\n                )\n            elif solver_type == 'taylor':\n                x_t = (\n                        expand_dims(sigma_t / sigma_s, dims) * x\n                        - expand_dims(alpha_t * phi_1, dims) * model_s\n                        + (1. / r1) * expand_dims(alpha_t * ((torch.exp(-h) - 1.) / h + 1.), dims) * (\n                                    model_s1 - model_s)\n                )\n        else:\n            phi_11 = torch.expm1(r1 * h)\n            phi_1 = torch.expm1(h)\n\n            if model_s is None:\n                model_s = self.model_fn(x, s)\n            x_s1 = (\n                    expand_dims(torch.exp(log_alpha_s1 - log_alpha_s), dims) * x\n                    - expand_dims(sigma_s1 * phi_11, dims) * model_s\n            )\n            model_s1 = self.model_fn(x_s1, s1)\n            if solver_type == 'dpm_solver':\n                x_t = (\n                        expand_dims(torch.exp(log_alpha_t - log_alpha_s), dims) * x\n                        - expand_dims(sigma_t * phi_1, dims) * model_s\n                        - (0.5 / r1) * expand_dims(sigma_t * phi_1, dims) * (model_s1 - model_s)\n                )\n            elif solver_type == 'taylor':\n                x_t = (\n                        expand_dims(torch.exp(log_alpha_t - log_alpha_s), dims) * x\n                        - expand_dims(sigma_t * phi_1, dims) * model_s\n                        - (1. / r1) * expand_dims(sigma_t * ((torch.exp(h) - 1.) / h - 1.), dims) * (model_s1 - model_s)\n                )\n        if return_intermediate:\n            return x_t, {'model_s': model_s, 'model_s1': model_s1}\n        else:\n            return x_t\n\n    def singlestep_dpm_solver_third_update(self, x, s, t, r1=1. / 3., r2=2. / 3., model_s=None, model_s1=None,\n                                           return_intermediate=False, solver_type='dpm_solver'):\n        \"\"\"\n        Singlestep solver DPM-Solver-3 from time `s` to time `t`.\n        Args:\n            x: A pytorch tensor. The initial value at time `s`.\n            s: A pytorch tensor. The starting time, with the shape (x.shape[0],).\n            t: A pytorch tensor. The ending time, with the shape (x.shape[0],).\n            r1: A `float`. The hyperparameter of the third-order solver.\n            r2: A `float`. The hyperparameter of the third-order solver.\n            model_s: A pytorch tensor. The model function evaluated at time `s`.\n                If `model_s` is None, we evaluate the model by `x` and `s`; otherwise we directly use it.\n            model_s1: A pytorch tensor. The model function evaluated at time `s1` (the intermediate time given by `r1`).\n                If `model_s1` is None, we evaluate the model at `s1`; otherwise we directly use it.\n            return_intermediate: A `bool`. If true, also return the model value at time `s`, `s1` and `s2` (the intermediate times).\n            solver_type: either 'dpm_solver' or 'taylor'. The type for the high-order solvers.\n                The type slightly impacts the performance. We recommend to use 'dpm_solver' type.\n        Returns:\n            x_t: A pytorch tensor. The approximated solution at time `t`.\n        \"\"\"\n        if solver_type not in ['dpm_solver', 'taylor']:\n            raise ValueError(\"'solver_type' must be either 'dpm_solver' or 'taylor', got {}\".format(solver_type))\n        if r1 is None:\n            r1 = 1. / 3.\n        if r2 is None:\n            r2 = 2. / 3.\n        ns = self.noise_schedule\n        dims = x.dim()\n        lambda_s, lambda_t = ns.marginal_lambda(s), ns.marginal_lambda(t)\n        h = lambda_t - lambda_s\n        lambda_s1 = lambda_s + r1 * h\n        lambda_s2 = lambda_s + r2 * h\n        s1 = ns.inverse_lambda(lambda_s1)\n        s2 = ns.inverse_lambda(lambda_s2)\n        log_alpha_s, log_alpha_s1, log_alpha_s2, log_alpha_t = ns.marginal_log_mean_coeff(\n            s), ns.marginal_log_mean_coeff(s1), ns.marginal_log_mean_coeff(s2), ns.marginal_log_mean_coeff(t)\n        sigma_s, sigma_s1, sigma_s2, sigma_t = ns.marginal_std(s), ns.marginal_std(s1), ns.marginal_std(\n            s2), ns.marginal_std(t)\n        alpha_s1, alpha_s2, alpha_t = torch.exp(log_alpha_s1), torch.exp(log_alpha_s2), torch.exp(log_alpha_t)\n\n        if self.predict_x0:\n            phi_11 = torch.expm1(-r1 * h)\n            phi_12 = torch.expm1(-r2 * h)\n            phi_1 = torch.expm1(-h)\n            phi_22 = torch.expm1(-r2 * h) / (r2 * h) + 1.\n            phi_2 = phi_1 / h + 1.\n            phi_3 = phi_2 / h - 0.5\n\n            if model_s is None:\n                model_s = self.model_fn(x, s)\n            if model_s1 is None:\n                x_s1 = (\n                        expand_dims(sigma_s1 / sigma_s, dims) * x\n                        - expand_dims(alpha_s1 * phi_11, dims) * model_s\n                )\n                model_s1 = self.model_fn(x_s1, s1)\n            x_s2 = (\n                    expand_dims(sigma_s2 / sigma_s, dims) * x\n                    - expand_dims(alpha_s2 * phi_12, dims) * model_s\n                    + r2 / r1 * expand_dims(alpha_s2 * phi_22, dims) * (model_s1 - model_s)\n            )\n            model_s2 = self.model_fn(x_s2, s2)\n            if solver_type == 'dpm_solver':\n                x_t = (\n                        expand_dims(sigma_t / sigma_s, dims) * x\n                        - expand_dims(alpha_t * phi_1, dims) * model_s\n                        + (1. / r2) * expand_dims(alpha_t * phi_2, dims) * (model_s2 - model_s)\n                )\n            elif solver_type == 'taylor':\n                D1_0 = (1. / r1) * (model_s1 - model_s)\n                D1_1 = (1. / r2) * (model_s2 - model_s)\n                D1 = (r2 * D1_0 - r1 * D1_1) / (r2 - r1)\n                D2 = 2. * (D1_1 - D1_0) / (r2 - r1)\n                x_t = (\n                        expand_dims(sigma_t / sigma_s, dims) * x\n                        - expand_dims(alpha_t * phi_1, dims) * model_s\n                        + expand_dims(alpha_t * phi_2, dims) * D1\n                        - expand_dims(alpha_t * phi_3, dims) * D2\n                )\n        else:\n            phi_11 = torch.expm1(r1 * h)\n            phi_12 = torch.expm1(r2 * h)\n            phi_1 = torch.expm1(h)\n            phi_22 = torch.expm1(r2 * h) / (r2 * h) - 1.\n            phi_2 = phi_1 / h - 1.\n            phi_3 = phi_2 / h - 0.5\n\n            if model_s is None:\n                model_s = self.model_fn(x, s)\n            if model_s1 is None:\n                x_s1 = (\n                        expand_dims(torch.exp(log_alpha_s1 - log_alpha_s), dims) * x\n                        - expand_dims(sigma_s1 * phi_11, dims) * model_s\n                )\n                model_s1 = self.model_fn(x_s1, s1)\n            x_s2 = (\n                    expand_dims(torch.exp(log_alpha_s2 - log_alpha_s), dims) * x\n                    - expand_dims(sigma_s2 * phi_12, dims) * model_s\n                    - r2 / r1 * expand_dims(sigma_s2 * phi_22, dims) * (model_s1 - model_s)\n            )\n            model_s2 = self.model_fn(x_s2, s2)\n            if solver_type == 'dpm_solver':\n                x_t = (\n                        expand_dims(torch.exp(log_alpha_t - log_alpha_s), dims) * x\n                        - expand_dims(sigma_t * phi_1, dims) * model_s\n                        - (1. / r2) * expand_dims(sigma_t * phi_2, dims) * (model_s2 - model_s)\n                )\n            elif solver_type == 'taylor':\n                D1_0 = (1. / r1) * (model_s1 - model_s)\n                D1_1 = (1. / r2) * (model_s2 - model_s)\n                D1 = (r2 * D1_0 - r1 * D1_1) / (r2 - r1)\n                D2 = 2. * (D1_1 - D1_0) / (r2 - r1)\n                x_t = (\n                        expand_dims(torch.exp(log_alpha_t - log_alpha_s), dims) * x\n                        - expand_dims(sigma_t * phi_1, dims) * model_s\n                        - expand_dims(sigma_t * phi_2, dims) * D1\n                        - expand_dims(sigma_t * phi_3, dims) * D2\n                )\n\n        if return_intermediate:\n            return x_t, {'model_s': model_s, 'model_s1': model_s1, 'model_s2': model_s2}\n        else:\n            return x_t\n\n    def multistep_dpm_solver_second_update(self, x, model_prev_list, t_prev_list, t, solver_type=\"dpm_solver\"):\n        \"\"\"\n        Multistep solver DPM-Solver-2 from time `t_prev_list[-1]` to time `t`.\n        Args:\n            x: A pytorch tensor. The initial value at time `s`.\n            model_prev_list: A list of pytorch tensor. The previous computed model values.\n            t_prev_list: A list of pytorch tensor. The previous times, each time has the shape (x.shape[0],)\n            t: A pytorch tensor. The ending time, with the shape (x.shape[0],).\n            solver_type: either 'dpm_solver' or 'taylor'. The type for the high-order solvers.\n                The type slightly impacts the performance. We recommend to use 'dpm_solver' type.\n        Returns:\n            x_t: A pytorch tensor. The approximated solution at time `t`.\n        \"\"\"\n        if solver_type not in ['dpm_solver', 'taylor']:\n            raise ValueError(\"'solver_type' must be either 'dpm_solver' or 'taylor', got {}\".format(solver_type))\n        ns = self.noise_schedule\n        dims = x.dim()\n        model_prev_1, model_prev_0 = model_prev_list\n        t_prev_1, t_prev_0 = t_prev_list\n        lambda_prev_1, lambda_prev_0, lambda_t = ns.marginal_lambda(t_prev_1), ns.marginal_lambda(\n            t_prev_0), ns.marginal_lambda(t)\n        log_alpha_prev_0, log_alpha_t = ns.marginal_log_mean_coeff(t_prev_0), ns.marginal_log_mean_coeff(t)\n        sigma_prev_0, sigma_t = ns.marginal_std(t_prev_0), ns.marginal_std(t)\n        alpha_t = torch.exp(log_alpha_t)\n\n        h_0 = lambda_prev_0 - lambda_prev_1\n        h = lambda_t - lambda_prev_0\n        r0 = h_0 / h\n        D1_0 = expand_dims(1. / r0, dims) * (model_prev_0 - model_prev_1)\n        if self.predict_x0:\n            if solver_type == 'dpm_solver':\n                x_t = (\n                        expand_dims(sigma_t / sigma_prev_0, dims) * x\n                        - expand_dims(alpha_t * (torch.exp(-h) - 1.), dims) * model_prev_0\n                        - 0.5 * expand_dims(alpha_t * (torch.exp(-h) - 1.), dims) * D1_0\n                )\n            elif solver_type == 'taylor':\n                x_t = (\n                        expand_dims(sigma_t / sigma_prev_0, dims) * x\n                        - expand_dims(alpha_t * (torch.exp(-h) - 1.), dims) * model_prev_0\n                        + expand_dims(alpha_t * ((torch.exp(-h) - 1.) / h + 1.), dims) * D1_0\n                )\n        else:\n            if solver_type == 'dpm_solver':\n                x_t = (\n                        expand_dims(torch.exp(log_alpha_t - log_alpha_prev_0), dims) * x\n                        - expand_dims(sigma_t * (torch.exp(h) - 1.), dims) * model_prev_0\n                        - 0.5 * expand_dims(sigma_t * (torch.exp(h) - 1.), dims) * D1_0\n                )\n            elif solver_type == 'taylor':\n                x_t = (\n                        expand_dims(torch.exp(log_alpha_t - log_alpha_prev_0), dims) * x\n                        - expand_dims(sigma_t * (torch.exp(h) - 1.), dims) * model_prev_0\n                        - expand_dims(sigma_t * ((torch.exp(h) - 1.) / h - 1.), dims) * D1_0\n                )\n        return x_t\n\n    def multistep_dpm_solver_third_update(self, x, model_prev_list, t_prev_list, t, solver_type='dpm_solver'):\n        \"\"\"\n        Multistep solver DPM-Solver-3 from time `t_prev_list[-1]` to time `t`.\n        Args:\n            x: A pytorch tensor. The initial value at time `s`.\n            model_prev_list: A list of pytorch tensor. The previous computed model values.\n            t_prev_list: A list of pytorch tensor. The previous times, each time has the shape (x.shape[0],)\n            t: A pytorch tensor. The ending time, with the shape (x.shape[0],).\n            solver_type: either 'dpm_solver' or 'taylor'. The type for the high-order solvers.\n                The type slightly impacts the performance. We recommend to use 'dpm_solver' type.\n        Returns:\n            x_t: A pytorch tensor. The approximated solution at time `t`.\n        \"\"\"\n        ns = self.noise_schedule\n        dims = x.dim()\n        model_prev_2, model_prev_1, model_prev_0 = model_prev_list\n        t_prev_2, t_prev_1, t_prev_0 = t_prev_list\n        lambda_prev_2, lambda_prev_1, lambda_prev_0, lambda_t = ns.marginal_lambda(t_prev_2), ns.marginal_lambda(\n            t_prev_1), ns.marginal_lambda(t_prev_0), ns.marginal_lambda(t)\n        log_alpha_prev_0, log_alpha_t = ns.marginal_log_mean_coeff(t_prev_0), ns.marginal_log_mean_coeff(t)\n        sigma_prev_0, sigma_t = ns.marginal_std(t_prev_0), ns.marginal_std(t)\n        alpha_t = torch.exp(log_alpha_t)\n\n        h_1 = lambda_prev_1 - lambda_prev_2\n        h_0 = lambda_prev_0 - lambda_prev_1\n        h = lambda_t - lambda_prev_0\n        r0, r1 = h_0 / h, h_1 / h\n        D1_0 = expand_dims(1. / r0, dims) * (model_prev_0 - model_prev_1)\n        D1_1 = expand_dims(1. / r1, dims) * (model_prev_1 - model_prev_2)\n        D1 = D1_0 + expand_dims(r0 / (r0 + r1), dims) * (D1_0 - D1_1)\n        D2 = expand_dims(1. / (r0 + r1), dims) * (D1_0 - D1_1)\n        if self.predict_x0:\n            x_t = (\n                    expand_dims(sigma_t / sigma_prev_0, dims) * x\n                    - expand_dims(alpha_t * (torch.exp(-h) - 1.), dims) * model_prev_0\n                    + expand_dims(alpha_t * ((torch.exp(-h) - 1.) / h + 1.), dims) * D1\n                    - expand_dims(alpha_t * ((torch.exp(-h) - 1. + h) / h ** 2 - 0.5), dims) * D2\n            )\n        else:\n            x_t = (\n                    expand_dims(torch.exp(log_alpha_t - log_alpha_prev_0), dims) * x\n                    - expand_dims(sigma_t * (torch.exp(h) - 1.), dims) * model_prev_0\n                    - expand_dims(sigma_t * ((torch.exp(h) - 1.) / h - 1.), dims) * D1\n                    - expand_dims(sigma_t * ((torch.exp(h) - 1. - h) / h ** 2 - 0.5), dims) * D2\n            )\n        return x_t\n\n    def singlestep_dpm_solver_update(self, x, s, t, order, return_intermediate=False, solver_type='dpm_solver', r1=None,\n                                     r2=None):\n        \"\"\"\n        Singlestep DPM-Solver with the order `order` from time `s` to time `t`.\n        Args:\n            x: A pytorch tensor. The initial value at time `s`.\n            s: A pytorch tensor. The starting time, with the shape (x.shape[0],).\n            t: A pytorch tensor. The ending time, with the shape (x.shape[0],).\n            order: A `int`. The order of DPM-Solver. We only support order == 1 or 2 or 3.\n            return_intermediate: A `bool`. If true, also return the model value at time `s`, `s1` and `s2` (the intermediate times).\n            solver_type: either 'dpm_solver' or 'taylor'. The type for the high-order solvers.\n                The type slightly impacts the performance. We recommend to use 'dpm_solver' type.\n            r1: A `float`. The hyperparameter of the second-order or third-order solver.\n            r2: A `float`. The hyperparameter of the third-order solver.\n        Returns:\n            x_t: A pytorch tensor. The approximated solution at time `t`.\n        \"\"\"\n        if order == 1:\n            return self.dpm_solver_first_update(x, s, t, return_intermediate=return_intermediate)\n        elif order == 2:\n            return self.singlestep_dpm_solver_second_update(x, s, t, return_intermediate=return_intermediate,\n                                                            solver_type=solver_type, r1=r1)\n        elif order == 3:\n            return self.singlestep_dpm_solver_third_update(x, s, t, return_intermediate=return_intermediate,\n                                                           solver_type=solver_type, r1=r1, r2=r2)\n        else:\n            raise ValueError(\"Solver order must be 1 or 2 or 3, got {}\".format(order))\n\n    def multistep_dpm_solver_update(self, x, model_prev_list, t_prev_list, t, order, solver_type='dpm_solver'):\n        \"\"\"\n        Multistep DPM-Solver with the order `order` from time `t_prev_list[-1]` to time `t`.\n        Args:\n            x: A pytorch tensor. The initial value at time `s`.\n            model_prev_list: A list of pytorch tensor. The previous computed model values.\n            t_prev_list: A list of pytorch tensor. The previous times, each time has the shape (x.shape[0],)\n            t: A pytorch tensor. The ending time, with the shape (x.shape[0],).\n            order: A `int`. The order of DPM-Solver. We only support order == 1 or 2 or 3.\n            solver_type: either 'dpm_solver' or 'taylor'. The type for the high-order solvers.\n                The type slightly impacts the performance. We recommend to use 'dpm_solver' type.\n        Returns:\n            x_t: A pytorch tensor. The approximated solution at time `t`.\n        \"\"\"\n        if order == 1:\n            return self.dpm_solver_first_update(x, t_prev_list[-1], t, model_s=model_prev_list[-1])\n        elif order == 2:\n            return self.multistep_dpm_solver_second_update(x, model_prev_list, t_prev_list, t, solver_type=solver_type)\n        elif order == 3:\n            return self.multistep_dpm_solver_third_update(x, model_prev_list, t_prev_list, t, solver_type=solver_type)\n        else:\n            raise ValueError(\"Solver order must be 1 or 2 or 3, got {}\".format(order))\n\n    def dpm_solver_adaptive(self, x, order, t_T, t_0, h_init=0.05, atol=0.0078, rtol=0.05, theta=0.9, t_err=1e-5,\n                            solver_type='dpm_solver'):\n        \"\"\"\n        The adaptive step size solver based on singlestep DPM-Solver.\n        Args:\n            x: A pytorch tensor. The initial value at time `t_T`.\n            order: A `int`. The (higher) order of the solver. We only support order == 2 or 3.\n            t_T: A `float`. The starting time of the sampling (default is T).\n            t_0: A `float`. The ending time of the sampling (default is epsilon).\n            h_init: A `float`. The initial step size (for logSNR).\n            atol: A `float`. The absolute tolerance of the solver. For image data, the default setting is 0.0078, followed [1].\n            rtol: A `float`. The relative tolerance of the solver. The default setting is 0.05.\n            theta: A `float`. The safety hyperparameter for adapting the step size. The default setting is 0.9, followed [1].\n            t_err: A `float`. The tolerance for the time. We solve the diffusion ODE until the absolute error between the\n                current time and `t_0` is less than `t_err`. The default setting is 1e-5.\n            solver_type: either 'dpm_solver' or 'taylor'. The type for the high-order solvers.\n                The type slightly impacts the performance. We recommend to use 'dpm_solver' type.\n        Returns:\n            x_0: A pytorch tensor. The approximated solution at time `t_0`.\n        [1] A. Jolicoeur-Martineau, K. Li, R. Pich\u00e9-Taillefer, T. Kachman, and I. Mitliagkas, \"Gotta go fast when generating data with score-based models,\" arXiv preprint arXiv:2105.14080, 2021.\n        \"\"\"\n        ns = self.noise_schedule\n        s = t_T * torch.ones((x.shape[0],)).to(x)\n        lambda_s = ns.marginal_lambda(s)\n        lambda_0 = ns.marginal_lambda(t_0 * torch.ones_like(s).to(x))\n        h = h_init * torch.ones_like(s).to(x)\n        x_prev = x\n        nfe = 0\n        if order == 2:\n            r1 = 0.5\n            lower_update = lambda x, s, t: self.dpm_solver_first_update(x, s, t, return_intermediate=True)\n            higher_update = lambda x, s, t, **kwargs: self.singlestep_dpm_solver_second_update(x, s, t, r1=r1,\n                                                                                               solver_type=solver_type,\n                                                                                               **kwargs)\n        elif order == 3:\n            r1, r2 = 1. / 3., 2. / 3.\n            lower_update = lambda x, s, t: self.singlestep_dpm_solver_second_update(x, s, t, r1=r1,\n                                                                                    return_intermediate=True,\n                                                                                    solver_type=solver_type)\n            higher_update = lambda x, s, t, **kwargs: self.singlestep_dpm_solver_third_update(x, s, t, r1=r1, r2=r2,\n                                                                                              solver_type=solver_type,\n                                                                                              **kwargs)\n        else:\n            raise ValueError(\"For adaptive step size solver, order must be 2 or 3, got {}\".format(order))\n        while torch.abs((s - t_0)).mean() > t_err:\n            t = ns.inverse_lambda(lambda_s + h)\n            x_lower, lower_noise_kwargs = lower_update(x, s, t)\n            x_higher = higher_update(x, s, t, **lower_noise_kwargs)\n            delta = torch.max(torch.ones_like(x).to(x) * atol, rtol * torch.max(torch.abs(x_lower), torch.abs(x_prev)))\n            norm_fn = lambda v: torch.sqrt(torch.square(v.reshape((v.shape[0], -1))).mean(dim=-1, keepdim=True))\n            E = norm_fn((x_higher - x_lower) / delta).max()\n            if torch.all(E <= 1.):\n                x = x_higher\n                s = t\n                x_prev = x_lower\n                lambda_s = ns.marginal_lambda(s)\n            h = torch.min(theta * h * torch.float_power(E, -1. / order).float(), lambda_0 - lambda_s)\n            nfe += order\n        print('adaptive solver nfe', nfe)\n        return x\n\n    def sample(self, x, steps=20, t_start=None, t_end=None, order=3, skip_type='time_uniform',\n               method='singlestep', lower_order_final=True, denoise_to_zero=False, solver_type='dpm_solver',\n               atol=0.0078, rtol=0.05,\n               ):\n        \"\"\"\n        Compute the sample at time `t_end` by DPM-Solver, given the initial `x` at time `t_start`.\n        =====================================================\n        We support the following algorithms for both noise prediction model and data prediction model:\n            - 'singlestep':\n                Singlestep DPM-Solver (i.e. \"DPM-Solver-fast\" in the paper), which combines different orders of singlestep DPM-Solver.\n                We combine all the singlestep solvers with order <= `order` to use up all the function evaluations (steps).\n                The total number of function evaluations (NFE) == `steps`.\n                Given a fixed NFE == `steps`, the sampling procedure is:\n                    - If `order` == 1:\n                        - Denote K = steps. We use K steps of DPM-Solver-1 (i.e. DDIM).\n                    - If `order` == 2:\n                        - Denote K = (steps // 2) + (steps % 2). We take K intermediate time steps for sampling.\n                        - If steps % 2 == 0, we use K steps of singlestep DPM-Solver-2.\n                        - If steps % 2 == 1, we use (K - 1) steps of singlestep DPM-Solver-2 and 1 step of DPM-Solver-1.\n                    - If `order` == 3:\n                        - Denote K = (steps // 3 + 1). We take K intermediate time steps for sampling.\n                        - If steps % 3 == 0, we use (K - 2) steps of singlestep DPM-Solver-3, and 1 step of singlestep DPM-Solver-2 and 1 step of DPM-Solver-1.\n                        - If steps % 3 == 1, we use (K - 1) steps of singlestep DPM-Solver-3 and 1 step of DPM-Solver-1.\n                        - If steps % 3 == 2, we use (K - 1) steps of singlestep DPM-Solver-3 and 1 step of singlestep DPM-Solver-2.\n            - 'multistep':\n                Multistep DPM-Solver with the order of `order`. The total number of function evaluations (NFE) == `steps`.\n                We initialize the first `order` values by lower order multistep solvers.\n                Given a fixed NFE == `steps`, the sampling procedure is:\n                    Denote K = steps.\n                    - If `order` == 1:\n                        - We use K steps of DPM-Solver-1 (i.e. DDIM).\n                    - If `order` == 2:\n                        - We firstly use 1 step of DPM-Solver-1, then use (K - 1) step of multistep DPM-Solver-2.\n                    - If `order` == 3:\n                        - We firstly use 1 step of DPM-Solver-1, then 1 step of multistep DPM-Solver-2, then (K - 2) step of multistep DPM-Solver-3.\n            - 'singlestep_fixed':\n                Fixed order singlestep DPM-Solver (i.e. DPM-Solver-1 or singlestep DPM-Solver-2 or singlestep DPM-Solver-3).\n                We use singlestep DPM-Solver-`order` for `order`=1 or 2 or 3, with total [`steps` // `order`] * `order` NFE.\n            - 'adaptive':\n                Adaptive step size DPM-Solver (i.e. \"DPM-Solver-12\" and \"DPM-Solver-23\" in the paper).\n                We ignore `steps` and use adaptive step size DPM-Solver with a higher order of `order`.\n                You can adjust the absolute tolerance `atol` and the relative tolerance `rtol` to balance the computatation costs\n                (NFE) and the sample quality.\n                    - If `order` == 2, we use DPM-Solver-12 which combines DPM-Solver-1 and singlestep DPM-Solver-2.\n                    - If `order` == 3, we use DPM-Solver-23 which combines singlestep DPM-Solver-2 and singlestep DPM-Solver-3.\n        =====================================================\n        Some advices for choosing the algorithm:\n            - For **unconditional sampling** or **guided sampling with small guidance scale** by DPMs:\n                Use singlestep DPM-Solver (\"DPM-Solver-fast\" in the paper) with `order = 3`.\n                e.g.\n                    >>> dpm_solver = DPM_Solver(model_fn, noise_schedule, predict_x0=False)\n                    >>> x_sample = dpm_solver.sample(x, steps=steps, t_start=t_start, t_end=t_end, order=3,\n                            skip_type='time_uniform', method='singlestep')\n            - For **guided sampling with large guidance scale** by DPMs:\n                Use multistep DPM-Solver with `predict_x0 = True` and `order = 2`.\n                e.g.\n                    >>> dpm_solver = DPM_Solver(model_fn, noise_schedule, predict_x0=True)\n                    >>> x_sample = dpm_solver.sample(x, steps=steps, t_start=t_start, t_end=t_end, order=2,\n                            skip_type='time_uniform', method='multistep')\n        We support three types of `skip_type`:\n            - 'logSNR': uniform logSNR for the time steps. **Recommended for low-resolutional images**\n            - 'time_uniform': uniform time for the time steps. **Recommended for high-resolutional images**.\n            - 'time_quadratic': quadratic time for the time steps.\n        =====================================================\n        Args:\n            x: A pytorch tensor. The initial value at time `t_start`\n                e.g. if `t_start` == T, then `x` is a sample from the standard normal distribution.\n            steps: A `int`. The total number of function evaluations (NFE).\n            t_start: A `float`. The starting time of the sampling.\n                If `T` is None, we use self.noise_schedule.T (default is 1.0).\n            t_end: A `float`. The ending time of the sampling.\n                If `t_end` is None, we use 1. / self.noise_schedule.total_N.\n                e.g. if total_N == 1000, we have `t_end` == 1e-3.\n                For discrete-time DPMs:\n                    - We recommend `t_end` == 1. / self.noise_schedule.total_N.\n                For continuous-time DPMs:\n                    - We recommend `t_end` == 1e-3 when `steps` <= 15; and `t_end` == 1e-4 when `steps` > 15.\n            order: A `int`. The order of DPM-Solver.\n            skip_type: A `str`. The type for the spacing of the time steps. 'time_uniform' or 'logSNR' or 'time_quadratic'.\n            method: A `str`. The method for sampling. 'singlestep' or 'multistep' or 'singlestep_fixed' or 'adaptive'.\n            denoise_to_zero: A `bool`. Whether to denoise to time 0 at the final step.\n                Default is `False`. If `denoise_to_zero` is `True`, the total NFE is (`steps` + 1).\n                This trick is firstly proposed by DDPM (https://arxiv.org/abs/2006.11239) and\n                score_sde (https://arxiv.org/abs/2011.13456). Such trick can improve the FID\n                for diffusion models sampling by diffusion SDEs for low-resolutional images\n                (such as CIFAR-10). However, we observed that such trick does not matter for\n                high-resolutional images. As it needs an additional NFE, we do not recommend\n                it for high-resolutional images.\n            lower_order_final: A `bool`. Whether to use lower order solvers at the final steps.\n                Only valid for `method=multistep` and `steps < 15`. We empirically find that\n                this trick is a key to stabilizing the sampling by DPM-Solver with very few steps\n                (especially for steps <= 10). So we recommend to set it to be `True`.\n            solver_type: A `str`. The taylor expansion type for the solver. `dpm_solver` or `taylor`. We recommend `dpm_solver`.\n            atol: A `float`. The absolute tolerance of the adaptive step size solver. Valid when `method` == 'adaptive'.\n            rtol: A `float`. The relative tolerance of the adaptive step size solver. Valid when `method` == 'adaptive'.\n        Returns:\n            x_end: A pytorch tensor. The approximated solution at time `t_end`.\n        \"\"\"\n        t_0 = 1. / self.noise_schedule.total_N if t_end is None else t_end\n        t_T = self.noise_schedule.T if t_start is None else t_start\n        device = x.device\n        if method == 'adaptive':\n            with torch.no_grad():\n                x = self.dpm_solver_adaptive(x, order=order, t_T=t_T, t_0=t_0, atol=atol, rtol=rtol,\n                                             solver_type=solver_type)\n        elif method == 'multistep':\n            assert steps >= order\n            timesteps = self.get_time_steps(skip_type=skip_type, t_T=t_T, t_0=t_0, N=steps, device=device)\n            assert timesteps.shape[0] - 1 == steps\n            with torch.no_grad():\n                vec_t = timesteps[0].expand((x.shape[0]))\n                model_prev_list = [self.model_fn(x, vec_t)]\n                t_prev_list = [vec_t]\n                # Init the first `order` values by lower order multistep DPM-Solver.\n                for init_order in tqdm(range(1, order), desc=\"DPM init order\"):\n                    vec_t = timesteps[init_order].expand(x.shape[0])\n                    x = self.multistep_dpm_solver_update(x, model_prev_list, t_prev_list, vec_t, init_order,\n                                                         solver_type=solver_type)\n                    model_prev_list.append(self.model_fn(x, vec_t))\n                    t_prev_list.append(vec_t)\n                # Compute the remaining values by `order`-th order multistep DPM-Solver.\n                for step in tqdm(range(order, steps + 1), desc=\"DPM multistep\"):\n                    vec_t = timesteps[step].expand(x.shape[0])\n                    if lower_order_final and steps < 15:\n                        step_order = min(order, steps + 1 - step)\n                    else:\n                        step_order = order\n                    x = self.multistep_dpm_solver_update(x, model_prev_list, t_prev_list, vec_t, step_order,\n                                                         solver_type=solver_type)\n                    for i in range(order - 1):\n                        t_prev_list[i] = t_prev_list[i + 1]\n                        model_prev_list[i] = model_prev_list[i + 1]\n                    t_prev_list[-1] = vec_t\n                    # We do not need to evaluate the final model value.\n                    if step < steps:\n                        model_prev_list[-1] = self.model_fn(x, vec_t)\n        elif method in ['singlestep', 'singlestep_fixed']:\n            if method == 'singlestep':\n                timesteps_outer, orders = self.get_orders_and_timesteps_for_singlestep_solver(steps=steps, order=order,\n                                                                                              skip_type=skip_type,\n                                                                                              t_T=t_T, t_0=t_0,\n                                                                                              device=device)\n            elif method == 'singlestep_fixed':\n                K = steps // order\n                orders = [order, ] * K\n                timesteps_outer = self.get_time_steps(skip_type=skip_type, t_T=t_T, t_0=t_0, N=K, device=device)\n            for i, order in enumerate(orders):\n                t_T_inner, t_0_inner = timesteps_outer[i], timesteps_outer[i + 1]\n                timesteps_inner = self.get_time_steps(skip_type=skip_type, t_T=t_T_inner.item(), t_0=t_0_inner.item(),\n                                                      N=order, device=device)\n                lambda_inner = self.noise_schedule.marginal_lambda(timesteps_inner)\n                vec_s, vec_t = t_T_inner.tile(x.shape[0]), t_0_inner.tile(x.shape[0])\n                h = lambda_inner[-1] - lambda_inner[0]\n                r1 = None if order <= 1 else (lambda_inner[1] - lambda_inner[0]) / h\n                r2 = None if order <= 2 else (lambda_inner[2] - lambda_inner[0]) / h\n                x = self.singlestep_dpm_solver_update(x, vec_s, vec_t, order, solver_type=solver_type, r1=r1, r2=r2)\n        if denoise_to_zero:\n            x = self.denoise_to_zero_fn(x, torch.ones((x.shape[0],)).to(device) * t_0)\n        return x\n\n\n#############################################################\n# other utility functions\n#############################################################\n\ndef interpolate_fn(x, xp, yp):\n    \"\"\"\n    A piecewise linear function y = f(x), using xp and yp as keypoints.\n    We implement f(x) in a differentiable way (i.e. applicable for autograd).\n    The function f(x) is well-defined for all x-axis. (For x beyond the bounds of xp, we use the outmost points of xp to define the linear function.)\n    Args:\n        x: PyTorch tensor with shape [N, C], where N is the batch size, C is the number of channels (we use C = 1 for DPM-Solver).\n        xp: PyTorch tensor with shape [C, K], where K is the number of keypoints.\n        yp: PyTorch tensor with shape [C, K].\n    Returns:\n        The function values f(x), with shape [N, C].\n    \"\"\"\n    N, K = x.shape[0], xp.shape[1]\n    all_x = torch.cat([x.unsqueeze(2), xp.unsqueeze(0).repeat((N, 1, 1))], dim=2)\n    sorted_all_x, x_indices = torch.sort(all_x, dim=2)\n    x_idx = torch.argmin(x_indices, dim=2)\n    cand_start_idx = x_idx - 1\n    start_idx = torch.where(\n        torch.eq(x_idx, 0),\n        torch.tensor(1, device=x.device),\n        torch.where(\n            torch.eq(x_idx, K), torch.tensor(K - 2, device=x.device), cand_start_idx,\n        ),\n    )\n    end_idx = torch.where(torch.eq(start_idx, cand_start_idx), start_idx + 2, start_idx + 1)\n    start_x = torch.gather(sorted_all_x, dim=2, index=start_idx.unsqueeze(2)).squeeze(2)\n    end_x = torch.gather(sorted_all_x, dim=2, index=end_idx.unsqueeze(2)).squeeze(2)\n    start_idx2 = torch.where(\n        torch.eq(x_idx, 0),\n        torch.tensor(0, device=x.device),\n        torch.where(\n            torch.eq(x_idx, K), torch.tensor(K - 2, device=x.device), cand_start_idx,\n        ),\n    )\n    y_positions_expanded = yp.unsqueeze(0).expand(N, -1, -1)\n    start_y = torch.gather(y_positions_expanded, dim=2, index=start_idx2.unsqueeze(2)).squeeze(2)\n    end_y = torch.gather(y_positions_expanded, dim=2, index=(start_idx2 + 1).unsqueeze(2)).squeeze(2)\n    cand = start_y + (x - start_x) * (end_y - start_y) / (end_x - start_x)\n    return cand\n\n\ndef expand_dims(v, dims):\n    \"\"\"\n    Expand the tensor `v` to the dim `dims`.\n    Args:\n        `v`: a PyTorch tensor with shape [N].\n        `dim`: a `int`.\n    Returns:\n        a PyTorch tensor with shape [N, 1, 1, ..., 1] and the total dimension is `dims`.\n    \"\"\"\n    return v[(...,) + (None,) * (dims - 1)]", "ldm/models/diffusion/dpm_solver/__init__.py": "from .sampler import DPMSolverSampler", "ldm/modules/attention.py": "from inspect import isfunction\nimport math\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn, einsum\nfrom einops import rearrange, repeat\nfrom typing import Optional, Any\n\nfrom ldm.modules.diffusionmodules.util import checkpoint\n\n\ntry:\n    import xformers\n    import xformers.ops\n    XFORMERS_IS_AVAILBLE = True\nexcept:\n    XFORMERS_IS_AVAILBLE = False\n\n# CrossAttn precision handling\nimport os\n_ATTN_PRECISION = os.environ.get(\"ATTN_PRECISION\", \"fp32\")\n\ndef exists(val):\n    return val is not None\n\n\ndef uniq(arr):\n    return{el: True for el in arr}.keys()\n\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if isfunction(d) else d\n\n\ndef max_neg_value(t):\n    return -torch.finfo(t.dtype).max\n\n\ndef init_(tensor):\n    dim = tensor.shape[-1]\n    std = 1 / math.sqrt(dim)\n    tensor.uniform_(-std, std)\n    return tensor\n\n\n# feedforward\nclass GEGLU(nn.Module):\n    def __init__(self, dim_in, dim_out):\n        super().__init__()\n        self.proj = nn.Linear(dim_in, dim_out * 2)\n\n    def forward(self, x):\n        x, gate = self.proj(x).chunk(2, dim=-1)\n        return x * F.gelu(gate)\n\n\nclass FeedForward(nn.Module):\n    def __init__(self, dim, dim_out=None, mult=4, glu=False, dropout=0.):\n        super().__init__()\n        inner_dim = int(dim * mult)\n        dim_out = default(dim_out, dim)\n        project_in = nn.Sequential(\n            nn.Linear(dim, inner_dim),\n            nn.GELU()\n        ) if not glu else GEGLU(dim, inner_dim)\n\n        self.net = nn.Sequential(\n            project_in,\n            nn.Dropout(dropout),\n            nn.Linear(inner_dim, dim_out)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\ndef zero_module(module):\n    \"\"\"\n    Zero out the parameters of a module and return it.\n    \"\"\"\n    for p in module.parameters():\n        p.detach().zero_()\n    return module\n\n\ndef Normalize(in_channels):\n    return torch.nn.GroupNorm(num_groups=32, num_channels=in_channels, eps=1e-6, affine=True)\n\n\nclass SpatialSelfAttention(nn.Module):\n    def __init__(self, in_channels):\n        super().__init__()\n        self.in_channels = in_channels\n\n        self.norm = Normalize(in_channels)\n        self.q = torch.nn.Conv2d(in_channels,\n                                 in_channels,\n                                 kernel_size=1,\n                                 stride=1,\n                                 padding=0)\n        self.k = torch.nn.Conv2d(in_channels,\n                                 in_channels,\n                                 kernel_size=1,\n                                 stride=1,\n                                 padding=0)\n        self.v = torch.nn.Conv2d(in_channels,\n                                 in_channels,\n                                 kernel_size=1,\n                                 stride=1,\n                                 padding=0)\n        self.proj_out = torch.nn.Conv2d(in_channels,\n                                        in_channels,\n                                        kernel_size=1,\n                                        stride=1,\n                                        padding=0)\n\n    def forward(self, x):\n        h_ = x\n        h_ = self.norm(h_)\n        q = self.q(h_)\n        k = self.k(h_)\n        v = self.v(h_)\n\n        # compute attention\n        b,c,h,w = q.shape\n        q = rearrange(q, 'b c h w -> b (h w) c')\n        k = rearrange(k, 'b c h w -> b c (h w)')\n        w_ = torch.einsum('bij,bjk->bik', q, k)\n\n        w_ = w_ * (int(c)**(-0.5))\n        w_ = torch.nn.functional.softmax(w_, dim=2)\n\n        # attend to values\n        v = rearrange(v, 'b c h w -> b c (h w)')\n        w_ = rearrange(w_, 'b i j -> b j i')\n        h_ = torch.einsum('bij,bjk->bik', v, w_)\n        h_ = rearrange(h_, 'b c (h w) -> b c h w', h=h)\n        h_ = self.proj_out(h_)\n\n        return x+h_\n\n\nclass CrossAttention(nn.Module):\n    def __init__(self, query_dim, context_dim=None, heads=8, dim_head=64, dropout=0.):\n        super().__init__()\n        inner_dim = dim_head * heads\n        context_dim = default(context_dim, query_dim)\n\n        self.scale = dim_head ** -0.5\n        self.heads = heads\n\n        self.to_q = nn.Linear(query_dim, inner_dim, bias=False)\n        self.to_k = nn.Linear(context_dim, inner_dim, bias=False)\n        self.to_v = nn.Linear(context_dim, inner_dim, bias=False)\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, query_dim),\n            nn.Dropout(dropout)\n        )\n\n    def forward(self, x, context=None, mask=None):\n        h = self.heads\n\n        q = self.to_q(x)\n        context = default(context, x)\n        k = self.to_k(context)\n        v = self.to_v(context)\n\n        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> (b h) n d', h=h), (q, k, v))\n\n        # force cast to fp32 to avoid overflowing\n        if _ATTN_PRECISION ==\"fp32\":\n            with torch.autocast(enabled=False, device_type = 'cuda'):\n                q, k = q.float(), k.float()\n                sim = einsum('b i d, b j d -> b i j', q, k) * self.scale\n        else:\n            sim = einsum('b i d, b j d -> b i j', q, k) * self.scale\n        \n        del q, k\n    \n        if exists(mask):\n            mask = rearrange(mask, 'b ... -> b (...)')\n            max_neg_value = -torch.finfo(sim.dtype).max\n            mask = repeat(mask, 'b j -> (b h) () j', h=h)\n            sim.masked_fill_(~mask, max_neg_value)\n\n        # attention, what we cannot get enough of\n        sim = sim.softmax(dim=-1)\n\n        out = einsum('b i j, b j d -> b i d', sim, v)\n        out = rearrange(out, '(b h) n d -> b n (h d)', h=h)\n        return self.to_out(out)\n\n\nclass MemoryEfficientCrossAttention(nn.Module):\n    # https://github.com/MatthieuTPHR/diffusers/blob/d80b531ff8060ec1ea982b65a1b8df70f73aa67c/src/diffusers/models/attention.py#L223\n    def __init__(self, query_dim, context_dim=None, heads=8, dim_head=64, dropout=0.0):\n        super().__init__()\n        print(f\"Setting up {self.__class__.__name__}. Query dim is {query_dim}, context_dim is {context_dim} and using \"\n              f\"{heads} heads.\")\n        inner_dim = dim_head * heads\n        context_dim = default(context_dim, query_dim)\n\n        self.heads = heads\n        self.dim_head = dim_head\n\n        self.to_q = nn.Linear(query_dim, inner_dim, bias=False)\n        self.to_k = nn.Linear(context_dim, inner_dim, bias=False)\n        self.to_v = nn.Linear(context_dim, inner_dim, bias=False)\n\n        self.to_out = nn.Sequential(nn.Linear(inner_dim, query_dim), nn.Dropout(dropout))\n        self.attention_op: Optional[Any] = None\n\n    def forward(self, x, context=None, mask=None):\n        q = self.to_q(x)\n        context = default(context, x)\n        k = self.to_k(context)\n        v = self.to_v(context)\n\n        b, _, _ = q.shape\n        q, k, v = map(\n            lambda t: t.unsqueeze(3)\n            .reshape(b, t.shape[1], self.heads, self.dim_head)\n            .permute(0, 2, 1, 3)\n            .reshape(b * self.heads, t.shape[1], self.dim_head)\n            .contiguous(),\n            (q, k, v),\n        )\n\n        # actually compute the attention, what we cannot get enough of\n        out = xformers.ops.memory_efficient_attention(q, k, v, attn_bias=None, op=self.attention_op)\n\n        if exists(mask):\n            raise NotImplementedError\n        out = (\n            out.unsqueeze(0)\n            .reshape(b, self.heads, out.shape[1], self.dim_head)\n            .permute(0, 2, 1, 3)\n            .reshape(b, out.shape[1], self.heads * self.dim_head)\n        )\n        return self.to_out(out)\n\n\nclass BasicTransformerBlock(nn.Module):\n    ATTENTION_MODES = {\n        \"softmax\": CrossAttention,  # vanilla attention\n        \"softmax-xformers\": MemoryEfficientCrossAttention\n    }\n    def __init__(self, dim, n_heads, d_head, dropout=0., context_dim=None, gated_ff=True, checkpoint=True,\n                 disable_self_attn=False):\n        super().__init__()\n        attn_mode = \"softmax-xformers\" if XFORMERS_IS_AVAILBLE else \"softmax\"\n        assert attn_mode in self.ATTENTION_MODES\n        attn_cls = self.ATTENTION_MODES[attn_mode]\n        self.disable_self_attn = disable_self_attn\n        self.attn1 = attn_cls(query_dim=dim, heads=n_heads, dim_head=d_head, dropout=dropout,\n                              context_dim=context_dim if self.disable_self_attn else None)  # is a self-attention if not self.disable_self_attn\n        self.ff = FeedForward(dim, dropout=dropout, glu=gated_ff)\n        self.attn2 = attn_cls(query_dim=dim, context_dim=context_dim,\n                              heads=n_heads, dim_head=d_head, dropout=dropout)  # is self-attn if context is none\n        self.norm1 = nn.LayerNorm(dim)\n        self.norm2 = nn.LayerNorm(dim)\n        self.norm3 = nn.LayerNorm(dim)\n        self.checkpoint = checkpoint\n\n    def forward(self, x, context=None):\n        return checkpoint(self._forward, (x, context), self.parameters(), self.checkpoint)\n\n    def _forward(self, x, context=None):\n        x = self.attn1(self.norm1(x), context=context if self.disable_self_attn else None) + x\n        x = self.attn2(self.norm2(x), context=context) + x\n        x = self.ff(self.norm3(x)) + x\n        return x\n\n\nclass SpatialTransformer(nn.Module):\n    \"\"\"\n    Transformer block for image-like data.\n    First, project the input (aka embedding)\n    and reshape to b, t, d.\n    Then apply standard transformer action.\n    Finally, reshape to image\n    NEW: use_linear for more efficiency instead of the 1x1 convs\n    \"\"\"\n    def __init__(self, in_channels, n_heads, d_head,\n                 depth=1, dropout=0., context_dim=None,\n                 disable_self_attn=False, use_linear=False,\n                 use_checkpoint=True):\n        super().__init__()\n        if exists(context_dim) and not isinstance(context_dim, list):\n            context_dim = [context_dim]\n        self.in_channels = in_channels\n        inner_dim = n_heads * d_head\n        self.norm = Normalize(in_channels)\n        if not use_linear:\n            self.proj_in = nn.Conv2d(in_channels,\n                                     inner_dim,\n                                     kernel_size=1,\n                                     stride=1,\n                                     padding=0)\n        else:\n            self.proj_in = nn.Linear(in_channels, inner_dim)\n\n        self.transformer_blocks = nn.ModuleList(\n            [BasicTransformerBlock(inner_dim, n_heads, d_head, dropout=dropout, context_dim=context_dim[d],\n                                   disable_self_attn=disable_self_attn, checkpoint=use_checkpoint)\n                for d in range(depth)]\n        )\n        if not use_linear:\n            self.proj_out = zero_module(nn.Conv2d(inner_dim,\n                                                  in_channels,\n                                                  kernel_size=1,\n                                                  stride=1,\n                                                  padding=0))\n        else:\n            self.proj_out = zero_module(nn.Linear(in_channels, inner_dim))\n        self.use_linear = use_linear\n\n    def forward(self, x, context=None):\n        # note: if no context is given, cross-attention defaults to self-attention\n        if not isinstance(context, list):\n            context = [context]\n        b, c, h, w = x.shape\n        x_in = x\n        x = self.norm(x)\n        if not self.use_linear:\n            x = self.proj_in(x)\n        x = rearrange(x, 'b c h w -> b (h w) c').contiguous()\n        if self.use_linear:\n            x = self.proj_in(x)\n        for i, block in enumerate(self.transformer_blocks):\n            x = block(x, context=context[i])\n        if self.use_linear:\n            x = self.proj_out(x)\n        x = rearrange(x, 'b (h w) c -> b c h w', h=h, w=w).contiguous()\n        if not self.use_linear:\n            x = self.proj_out(x)\n        return x + x_in\n\n", "ldm/modules/ema.py": "import torch\nfrom torch import nn\n\n\nclass LitEma(nn.Module):\n    def __init__(self, model, decay=0.9999, use_num_upates=True):\n        super().__init__()\n        if decay < 0.0 or decay > 1.0:\n            raise ValueError('Decay must be between 0 and 1')\n\n        self.m_name2s_name = {}\n        self.register_buffer('decay', torch.tensor(decay, dtype=torch.float32))\n        self.register_buffer('num_updates', torch.tensor(0, dtype=torch.int) if use_num_upates\n        else torch.tensor(-1, dtype=torch.int))\n\n        for name, p in model.named_parameters():\n            if p.requires_grad:\n                # remove as '.'-character is not allowed in buffers\n                s_name = name.replace('.', '')\n                self.m_name2s_name.update({name: s_name})\n                self.register_buffer(s_name, p.clone().detach().data)\n\n        self.collected_params = []\n\n    def reset_num_updates(self):\n        del self.num_updates\n        self.register_buffer('num_updates', torch.tensor(0, dtype=torch.int))\n\n    def forward(self, model):\n        decay = self.decay\n\n        if self.num_updates >= 0:\n            self.num_updates += 1\n            decay = min(self.decay, (1 + self.num_updates) / (10 + self.num_updates))\n\n        one_minus_decay = 1.0 - decay\n\n        with torch.no_grad():\n            m_param = dict(model.named_parameters())\n            shadow_params = dict(self.named_buffers())\n\n            for key in m_param:\n                if m_param[key].requires_grad:\n                    sname = self.m_name2s_name[key]\n                    shadow_params[sname] = shadow_params[sname].type_as(m_param[key])\n                    shadow_params[sname].sub_(one_minus_decay * (shadow_params[sname] - m_param[key]))\n                else:\n                    assert not key in self.m_name2s_name\n\n    def copy_to(self, model):\n        m_param = dict(model.named_parameters())\n        shadow_params = dict(self.named_buffers())\n        for key in m_param:\n            if m_param[key].requires_grad:\n                m_param[key].data.copy_(shadow_params[self.m_name2s_name[key]].data)\n            else:\n                assert not key in self.m_name2s_name\n\n    def store(self, parameters):\n        \"\"\"\n        Save the current parameters for restoring later.\n        Args:\n          parameters: Iterable of `torch.nn.Parameter`; the parameters to be\n            temporarily stored.\n        \"\"\"\n        self.collected_params = [param.clone() for param in parameters]\n\n    def restore(self, parameters):\n        \"\"\"\n        Restore the parameters stored with the `store` method.\n        Useful to validate the model with EMA parameters without affecting the\n        original optimization process. Store the parameters before the\n        `copy_to` method. After validation (or model saving), use this to\n        restore the former parameters.\n        Args:\n          parameters: Iterable of `torch.nn.Parameter`; the parameters to be\n            updated with the stored parameters.\n        \"\"\"\n        for c_param, param in zip(self.collected_params, parameters):\n            param.data.copy_(c_param.data)\n", "ldm/modules/encoders/modules.py": "import torch\nimport torch.nn as nn\nimport kornia\nfrom torch.utils.checkpoint import checkpoint\n\nfrom transformers import T5Tokenizer, T5EncoderModel, CLIPTokenizer, CLIPTextModel\n\nimport open_clip\nfrom ldm.util import default, count_params, autocast\n\n\nclass AbstractEncoder(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def encode(self, *args, **kwargs):\n        raise NotImplementedError\n\n\nclass IdentityEncoder(AbstractEncoder):\n\n    def encode(self, x):\n        return x\n\n\nclass ClassEmbedder(nn.Module):\n    def __init__(self, embed_dim, n_classes=1000, key='class', ucg_rate=0.1):\n        super().__init__()\n        self.key = key\n        self.embedding = nn.Embedding(n_classes, embed_dim)\n        self.n_classes = n_classes\n        self.ucg_rate = ucg_rate\n\n    def forward(self, batch, key=None, disable_dropout=False):\n        if key is None:\n            key = self.key\n        # this is for use in crossattn\n        c = batch[key][:, None]\n        if self.ucg_rate > 0. and not disable_dropout:\n            mask = 1. - torch.bernoulli(torch.ones_like(c) * self.ucg_rate)\n            c = mask * c + (1 - mask) * torch.ones_like(c) * (self.n_classes - 1)\n            c = c.long()\n        c = self.embedding(c)\n        return c\n\n    def get_unconditional_conditioning(self, bs, device=\"cuda\"):\n        uc_class = self.n_classes - 1  # 1000 classes --> 0 ... 999, one extra class for ucg (class 1000)\n        uc = torch.ones((bs,), device=device) * uc_class\n        uc = {self.key: uc}\n        return uc\n\n\ndef disabled_train(self, mode=True):\n    \"\"\"Overwrite model.train with this function to make sure train/eval mode\n    does not change anymore.\"\"\"\n    return self\n\n\nclass FrozenT5Embedder(AbstractEncoder):\n    \"\"\"Uses the T5 transformer encoder for text\"\"\"\n\n    def __init__(self, version=\"google/t5-v1_1-large\", device=\"cuda\", max_length=77,\n                 freeze=True):  # others are google/t5-v1_1-xl and google/t5-v1_1-xxl\n        super().__init__()\n        self.tokenizer = T5Tokenizer.from_pretrained(version)\n        self.transformer = T5EncoderModel.from_pretrained(version)\n        self.device = device\n        self.max_length = max_length  # TODO: typical value?\n        if freeze:\n            self.freeze()\n\n    def freeze(self):\n        self.transformer = self.transformer.eval()\n        # self.train = disabled_train\n        for param in self.parameters():\n            param.requires_grad = False\n\n    def forward(self, text):\n        batch_encoding = self.tokenizer(text, truncation=True, max_length=self.max_length, return_length=True,\n                                        return_overflowing_tokens=False, padding=\"max_length\", return_tensors=\"pt\")\n        tokens = batch_encoding[\"input_ids\"].to(self.device)\n        outputs = self.transformer(input_ids=tokens)\n\n        z = outputs.last_hidden_state\n        return z\n\n    def encode(self, text):\n        return self(text)\n\n\nclass FrozenCLIPEmbedder(AbstractEncoder):\n    \"\"\"Uses the CLIP transformer encoder for text (from huggingface)\"\"\"\n    LAYERS = [\n        \"last\",\n        \"pooled\",\n        \"hidden\"\n    ]\n\n    def __init__(self, version=\"openai/clip-vit-large-patch14\", device=\"cuda\", max_length=77,\n                 freeze=True, layer=\"last\", layer_idx=None):  # clip-vit-base-patch32\n        super().__init__()\n        assert layer in self.LAYERS\n        self.tokenizer = CLIPTokenizer.from_pretrained(version)\n        self.transformer = CLIPTextModel.from_pretrained(version)\n        self.device = device\n        self.max_length = max_length\n        if freeze:\n            self.freeze()\n        self.layer = layer\n        self.layer_idx = layer_idx\n        if layer == \"hidden\":\n            assert layer_idx is not None\n            assert 0 <= abs(layer_idx) <= 12\n\n    def freeze(self):\n        self.transformer = self.transformer.eval()\n        # self.train = disabled_train\n        for param in self.parameters():\n            param.requires_grad = False\n\n    def forward(self, text):\n        batch_encoding = self.tokenizer(text, truncation=True, max_length=self.max_length, return_length=True,\n                                        return_overflowing_tokens=False, padding=\"max_length\", return_tensors=\"pt\")\n        tokens = batch_encoding[\"input_ids\"].to(self.device)\n        outputs = self.transformer(input_ids=tokens, output_hidden_states=self.layer == \"hidden\")\n        if self.layer == \"last\":\n            z = outputs.last_hidden_state\n        elif self.layer == \"pooled\":\n            z = outputs.pooler_output[:, None, :]\n        else:\n            z = outputs.hidden_states[self.layer_idx]\n        return z\n\n    def encode(self, text):\n        return self(text)\n\n\nclass ClipImageEmbedder(nn.Module):\n    def __init__(\n            self,\n            model,\n            jit=False,\n            device='cuda' if torch.cuda.is_available() else 'cpu',\n            antialias=True,\n            ucg_rate=0.\n    ):\n        super().__init__()\n        from clip import load as load_clip\n        self.model, _ = load_clip(name=model, device=device, jit=jit)\n\n        self.antialias = antialias\n\n        self.register_buffer('mean', torch.Tensor([0.48145466, 0.4578275, 0.40821073]), persistent=False)\n        self.register_buffer('std', torch.Tensor([0.26862954, 0.26130258, 0.27577711]), persistent=False)\n        self.ucg_rate = ucg_rate\n\n    def preprocess(self, x):\n        # normalize to [0,1]\n        x = kornia.geometry.resize(x, (224, 224),\n                                   interpolation='bicubic', align_corners=True,\n                                   antialias=self.antialias)\n        x = (x + 1.) / 2.\n        # re-normalize according to clip\n        x = kornia.enhance.normalize(x, self.mean, self.std)\n        return x\n\n    def forward(self, x, no_dropout=False):\n        # x is assumed to be in range [-1,1]\n        out = self.model.encode_image(self.preprocess(x))\n        out = out.to(x.dtype)\n        if self.ucg_rate > 0. and not no_dropout:\n            out = torch.bernoulli((1. - self.ucg_rate) * torch.ones(out.shape[0], device=out.device))[:, None] * out\n        return out\n\n\nclass FrozenOpenCLIPEmbedder(AbstractEncoder):\n    \"\"\"\n    Uses the OpenCLIP transformer encoder for text\n    \"\"\"\n    LAYERS = [\n        # \"pooled\",\n        \"last\",\n        \"penultimate\"\n    ]\n\n    def __init__(self, arch=\"ViT-H-14\", version=\"laion2b_s32b_b79k\", device=\"cuda\", max_length=77,\n                 freeze=True, layer=\"last\"):\n        super().__init__()\n        assert layer in self.LAYERS\n        model, _, _ = open_clip.create_model_and_transforms(arch, device=torch.device('cpu'), pretrained=version)\n        del model.visual\n        self.model = model\n\n        self.device = device\n        self.max_length = max_length\n        if freeze:\n            self.freeze()\n        self.layer = layer\n        if self.layer == \"last\":\n            self.layer_idx = 0\n        elif self.layer == \"penultimate\":\n            self.layer_idx = 1\n        else:\n            raise NotImplementedError()\n\n    def freeze(self):\n        self.model = self.model.eval()\n        for param in self.parameters():\n            param.requires_grad = False\n\n    def forward(self, text):\n        tokens = open_clip.tokenize(text)\n        z = self.encode_with_transformer(tokens.to(self.device))\n        return z\n\n    def encode_with_transformer(self, text):\n        x = self.model.token_embedding(text)  # [batch_size, n_ctx, d_model]\n        x = x + self.model.positional_embedding\n        x = x.permute(1, 0, 2)  # NLD -> LND\n        x = self.text_transformer_forward(x, attn_mask=self.model.attn_mask)\n        x = x.permute(1, 0, 2)  # LND -> NLD\n        x = self.model.ln_final(x)\n        return x\n\n    def text_transformer_forward(self, x: torch.Tensor, attn_mask=None):\n        for i, r in enumerate(self.model.transformer.resblocks):\n            if i == len(self.model.transformer.resblocks) - self.layer_idx:\n                break\n            if self.model.transformer.grad_checkpointing and not torch.jit.is_scripting():\n                x = checkpoint(r, x, attn_mask)\n            else:\n                x = r(x, attn_mask=attn_mask)\n        return x\n\n    def encode(self, text):\n        return self(text)\n\n\nclass FrozenOpenCLIPImageEmbedder(AbstractEncoder):\n    \"\"\"\n    Uses the OpenCLIP vision transformer encoder for images\n    \"\"\"\n\n    def __init__(self, arch=\"ViT-H-14\", version=\"laion2b_s32b_b79k\", device=\"cuda\", max_length=77,\n                 freeze=True, layer=\"pooled\", antialias=True, ucg_rate=0.):\n        super().__init__()\n        model, _, _ = open_clip.create_model_and_transforms(arch, device=torch.device('cpu'),\n                                                            pretrained=version, )\n        del model.transformer\n        self.model = model\n\n        self.device = device\n        self.max_length = max_length\n        if freeze:\n            self.freeze()\n        self.layer = layer\n        if self.layer == \"penultimate\":\n            raise NotImplementedError()\n            self.layer_idx = 1\n\n        self.antialias = antialias\n\n        self.register_buffer('mean', torch.Tensor([0.48145466, 0.4578275, 0.40821073]), persistent=False)\n        self.register_buffer('std', torch.Tensor([0.26862954, 0.26130258, 0.27577711]), persistent=False)\n        self.ucg_rate = ucg_rate\n\n    def preprocess(self, x):\n        # normalize to [0,1]\n        x = kornia.geometry.resize(x, (224, 224),\n                                   interpolation='bicubic', align_corners=True,\n                                   antialias=self.antialias)\n        x = (x + 1.) / 2.\n        # renormalize according to clip\n        x = kornia.enhance.normalize(x, self.mean, self.std)\n        return x\n\n    def freeze(self):\n        self.model = self.model.eval()\n        for param in self.parameters():\n            param.requires_grad = False\n\n    @autocast\n    def forward(self, image, no_dropout=False):\n        z = self.encode_with_vision_transformer(image)\n        if self.ucg_rate > 0. and not no_dropout:\n            z = torch.bernoulli((1. - self.ucg_rate) * torch.ones(z.shape[0], device=z.device))[:, None] * z\n        return z\n\n    def encode_with_vision_transformer(self, img):\n        img = self.preprocess(img)\n        x = self.model.visual(img)\n        return x\n\n    def encode(self, text):\n        return self(text)\n\n\nclass FrozenCLIPT5Encoder(AbstractEncoder):\n    def __init__(self, clip_version=\"openai/clip-vit-large-patch14\", t5_version=\"google/t5-v1_1-xl\", device=\"cuda\",\n                 clip_max_length=77, t5_max_length=77):\n        super().__init__()\n        self.clip_encoder = FrozenCLIPEmbedder(clip_version, device, max_length=clip_max_length)\n        self.t5_encoder = FrozenT5Embedder(t5_version, device, max_length=t5_max_length)\n        print(f\"{self.clip_encoder.__class__.__name__} has {count_params(self.clip_encoder) * 1.e-6:.2f} M parameters, \"\n              f\"{self.t5_encoder.__class__.__name__} comes with {count_params(self.t5_encoder) * 1.e-6:.2f} M params.\")\n\n    def encode(self, text):\n        return self(text)\n\n    def forward(self, text):\n        clip_z = self.clip_encoder.encode(text)\n        t5_z = self.t5_encoder.encode(text)\n        return [clip_z, t5_z]\n\n\nfrom ldm.modules.diffusionmodules.upscaling import ImageConcatWithNoiseAugmentation\nfrom ldm.modules.diffusionmodules.openaimodel import Timestep\n\n\nclass CLIPEmbeddingNoiseAugmentation(ImageConcatWithNoiseAugmentation):\n    def __init__(self, *args, clip_stats_path=None, timestep_dim=256, **kwargs):\n        super().__init__(*args, **kwargs)\n        if clip_stats_path is None:\n            clip_mean, clip_std = torch.zeros(timestep_dim), torch.ones(timestep_dim)\n        else:\n            clip_mean, clip_std = torch.load(clip_stats_path, map_location=\"cpu\")\n        self.register_buffer(\"data_mean\", clip_mean[None, :], persistent=False)\n        self.register_buffer(\"data_std\", clip_std[None, :], persistent=False)\n        self.time_embed = Timestep(timestep_dim)\n\n    def scale(self, x):\n        # re-normalize to centered mean and unit variance\n        x = (x - self.data_mean) * 1. / self.data_std\n        return x\n\n    def unscale(self, x):\n        # back to original data stats\n        x = (x * self.data_std) + self.data_mean\n        return x\n\n    def forward(self, x, noise_level=None):\n        if noise_level is None:\n            noise_level = torch.randint(0, self.max_noise_level, (x.shape[0],), device=x.device).long()\n        else:\n            assert isinstance(noise_level, torch.Tensor)\n        x = self.scale(x)\n        z = self.q_sample(x, noise_level)\n        z = self.unscale(z)\n        noise_level = self.time_embed(noise_level)\n        return z, noise_level\n", "ldm/modules/encoders/__init__.py": "", "ldm/modules/diffusionmodules/model.py": "# pytorch_diffusion + derived encoder decoder\nimport math\nimport torch\nimport torch.nn as nn\nimport numpy as np\nfrom einops import rearrange\nfrom typing import Optional, Any\n\nfrom ldm.modules.attention import MemoryEfficientCrossAttention\n\ntry:\n    import xformers\n    import xformers.ops\n    XFORMERS_IS_AVAILBLE = True\nexcept:\n    XFORMERS_IS_AVAILBLE = False\n    print(\"No module 'xformers'. Proceeding without it.\")\n\n\ndef get_timestep_embedding(timesteps, embedding_dim):\n    \"\"\"\n    This matches the implementation in Denoising Diffusion Probabilistic Models:\n    From Fairseq.\n    Build sinusoidal embeddings.\n    This matches the implementation in tensor2tensor, but differs slightly\n    from the description in Section 3.5 of \"Attention Is All You Need\".\n    \"\"\"\n    assert len(timesteps.shape) == 1\n\n    half_dim = embedding_dim // 2\n    emb = math.log(10000) / (half_dim - 1)\n    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)\n    emb = emb.to(device=timesteps.device)\n    emb = timesteps.float()[:, None] * emb[None, :]\n    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n    if embedding_dim % 2 == 1:  # zero pad\n        emb = torch.nn.functional.pad(emb, (0,1,0,0))\n    return emb\n\n\ndef nonlinearity(x):\n    # swish\n    return x*torch.sigmoid(x)\n\n\ndef Normalize(in_channels, num_groups=32):\n    return torch.nn.GroupNorm(num_groups=num_groups, num_channels=in_channels, eps=1e-6, affine=True)\n\n\nclass Upsample(nn.Module):\n    def __init__(self, in_channels, with_conv):\n        super().__init__()\n        self.with_conv = with_conv\n        if self.with_conv:\n            self.conv = torch.nn.Conv2d(in_channels,\n                                        in_channels,\n                                        kernel_size=3,\n                                        stride=1,\n                                        padding=1)\n\n    def forward(self, x):\n        x = torch.nn.functional.interpolate(x, scale_factor=2.0, mode=\"nearest\")\n        if self.with_conv:\n            x = self.conv(x)\n        return x\n\n\nclass Downsample(nn.Module):\n    def __init__(self, in_channels, with_conv):\n        super().__init__()\n        self.with_conv = with_conv\n        if self.with_conv:\n            # no asymmetric padding in torch conv, must do it ourselves\n            self.conv = torch.nn.Conv2d(in_channels,\n                                        in_channels,\n                                        kernel_size=3,\n                                        stride=2,\n                                        padding=0)\n\n    def forward(self, x):\n        if self.with_conv:\n            pad = (0,1,0,1)\n            x = torch.nn.functional.pad(x, pad, mode=\"constant\", value=0)\n            x = self.conv(x)\n        else:\n            x = torch.nn.functional.avg_pool2d(x, kernel_size=2, stride=2)\n        return x\n\n\nclass ResnetBlock(nn.Module):\n    def __init__(self, *, in_channels, out_channels=None, conv_shortcut=False,\n                 dropout, temb_channels=512):\n        super().__init__()\n        self.in_channels = in_channels\n        out_channels = in_channels if out_channels is None else out_channels\n        self.out_channels = out_channels\n        self.use_conv_shortcut = conv_shortcut\n\n        self.norm1 = Normalize(in_channels)\n        self.conv1 = torch.nn.Conv2d(in_channels,\n                                     out_channels,\n                                     kernel_size=3,\n                                     stride=1,\n                                     padding=1)\n        if temb_channels > 0:\n            self.temb_proj = torch.nn.Linear(temb_channels,\n                                             out_channels)\n        self.norm2 = Normalize(out_channels)\n        self.dropout = torch.nn.Dropout(dropout)\n        self.conv2 = torch.nn.Conv2d(out_channels,\n                                     out_channels,\n                                     kernel_size=3,\n                                     stride=1,\n                                     padding=1)\n        if self.in_channels != self.out_channels:\n            if self.use_conv_shortcut:\n                self.conv_shortcut = torch.nn.Conv2d(in_channels,\n                                                     out_channels,\n                                                     kernel_size=3,\n                                                     stride=1,\n                                                     padding=1)\n            else:\n                self.nin_shortcut = torch.nn.Conv2d(in_channels,\n                                                    out_channels,\n                                                    kernel_size=1,\n                                                    stride=1,\n                                                    padding=0)\n\n    def forward(self, x, temb):\n        h = x\n        h = self.norm1(h)\n        h = nonlinearity(h)\n        h = self.conv1(h)\n\n        if temb is not None:\n            h = h + self.temb_proj(nonlinearity(temb))[:,:,None,None]\n\n        h = self.norm2(h)\n        h = nonlinearity(h)\n        h = self.dropout(h)\n        h = self.conv2(h)\n\n        if self.in_channels != self.out_channels:\n            if self.use_conv_shortcut:\n                x = self.conv_shortcut(x)\n            else:\n                x = self.nin_shortcut(x)\n\n        return x+h\n\n\nclass AttnBlock(nn.Module):\n    def __init__(self, in_channels):\n        super().__init__()\n        self.in_channels = in_channels\n\n        self.norm = Normalize(in_channels)\n        self.q = torch.nn.Conv2d(in_channels,\n                                 in_channels,\n                                 kernel_size=1,\n                                 stride=1,\n                                 padding=0)\n        self.k = torch.nn.Conv2d(in_channels,\n                                 in_channels,\n                                 kernel_size=1,\n                                 stride=1,\n                                 padding=0)\n        self.v = torch.nn.Conv2d(in_channels,\n                                 in_channels,\n                                 kernel_size=1,\n                                 stride=1,\n                                 padding=0)\n        self.proj_out = torch.nn.Conv2d(in_channels,\n                                        in_channels,\n                                        kernel_size=1,\n                                        stride=1,\n                                        padding=0)\n\n    def forward(self, x):\n        h_ = x\n        h_ = self.norm(h_)\n        q = self.q(h_)\n        k = self.k(h_)\n        v = self.v(h_)\n\n        # compute attention\n        b,c,h,w = q.shape\n        q = q.reshape(b,c,h*w)\n        q = q.permute(0,2,1)   # b,hw,c\n        k = k.reshape(b,c,h*w) # b,c,hw\n        w_ = torch.bmm(q,k)     # b,hw,hw    w[b,i,j]=sum_c q[b,i,c]k[b,c,j]\n        w_ = w_ * (int(c)**(-0.5))\n        w_ = torch.nn.functional.softmax(w_, dim=2)\n\n        # attend to values\n        v = v.reshape(b,c,h*w)\n        w_ = w_.permute(0,2,1)   # b,hw,hw (first hw of k, second of q)\n        h_ = torch.bmm(v,w_)     # b, c,hw (hw of q) h_[b,c,j] = sum_i v[b,c,i] w_[b,i,j]\n        h_ = h_.reshape(b,c,h,w)\n\n        h_ = self.proj_out(h_)\n\n        return x+h_\n\nclass MemoryEfficientAttnBlock(nn.Module):\n    \"\"\"\n        Uses xformers efficient implementation,\n        see https://github.com/MatthieuTPHR/diffusers/blob/d80b531ff8060ec1ea982b65a1b8df70f73aa67c/src/diffusers/models/attention.py#L223\n        Note: this is a single-head self-attention operation\n    \"\"\"\n    #\n    def __init__(self, in_channels):\n        super().__init__()\n        self.in_channels = in_channels\n\n        self.norm = Normalize(in_channels)\n        self.q = torch.nn.Conv2d(in_channels,\n                                 in_channels,\n                                 kernel_size=1,\n                                 stride=1,\n                                 padding=0)\n        self.k = torch.nn.Conv2d(in_channels,\n                                 in_channels,\n                                 kernel_size=1,\n                                 stride=1,\n                                 padding=0)\n        self.v = torch.nn.Conv2d(in_channels,\n                                 in_channels,\n                                 kernel_size=1,\n                                 stride=1,\n                                 padding=0)\n        self.proj_out = torch.nn.Conv2d(in_channels,\n                                        in_channels,\n                                        kernel_size=1,\n                                        stride=1,\n                                        padding=0)\n        self.attention_op: Optional[Any] = None\n\n    def forward(self, x):\n        h_ = x\n        h_ = self.norm(h_)\n        q = self.q(h_)\n        k = self.k(h_)\n        v = self.v(h_)\n\n        # compute attention\n        B, C, H, W = q.shape\n        q, k, v = map(lambda x: rearrange(x, 'b c h w -> b (h w) c'), (q, k, v))\n\n        q, k, v = map(\n            lambda t: t.unsqueeze(3)\n            .reshape(B, t.shape[1], 1, C)\n            .permute(0, 2, 1, 3)\n            .reshape(B * 1, t.shape[1], C)\n            .contiguous(),\n            (q, k, v),\n        )\n        out = xformers.ops.memory_efficient_attention(q, k, v, attn_bias=None, op=self.attention_op)\n\n        out = (\n            out.unsqueeze(0)\n            .reshape(B, 1, out.shape[1], C)\n            .permute(0, 2, 1, 3)\n            .reshape(B, out.shape[1], C)\n        )\n        out = rearrange(out, 'b (h w) c -> b c h w', b=B, h=H, w=W, c=C)\n        out = self.proj_out(out)\n        return x+out\n\n\nclass MemoryEfficientCrossAttentionWrapper(MemoryEfficientCrossAttention):\n    def forward(self, x, context=None, mask=None):\n        b, c, h, w = x.shape\n        x = rearrange(x, 'b c h w -> b (h w) c')\n        out = super().forward(x, context=context, mask=mask)\n        out = rearrange(out, 'b (h w) c -> b c h w', h=h, w=w, c=c)\n        return x + out\n\n\ndef make_attn(in_channels, attn_type=\"vanilla\", attn_kwargs=None):\n    assert attn_type in [\"vanilla\", \"vanilla-xformers\", \"memory-efficient-cross-attn\", \"linear\", \"none\"], f'attn_type {attn_type} unknown'\n    if XFORMERS_IS_AVAILBLE and attn_type == \"vanilla\":\n        attn_type = \"vanilla-xformers\"\n    print(f\"making attention of type '{attn_type}' with {in_channels} in_channels\")\n    if attn_type == \"vanilla\":\n        assert attn_kwargs is None\n        return AttnBlock(in_channels)\n    elif attn_type == \"vanilla-xformers\":\n        print(f\"building MemoryEfficientAttnBlock with {in_channels} in_channels...\")\n        return MemoryEfficientAttnBlock(in_channels)\n    elif type == \"memory-efficient-cross-attn\":\n        attn_kwargs[\"query_dim\"] = in_channels\n        return MemoryEfficientCrossAttentionWrapper(**attn_kwargs)\n    elif attn_type == \"none\":\n        return nn.Identity(in_channels)\n    else:\n        raise NotImplementedError()\n\n\nclass Model(nn.Module):\n    def __init__(self, *, ch, out_ch, ch_mult=(1,2,4,8), num_res_blocks,\n                 attn_resolutions, dropout=0.0, resamp_with_conv=True, in_channels,\n                 resolution, use_timestep=True, use_linear_attn=False, attn_type=\"vanilla\"):\n        super().__init__()\n        if use_linear_attn: attn_type = \"linear\"\n        self.ch = ch\n        self.temb_ch = self.ch*4\n        self.num_resolutions = len(ch_mult)\n        self.num_res_blocks = num_res_blocks\n        self.resolution = resolution\n        self.in_channels = in_channels\n\n        self.use_timestep = use_timestep\n        if self.use_timestep:\n            # timestep embedding\n            self.temb = nn.Module()\n            self.temb.dense = nn.ModuleList([\n                torch.nn.Linear(self.ch,\n                                self.temb_ch),\n                torch.nn.Linear(self.temb_ch,\n                                self.temb_ch),\n            ])\n\n        # downsampling\n        self.conv_in = torch.nn.Conv2d(in_channels,\n                                       self.ch,\n                                       kernel_size=3,\n                                       stride=1,\n                                       padding=1)\n\n        curr_res = resolution\n        in_ch_mult = (1,)+tuple(ch_mult)\n        self.down = nn.ModuleList()\n        for i_level in range(self.num_resolutions):\n            block = nn.ModuleList()\n            attn = nn.ModuleList()\n            block_in = ch*in_ch_mult[i_level]\n            block_out = ch*ch_mult[i_level]\n            for i_block in range(self.num_res_blocks):\n                block.append(ResnetBlock(in_channels=block_in,\n                                         out_channels=block_out,\n                                         temb_channels=self.temb_ch,\n                                         dropout=dropout))\n                block_in = block_out\n                if curr_res in attn_resolutions:\n                    attn.append(make_attn(block_in, attn_type=attn_type))\n            down = nn.Module()\n            down.block = block\n            down.attn = attn\n            if i_level != self.num_resolutions-1:\n                down.downsample = Downsample(block_in, resamp_with_conv)\n                curr_res = curr_res // 2\n            self.down.append(down)\n\n        # middle\n        self.mid = nn.Module()\n        self.mid.block_1 = ResnetBlock(in_channels=block_in,\n                                       out_channels=block_in,\n                                       temb_channels=self.temb_ch,\n                                       dropout=dropout)\n        self.mid.attn_1 = make_attn(block_in, attn_type=attn_type)\n        self.mid.block_2 = ResnetBlock(in_channels=block_in,\n                                       out_channels=block_in,\n                                       temb_channels=self.temb_ch,\n                                       dropout=dropout)\n\n        # upsampling\n        self.up = nn.ModuleList()\n        for i_level in reversed(range(self.num_resolutions)):\n            block = nn.ModuleList()\n            attn = nn.ModuleList()\n            block_out = ch*ch_mult[i_level]\n            skip_in = ch*ch_mult[i_level]\n            for i_block in range(self.num_res_blocks+1):\n                if i_block == self.num_res_blocks:\n                    skip_in = ch*in_ch_mult[i_level]\n                block.append(ResnetBlock(in_channels=block_in+skip_in,\n                                         out_channels=block_out,\n                                         temb_channels=self.temb_ch,\n                                         dropout=dropout))\n                block_in = block_out\n                if curr_res in attn_resolutions:\n                    attn.append(make_attn(block_in, attn_type=attn_type))\n            up = nn.Module()\n            up.block = block\n            up.attn = attn\n            if i_level != 0:\n                up.upsample = Upsample(block_in, resamp_with_conv)\n                curr_res = curr_res * 2\n            self.up.insert(0, up) # prepend to get consistent order\n\n        # end\n        self.norm_out = Normalize(block_in)\n        self.conv_out = torch.nn.Conv2d(block_in,\n                                        out_ch,\n                                        kernel_size=3,\n                                        stride=1,\n                                        padding=1)\n\n    def forward(self, x, t=None, context=None):\n        #assert x.shape[2] == x.shape[3] == self.resolution\n        if context is not None:\n            # assume aligned context, cat along channel axis\n            x = torch.cat((x, context), dim=1)\n        if self.use_timestep:\n            # timestep embedding\n            assert t is not None\n            temb = get_timestep_embedding(t, self.ch)\n            temb = self.temb.dense[0](temb)\n            temb = nonlinearity(temb)\n            temb = self.temb.dense[1](temb)\n        else:\n            temb = None\n\n        # downsampling\n        hs = [self.conv_in(x)]\n        for i_level in range(self.num_resolutions):\n            for i_block in range(self.num_res_blocks):\n                h = self.down[i_level].block[i_block](hs[-1], temb)\n                if len(self.down[i_level].attn) > 0:\n                    h = self.down[i_level].attn[i_block](h)\n                hs.append(h)\n            if i_level != self.num_resolutions-1:\n                hs.append(self.down[i_level].downsample(hs[-1]))\n\n        # middle\n        h = hs[-1]\n        h = self.mid.block_1(h, temb)\n        h = self.mid.attn_1(h)\n        h = self.mid.block_2(h, temb)\n\n        # upsampling\n        for i_level in reversed(range(self.num_resolutions)):\n            for i_block in range(self.num_res_blocks+1):\n                h = self.up[i_level].block[i_block](\n                    torch.cat([h, hs.pop()], dim=1), temb)\n                if len(self.up[i_level].attn) > 0:\n                    h = self.up[i_level].attn[i_block](h)\n            if i_level != 0:\n                h = self.up[i_level].upsample(h)\n\n        # end\n        h = self.norm_out(h)\n        h = nonlinearity(h)\n        h = self.conv_out(h)\n        return h\n\n    def get_last_layer(self):\n        return self.conv_out.weight\n\n\nclass Encoder(nn.Module):\n    def __init__(self, *, ch, out_ch, ch_mult=(1,2,4,8), num_res_blocks,\n                 attn_resolutions, dropout=0.0, resamp_with_conv=True, in_channels,\n                 resolution, z_channels, double_z=True, use_linear_attn=False, attn_type=\"vanilla\",\n                 **ignore_kwargs):\n        super().__init__()\n        if use_linear_attn: attn_type = \"linear\"\n        self.ch = ch\n        self.temb_ch = 0\n        self.num_resolutions = len(ch_mult)\n        self.num_res_blocks = num_res_blocks\n        self.resolution = resolution\n        self.in_channels = in_channels\n\n        # downsampling\n        self.conv_in = torch.nn.Conv2d(in_channels,\n                                       self.ch,\n                                       kernel_size=3,\n                                       stride=1,\n                                       padding=1)\n\n        curr_res = resolution\n        in_ch_mult = (1,)+tuple(ch_mult)\n        self.in_ch_mult = in_ch_mult\n        self.down = nn.ModuleList()\n        for i_level in range(self.num_resolutions):\n            block = nn.ModuleList()\n            attn = nn.ModuleList()\n            block_in = ch*in_ch_mult[i_level]\n            block_out = ch*ch_mult[i_level]\n            for i_block in range(self.num_res_blocks):\n                block.append(ResnetBlock(in_channels=block_in,\n                                         out_channels=block_out,\n                                         temb_channels=self.temb_ch,\n                                         dropout=dropout))\n                block_in = block_out\n                if curr_res in attn_resolutions:\n                    attn.append(make_attn(block_in, attn_type=attn_type))\n            down = nn.Module()\n            down.block = block\n            down.attn = attn\n            if i_level != self.num_resolutions-1:\n                down.downsample = Downsample(block_in, resamp_with_conv)\n                curr_res = curr_res // 2\n            self.down.append(down)\n\n        # middle\n        self.mid = nn.Module()\n        self.mid.block_1 = ResnetBlock(in_channels=block_in,\n                                       out_channels=block_in,\n                                       temb_channels=self.temb_ch,\n                                       dropout=dropout)\n        self.mid.attn_1 = make_attn(block_in, attn_type=attn_type)\n        self.mid.block_2 = ResnetBlock(in_channels=block_in,\n                                       out_channels=block_in,\n                                       temb_channels=self.temb_ch,\n                                       dropout=dropout)\n\n        # end\n        self.norm_out = Normalize(block_in)\n        self.conv_out = torch.nn.Conv2d(block_in,\n                                        2*z_channels if double_z else z_channels,\n                                        kernel_size=3,\n                                        stride=1,\n                                        padding=1)\n\n    def forward(self, x):\n        # timestep embedding\n        temb = None\n\n        # downsampling\n        hs = [self.conv_in(x)]\n        for i_level in range(self.num_resolutions):\n            for i_block in range(self.num_res_blocks):\n                h = self.down[i_level].block[i_block](hs[-1], temb)\n                if len(self.down[i_level].attn) > 0:\n                    h = self.down[i_level].attn[i_block](h)\n                hs.append(h)\n            if i_level != self.num_resolutions-1:\n                hs.append(self.down[i_level].downsample(hs[-1]))\n\n        # middle\n        h = hs[-1]\n        h = self.mid.block_1(h, temb)\n        h = self.mid.attn_1(h)\n        h = self.mid.block_2(h, temb)\n\n        # end\n        h = self.norm_out(h)\n        h = nonlinearity(h)\n        h = self.conv_out(h)\n        return h\n\n\nclass Decoder(nn.Module):\n    def __init__(self, *, ch, out_ch, ch_mult=(1,2,4,8), num_res_blocks,\n                 attn_resolutions, dropout=0.0, resamp_with_conv=True, in_channels,\n                 resolution, z_channels, give_pre_end=False, tanh_out=False, use_linear_attn=False,\n                 attn_type=\"vanilla\", **ignorekwargs):\n        super().__init__()\n        if use_linear_attn: attn_type = \"linear\"\n        self.ch = ch\n        self.temb_ch = 0\n        self.num_resolutions = len(ch_mult)\n        self.num_res_blocks = num_res_blocks\n        self.resolution = resolution\n        self.in_channels = in_channels\n        self.give_pre_end = give_pre_end\n        self.tanh_out = tanh_out\n\n        # compute in_ch_mult, block_in and curr_res at lowest res\n        in_ch_mult = (1,)+tuple(ch_mult)\n        block_in = ch*ch_mult[self.num_resolutions-1]\n        curr_res = resolution // 2**(self.num_resolutions-1)\n        self.z_shape = (1,z_channels,curr_res,curr_res)\n        print(\"Working with z of shape {} = {} dimensions.\".format(\n            self.z_shape, np.prod(self.z_shape)))\n\n        # z to block_in\n        self.conv_in = torch.nn.Conv2d(z_channels,\n                                       block_in,\n                                       kernel_size=3,\n                                       stride=1,\n                                       padding=1)\n\n        # middle\n        self.mid = nn.Module()\n        self.mid.block_1 = ResnetBlock(in_channels=block_in,\n                                       out_channels=block_in,\n                                       temb_channels=self.temb_ch,\n                                       dropout=dropout)\n        self.mid.attn_1 = make_attn(block_in, attn_type=attn_type)\n        self.mid.block_2 = ResnetBlock(in_channels=block_in,\n                                       out_channels=block_in,\n                                       temb_channels=self.temb_ch,\n                                       dropout=dropout)\n\n        # upsampling\n        self.up = nn.ModuleList()\n        for i_level in reversed(range(self.num_resolutions)):\n            block = nn.ModuleList()\n            attn = nn.ModuleList()\n            block_out = ch*ch_mult[i_level]\n            for i_block in range(self.num_res_blocks+1):\n                block.append(ResnetBlock(in_channels=block_in,\n                                         out_channels=block_out,\n                                         temb_channels=self.temb_ch,\n                                         dropout=dropout))\n                block_in = block_out\n                if curr_res in attn_resolutions:\n                    attn.append(make_attn(block_in, attn_type=attn_type))\n            up = nn.Module()\n            up.block = block\n            up.attn = attn\n            if i_level != 0:\n                up.upsample = Upsample(block_in, resamp_with_conv)\n                curr_res = curr_res * 2\n            self.up.insert(0, up) # prepend to get consistent order\n\n        # end\n        self.norm_out = Normalize(block_in)\n        self.conv_out = torch.nn.Conv2d(block_in,\n                                        out_ch,\n                                        kernel_size=3,\n                                        stride=1,\n                                        padding=1)\n\n    def forward(self, z):\n        #assert z.shape[1:] == self.z_shape[1:]\n        self.last_z_shape = z.shape\n\n        # timestep embedding\n        temb = None\n\n        # z to block_in\n        h = self.conv_in(z)\n\n        # middle\n        h = self.mid.block_1(h, temb)\n        h = self.mid.attn_1(h)\n        h = self.mid.block_2(h, temb)\n\n        # upsampling\n        for i_level in reversed(range(self.num_resolutions)):\n            for i_block in range(self.num_res_blocks+1):\n                h = self.up[i_level].block[i_block](h, temb)\n                if len(self.up[i_level].attn) > 0:\n                    h = self.up[i_level].attn[i_block](h)\n            if i_level != 0:\n                h = self.up[i_level].upsample(h)\n\n        # end\n        if self.give_pre_end:\n            return h\n\n        h = self.norm_out(h)\n        h = nonlinearity(h)\n        h = self.conv_out(h)\n        if self.tanh_out:\n            h = torch.tanh(h)\n        return h\n\n\nclass SimpleDecoder(nn.Module):\n    def __init__(self, in_channels, out_channels, *args, **kwargs):\n        super().__init__()\n        self.model = nn.ModuleList([nn.Conv2d(in_channels, in_channels, 1),\n                                     ResnetBlock(in_channels=in_channels,\n                                                 out_channels=2 * in_channels,\n                                                 temb_channels=0, dropout=0.0),\n                                     ResnetBlock(in_channels=2 * in_channels,\n                                                out_channels=4 * in_channels,\n                                                temb_channels=0, dropout=0.0),\n                                     ResnetBlock(in_channels=4 * in_channels,\n                                                out_channels=2 * in_channels,\n                                                temb_channels=0, dropout=0.0),\n                                     nn.Conv2d(2*in_channels, in_channels, 1),\n                                     Upsample(in_channels, with_conv=True)])\n        # end\n        self.norm_out = Normalize(in_channels)\n        self.conv_out = torch.nn.Conv2d(in_channels,\n                                        out_channels,\n                                        kernel_size=3,\n                                        stride=1,\n                                        padding=1)\n\n    def forward(self, x):\n        for i, layer in enumerate(self.model):\n            if i in [1,2,3]:\n                x = layer(x, None)\n            else:\n                x = layer(x)\n\n        h = self.norm_out(x)\n        h = nonlinearity(h)\n        x = self.conv_out(h)\n        return x\n\n\nclass UpsampleDecoder(nn.Module):\n    def __init__(self, in_channels, out_channels, ch, num_res_blocks, resolution,\n                 ch_mult=(2,2), dropout=0.0):\n        super().__init__()\n        # upsampling\n        self.temb_ch = 0\n        self.num_resolutions = len(ch_mult)\n        self.num_res_blocks = num_res_blocks\n        block_in = in_channels\n        curr_res = resolution // 2 ** (self.num_resolutions - 1)\n        self.res_blocks = nn.ModuleList()\n        self.upsample_blocks = nn.ModuleList()\n        for i_level in range(self.num_resolutions):\n            res_block = []\n            block_out = ch * ch_mult[i_level]\n            for i_block in range(self.num_res_blocks + 1):\n                res_block.append(ResnetBlock(in_channels=block_in,\n                                         out_channels=block_out,\n                                         temb_channels=self.temb_ch,\n                                         dropout=dropout))\n                block_in = block_out\n            self.res_blocks.append(nn.ModuleList(res_block))\n            if i_level != self.num_resolutions - 1:\n                self.upsample_blocks.append(Upsample(block_in, True))\n                curr_res = curr_res * 2\n\n        # end\n        self.norm_out = Normalize(block_in)\n        self.conv_out = torch.nn.Conv2d(block_in,\n                                        out_channels,\n                                        kernel_size=3,\n                                        stride=1,\n                                        padding=1)\n\n    def forward(self, x):\n        # upsampling\n        h = x\n        for k, i_level in enumerate(range(self.num_resolutions)):\n            for i_block in range(self.num_res_blocks + 1):\n                h = self.res_blocks[i_level][i_block](h, None)\n            if i_level != self.num_resolutions - 1:\n                h = self.upsample_blocks[k](h)\n        h = self.norm_out(h)\n        h = nonlinearity(h)\n        h = self.conv_out(h)\n        return h\n\n\nclass LatentRescaler(nn.Module):\n    def __init__(self, factor, in_channels, mid_channels, out_channels, depth=2):\n        super().__init__()\n        # residual block, interpolate, residual block\n        self.factor = factor\n        self.conv_in = nn.Conv2d(in_channels,\n                                 mid_channels,\n                                 kernel_size=3,\n                                 stride=1,\n                                 padding=1)\n        self.res_block1 = nn.ModuleList([ResnetBlock(in_channels=mid_channels,\n                                                     out_channels=mid_channels,\n                                                     temb_channels=0,\n                                                     dropout=0.0) for _ in range(depth)])\n        self.attn = AttnBlock(mid_channels)\n        self.res_block2 = nn.ModuleList([ResnetBlock(in_channels=mid_channels,\n                                                     out_channels=mid_channels,\n                                                     temb_channels=0,\n                                                     dropout=0.0) for _ in range(depth)])\n\n        self.conv_out = nn.Conv2d(mid_channels,\n                                  out_channels,\n                                  kernel_size=1,\n                                  )\n\n    def forward(self, x):\n        x = self.conv_in(x)\n        for block in self.res_block1:\n            x = block(x, None)\n        x = torch.nn.functional.interpolate(x, size=(int(round(x.shape[2]*self.factor)), int(round(x.shape[3]*self.factor))))\n        x = self.attn(x)\n        for block in self.res_block2:\n            x = block(x, None)\n        x = self.conv_out(x)\n        return x\n\n\nclass MergedRescaleEncoder(nn.Module):\n    def __init__(self, in_channels, ch, resolution, out_ch, num_res_blocks,\n                 attn_resolutions, dropout=0.0, resamp_with_conv=True,\n                 ch_mult=(1,2,4,8), rescale_factor=1.0, rescale_module_depth=1):\n        super().__init__()\n        intermediate_chn = ch * ch_mult[-1]\n        self.encoder = Encoder(in_channels=in_channels, num_res_blocks=num_res_blocks, ch=ch, ch_mult=ch_mult,\n                               z_channels=intermediate_chn, double_z=False, resolution=resolution,\n                               attn_resolutions=attn_resolutions, dropout=dropout, resamp_with_conv=resamp_with_conv,\n                               out_ch=None)\n        self.rescaler = LatentRescaler(factor=rescale_factor, in_channels=intermediate_chn,\n                                       mid_channels=intermediate_chn, out_channels=out_ch, depth=rescale_module_depth)\n\n    def forward(self, x):\n        x = self.encoder(x)\n        x = self.rescaler(x)\n        return x\n\n\nclass MergedRescaleDecoder(nn.Module):\n    def __init__(self, z_channels, out_ch, resolution, num_res_blocks, attn_resolutions, ch, ch_mult=(1,2,4,8),\n                 dropout=0.0, resamp_with_conv=True, rescale_factor=1.0, rescale_module_depth=1):\n        super().__init__()\n        tmp_chn = z_channels*ch_mult[-1]\n        self.decoder = Decoder(out_ch=out_ch, z_channels=tmp_chn, attn_resolutions=attn_resolutions, dropout=dropout,\n                               resamp_with_conv=resamp_with_conv, in_channels=None, num_res_blocks=num_res_blocks,\n                               ch_mult=ch_mult, resolution=resolution, ch=ch)\n        self.rescaler = LatentRescaler(factor=rescale_factor, in_channels=z_channels, mid_channels=tmp_chn,\n                                       out_channels=tmp_chn, depth=rescale_module_depth)\n\n    def forward(self, x):\n        x = self.rescaler(x)\n        x = self.decoder(x)\n        return x\n\n\nclass Upsampler(nn.Module):\n    def __init__(self, in_size, out_size, in_channels, out_channels, ch_mult=2):\n        super().__init__()\n        assert out_size >= in_size\n        num_blocks = int(np.log2(out_size//in_size))+1\n        factor_up = 1.+ (out_size % in_size)\n        print(f\"Building {self.__class__.__name__} with in_size: {in_size} --> out_size {out_size} and factor {factor_up}\")\n        self.rescaler = LatentRescaler(factor=factor_up, in_channels=in_channels, mid_channels=2*in_channels,\n                                       out_channels=in_channels)\n        self.decoder = Decoder(out_ch=out_channels, resolution=out_size, z_channels=in_channels, num_res_blocks=2,\n                               attn_resolutions=[], in_channels=None, ch=in_channels,\n                               ch_mult=[ch_mult for _ in range(num_blocks)])\n\n    def forward(self, x):\n        x = self.rescaler(x)\n        x = self.decoder(x)\n        return x\n\n\nclass Resize(nn.Module):\n    def __init__(self, in_channels=None, learned=False, mode=\"bilinear\"):\n        super().__init__()\n        self.with_conv = learned\n        self.mode = mode\n        if self.with_conv:\n            print(f\"Note: {self.__class__.__name} uses learned downsampling and will ignore the fixed {mode} mode\")\n            raise NotImplementedError()\n            assert in_channels is not None\n            # no asymmetric padding in torch conv, must do it ourselves\n            self.conv = torch.nn.Conv2d(in_channels,\n                                        in_channels,\n                                        kernel_size=4,\n                                        stride=2,\n                                        padding=1)\n\n    def forward(self, x, scale_factor=1.0):\n        if scale_factor==1.0:\n            return x\n        else:\n            x = torch.nn.functional.interpolate(x, mode=self.mode, align_corners=False, scale_factor=scale_factor)\n        return x\n", "ldm/modules/diffusionmodules/util.py": "# adopted from\n# https://github.com/openai/improved-diffusion/blob/main/improved_diffusion/gaussian_diffusion.py\n# and\n# https://github.com/lucidrains/denoising-diffusion-pytorch/blob/7706bdfc6f527f58d33f84b7b522e61e6e3164b3/denoising_diffusion_pytorch/denoising_diffusion_pytorch.py\n# and\n# https://github.com/openai/guided-diffusion/blob/0ba878e517b276c45d1195eb29f6f5f72659a05b/guided_diffusion/nn.py\n#\n# thanks!\n\n\nimport os\nimport math\nimport torch\nimport torch.nn as nn\nimport numpy as np\nfrom einops import repeat\n\nfrom ldm.util import instantiate_from_config\n\n\ndef make_beta_schedule(schedule, n_timestep, linear_start=1e-4, linear_end=2e-2, cosine_s=8e-3):\n    if schedule == \"linear\":\n        betas = (\n                torch.linspace(linear_start ** 0.5, linear_end ** 0.5, n_timestep, dtype=torch.float64) ** 2\n        )\n\n    elif schedule == \"cosine\":\n        timesteps = (\n                torch.arange(n_timestep + 1, dtype=torch.float64) / n_timestep + cosine_s\n        )\n        alphas = timesteps / (1 + cosine_s) * np.pi / 2\n        alphas = torch.cos(alphas).pow(2)\n        alphas = alphas / alphas[0]\n        betas = 1 - alphas[1:] / alphas[:-1]\n        betas = np.clip(betas, a_min=0, a_max=0.999)\n\n    elif schedule == \"squaredcos_cap_v2\":  # used for karlo prior\n        # return early\n        return betas_for_alpha_bar(\n            n_timestep,\n            lambda t: math.cos((t + 0.008) / 1.008 * math.pi / 2) ** 2,\n        )\n\n    elif schedule == \"sqrt_linear\":\n        betas = torch.linspace(linear_start, linear_end, n_timestep, dtype=torch.float64)\n    elif schedule == \"sqrt\":\n        betas = torch.linspace(linear_start, linear_end, n_timestep, dtype=torch.float64) ** 0.5\n    else:\n        raise ValueError(f\"schedule '{schedule}' unknown.\")\n    return betas.numpy()\n\n\ndef make_ddim_timesteps(ddim_discr_method, num_ddim_timesteps, num_ddpm_timesteps, verbose=True):\n    if ddim_discr_method == 'uniform':\n        c = num_ddpm_timesteps // num_ddim_timesteps\n        ddim_timesteps = np.asarray(list(range(0, num_ddpm_timesteps, c)))\n    elif ddim_discr_method == 'quad':\n        ddim_timesteps = ((np.linspace(0, np.sqrt(num_ddpm_timesteps * .8), num_ddim_timesteps)) ** 2).astype(int)\n    else:\n        raise NotImplementedError(f'There is no ddim discretization method called \"{ddim_discr_method}\"')\n\n    # assert ddim_timesteps.shape[0] == num_ddim_timesteps\n    # add one to get the final alpha values right (the ones from first scale to data during sampling)\n    steps_out = ddim_timesteps + 1\n    if verbose:\n        print(f'Selected timesteps for ddim sampler: {steps_out}')\n    return steps_out\n\n\ndef make_ddim_sampling_parameters(alphacums, ddim_timesteps, eta, verbose=True):\n    # select alphas for computing the variance schedule\n    alphas = alphacums[ddim_timesteps]\n    alphas_prev = np.asarray([alphacums[0]] + alphacums[ddim_timesteps[:-1]].tolist())\n\n    # according the the formula provided in https://arxiv.org/abs/2010.02502\n    sigmas = eta * np.sqrt((1 - alphas_prev) / (1 - alphas) * (1 - alphas / alphas_prev))\n    if verbose:\n        print(f'Selected alphas for ddim sampler: a_t: {alphas}; a_(t-1): {alphas_prev}')\n        print(f'For the chosen value of eta, which is {eta}, '\n              f'this results in the following sigma_t schedule for ddim sampler {sigmas}')\n    return sigmas, alphas, alphas_prev\n\n\ndef betas_for_alpha_bar(num_diffusion_timesteps, alpha_bar, max_beta=0.999):\n    \"\"\"\n    Create a beta schedule that discretizes the given alpha_t_bar function,\n    which defines the cumulative product of (1-beta) over time from t = [0,1].\n    :param num_diffusion_timesteps: the number of betas to produce.\n    :param alpha_bar: a lambda that takes an argument t from 0 to 1 and\n                      produces the cumulative product of (1-beta) up to that\n                      part of the diffusion process.\n    :param max_beta: the maximum beta to use; use values lower than 1 to\n                     prevent singularities.\n    \"\"\"\n    betas = []\n    for i in range(num_diffusion_timesteps):\n        t1 = i / num_diffusion_timesteps\n        t2 = (i + 1) / num_diffusion_timesteps\n        betas.append(min(1 - alpha_bar(t2) / alpha_bar(t1), max_beta))\n    return np.array(betas)\n\n\ndef extract_into_tensor(a, t, x_shape):\n    b, *_ = t.shape\n    out = a.gather(-1, t)\n    return out.reshape(b, *((1,) * (len(x_shape) - 1)))\n\n\ndef checkpoint(func, inputs, params, flag):\n    \"\"\"\n    Evaluate a function without caching intermediate activations, allowing for\n    reduced memory at the expense of extra compute in the backward pass.\n    :param func: the function to evaluate.\n    :param inputs: the argument sequence to pass to `func`.\n    :param params: a sequence of parameters `func` depends on but does not\n                   explicitly take as arguments.\n    :param flag: if False, disable gradient checkpointing.\n    \"\"\"\n    if flag:\n        args = tuple(inputs) + tuple(params)\n        return CheckpointFunction.apply(func, len(inputs), *args)\n    else:\n        return func(*inputs)\n\n\nclass CheckpointFunction(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, run_function, length, *args):\n        ctx.run_function = run_function\n        ctx.input_tensors = list(args[:length])\n        ctx.input_params = list(args[length:])\n        ctx.gpu_autocast_kwargs = {\"enabled\": torch.is_autocast_enabled(),\n                                   \"dtype\": torch.get_autocast_gpu_dtype(),\n                                   \"cache_enabled\": torch.is_autocast_cache_enabled()}\n        with torch.no_grad():\n            output_tensors = ctx.run_function(*ctx.input_tensors)\n        return output_tensors\n\n    @staticmethod\n    def backward(ctx, *output_grads):\n        ctx.input_tensors = [x.detach().requires_grad_(True) for x in ctx.input_tensors]\n        with torch.enable_grad(), \\\n                torch.cuda.amp.autocast(**ctx.gpu_autocast_kwargs):\n            # Fixes a bug where the first op in run_function modifies the\n            # Tensor storage in place, which is not allowed for detach()'d\n            # Tensors.\n            shallow_copies = [x.view_as(x) for x in ctx.input_tensors]\n            output_tensors = ctx.run_function(*shallow_copies)\n        input_grads = torch.autograd.grad(\n            output_tensors,\n            ctx.input_tensors + ctx.input_params,\n            output_grads,\n            allow_unused=True,\n        )\n        del ctx.input_tensors\n        del ctx.input_params\n        del output_tensors\n        return (None, None) + input_grads\n\n\ndef timestep_embedding(timesteps, dim, max_period=10000, repeat_only=False):\n    \"\"\"\n    Create sinusoidal timestep embeddings.\n    :param timesteps: a 1-D Tensor of N indices, one per batch element.\n                      These may be fractional.\n    :param dim: the dimension of the output.\n    :param max_period: controls the minimum frequency of the embeddings.\n    :return: an [N x dim] Tensor of positional embeddings.\n    \"\"\"\n    if not repeat_only:\n        half = dim // 2\n        freqs = torch.exp(\n            -math.log(max_period) * torch.arange(start=0, end=half, dtype=torch.float32) / half\n        ).to(device=timesteps.device)\n        args = timesteps[:, None].float() * freqs[None]\n        embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n        if dim % 2:\n            embedding = torch.cat([embedding, torch.zeros_like(embedding[:, :1])], dim=-1)\n    else:\n        embedding = repeat(timesteps, 'b -> b d', d=dim)\n    return embedding\n\n\ndef zero_module(module):\n    \"\"\"\n    Zero out the parameters of a module and return it.\n    \"\"\"\n    for p in module.parameters():\n        p.detach().zero_()\n    return module\n\n\ndef scale_module(module, scale):\n    \"\"\"\n    Scale the parameters of a module and return it.\n    \"\"\"\n    for p in module.parameters():\n        p.detach().mul_(scale)\n    return module\n\n\ndef mean_flat(tensor):\n    \"\"\"\n    Take the mean over all non-batch dimensions.\n    \"\"\"\n    return tensor.mean(dim=list(range(1, len(tensor.shape))))\n\n\ndef normalization(channels):\n    \"\"\"\n    Make a standard normalization layer.\n    :param channels: number of input channels.\n    :return: an nn.Module for normalization.\n    \"\"\"\n    return GroupNorm32(32, channels)\n\n\n# PyTorch 1.7 has SiLU, but we support PyTorch 1.5.\nclass SiLU(nn.Module):\n    def forward(self, x):\n        return x * torch.sigmoid(x)\n\n\nclass GroupNorm32(nn.GroupNorm):\n    def forward(self, x):\n        return super().forward(x.float()).type(x.dtype)\n\n\ndef conv_nd(dims, *args, **kwargs):\n    \"\"\"\n    Create a 1D, 2D, or 3D convolution module.\n    \"\"\"\n    if dims == 1:\n        return nn.Conv1d(*args, **kwargs)\n    elif dims == 2:\n        return nn.Conv2d(*args, **kwargs)\n    elif dims == 3:\n        return nn.Conv3d(*args, **kwargs)\n    raise ValueError(f\"unsupported dimensions: {dims}\")\n\n\ndef linear(*args, **kwargs):\n    \"\"\"\n    Create a linear module.\n    \"\"\"\n    return nn.Linear(*args, **kwargs)\n\n\ndef avg_pool_nd(dims, *args, **kwargs):\n    \"\"\"\n    Create a 1D, 2D, or 3D average pooling module.\n    \"\"\"\n    if dims == 1:\n        return nn.AvgPool1d(*args, **kwargs)\n    elif dims == 2:\n        return nn.AvgPool2d(*args, **kwargs)\n    elif dims == 3:\n        return nn.AvgPool3d(*args, **kwargs)\n    raise ValueError(f\"unsupported dimensions: {dims}\")\n\n\nclass HybridConditioner(nn.Module):\n\n    def __init__(self, c_concat_config, c_crossattn_config):\n        super().__init__()\n        self.concat_conditioner = instantiate_from_config(c_concat_config)\n        self.crossattn_conditioner = instantiate_from_config(c_crossattn_config)\n\n    def forward(self, c_concat, c_crossattn):\n        c_concat = self.concat_conditioner(c_concat)\n        c_crossattn = self.crossattn_conditioner(c_crossattn)\n        return {'c_concat': [c_concat], 'c_crossattn': [c_crossattn]}\n\n\ndef noise_like(shape, device, repeat=False):\n    repeat_noise = lambda: torch.randn((1, *shape[1:]), device=device).repeat(shape[0], *((1,) * (len(shape) - 1)))\n    noise = lambda: torch.randn(shape, device=device)\n    return repeat_noise() if repeat else noise()\n", "ldm/modules/diffusionmodules/upscaling.py": "import torch\nimport torch.nn as nn\nimport numpy as np\nfrom functools import partial\n\nfrom ldm.modules.diffusionmodules.util import extract_into_tensor, make_beta_schedule\nfrom ldm.util import default\n\n\nclass AbstractLowScaleModel(nn.Module):\n    # for concatenating a downsampled image to the latent representation\n    def __init__(self, noise_schedule_config=None):\n        super(AbstractLowScaleModel, self).__init__()\n        if noise_schedule_config is not None:\n            self.register_schedule(**noise_schedule_config)\n\n    def register_schedule(self, beta_schedule=\"linear\", timesteps=1000,\n                          linear_start=1e-4, linear_end=2e-2, cosine_s=8e-3):\n        betas = make_beta_schedule(beta_schedule, timesteps, linear_start=linear_start, linear_end=linear_end,\n                                   cosine_s=cosine_s)\n        alphas = 1. - betas\n        alphas_cumprod = np.cumprod(alphas, axis=0)\n        alphas_cumprod_prev = np.append(1., alphas_cumprod[:-1])\n\n        timesteps, = betas.shape\n        self.num_timesteps = int(timesteps)\n        self.linear_start = linear_start\n        self.linear_end = linear_end\n        assert alphas_cumprod.shape[0] == self.num_timesteps, 'alphas have to be defined for each timestep'\n\n        to_torch = partial(torch.tensor, dtype=torch.float32)\n\n        self.register_buffer('betas', to_torch(betas))\n        self.register_buffer('alphas_cumprod', to_torch(alphas_cumprod))\n        self.register_buffer('alphas_cumprod_prev', to_torch(alphas_cumprod_prev))\n\n        # calculations for diffusion q(x_t | x_{t-1}) and others\n        self.register_buffer('sqrt_alphas_cumprod', to_torch(np.sqrt(alphas_cumprod)))\n        self.register_buffer('sqrt_one_minus_alphas_cumprod', to_torch(np.sqrt(1. - alphas_cumprod)))\n        self.register_buffer('log_one_minus_alphas_cumprod', to_torch(np.log(1. - alphas_cumprod)))\n        self.register_buffer('sqrt_recip_alphas_cumprod', to_torch(np.sqrt(1. / alphas_cumprod)))\n        self.register_buffer('sqrt_recipm1_alphas_cumprod', to_torch(np.sqrt(1. / alphas_cumprod - 1)))\n\n    def q_sample(self, x_start, t, noise=None):\n        noise = default(noise, lambda: torch.randn_like(x_start))\n        return (extract_into_tensor(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start +\n                extract_into_tensor(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape) * noise)\n\n    def forward(self, x):\n        return x, None\n\n    def decode(self, x):\n        return x\n\n\nclass SimpleImageConcat(AbstractLowScaleModel):\n    # no noise level conditioning\n    def __init__(self):\n        super(SimpleImageConcat, self).__init__(noise_schedule_config=None)\n        self.max_noise_level = 0\n\n    def forward(self, x):\n        # fix to constant noise level\n        return x, torch.zeros(x.shape[0], device=x.device).long()\n\n\nclass ImageConcatWithNoiseAugmentation(AbstractLowScaleModel):\n    def __init__(self, noise_schedule_config, max_noise_level=1000, to_cuda=False):\n        super().__init__(noise_schedule_config=noise_schedule_config)\n        self.max_noise_level = max_noise_level\n\n    def forward(self, x, noise_level=None):\n        if noise_level is None:\n            noise_level = torch.randint(0, self.max_noise_level, (x.shape[0],), device=x.device).long()\n        else:\n            assert isinstance(noise_level, torch.Tensor)\n        z = self.q_sample(x, noise_level)\n        return z, noise_level\n\n\n\n", "ldm/modules/diffusionmodules/__init__.py": "", "ldm/modules/diffusionmodules/openaimodel.py": "from abc import abstractmethod\nimport math\n\nimport numpy as np\nimport torch as th\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom ldm.modules.diffusionmodules.util import (\n    checkpoint,\n    conv_nd,\n    linear,\n    avg_pool_nd,\n    zero_module,\n    normalization,\n    timestep_embedding,\n)\nfrom ldm.modules.attention import SpatialTransformer\nfrom ldm.util import exists\n\n\n# dummy replace\ndef convert_module_to_f16(x):\n    pass\n\ndef convert_module_to_f32(x):\n    pass\n\n\n## go\nclass AttentionPool2d(nn.Module):\n    \"\"\"\n    Adapted from CLIP: https://github.com/openai/CLIP/blob/main/clip/model.py\n    \"\"\"\n\n    def __init__(\n        self,\n        spacial_dim: int,\n        embed_dim: int,\n        num_heads_channels: int,\n        output_dim: int = None,\n    ):\n        super().__init__()\n        self.positional_embedding = nn.Parameter(th.randn(embed_dim, spacial_dim ** 2 + 1) / embed_dim ** 0.5)\n        self.qkv_proj = conv_nd(1, embed_dim, 3 * embed_dim, 1)\n        self.c_proj = conv_nd(1, embed_dim, output_dim or embed_dim, 1)\n        self.num_heads = embed_dim // num_heads_channels\n        self.attention = QKVAttention(self.num_heads)\n\n    def forward(self, x):\n        b, c, *_spatial = x.shape\n        x = x.reshape(b, c, -1)  # NC(HW)\n        x = th.cat([x.mean(dim=-1, keepdim=True), x], dim=-1)  # NC(HW+1)\n        x = x + self.positional_embedding[None, :, :].to(x.dtype)  # NC(HW+1)\n        x = self.qkv_proj(x)\n        x = self.attention(x)\n        x = self.c_proj(x)\n        return x[:, :, 0]\n\n\nclass TimestepBlock(nn.Module):\n    \"\"\"\n    Any module where forward() takes timestep embeddings as a second argument.\n    \"\"\"\n\n    @abstractmethod\n    def forward(self, x, emb):\n        \"\"\"\n        Apply the module to `x` given `emb` timestep embeddings.\n        \"\"\"\n\n\nclass TimestepEmbedSequential(nn.Sequential, TimestepBlock):\n    \"\"\"\n    A sequential module that passes timestep embeddings to the children that\n    support it as an extra input.\n    \"\"\"\n\n    def forward(self, x, emb, context=None):\n        for layer in self:\n            if isinstance(layer, TimestepBlock):\n                x = layer(x, emb)\n            elif isinstance(layer, SpatialTransformer):\n                x = layer(x, context)\n            else:\n                x = layer(x)\n        return x\n\n\nclass Upsample(nn.Module):\n    \"\"\"\n    An upsampling layer with an optional convolution.\n    :param channels: channels in the inputs and outputs.\n    :param use_conv: a bool determining if a convolution is applied.\n    :param dims: determines if the signal is 1D, 2D, or 3D. If 3D, then\n                 upsampling occurs in the inner-two dimensions.\n    \"\"\"\n\n    def __init__(self, channels, use_conv, dims=2, out_channels=None, padding=1):\n        super().__init__()\n        self.channels = channels\n        self.out_channels = out_channels or channels\n        self.use_conv = use_conv\n        self.dims = dims\n        if use_conv:\n            self.conv = conv_nd(dims, self.channels, self.out_channels, 3, padding=padding)\n\n    def forward(self, x):\n        assert x.shape[1] == self.channels\n        if self.dims == 3:\n            x = F.interpolate(\n                x, (x.shape[2], x.shape[3] * 2, x.shape[4] * 2), mode=\"nearest\"\n            )\n        else:\n            x = F.interpolate(x, scale_factor=2, mode=\"nearest\")\n        if self.use_conv:\n            x = self.conv(x)\n        return x\n\nclass TransposedUpsample(nn.Module):\n    'Learned 2x upsampling without padding'\n    def __init__(self, channels, out_channels=None, ks=5):\n        super().__init__()\n        self.channels = channels\n        self.out_channels = out_channels or channels\n\n        self.up = nn.ConvTranspose2d(self.channels,self.out_channels,kernel_size=ks,stride=2)\n\n    def forward(self,x):\n        return self.up(x)\n\n\nclass Downsample(nn.Module):\n    \"\"\"\n    A downsampling layer with an optional convolution.\n    :param channels: channels in the inputs and outputs.\n    :param use_conv: a bool determining if a convolution is applied.\n    :param dims: determines if the signal is 1D, 2D, or 3D. If 3D, then\n                 downsampling occurs in the inner-two dimensions.\n    \"\"\"\n\n    def __init__(self, channels, use_conv, dims=2, out_channels=None,padding=1):\n        super().__init__()\n        self.channels = channels\n        self.out_channels = out_channels or channels\n        self.use_conv = use_conv\n        self.dims = dims\n        stride = 2 if dims != 3 else (1, 2, 2)\n        if use_conv:\n            self.op = conv_nd(\n                dims, self.channels, self.out_channels, 3, stride=stride, padding=padding\n            )\n        else:\n            assert self.channels == self.out_channels\n            self.op = avg_pool_nd(dims, kernel_size=stride, stride=stride)\n\n    def forward(self, x):\n        assert x.shape[1] == self.channels\n        return self.op(x)\n\n\nclass ResBlock(TimestepBlock):\n    \"\"\"\n    A residual block that can optionally change the number of channels.\n    :param channels: the number of input channels.\n    :param emb_channels: the number of timestep embedding channels.\n    :param dropout: the rate of dropout.\n    :param out_channels: if specified, the number of out channels.\n    :param use_conv: if True and out_channels is specified, use a spatial\n        convolution instead of a smaller 1x1 convolution to change the\n        channels in the skip connection.\n    :param dims: determines if the signal is 1D, 2D, or 3D.\n    :param use_checkpoint: if True, use gradient checkpointing on this module.\n    :param up: if True, use this block for upsampling.\n    :param down: if True, use this block for downsampling.\n    \"\"\"\n\n    def __init__(\n        self,\n        channels,\n        emb_channels,\n        dropout,\n        out_channels=None,\n        use_conv=False,\n        use_scale_shift_norm=False,\n        dims=2,\n        use_checkpoint=False,\n        up=False,\n        down=False,\n    ):\n        super().__init__()\n        self.channels = channels\n        self.emb_channels = emb_channels\n        self.dropout = dropout\n        self.out_channels = out_channels or channels\n        self.use_conv = use_conv\n        self.use_checkpoint = use_checkpoint\n        self.use_scale_shift_norm = use_scale_shift_norm\n\n        self.in_layers = nn.Sequential(\n            normalization(channels),\n            nn.SiLU(),\n            conv_nd(dims, channels, self.out_channels, 3, padding=1),\n        )\n\n        self.updown = up or down\n\n        if up:\n            self.h_upd = Upsample(channels, False, dims)\n            self.x_upd = Upsample(channels, False, dims)\n        elif down:\n            self.h_upd = Downsample(channels, False, dims)\n            self.x_upd = Downsample(channels, False, dims)\n        else:\n            self.h_upd = self.x_upd = nn.Identity()\n\n        self.emb_layers = nn.Sequential(\n            nn.SiLU(),\n            linear(\n                emb_channels,\n                2 * self.out_channels if use_scale_shift_norm else self.out_channels,\n            ),\n        )\n        self.out_layers = nn.Sequential(\n            normalization(self.out_channels),\n            nn.SiLU(),\n            nn.Dropout(p=dropout),\n            zero_module(\n                conv_nd(dims, self.out_channels, self.out_channels, 3, padding=1)\n            ),\n        )\n\n        if self.out_channels == channels:\n            self.skip_connection = nn.Identity()\n        elif use_conv:\n            self.skip_connection = conv_nd(\n                dims, channels, self.out_channels, 3, padding=1\n            )\n        else:\n            self.skip_connection = conv_nd(dims, channels, self.out_channels, 1)\n\n    def forward(self, x, emb):\n        \"\"\"\n        Apply the block to a Tensor, conditioned on a timestep embedding.\n        :param x: an [N x C x ...] Tensor of features.\n        :param emb: an [N x emb_channels] Tensor of timestep embeddings.\n        :return: an [N x C x ...] Tensor of outputs.\n        \"\"\"\n        return checkpoint(\n            self._forward, (x, emb), self.parameters(), self.use_checkpoint\n        )\n\n\n    def _forward(self, x, emb):\n        if self.updown:\n            in_rest, in_conv = self.in_layers[:-1], self.in_layers[-1]\n            h = in_rest(x)\n            h = self.h_upd(h)\n            x = self.x_upd(x)\n            h = in_conv(h)\n        else:\n            h = self.in_layers(x)\n        emb_out = self.emb_layers(emb).type(h.dtype)\n        while len(emb_out.shape) < len(h.shape):\n            emb_out = emb_out[..., None]\n        if self.use_scale_shift_norm:\n            out_norm, out_rest = self.out_layers[0], self.out_layers[1:]\n            scale, shift = th.chunk(emb_out, 2, dim=1)\n            h = out_norm(h) * (1 + scale) + shift\n            h = out_rest(h)\n        else:\n            h = h + emb_out\n            h = self.out_layers(h)\n        return self.skip_connection(x) + h\n\n\nclass AttentionBlock(nn.Module):\n    \"\"\"\n    An attention block that allows spatial positions to attend to each other.\n    Originally ported from here, but adapted to the N-d case.\n    https://github.com/hojonathanho/diffusion/blob/1e0dceb3b3495bbe19116a5e1b3596cd0706c543/diffusion_tf/models/unet.py#L66.\n    \"\"\"\n\n    def __init__(\n        self,\n        channels,\n        num_heads=1,\n        num_head_channels=-1,\n        use_checkpoint=False,\n        use_new_attention_order=False,\n    ):\n        super().__init__()\n        self.channels = channels\n        if num_head_channels == -1:\n            self.num_heads = num_heads\n        else:\n            assert (\n                channels % num_head_channels == 0\n            ), f\"q,k,v channels {channels} is not divisible by num_head_channels {num_head_channels}\"\n            self.num_heads = channels // num_head_channels\n        self.use_checkpoint = use_checkpoint\n        self.norm = normalization(channels)\n        self.qkv = conv_nd(1, channels, channels * 3, 1)\n        if use_new_attention_order:\n            # split qkv before split heads\n            self.attention = QKVAttention(self.num_heads)\n        else:\n            # split heads before split qkv\n            self.attention = QKVAttentionLegacy(self.num_heads)\n\n        self.proj_out = zero_module(conv_nd(1, channels, channels, 1))\n\n    def forward(self, x):\n        return checkpoint(self._forward, (x,), self.parameters(), True)   # TODO: check checkpoint usage, is True # TODO: fix the .half call!!!\n        #return pt_checkpoint(self._forward, x)  # pytorch\n\n    def _forward(self, x):\n        b, c, *spatial = x.shape\n        x = x.reshape(b, c, -1)\n        qkv = self.qkv(self.norm(x))\n        h = self.attention(qkv)\n        h = self.proj_out(h)\n        return (x + h).reshape(b, c, *spatial)\n\n\ndef count_flops_attn(model, _x, y):\n    \"\"\"\n    A counter for the `thop` package to count the operations in an\n    attention operation.\n    Meant to be used like:\n        macs, params = thop.profile(\n            model,\n            inputs=(inputs, timestamps),\n            custom_ops={QKVAttention: QKVAttention.count_flops},\n        )\n    \"\"\"\n    b, c, *spatial = y[0].shape\n    num_spatial = int(np.prod(spatial))\n    # We perform two matmuls with the same number of ops.\n    # The first computes the weight matrix, the second computes\n    # the combination of the value vectors.\n    matmul_ops = 2 * b * (num_spatial ** 2) * c\n    model.total_ops += th.DoubleTensor([matmul_ops])\n\n\nclass QKVAttentionLegacy(nn.Module):\n    \"\"\"\n    A module which performs QKV attention. Matches legacy QKVAttention + input/ouput heads shaping\n    \"\"\"\n\n    def __init__(self, n_heads):\n        super().__init__()\n        self.n_heads = n_heads\n\n    def forward(self, qkv):\n        \"\"\"\n        Apply QKV attention.\n        :param qkv: an [N x (H * 3 * C) x T] tensor of Qs, Ks, and Vs.\n        :return: an [N x (H * C) x T] tensor after attention.\n        \"\"\"\n        bs, width, length = qkv.shape\n        assert width % (3 * self.n_heads) == 0\n        ch = width // (3 * self.n_heads)\n        q, k, v = qkv.reshape(bs * self.n_heads, ch * 3, length).split(ch, dim=1)\n        scale = 1 / math.sqrt(math.sqrt(ch))\n        weight = th.einsum(\n            \"bct,bcs->bts\", q * scale, k * scale\n        )  # More stable with f16 than dividing afterwards\n        weight = th.softmax(weight.float(), dim=-1).type(weight.dtype)\n        a = th.einsum(\"bts,bcs->bct\", weight, v)\n        return a.reshape(bs, -1, length)\n\n    @staticmethod\n    def count_flops(model, _x, y):\n        return count_flops_attn(model, _x, y)\n\n\nclass QKVAttention(nn.Module):\n    \"\"\"\n    A module which performs QKV attention and splits in a different order.\n    \"\"\"\n\n    def __init__(self, n_heads):\n        super().__init__()\n        self.n_heads = n_heads\n\n    def forward(self, qkv):\n        \"\"\"\n        Apply QKV attention.\n        :param qkv: an [N x (3 * H * C) x T] tensor of Qs, Ks, and Vs.\n        :return: an [N x (H * C) x T] tensor after attention.\n        \"\"\"\n        bs, width, length = qkv.shape\n        assert width % (3 * self.n_heads) == 0\n        ch = width // (3 * self.n_heads)\n        q, k, v = qkv.chunk(3, dim=1)\n        scale = 1 / math.sqrt(math.sqrt(ch))\n        weight = th.einsum(\n            \"bct,bcs->bts\",\n            (q * scale).view(bs * self.n_heads, ch, length),\n            (k * scale).view(bs * self.n_heads, ch, length),\n        )  # More stable with f16 than dividing afterwards\n        weight = th.softmax(weight.float(), dim=-1).type(weight.dtype)\n        a = th.einsum(\"bts,bcs->bct\", weight, v.reshape(bs * self.n_heads, ch, length))\n        return a.reshape(bs, -1, length)\n\n    @staticmethod\n    def count_flops(model, _x, y):\n        return count_flops_attn(model, _x, y)\n\n\nclass Timestep(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.dim = dim\n\n    def forward(self, t):\n        return timestep_embedding(t, self.dim)\n\n\nclass UNetModel(nn.Module):\n    \"\"\"\n    The full UNet model with attention and timestep embedding.\n    :param in_channels: channels in the input Tensor.\n    :param model_channels: base channel count for the model.\n    :param out_channels: channels in the output Tensor.\n    :param num_res_blocks: number of residual blocks per downsample.\n    :param attention_resolutions: a collection of downsample rates at which\n        attention will take place. May be a set, list, or tuple.\n        For example, if this contains 4, then at 4x downsampling, attention\n        will be used.\n    :param dropout: the dropout probability.\n    :param channel_mult: channel multiplier for each level of the UNet.\n    :param conv_resample: if True, use learned convolutions for upsampling and\n        downsampling.\n    :param dims: determines if the signal is 1D, 2D, or 3D.\n    :param num_classes: if specified (as an int), then this model will be\n        class-conditional with `num_classes` classes.\n    :param use_checkpoint: use gradient checkpointing to reduce memory usage.\n    :param num_heads: the number of attention heads in each attention layer.\n    :param num_heads_channels: if specified, ignore num_heads and instead use\n                               a fixed channel width per attention head.\n    :param num_heads_upsample: works with num_heads to set a different number\n                               of heads for upsampling. Deprecated.\n    :param use_scale_shift_norm: use a FiLM-like conditioning mechanism.\n    :param resblock_updown: use residual blocks for up/downsampling.\n    :param use_new_attention_order: use a different attention pattern for potentially\n                                    increased efficiency.\n    \"\"\"\n\n    def __init__(\n        self,\n        image_size,\n        in_channels,\n        model_channels,\n        out_channels,\n        num_res_blocks,\n        attention_resolutions,\n        dropout=0,\n        channel_mult=(1, 2, 4, 8),\n        conv_resample=True,\n        dims=2,\n        num_classes=None,\n        use_checkpoint=False,\n        use_fp16=False,\n        use_bf16=False,\n        num_heads=-1,\n        num_head_channels=-1,\n        num_heads_upsample=-1,\n        use_scale_shift_norm=False,\n        resblock_updown=False,\n        use_new_attention_order=False,\n        use_spatial_transformer=False,    # custom transformer support\n        transformer_depth=1,              # custom transformer support\n        context_dim=None,                 # custom transformer support\n        n_embed=None,                     # custom support for prediction of discrete ids into codebook of first stage vq model\n        legacy=True,\n        disable_self_attentions=None,\n        num_attention_blocks=None,\n        disable_middle_self_attn=False,\n        use_linear_in_transformer=False,\n        adm_in_channels=None,\n    ):\n        super().__init__()\n        if use_spatial_transformer:\n            assert context_dim is not None, 'Fool!! You forgot to include the dimension of your cross-attention conditioning...'\n\n        if context_dim is not None:\n            assert use_spatial_transformer, 'Fool!! You forgot to use the spatial transformer for your cross-attention conditioning...'\n            from omegaconf.listconfig import ListConfig\n            if type(context_dim) == ListConfig:\n                context_dim = list(context_dim)\n\n        if num_heads_upsample == -1:\n            num_heads_upsample = num_heads\n\n        if num_heads == -1:\n            assert num_head_channels != -1, 'Either num_heads or num_head_channels has to be set'\n\n        if num_head_channels == -1:\n            assert num_heads != -1, 'Either num_heads or num_head_channels has to be set'\n\n        self.image_size = image_size\n        self.in_channels = in_channels\n        self.model_channels = model_channels\n        self.out_channels = out_channels\n        if isinstance(num_res_blocks, int):\n            self.num_res_blocks = len(channel_mult) * [num_res_blocks]\n        else:\n            if len(num_res_blocks) != len(channel_mult):\n                raise ValueError(\"provide num_res_blocks either as an int (globally constant) or \"\n                                 \"as a list/tuple (per-level) with the same length as channel_mult\")\n            self.num_res_blocks = num_res_blocks\n        if disable_self_attentions is not None:\n            # should be a list of booleans, indicating whether to disable self-attention in TransformerBlocks or not\n            assert len(disable_self_attentions) == len(channel_mult)\n        if num_attention_blocks is not None:\n            assert len(num_attention_blocks) == len(self.num_res_blocks)\n            assert all(map(lambda i: self.num_res_blocks[i] >= num_attention_blocks[i], range(len(num_attention_blocks))))\n            print(f\"Constructor of UNetModel received num_attention_blocks={num_attention_blocks}. \"\n                  f\"This option has LESS priority than attention_resolutions {attention_resolutions}, \"\n                  f\"i.e., in cases where num_attention_blocks[i] > 0 but 2**i not in attention_resolutions, \"\n                  f\"attention will still not be set.\")\n\n        self.attention_resolutions = attention_resolutions\n        self.dropout = dropout\n        self.channel_mult = channel_mult\n        self.conv_resample = conv_resample\n        self.num_classes = num_classes\n        self.use_checkpoint = use_checkpoint\n        self.dtype = th.float16 if use_fp16 else th.float32\n        self.dtype = th.bfloat16 if use_bf16 else self.dtype\n        self.num_heads = num_heads\n        self.num_head_channels = num_head_channels\n        self.num_heads_upsample = num_heads_upsample\n        self.predict_codebook_ids = n_embed is not None\n\n        time_embed_dim = model_channels * 4\n        self.time_embed = nn.Sequential(\n            linear(model_channels, time_embed_dim),\n            nn.SiLU(),\n            linear(time_embed_dim, time_embed_dim),\n        )\n\n        if self.num_classes is not None:\n            if isinstance(self.num_classes, int):\n                self.label_emb = nn.Embedding(num_classes, time_embed_dim)\n            elif self.num_classes == \"continuous\":\n                print(\"setting up linear c_adm embedding layer\")\n                self.label_emb = nn.Linear(1, time_embed_dim)\n            elif self.num_classes == \"sequential\":\n                assert adm_in_channels is not None\n                self.label_emb = nn.Sequential(\n                    nn.Sequential(\n                        linear(adm_in_channels, time_embed_dim),\n                        nn.SiLU(),\n                        linear(time_embed_dim, time_embed_dim),\n                    )\n                )\n            else:\n                raise ValueError()\n\n        self.input_blocks = nn.ModuleList(\n            [\n                TimestepEmbedSequential(\n                    conv_nd(dims, in_channels, model_channels, 3, padding=1)\n                )\n            ]\n        )\n        self._feature_size = model_channels\n        input_block_chans = [model_channels]\n        ch = model_channels\n        ds = 1\n        for level, mult in enumerate(channel_mult):\n            for nr in range(self.num_res_blocks[level]):\n                layers = [\n                    ResBlock(\n                        ch,\n                        time_embed_dim,\n                        dropout,\n                        out_channels=mult * model_channels,\n                        dims=dims,\n                        use_checkpoint=use_checkpoint,\n                        use_scale_shift_norm=use_scale_shift_norm,\n                    )\n                ]\n                ch = mult * model_channels\n                if ds in attention_resolutions:\n                    if num_head_channels == -1:\n                        dim_head = ch // num_heads\n                    else:\n                        num_heads = ch // num_head_channels\n                        dim_head = num_head_channels\n                    if legacy:\n                        #num_heads = 1\n                        dim_head = ch // num_heads if use_spatial_transformer else num_head_channels\n                    if exists(disable_self_attentions):\n                        disabled_sa = disable_self_attentions[level]\n                    else:\n                        disabled_sa = False\n\n                    if not exists(num_attention_blocks) or nr < num_attention_blocks[level]:\n                        layers.append(\n                            AttentionBlock(\n                                ch,\n                                use_checkpoint=use_checkpoint,\n                                num_heads=num_heads,\n                                num_head_channels=dim_head,\n                                use_new_attention_order=use_new_attention_order,\n                            ) if not use_spatial_transformer else SpatialTransformer(\n                                ch, num_heads, dim_head, depth=transformer_depth, context_dim=context_dim,\n                                disable_self_attn=disabled_sa, use_linear=use_linear_in_transformer,\n                                use_checkpoint=use_checkpoint\n                            )\n                        )\n                self.input_blocks.append(TimestepEmbedSequential(*layers))\n                self._feature_size += ch\n                input_block_chans.append(ch)\n            if level != len(channel_mult) - 1:\n                out_ch = ch\n                self.input_blocks.append(\n                    TimestepEmbedSequential(\n                        ResBlock(\n                            ch,\n                            time_embed_dim,\n                            dropout,\n                            out_channels=out_ch,\n                            dims=dims,\n                            use_checkpoint=use_checkpoint,\n                            use_scale_shift_norm=use_scale_shift_norm,\n                            down=True,\n                        )\n                        if resblock_updown\n                        else Downsample(\n                            ch, conv_resample, dims=dims, out_channels=out_ch\n                        )\n                    )\n                )\n                ch = out_ch\n                input_block_chans.append(ch)\n                ds *= 2\n                self._feature_size += ch\n\n        if num_head_channels == -1:\n            dim_head = ch // num_heads\n        else:\n            num_heads = ch // num_head_channels\n            dim_head = num_head_channels\n        if legacy:\n            #num_heads = 1\n            dim_head = ch // num_heads if use_spatial_transformer else num_head_channels\n        self.middle_block = TimestepEmbedSequential(\n            ResBlock(\n                ch,\n                time_embed_dim,\n                dropout,\n                dims=dims,\n                use_checkpoint=use_checkpoint,\n                use_scale_shift_norm=use_scale_shift_norm,\n            ),\n            AttentionBlock(\n                ch,\n                use_checkpoint=use_checkpoint,\n                num_heads=num_heads,\n                num_head_channels=dim_head,\n                use_new_attention_order=use_new_attention_order,\n            ) if not use_spatial_transformer else SpatialTransformer(  # always uses a self-attn\n                            ch, num_heads, dim_head, depth=transformer_depth, context_dim=context_dim,\n                            disable_self_attn=disable_middle_self_attn, use_linear=use_linear_in_transformer,\n                            use_checkpoint=use_checkpoint\n                        ),\n            ResBlock(\n                ch,\n                time_embed_dim,\n                dropout,\n                dims=dims,\n                use_checkpoint=use_checkpoint,\n                use_scale_shift_norm=use_scale_shift_norm,\n            ),\n        )\n        self._feature_size += ch\n\n        self.output_blocks = nn.ModuleList([])\n        for level, mult in list(enumerate(channel_mult))[::-1]:\n            for i in range(self.num_res_blocks[level] + 1):\n                ich = input_block_chans.pop()\n                layers = [\n                    ResBlock(\n                        ch + ich,\n                        time_embed_dim,\n                        dropout,\n                        out_channels=model_channels * mult,\n                        dims=dims,\n                        use_checkpoint=use_checkpoint,\n                        use_scale_shift_norm=use_scale_shift_norm,\n                    )\n                ]\n                ch = model_channels * mult\n                if ds in attention_resolutions:\n                    if num_head_channels == -1:\n                        dim_head = ch // num_heads\n                    else:\n                        num_heads = ch // num_head_channels\n                        dim_head = num_head_channels\n                    if legacy:\n                        #num_heads = 1\n                        dim_head = ch // num_heads if use_spatial_transformer else num_head_channels\n                    if exists(disable_self_attentions):\n                        disabled_sa = disable_self_attentions[level]\n                    else:\n                        disabled_sa = False\n\n                    if not exists(num_attention_blocks) or i < num_attention_blocks[level]:\n                        layers.append(\n                            AttentionBlock(\n                                ch,\n                                use_checkpoint=use_checkpoint,\n                                num_heads=num_heads_upsample,\n                                num_head_channels=dim_head,\n                                use_new_attention_order=use_new_attention_order,\n                            ) if not use_spatial_transformer else SpatialTransformer(\n                                ch, num_heads, dim_head, depth=transformer_depth, context_dim=context_dim,\n                                disable_self_attn=disabled_sa, use_linear=use_linear_in_transformer,\n                                use_checkpoint=use_checkpoint\n                            )\n                        )\n                if level and i == self.num_res_blocks[level]:\n                    out_ch = ch\n                    layers.append(\n                        ResBlock(\n                            ch,\n                            time_embed_dim,\n                            dropout,\n                            out_channels=out_ch,\n                            dims=dims,\n                            use_checkpoint=use_checkpoint,\n                            use_scale_shift_norm=use_scale_shift_norm,\n                            up=True,\n                        )\n                        if resblock_updown\n                        else Upsample(ch, conv_resample, dims=dims, out_channels=out_ch)\n                    )\n                    ds //= 2\n                self.output_blocks.append(TimestepEmbedSequential(*layers))\n                self._feature_size += ch\n\n        self.out = nn.Sequential(\n            normalization(ch),\n            nn.SiLU(),\n            zero_module(conv_nd(dims, model_channels, out_channels, 3, padding=1)),\n        )\n        if self.predict_codebook_ids:\n            self.id_predictor = nn.Sequential(\n            normalization(ch),\n            conv_nd(dims, model_channels, n_embed, 1),\n            #nn.LogSoftmax(dim=1)  # change to cross_entropy and produce non-normalized logits\n        )\n\n    def convert_to_fp16(self):\n        \"\"\"\n        Convert the torso of the model to float16.\n        \"\"\"\n        self.input_blocks.apply(convert_module_to_f16)\n        self.middle_block.apply(convert_module_to_f16)\n        self.output_blocks.apply(convert_module_to_f16)\n\n    def convert_to_fp32(self):\n        \"\"\"\n        Convert the torso of the model to float32.\n        \"\"\"\n        self.input_blocks.apply(convert_module_to_f32)\n        self.middle_block.apply(convert_module_to_f32)\n        self.output_blocks.apply(convert_module_to_f32)\n\n    def forward(self, x, timesteps=None, context=None, y=None,**kwargs):\n        \"\"\"\n        Apply the model to an input batch.\n        :param x: an [N x C x ...] Tensor of inputs.\n        :param timesteps: a 1-D batch of timesteps.\n        :param context: conditioning plugged in via crossattn\n        :param y: an [N] Tensor of labels, if class-conditional.\n        :return: an [N x C x ...] Tensor of outputs.\n        \"\"\"\n        assert (y is not None) == (\n            self.num_classes is not None\n        ), \"must specify y if and only if the model is class-conditional\"\n        hs = []\n        t_emb = timestep_embedding(timesteps, self.model_channels, repeat_only=False)\n        emb = self.time_embed(t_emb)\n\n        if self.num_classes is not None:\n            assert y.shape[0] == x.shape[0]\n            emb = emb + self.label_emb(y)\n\n        h = x.type(self.dtype)\n        for module in self.input_blocks:\n            h = module(h, emb, context)\n            hs.append(h)\n        h = self.middle_block(h, emb, context)\n        for module in self.output_blocks:\n            h = th.cat([h, hs.pop()], dim=1)\n            h = module(h, emb, context)\n        h = h.type(x.dtype)\n        if self.predict_codebook_ids:\n            return self.id_predictor(h)\n        else:\n            return self.out(h)\n", "ldm/modules/image_degradation/bsrgan_light.py": "# -*- coding: utf-8 -*-\nimport numpy as np\nimport cv2\nimport torch\n\nfrom functools import partial\nimport random\nfrom scipy import ndimage\nimport scipy\nimport scipy.stats as ss\nfrom scipy.interpolate import interp2d\nfrom scipy.linalg import orth\nimport albumentations\n\nimport ldm.modules.image_degradation.utils_image as util\n\n\"\"\"\n# --------------------------------------------\n# Super-Resolution\n# --------------------------------------------\n#\n# Kai Zhang (cskaizhang@gmail.com)\n# https://github.com/cszn\n# From 2019/03--2021/08\n# --------------------------------------------\n\"\"\"\n\ndef modcrop_np(img, sf):\n    '''\n    Args:\n        img: numpy image, WxH or WxHxC\n        sf: scale factor\n    Return:\n        cropped image\n    '''\n    w, h = img.shape[:2]\n    im = np.copy(img)\n    return im[:w - w % sf, :h - h % sf, ...]\n\n\n\"\"\"\n# --------------------------------------------\n# anisotropic Gaussian kernels\n# --------------------------------------------\n\"\"\"\n\n\ndef analytic_kernel(k):\n    \"\"\"Calculate the X4 kernel from the X2 kernel (for proof see appendix in paper)\"\"\"\n    k_size = k.shape[0]\n    # Calculate the big kernels size\n    big_k = np.zeros((3 * k_size - 2, 3 * k_size - 2))\n    # Loop over the small kernel to fill the big one\n    for r in range(k_size):\n        for c in range(k_size):\n            big_k[2 * r:2 * r + k_size, 2 * c:2 * c + k_size] += k[r, c] * k\n    # Crop the edges of the big kernel to ignore very small values and increase run time of SR\n    crop = k_size // 2\n    cropped_big_k = big_k[crop:-crop, crop:-crop]\n    # Normalize to 1\n    return cropped_big_k / cropped_big_k.sum()\n\n\ndef anisotropic_Gaussian(ksize=15, theta=np.pi, l1=6, l2=6):\n    \"\"\" generate an anisotropic Gaussian kernel\n    Args:\n        ksize : e.g., 15, kernel size\n        theta : [0,  pi], rotation angle range\n        l1    : [0.1,50], scaling of eigenvalues\n        l2    : [0.1,l1], scaling of eigenvalues\n        If l1 = l2, will get an isotropic Gaussian kernel.\n    Returns:\n        k     : kernel\n    \"\"\"\n\n    v = np.dot(np.array([[np.cos(theta), -np.sin(theta)], [np.sin(theta), np.cos(theta)]]), np.array([1., 0.]))\n    V = np.array([[v[0], v[1]], [v[1], -v[0]]])\n    D = np.array([[l1, 0], [0, l2]])\n    Sigma = np.dot(np.dot(V, D), np.linalg.inv(V))\n    k = gm_blur_kernel(mean=[0, 0], cov=Sigma, size=ksize)\n\n    return k\n\n\ndef gm_blur_kernel(mean, cov, size=15):\n    center = size / 2.0 + 0.5\n    k = np.zeros([size, size])\n    for y in range(size):\n        for x in range(size):\n            cy = y - center + 1\n            cx = x - center + 1\n            k[y, x] = ss.multivariate_normal.pdf([cx, cy], mean=mean, cov=cov)\n\n    k = k / np.sum(k)\n    return k\n\n\ndef shift_pixel(x, sf, upper_left=True):\n    \"\"\"shift pixel for super-resolution with different scale factors\n    Args:\n        x: WxHxC or WxH\n        sf: scale factor\n        upper_left: shift direction\n    \"\"\"\n    h, w = x.shape[:2]\n    shift = (sf - 1) * 0.5\n    xv, yv = np.arange(0, w, 1.0), np.arange(0, h, 1.0)\n    if upper_left:\n        x1 = xv + shift\n        y1 = yv + shift\n    else:\n        x1 = xv - shift\n        y1 = yv - shift\n\n    x1 = np.clip(x1, 0, w - 1)\n    y1 = np.clip(y1, 0, h - 1)\n\n    if x.ndim == 2:\n        x = interp2d(xv, yv, x)(x1, y1)\n    if x.ndim == 3:\n        for i in range(x.shape[-1]):\n            x[:, :, i] = interp2d(xv, yv, x[:, :, i])(x1, y1)\n\n    return x\n\n\ndef blur(x, k):\n    '''\n    x: image, NxcxHxW\n    k: kernel, Nx1xhxw\n    '''\n    n, c = x.shape[:2]\n    p1, p2 = (k.shape[-2] - 1) // 2, (k.shape[-1] - 1) // 2\n    x = torch.nn.functional.pad(x, pad=(p1, p2, p1, p2), mode='replicate')\n    k = k.repeat(1, c, 1, 1)\n    k = k.view(-1, 1, k.shape[2], k.shape[3])\n    x = x.view(1, -1, x.shape[2], x.shape[3])\n    x = torch.nn.functional.conv2d(x, k, bias=None, stride=1, padding=0, groups=n * c)\n    x = x.view(n, c, x.shape[2], x.shape[3])\n\n    return x\n\n\ndef gen_kernel(k_size=np.array([15, 15]), scale_factor=np.array([4, 4]), min_var=0.6, max_var=10., noise_level=0):\n    \"\"\"\"\n    # modified version of https://github.com/assafshocher/BlindSR_dataset_generator\n    # Kai Zhang\n    # min_var = 0.175 * sf  # variance of the gaussian kernel will be sampled between min_var and max_var\n    # max_var = 2.5 * sf\n    \"\"\"\n    # Set random eigen-vals (lambdas) and angle (theta) for COV matrix\n    lambda_1 = min_var + np.random.rand() * (max_var - min_var)\n    lambda_2 = min_var + np.random.rand() * (max_var - min_var)\n    theta = np.random.rand() * np.pi  # random theta\n    noise = -noise_level + np.random.rand(*k_size) * noise_level * 2\n\n    # Set COV matrix using Lambdas and Theta\n    LAMBDA = np.diag([lambda_1, lambda_2])\n    Q = np.array([[np.cos(theta), -np.sin(theta)],\n                  [np.sin(theta), np.cos(theta)]])\n    SIGMA = Q @ LAMBDA @ Q.T\n    INV_SIGMA = np.linalg.inv(SIGMA)[None, None, :, :]\n\n    # Set expectation position (shifting kernel for aligned image)\n    MU = k_size // 2 - 0.5 * (scale_factor - 1)  # - 0.5 * (scale_factor - k_size % 2)\n    MU = MU[None, None, :, None]\n\n    # Create meshgrid for Gaussian\n    [X, Y] = np.meshgrid(range(k_size[0]), range(k_size[1]))\n    Z = np.stack([X, Y], 2)[:, :, :, None]\n\n    # Calcualte Gaussian for every pixel of the kernel\n    ZZ = Z - MU\n    ZZ_t = ZZ.transpose(0, 1, 3, 2)\n    raw_kernel = np.exp(-0.5 * np.squeeze(ZZ_t @ INV_SIGMA @ ZZ)) * (1 + noise)\n\n    # shift the kernel so it will be centered\n    # raw_kernel_centered = kernel_shift(raw_kernel, scale_factor)\n\n    # Normalize the kernel and return\n    # kernel = raw_kernel_centered / np.sum(raw_kernel_centered)\n    kernel = raw_kernel / np.sum(raw_kernel)\n    return kernel\n\n\ndef fspecial_gaussian(hsize, sigma):\n    hsize = [hsize, hsize]\n    siz = [(hsize[0] - 1.0) / 2.0, (hsize[1] - 1.0) / 2.0]\n    std = sigma\n    [x, y] = np.meshgrid(np.arange(-siz[1], siz[1] + 1), np.arange(-siz[0], siz[0] + 1))\n    arg = -(x * x + y * y) / (2 * std * std)\n    h = np.exp(arg)\n    h[h < scipy.finfo(float).eps * h.max()] = 0\n    sumh = h.sum()\n    if sumh != 0:\n        h = h / sumh\n    return h\n\n\ndef fspecial_laplacian(alpha):\n    alpha = max([0, min([alpha, 1])])\n    h1 = alpha / (alpha + 1)\n    h2 = (1 - alpha) / (alpha + 1)\n    h = [[h1, h2, h1], [h2, -4 / (alpha + 1), h2], [h1, h2, h1]]\n    h = np.array(h)\n    return h\n\n\ndef fspecial(filter_type, *args, **kwargs):\n    '''\n    python code from:\n    https://github.com/ronaldosena/imagens-medicas-2/blob/40171a6c259edec7827a6693a93955de2bd39e76/Aulas/aula_2_-_uniform_filter/matlab_fspecial.py\n    '''\n    if filter_type == 'gaussian':\n        return fspecial_gaussian(*args, **kwargs)\n    if filter_type == 'laplacian':\n        return fspecial_laplacian(*args, **kwargs)\n\n\n\"\"\"\n# --------------------------------------------\n# degradation models\n# --------------------------------------------\n\"\"\"\n\n\ndef bicubic_degradation(x, sf=3):\n    '''\n    Args:\n        x: HxWxC image, [0, 1]\n        sf: down-scale factor\n    Return:\n        bicubicly downsampled LR image\n    '''\n    x = util.imresize_np(x, scale=1 / sf)\n    return x\n\n\ndef srmd_degradation(x, k, sf=3):\n    ''' blur + bicubic downsampling\n    Args:\n        x: HxWxC image, [0, 1]\n        k: hxw, double\n        sf: down-scale factor\n    Return:\n        downsampled LR image\n    Reference:\n        @inproceedings{zhang2018learning,\n          title={Learning a single convolutional super-resolution network for multiple degradations},\n          author={Zhang, Kai and Zuo, Wangmeng and Zhang, Lei},\n          booktitle={IEEE Conference on Computer Vision and Pattern Recognition},\n          pages={3262--3271},\n          year={2018}\n        }\n    '''\n    x = ndimage.convolve(x, np.expand_dims(k, axis=2), mode='wrap')  # 'nearest' | 'mirror'\n    x = bicubic_degradation(x, sf=sf)\n    return x\n\n\ndef dpsr_degradation(x, k, sf=3):\n    ''' bicubic downsampling + blur\n    Args:\n        x: HxWxC image, [0, 1]\n        k: hxw, double\n        sf: down-scale factor\n    Return:\n        downsampled LR image\n    Reference:\n        @inproceedings{zhang2019deep,\n          title={Deep Plug-and-Play Super-Resolution for Arbitrary Blur Kernels},\n          author={Zhang, Kai and Zuo, Wangmeng and Zhang, Lei},\n          booktitle={IEEE Conference on Computer Vision and Pattern Recognition},\n          pages={1671--1681},\n          year={2019}\n        }\n    '''\n    x = bicubic_degradation(x, sf=sf)\n    x = ndimage.convolve(x, np.expand_dims(k, axis=2), mode='wrap')\n    return x\n\n\ndef classical_degradation(x, k, sf=3):\n    ''' blur + downsampling\n    Args:\n        x: HxWxC image, [0, 1]/[0, 255]\n        k: hxw, double\n        sf: down-scale factor\n    Return:\n        downsampled LR image\n    '''\n    x = ndimage.convolve(x, np.expand_dims(k, axis=2), mode='wrap')\n    # x = filters.correlate(x, np.expand_dims(np.flip(k), axis=2))\n    st = 0\n    return x[st::sf, st::sf, ...]\n\n\ndef add_sharpening(img, weight=0.5, radius=50, threshold=10):\n    \"\"\"USM sharpening. borrowed from real-ESRGAN\n    Input image: I; Blurry image: B.\n    1. K = I + weight * (I - B)\n    2. Mask = 1 if abs(I - B) > threshold, else: 0\n    3. Blur mask:\n    4. Out = Mask * K + (1 - Mask) * I\n    Args:\n        img (Numpy array): Input image, HWC, BGR; float32, [0, 1].\n        weight (float): Sharp weight. Default: 1.\n        radius (float): Kernel size of Gaussian blur. Default: 50.\n        threshold (int):\n    \"\"\"\n    if radius % 2 == 0:\n        radius += 1\n    blur = cv2.GaussianBlur(img, (radius, radius), 0)\n    residual = img - blur\n    mask = np.abs(residual) * 255 > threshold\n    mask = mask.astype('float32')\n    soft_mask = cv2.GaussianBlur(mask, (radius, radius), 0)\n\n    K = img + weight * residual\n    K = np.clip(K, 0, 1)\n    return soft_mask * K + (1 - soft_mask) * img\n\n\ndef add_blur(img, sf=4):\n    wd2 = 4.0 + sf\n    wd = 2.0 + 0.2 * sf\n\n    wd2 = wd2/4\n    wd = wd/4\n\n    if random.random() < 0.5:\n        l1 = wd2 * random.random()\n        l2 = wd2 * random.random()\n        k = anisotropic_Gaussian(ksize=random.randint(2, 11) + 3, theta=random.random() * np.pi, l1=l1, l2=l2)\n    else:\n        k = fspecial('gaussian', random.randint(2, 4) + 3, wd * random.random())\n    img = ndimage.convolve(img, np.expand_dims(k, axis=2), mode='mirror')\n\n    return img\n\n\ndef add_resize(img, sf=4):\n    rnum = np.random.rand()\n    if rnum > 0.8:  # up\n        sf1 = random.uniform(1, 2)\n    elif rnum < 0.7:  # down\n        sf1 = random.uniform(0.5 / sf, 1)\n    else:\n        sf1 = 1.0\n    img = cv2.resize(img, (int(sf1 * img.shape[1]), int(sf1 * img.shape[0])), interpolation=random.choice([1, 2, 3]))\n    img = np.clip(img, 0.0, 1.0)\n\n    return img\n\n\n# def add_Gaussian_noise(img, noise_level1=2, noise_level2=25):\n#     noise_level = random.randint(noise_level1, noise_level2)\n#     rnum = np.random.rand()\n#     if rnum > 0.6:  # add color Gaussian noise\n#         img += np.random.normal(0, noise_level / 255.0, img.shape).astype(np.float32)\n#     elif rnum < 0.4:  # add grayscale Gaussian noise\n#         img += np.random.normal(0, noise_level / 255.0, (*img.shape[:2], 1)).astype(np.float32)\n#     else:  # add  noise\n#         L = noise_level2 / 255.\n#         D = np.diag(np.random.rand(3))\n#         U = orth(np.random.rand(3, 3))\n#         conv = np.dot(np.dot(np.transpose(U), D), U)\n#         img += np.random.multivariate_normal([0, 0, 0], np.abs(L ** 2 * conv), img.shape[:2]).astype(np.float32)\n#     img = np.clip(img, 0.0, 1.0)\n#     return img\n\ndef add_Gaussian_noise(img, noise_level1=2, noise_level2=25):\n    noise_level = random.randint(noise_level1, noise_level2)\n    rnum = np.random.rand()\n    if rnum > 0.6:  # add color Gaussian noise\n        img = img + np.random.normal(0, noise_level / 255.0, img.shape).astype(np.float32)\n    elif rnum < 0.4:  # add grayscale Gaussian noise\n        img = img + np.random.normal(0, noise_level / 255.0, (*img.shape[:2], 1)).astype(np.float32)\n    else:  # add  noise\n        L = noise_level2 / 255.\n        D = np.diag(np.random.rand(3))\n        U = orth(np.random.rand(3, 3))\n        conv = np.dot(np.dot(np.transpose(U), D), U)\n        img = img + np.random.multivariate_normal([0, 0, 0], np.abs(L ** 2 * conv), img.shape[:2]).astype(np.float32)\n    img = np.clip(img, 0.0, 1.0)\n    return img\n\n\ndef add_speckle_noise(img, noise_level1=2, noise_level2=25):\n    noise_level = random.randint(noise_level1, noise_level2)\n    img = np.clip(img, 0.0, 1.0)\n    rnum = random.random()\n    if rnum > 0.6:\n        img += img * np.random.normal(0, noise_level / 255.0, img.shape).astype(np.float32)\n    elif rnum < 0.4:\n        img += img * np.random.normal(0, noise_level / 255.0, (*img.shape[:2], 1)).astype(np.float32)\n    else:\n        L = noise_level2 / 255.\n        D = np.diag(np.random.rand(3))\n        U = orth(np.random.rand(3, 3))\n        conv = np.dot(np.dot(np.transpose(U), D), U)\n        img += img * np.random.multivariate_normal([0, 0, 0], np.abs(L ** 2 * conv), img.shape[:2]).astype(np.float32)\n    img = np.clip(img, 0.0, 1.0)\n    return img\n\n\ndef add_Poisson_noise(img):\n    img = np.clip((img * 255.0).round(), 0, 255) / 255.\n    vals = 10 ** (2 * random.random() + 2.0)  # [2, 4]\n    if random.random() < 0.5:\n        img = np.random.poisson(img * vals).astype(np.float32) / vals\n    else:\n        img_gray = np.dot(img[..., :3], [0.299, 0.587, 0.114])\n        img_gray = np.clip((img_gray * 255.0).round(), 0, 255) / 255.\n        noise_gray = np.random.poisson(img_gray * vals).astype(np.float32) / vals - img_gray\n        img += noise_gray[:, :, np.newaxis]\n    img = np.clip(img, 0.0, 1.0)\n    return img\n\n\ndef add_JPEG_noise(img):\n    quality_factor = random.randint(80, 95)\n    img = cv2.cvtColor(util.single2uint(img), cv2.COLOR_RGB2BGR)\n    result, encimg = cv2.imencode('.jpg', img, [int(cv2.IMWRITE_JPEG_QUALITY), quality_factor])\n    img = cv2.imdecode(encimg, 1)\n    img = cv2.cvtColor(util.uint2single(img), cv2.COLOR_BGR2RGB)\n    return img\n\n\ndef random_crop(lq, hq, sf=4, lq_patchsize=64):\n    h, w = lq.shape[:2]\n    rnd_h = random.randint(0, h - lq_patchsize)\n    rnd_w = random.randint(0, w - lq_patchsize)\n    lq = lq[rnd_h:rnd_h + lq_patchsize, rnd_w:rnd_w + lq_patchsize, :]\n\n    rnd_h_H, rnd_w_H = int(rnd_h * sf), int(rnd_w * sf)\n    hq = hq[rnd_h_H:rnd_h_H + lq_patchsize * sf, rnd_w_H:rnd_w_H + lq_patchsize * sf, :]\n    return lq, hq\n\n\ndef degradation_bsrgan(img, sf=4, lq_patchsize=72, isp_model=None):\n    \"\"\"\n    This is the degradation model of BSRGAN from the paper\n    \"Designing a Practical Degradation Model for Deep Blind Image Super-Resolution\"\n    ----------\n    img: HXWXC, [0, 1], its size should be large than (lq_patchsizexsf)x(lq_patchsizexsf)\n    sf: scale factor\n    isp_model: camera ISP model\n    Returns\n    -------\n    img: low-quality patch, size: lq_patchsizeXlq_patchsizeXC, range: [0, 1]\n    hq: corresponding high-quality patch, size: (lq_patchsizexsf)X(lq_patchsizexsf)XC, range: [0, 1]\n    \"\"\"\n    isp_prob, jpeg_prob, scale2_prob = 0.25, 0.9, 0.25\n    sf_ori = sf\n\n    h1, w1 = img.shape[:2]\n    img = img.copy()[:w1 - w1 % sf, :h1 - h1 % sf, ...]  # mod crop\n    h, w = img.shape[:2]\n\n    if h < lq_patchsize * sf or w < lq_patchsize * sf:\n        raise ValueError(f'img size ({h1}X{w1}) is too small!')\n\n    hq = img.copy()\n\n    if sf == 4 and random.random() < scale2_prob:  # downsample1\n        if np.random.rand() < 0.5:\n            img = cv2.resize(img, (int(1 / 2 * img.shape[1]), int(1 / 2 * img.shape[0])),\n                             interpolation=random.choice([1, 2, 3]))\n        else:\n            img = util.imresize_np(img, 1 / 2, True)\n        img = np.clip(img, 0.0, 1.0)\n        sf = 2\n\n    shuffle_order = random.sample(range(7), 7)\n    idx1, idx2 = shuffle_order.index(2), shuffle_order.index(3)\n    if idx1 > idx2:  # keep downsample3 last\n        shuffle_order[idx1], shuffle_order[idx2] = shuffle_order[idx2], shuffle_order[idx1]\n\n    for i in shuffle_order:\n\n        if i == 0:\n            img = add_blur(img, sf=sf)\n\n        elif i == 1:\n            img = add_blur(img, sf=sf)\n\n        elif i == 2:\n            a, b = img.shape[1], img.shape[0]\n            # downsample2\n            if random.random() < 0.75:\n                sf1 = random.uniform(1, 2 * sf)\n                img = cv2.resize(img, (int(1 / sf1 * img.shape[1]), int(1 / sf1 * img.shape[0])),\n                                 interpolation=random.choice([1, 2, 3]))\n            else:\n                k = fspecial('gaussian', 25, random.uniform(0.1, 0.6 * sf))\n                k_shifted = shift_pixel(k, sf)\n                k_shifted = k_shifted / k_shifted.sum()  # blur with shifted kernel\n                img = ndimage.convolve(img, np.expand_dims(k_shifted, axis=2), mode='mirror')\n                img = img[0::sf, 0::sf, ...]  # nearest downsampling\n            img = np.clip(img, 0.0, 1.0)\n\n        elif i == 3:\n            # downsample3\n            img = cv2.resize(img, (int(1 / sf * a), int(1 / sf * b)), interpolation=random.choice([1, 2, 3]))\n            img = np.clip(img, 0.0, 1.0)\n\n        elif i == 4:\n            # add Gaussian noise\n            img = add_Gaussian_noise(img, noise_level1=2, noise_level2=8)\n\n        elif i == 5:\n            # add JPEG noise\n            if random.random() < jpeg_prob:\n                img = add_JPEG_noise(img)\n\n        elif i == 6:\n            # add processed camera sensor noise\n            if random.random() < isp_prob and isp_model is not None:\n                with torch.no_grad():\n                    img, hq = isp_model.forward(img.copy(), hq)\n\n    # add final JPEG compression noise\n    img = add_JPEG_noise(img)\n\n    # random crop\n    img, hq = random_crop(img, hq, sf_ori, lq_patchsize)\n\n    return img, hq\n\n\n# todo no isp_model?\ndef degradation_bsrgan_variant(image, sf=4, isp_model=None, up=False):\n    \"\"\"\n    This is the degradation model of BSRGAN from the paper\n    \"Designing a Practical Degradation Model for Deep Blind Image Super-Resolution\"\n    ----------\n    sf: scale factor\n    isp_model: camera ISP model\n    Returns\n    -------\n    img: low-quality patch, size: lq_patchsizeXlq_patchsizeXC, range: [0, 1]\n    hq: corresponding high-quality patch, size: (lq_patchsizexsf)X(lq_patchsizexsf)XC, range: [0, 1]\n    \"\"\"\n    image = util.uint2single(image)\n    isp_prob, jpeg_prob, scale2_prob = 0.25, 0.9, 0.25\n    sf_ori = sf\n\n    h1, w1 = image.shape[:2]\n    image = image.copy()[:w1 - w1 % sf, :h1 - h1 % sf, ...]  # mod crop\n    h, w = image.shape[:2]\n\n    hq = image.copy()\n\n    if sf == 4 and random.random() < scale2_prob:  # downsample1\n        if np.random.rand() < 0.5:\n            image = cv2.resize(image, (int(1 / 2 * image.shape[1]), int(1 / 2 * image.shape[0])),\n                               interpolation=random.choice([1, 2, 3]))\n        else:\n            image = util.imresize_np(image, 1 / 2, True)\n        image = np.clip(image, 0.0, 1.0)\n        sf = 2\n\n    shuffle_order = random.sample(range(7), 7)\n    idx1, idx2 = shuffle_order.index(2), shuffle_order.index(3)\n    if idx1 > idx2:  # keep downsample3 last\n        shuffle_order[idx1], shuffle_order[idx2] = shuffle_order[idx2], shuffle_order[idx1]\n\n    for i in shuffle_order:\n\n        if i == 0:\n            image = add_blur(image, sf=sf)\n\n        # elif i == 1:\n        #     image = add_blur(image, sf=sf)\n\n        if i == 0:\n            pass\n\n        elif i == 2:\n            a, b = image.shape[1], image.shape[0]\n            # downsample2\n            if random.random() < 0.8:\n                sf1 = random.uniform(1, 2 * sf)\n                image = cv2.resize(image, (int(1 / sf1 * image.shape[1]), int(1 / sf1 * image.shape[0])),\n                                   interpolation=random.choice([1, 2, 3]))\n            else:\n                k = fspecial('gaussian', 25, random.uniform(0.1, 0.6 * sf))\n                k_shifted = shift_pixel(k, sf)\n                k_shifted = k_shifted / k_shifted.sum()  # blur with shifted kernel\n                image = ndimage.convolve(image, np.expand_dims(k_shifted, axis=2), mode='mirror')\n                image = image[0::sf, 0::sf, ...]  # nearest downsampling\n\n            image = np.clip(image, 0.0, 1.0)\n\n        elif i == 3:\n            # downsample3\n            image = cv2.resize(image, (int(1 / sf * a), int(1 / sf * b)), interpolation=random.choice([1, 2, 3]))\n            image = np.clip(image, 0.0, 1.0)\n\n        elif i == 4:\n            # add Gaussian noise\n            image = add_Gaussian_noise(image, noise_level1=1, noise_level2=2)\n\n        elif i == 5:\n            # add JPEG noise\n            if random.random() < jpeg_prob:\n                image = add_JPEG_noise(image)\n        #\n        # elif i == 6:\n        #     # add processed camera sensor noise\n        #     if random.random() < isp_prob and isp_model is not None:\n        #         with torch.no_grad():\n        #             img, hq = isp_model.forward(img.copy(), hq)\n\n    # add final JPEG compression noise\n    image = add_JPEG_noise(image)\n    image = util.single2uint(image)\n    if up:\n        image = cv2.resize(image, (w1, h1), interpolation=cv2.INTER_CUBIC)  # todo: random, as above? want to condition on it then\n    example = {\"image\": image}\n    return example\n\n\n\n\nif __name__ == '__main__':\n    print(\"hey\")\n    img = util.imread_uint('utils/test.png', 3)\n    img = img[:448, :448]\n    h = img.shape[0] // 4\n    print(\"resizing to\", h)\n    sf = 4\n    deg_fn = partial(degradation_bsrgan_variant, sf=sf)\n    for i in range(20):\n        print(i)\n        img_hq = img\n        img_lq = deg_fn(img)[\"image\"]\n        img_hq, img_lq = util.uint2single(img_hq), util.uint2single(img_lq)\n        print(img_lq)\n        img_lq_bicubic = albumentations.SmallestMaxSize(max_size=h, interpolation=cv2.INTER_CUBIC)(image=img_hq)[\"image\"]\n        print(img_lq.shape)\n        print(\"bicubic\", img_lq_bicubic.shape)\n        print(img_hq.shape)\n        lq_nearest = cv2.resize(util.single2uint(img_lq), (int(sf * img_lq.shape[1]), int(sf * img_lq.shape[0])),\n                                interpolation=0)\n        lq_bicubic_nearest = cv2.resize(util.single2uint(img_lq_bicubic),\n                                        (int(sf * img_lq.shape[1]), int(sf * img_lq.shape[0])),\n                                        interpolation=0)\n        img_concat = np.concatenate([lq_bicubic_nearest, lq_nearest, util.single2uint(img_hq)], axis=1)\n        util.imsave(img_concat, str(i) + '.png')\n", "ldm/modules/image_degradation/bsrgan.py": "# -*- coding: utf-8 -*-\n\"\"\"\n# --------------------------------------------\n# Super-Resolution\n# --------------------------------------------\n#\n# Kai Zhang (cskaizhang@gmail.com)\n# https://github.com/cszn\n# From 2019/03--2021/08\n# --------------------------------------------\n\"\"\"\n\nimport numpy as np\nimport cv2\nimport torch\n\nfrom functools import partial\nimport random\nfrom scipy import ndimage\nimport scipy\nimport scipy.stats as ss\nfrom scipy.interpolate import interp2d\nfrom scipy.linalg import orth\nimport albumentations\n\nimport ldm.modules.image_degradation.utils_image as util\n\n\ndef modcrop_np(img, sf):\n    '''\n    Args:\n        img: numpy image, WxH or WxHxC\n        sf: scale factor\n    Return:\n        cropped image\n    '''\n    w, h = img.shape[:2]\n    im = np.copy(img)\n    return im[:w - w % sf, :h - h % sf, ...]\n\n\n\"\"\"\n# --------------------------------------------\n# anisotropic Gaussian kernels\n# --------------------------------------------\n\"\"\"\n\n\ndef analytic_kernel(k):\n    \"\"\"Calculate the X4 kernel from the X2 kernel (for proof see appendix in paper)\"\"\"\n    k_size = k.shape[0]\n    # Calculate the big kernels size\n    big_k = np.zeros((3 * k_size - 2, 3 * k_size - 2))\n    # Loop over the small kernel to fill the big one\n    for r in range(k_size):\n        for c in range(k_size):\n            big_k[2 * r:2 * r + k_size, 2 * c:2 * c + k_size] += k[r, c] * k\n    # Crop the edges of the big kernel to ignore very small values and increase run time of SR\n    crop = k_size // 2\n    cropped_big_k = big_k[crop:-crop, crop:-crop]\n    # Normalize to 1\n    return cropped_big_k / cropped_big_k.sum()\n\n\ndef anisotropic_Gaussian(ksize=15, theta=np.pi, l1=6, l2=6):\n    \"\"\" generate an anisotropic Gaussian kernel\n    Args:\n        ksize : e.g., 15, kernel size\n        theta : [0,  pi], rotation angle range\n        l1    : [0.1,50], scaling of eigenvalues\n        l2    : [0.1,l1], scaling of eigenvalues\n        If l1 = l2, will get an isotropic Gaussian kernel.\n    Returns:\n        k     : kernel\n    \"\"\"\n\n    v = np.dot(np.array([[np.cos(theta), -np.sin(theta)], [np.sin(theta), np.cos(theta)]]), np.array([1., 0.]))\n    V = np.array([[v[0], v[1]], [v[1], -v[0]]])\n    D = np.array([[l1, 0], [0, l2]])\n    Sigma = np.dot(np.dot(V, D), np.linalg.inv(V))\n    k = gm_blur_kernel(mean=[0, 0], cov=Sigma, size=ksize)\n\n    return k\n\n\ndef gm_blur_kernel(mean, cov, size=15):\n    center = size / 2.0 + 0.5\n    k = np.zeros([size, size])\n    for y in range(size):\n        for x in range(size):\n            cy = y - center + 1\n            cx = x - center + 1\n            k[y, x] = ss.multivariate_normal.pdf([cx, cy], mean=mean, cov=cov)\n\n    k = k / np.sum(k)\n    return k\n\n\ndef shift_pixel(x, sf, upper_left=True):\n    \"\"\"shift pixel for super-resolution with different scale factors\n    Args:\n        x: WxHxC or WxH\n        sf: scale factor\n        upper_left: shift direction\n    \"\"\"\n    h, w = x.shape[:2]\n    shift = (sf - 1) * 0.5\n    xv, yv = np.arange(0, w, 1.0), np.arange(0, h, 1.0)\n    if upper_left:\n        x1 = xv + shift\n        y1 = yv + shift\n    else:\n        x1 = xv - shift\n        y1 = yv - shift\n\n    x1 = np.clip(x1, 0, w - 1)\n    y1 = np.clip(y1, 0, h - 1)\n\n    if x.ndim == 2:\n        x = interp2d(xv, yv, x)(x1, y1)\n    if x.ndim == 3:\n        for i in range(x.shape[-1]):\n            x[:, :, i] = interp2d(xv, yv, x[:, :, i])(x1, y1)\n\n    return x\n\n\ndef blur(x, k):\n    '''\n    x: image, NxcxHxW\n    k: kernel, Nx1xhxw\n    '''\n    n, c = x.shape[:2]\n    p1, p2 = (k.shape[-2] - 1) // 2, (k.shape[-1] - 1) // 2\n    x = torch.nn.functional.pad(x, pad=(p1, p2, p1, p2), mode='replicate')\n    k = k.repeat(1, c, 1, 1)\n    k = k.view(-1, 1, k.shape[2], k.shape[3])\n    x = x.view(1, -1, x.shape[2], x.shape[3])\n    x = torch.nn.functional.conv2d(x, k, bias=None, stride=1, padding=0, groups=n * c)\n    x = x.view(n, c, x.shape[2], x.shape[3])\n\n    return x\n\n\ndef gen_kernel(k_size=np.array([15, 15]), scale_factor=np.array([4, 4]), min_var=0.6, max_var=10., noise_level=0):\n    \"\"\"\"\n    # modified version of https://github.com/assafshocher/BlindSR_dataset_generator\n    # Kai Zhang\n    # min_var = 0.175 * sf  # variance of the gaussian kernel will be sampled between min_var and max_var\n    # max_var = 2.5 * sf\n    \"\"\"\n    # Set random eigen-vals (lambdas) and angle (theta) for COV matrix\n    lambda_1 = min_var + np.random.rand() * (max_var - min_var)\n    lambda_2 = min_var + np.random.rand() * (max_var - min_var)\n    theta = np.random.rand() * np.pi  # random theta\n    noise = -noise_level + np.random.rand(*k_size) * noise_level * 2\n\n    # Set COV matrix using Lambdas and Theta\n    LAMBDA = np.diag([lambda_1, lambda_2])\n    Q = np.array([[np.cos(theta), -np.sin(theta)],\n                  [np.sin(theta), np.cos(theta)]])\n    SIGMA = Q @ LAMBDA @ Q.T\n    INV_SIGMA = np.linalg.inv(SIGMA)[None, None, :, :]\n\n    # Set expectation position (shifting kernel for aligned image)\n    MU = k_size // 2 - 0.5 * (scale_factor - 1)  # - 0.5 * (scale_factor - k_size % 2)\n    MU = MU[None, None, :, None]\n\n    # Create meshgrid for Gaussian\n    [X, Y] = np.meshgrid(range(k_size[0]), range(k_size[1]))\n    Z = np.stack([X, Y], 2)[:, :, :, None]\n\n    # Calcualte Gaussian for every pixel of the kernel\n    ZZ = Z - MU\n    ZZ_t = ZZ.transpose(0, 1, 3, 2)\n    raw_kernel = np.exp(-0.5 * np.squeeze(ZZ_t @ INV_SIGMA @ ZZ)) * (1 + noise)\n\n    # shift the kernel so it will be centered\n    # raw_kernel_centered = kernel_shift(raw_kernel, scale_factor)\n\n    # Normalize the kernel and return\n    # kernel = raw_kernel_centered / np.sum(raw_kernel_centered)\n    kernel = raw_kernel / np.sum(raw_kernel)\n    return kernel\n\n\ndef fspecial_gaussian(hsize, sigma):\n    hsize = [hsize, hsize]\n    siz = [(hsize[0] - 1.0) / 2.0, (hsize[1] - 1.0) / 2.0]\n    std = sigma\n    [x, y] = np.meshgrid(np.arange(-siz[1], siz[1] + 1), np.arange(-siz[0], siz[0] + 1))\n    arg = -(x * x + y * y) / (2 * std * std)\n    h = np.exp(arg)\n    h[h < scipy.finfo(float).eps * h.max()] = 0\n    sumh = h.sum()\n    if sumh != 0:\n        h = h / sumh\n    return h\n\n\ndef fspecial_laplacian(alpha):\n    alpha = max([0, min([alpha, 1])])\n    h1 = alpha / (alpha + 1)\n    h2 = (1 - alpha) / (alpha + 1)\n    h = [[h1, h2, h1], [h2, -4 / (alpha + 1), h2], [h1, h2, h1]]\n    h = np.array(h)\n    return h\n\n\ndef fspecial(filter_type, *args, **kwargs):\n    '''\n    python code from:\n    https://github.com/ronaldosena/imagens-medicas-2/blob/40171a6c259edec7827a6693a93955de2bd39e76/Aulas/aula_2_-_uniform_filter/matlab_fspecial.py\n    '''\n    if filter_type == 'gaussian':\n        return fspecial_gaussian(*args, **kwargs)\n    if filter_type == 'laplacian':\n        return fspecial_laplacian(*args, **kwargs)\n\n\n\"\"\"\n# --------------------------------------------\n# degradation models\n# --------------------------------------------\n\"\"\"\n\n\ndef bicubic_degradation(x, sf=3):\n    '''\n    Args:\n        x: HxWxC image, [0, 1]\n        sf: down-scale factor\n    Return:\n        bicubicly downsampled LR image\n    '''\n    x = util.imresize_np(x, scale=1 / sf)\n    return x\n\n\ndef srmd_degradation(x, k, sf=3):\n    ''' blur + bicubic downsampling\n    Args:\n        x: HxWxC image, [0, 1]\n        k: hxw, double\n        sf: down-scale factor\n    Return:\n        downsampled LR image\n    Reference:\n        @inproceedings{zhang2018learning,\n          title={Learning a single convolutional super-resolution network for multiple degradations},\n          author={Zhang, Kai and Zuo, Wangmeng and Zhang, Lei},\n          booktitle={IEEE Conference on Computer Vision and Pattern Recognition},\n          pages={3262--3271},\n          year={2018}\n        }\n    '''\n    x = ndimage.filters.convolve(x, np.expand_dims(k, axis=2), mode='wrap')  # 'nearest' | 'mirror'\n    x = bicubic_degradation(x, sf=sf)\n    return x\n\n\ndef dpsr_degradation(x, k, sf=3):\n    ''' bicubic downsampling + blur\n    Args:\n        x: HxWxC image, [0, 1]\n        k: hxw, double\n        sf: down-scale factor\n    Return:\n        downsampled LR image\n    Reference:\n        @inproceedings{zhang2019deep,\n          title={Deep Plug-and-Play Super-Resolution for Arbitrary Blur Kernels},\n          author={Zhang, Kai and Zuo, Wangmeng and Zhang, Lei},\n          booktitle={IEEE Conference on Computer Vision and Pattern Recognition},\n          pages={1671--1681},\n          year={2019}\n        }\n    '''\n    x = bicubic_degradation(x, sf=sf)\n    x = ndimage.filters.convolve(x, np.expand_dims(k, axis=2), mode='wrap')\n    return x\n\n\ndef classical_degradation(x, k, sf=3):\n    ''' blur + downsampling\n    Args:\n        x: HxWxC image, [0, 1]/[0, 255]\n        k: hxw, double\n        sf: down-scale factor\n    Return:\n        downsampled LR image\n    '''\n    x = ndimage.filters.convolve(x, np.expand_dims(k, axis=2), mode='wrap')\n    # x = filters.correlate(x, np.expand_dims(np.flip(k), axis=2))\n    st = 0\n    return x[st::sf, st::sf, ...]\n\n\ndef add_sharpening(img, weight=0.5, radius=50, threshold=10):\n    \"\"\"USM sharpening. borrowed from real-ESRGAN\n    Input image: I; Blurry image: B.\n    1. K = I + weight * (I - B)\n    2. Mask = 1 if abs(I - B) > threshold, else: 0\n    3. Blur mask:\n    4. Out = Mask * K + (1 - Mask) * I\n    Args:\n        img (Numpy array): Input image, HWC, BGR; float32, [0, 1].\n        weight (float): Sharp weight. Default: 1.\n        radius (float): Kernel size of Gaussian blur. Default: 50.\n        threshold (int):\n    \"\"\"\n    if radius % 2 == 0:\n        radius += 1\n    blur = cv2.GaussianBlur(img, (radius, radius), 0)\n    residual = img - blur\n    mask = np.abs(residual) * 255 > threshold\n    mask = mask.astype('float32')\n    soft_mask = cv2.GaussianBlur(mask, (radius, radius), 0)\n\n    K = img + weight * residual\n    K = np.clip(K, 0, 1)\n    return soft_mask * K + (1 - soft_mask) * img\n\n\ndef add_blur(img, sf=4):\n    wd2 = 4.0 + sf\n    wd = 2.0 + 0.2 * sf\n    if random.random() < 0.5:\n        l1 = wd2 * random.random()\n        l2 = wd2 * random.random()\n        k = anisotropic_Gaussian(ksize=2 * random.randint(2, 11) + 3, theta=random.random() * np.pi, l1=l1, l2=l2)\n    else:\n        k = fspecial('gaussian', 2 * random.randint(2, 11) + 3, wd * random.random())\n    img = ndimage.filters.convolve(img, np.expand_dims(k, axis=2), mode='mirror')\n\n    return img\n\n\ndef add_resize(img, sf=4):\n    rnum = np.random.rand()\n    if rnum > 0.8:  # up\n        sf1 = random.uniform(1, 2)\n    elif rnum < 0.7:  # down\n        sf1 = random.uniform(0.5 / sf, 1)\n    else:\n        sf1 = 1.0\n    img = cv2.resize(img, (int(sf1 * img.shape[1]), int(sf1 * img.shape[0])), interpolation=random.choice([1, 2, 3]))\n    img = np.clip(img, 0.0, 1.0)\n\n    return img\n\n\n# def add_Gaussian_noise(img, noise_level1=2, noise_level2=25):\n#     noise_level = random.randint(noise_level1, noise_level2)\n#     rnum = np.random.rand()\n#     if rnum > 0.6:  # add color Gaussian noise\n#         img += np.random.normal(0, noise_level / 255.0, img.shape).astype(np.float32)\n#     elif rnum < 0.4:  # add grayscale Gaussian noise\n#         img += np.random.normal(0, noise_level / 255.0, (*img.shape[:2], 1)).astype(np.float32)\n#     else:  # add  noise\n#         L = noise_level2 / 255.\n#         D = np.diag(np.random.rand(3))\n#         U = orth(np.random.rand(3, 3))\n#         conv = np.dot(np.dot(np.transpose(U), D), U)\n#         img += np.random.multivariate_normal([0, 0, 0], np.abs(L ** 2 * conv), img.shape[:2]).astype(np.float32)\n#     img = np.clip(img, 0.0, 1.0)\n#     return img\n\ndef add_Gaussian_noise(img, noise_level1=2, noise_level2=25):\n    noise_level = random.randint(noise_level1, noise_level2)\n    rnum = np.random.rand()\n    if rnum > 0.6:  # add color Gaussian noise\n        img = img + np.random.normal(0, noise_level / 255.0, img.shape).astype(np.float32)\n    elif rnum < 0.4:  # add grayscale Gaussian noise\n        img = img + np.random.normal(0, noise_level / 255.0, (*img.shape[:2], 1)).astype(np.float32)\n    else:  # add  noise\n        L = noise_level2 / 255.\n        D = np.diag(np.random.rand(3))\n        U = orth(np.random.rand(3, 3))\n        conv = np.dot(np.dot(np.transpose(U), D), U)\n        img = img + np.random.multivariate_normal([0, 0, 0], np.abs(L ** 2 * conv), img.shape[:2]).astype(np.float32)\n    img = np.clip(img, 0.0, 1.0)\n    return img\n\n\ndef add_speckle_noise(img, noise_level1=2, noise_level2=25):\n    noise_level = random.randint(noise_level1, noise_level2)\n    img = np.clip(img, 0.0, 1.0)\n    rnum = random.random()\n    if rnum > 0.6:\n        img += img * np.random.normal(0, noise_level / 255.0, img.shape).astype(np.float32)\n    elif rnum < 0.4:\n        img += img * np.random.normal(0, noise_level / 255.0, (*img.shape[:2], 1)).astype(np.float32)\n    else:\n        L = noise_level2 / 255.\n        D = np.diag(np.random.rand(3))\n        U = orth(np.random.rand(3, 3))\n        conv = np.dot(np.dot(np.transpose(U), D), U)\n        img += img * np.random.multivariate_normal([0, 0, 0], np.abs(L ** 2 * conv), img.shape[:2]).astype(np.float32)\n    img = np.clip(img, 0.0, 1.0)\n    return img\n\n\ndef add_Poisson_noise(img):\n    img = np.clip((img * 255.0).round(), 0, 255) / 255.\n    vals = 10 ** (2 * random.random() + 2.0)  # [2, 4]\n    if random.random() < 0.5:\n        img = np.random.poisson(img * vals).astype(np.float32) / vals\n    else:\n        img_gray = np.dot(img[..., :3], [0.299, 0.587, 0.114])\n        img_gray = np.clip((img_gray * 255.0).round(), 0, 255) / 255.\n        noise_gray = np.random.poisson(img_gray * vals).astype(np.float32) / vals - img_gray\n        img += noise_gray[:, :, np.newaxis]\n    img = np.clip(img, 0.0, 1.0)\n    return img\n\n\ndef add_JPEG_noise(img):\n    quality_factor = random.randint(30, 95)\n    img = cv2.cvtColor(util.single2uint(img), cv2.COLOR_RGB2BGR)\n    result, encimg = cv2.imencode('.jpg', img, [int(cv2.IMWRITE_JPEG_QUALITY), quality_factor])\n    img = cv2.imdecode(encimg, 1)\n    img = cv2.cvtColor(util.uint2single(img), cv2.COLOR_BGR2RGB)\n    return img\n\n\ndef random_crop(lq, hq, sf=4, lq_patchsize=64):\n    h, w = lq.shape[:2]\n    rnd_h = random.randint(0, h - lq_patchsize)\n    rnd_w = random.randint(0, w - lq_patchsize)\n    lq = lq[rnd_h:rnd_h + lq_patchsize, rnd_w:rnd_w + lq_patchsize, :]\n\n    rnd_h_H, rnd_w_H = int(rnd_h * sf), int(rnd_w * sf)\n    hq = hq[rnd_h_H:rnd_h_H + lq_patchsize * sf, rnd_w_H:rnd_w_H + lq_patchsize * sf, :]\n    return lq, hq\n\n\ndef degradation_bsrgan(img, sf=4, lq_patchsize=72, isp_model=None):\n    \"\"\"\n    This is the degradation model of BSRGAN from the paper\n    \"Designing a Practical Degradation Model for Deep Blind Image Super-Resolution\"\n    ----------\n    img: HXWXC, [0, 1], its size should be large than (lq_patchsizexsf)x(lq_patchsizexsf)\n    sf: scale factor\n    isp_model: camera ISP model\n    Returns\n    -------\n    img: low-quality patch, size: lq_patchsizeXlq_patchsizeXC, range: [0, 1]\n    hq: corresponding high-quality patch, size: (lq_patchsizexsf)X(lq_patchsizexsf)XC, range: [0, 1]\n    \"\"\"\n    isp_prob, jpeg_prob, scale2_prob = 0.25, 0.9, 0.25\n    sf_ori = sf\n\n    h1, w1 = img.shape[:2]\n    img = img.copy()[:w1 - w1 % sf, :h1 - h1 % sf, ...]  # mod crop\n    h, w = img.shape[:2]\n\n    if h < lq_patchsize * sf or w < lq_patchsize * sf:\n        raise ValueError(f'img size ({h1}X{w1}) is too small!')\n\n    hq = img.copy()\n\n    if sf == 4 and random.random() < scale2_prob:  # downsample1\n        if np.random.rand() < 0.5:\n            img = cv2.resize(img, (int(1 / 2 * img.shape[1]), int(1 / 2 * img.shape[0])),\n                             interpolation=random.choice([1, 2, 3]))\n        else:\n            img = util.imresize_np(img, 1 / 2, True)\n        img = np.clip(img, 0.0, 1.0)\n        sf = 2\n\n    shuffle_order = random.sample(range(7), 7)\n    idx1, idx2 = shuffle_order.index(2), shuffle_order.index(3)\n    if idx1 > idx2:  # keep downsample3 last\n        shuffle_order[idx1], shuffle_order[idx2] = shuffle_order[idx2], shuffle_order[idx1]\n\n    for i in shuffle_order:\n\n        if i == 0:\n            img = add_blur(img, sf=sf)\n\n        elif i == 1:\n            img = add_blur(img, sf=sf)\n\n        elif i == 2:\n            a, b = img.shape[1], img.shape[0]\n            # downsample2\n            if random.random() < 0.75:\n                sf1 = random.uniform(1, 2 * sf)\n                img = cv2.resize(img, (int(1 / sf1 * img.shape[1]), int(1 / sf1 * img.shape[0])),\n                                 interpolation=random.choice([1, 2, 3]))\n            else:\n                k = fspecial('gaussian', 25, random.uniform(0.1, 0.6 * sf))\n                k_shifted = shift_pixel(k, sf)\n                k_shifted = k_shifted / k_shifted.sum()  # blur with shifted kernel\n                img = ndimage.filters.convolve(img, np.expand_dims(k_shifted, axis=2), mode='mirror')\n                img = img[0::sf, 0::sf, ...]  # nearest downsampling\n            img = np.clip(img, 0.0, 1.0)\n\n        elif i == 3:\n            # downsample3\n            img = cv2.resize(img, (int(1 / sf * a), int(1 / sf * b)), interpolation=random.choice([1, 2, 3]))\n            img = np.clip(img, 0.0, 1.0)\n\n        elif i == 4:\n            # add Gaussian noise\n            img = add_Gaussian_noise(img, noise_level1=2, noise_level2=25)\n\n        elif i == 5:\n            # add JPEG noise\n            if random.random() < jpeg_prob:\n                img = add_JPEG_noise(img)\n\n        elif i == 6:\n            # add processed camera sensor noise\n            if random.random() < isp_prob and isp_model is not None:\n                with torch.no_grad():\n                    img, hq = isp_model.forward(img.copy(), hq)\n\n    # add final JPEG compression noise\n    img = add_JPEG_noise(img)\n\n    # random crop\n    img, hq = random_crop(img, hq, sf_ori, lq_patchsize)\n\n    return img, hq\n\n\n# todo no isp_model?\ndef degradation_bsrgan_variant(image, sf=4, isp_model=None):\n    \"\"\"\n    This is the degradation model of BSRGAN from the paper\n    \"Designing a Practical Degradation Model for Deep Blind Image Super-Resolution\"\n    ----------\n    sf: scale factor\n    isp_model: camera ISP model\n    Returns\n    -------\n    img: low-quality patch, size: lq_patchsizeXlq_patchsizeXC, range: [0, 1]\n    hq: corresponding high-quality patch, size: (lq_patchsizexsf)X(lq_patchsizexsf)XC, range: [0, 1]\n    \"\"\"\n    image = util.uint2single(image)\n    isp_prob, jpeg_prob, scale2_prob = 0.25, 0.9, 0.25\n    sf_ori = sf\n\n    h1, w1 = image.shape[:2]\n    image = image.copy()[:w1 - w1 % sf, :h1 - h1 % sf, ...]  # mod crop\n    h, w = image.shape[:2]\n\n    hq = image.copy()\n\n    if sf == 4 and random.random() < scale2_prob:  # downsample1\n        if np.random.rand() < 0.5:\n            image = cv2.resize(image, (int(1 / 2 * image.shape[1]), int(1 / 2 * image.shape[0])),\n                               interpolation=random.choice([1, 2, 3]))\n        else:\n            image = util.imresize_np(image, 1 / 2, True)\n        image = np.clip(image, 0.0, 1.0)\n        sf = 2\n\n    shuffle_order = random.sample(range(7), 7)\n    idx1, idx2 = shuffle_order.index(2), shuffle_order.index(3)\n    if idx1 > idx2:  # keep downsample3 last\n        shuffle_order[idx1], shuffle_order[idx2] = shuffle_order[idx2], shuffle_order[idx1]\n\n    for i in shuffle_order:\n\n        if i == 0:\n            image = add_blur(image, sf=sf)\n\n        elif i == 1:\n            image = add_blur(image, sf=sf)\n\n        elif i == 2:\n            a, b = image.shape[1], image.shape[0]\n            # downsample2\n            if random.random() < 0.75:\n                sf1 = random.uniform(1, 2 * sf)\n                image = cv2.resize(image, (int(1 / sf1 * image.shape[1]), int(1 / sf1 * image.shape[0])),\n                                   interpolation=random.choice([1, 2, 3]))\n            else:\n                k = fspecial('gaussian', 25, random.uniform(0.1, 0.6 * sf))\n                k_shifted = shift_pixel(k, sf)\n                k_shifted = k_shifted / k_shifted.sum()  # blur with shifted kernel\n                image = ndimage.filters.convolve(image, np.expand_dims(k_shifted, axis=2), mode='mirror')\n                image = image[0::sf, 0::sf, ...]  # nearest downsampling\n            image = np.clip(image, 0.0, 1.0)\n\n        elif i == 3:\n            # downsample3\n            image = cv2.resize(image, (int(1 / sf * a), int(1 / sf * b)), interpolation=random.choice([1, 2, 3]))\n            image = np.clip(image, 0.0, 1.0)\n\n        elif i == 4:\n            # add Gaussian noise\n            image = add_Gaussian_noise(image, noise_level1=2, noise_level2=25)\n\n        elif i == 5:\n            # add JPEG noise\n            if random.random() < jpeg_prob:\n                image = add_JPEG_noise(image)\n\n        # elif i == 6:\n        #     # add processed camera sensor noise\n        #     if random.random() < isp_prob and isp_model is not None:\n        #         with torch.no_grad():\n        #             img, hq = isp_model.forward(img.copy(), hq)\n\n    # add final JPEG compression noise\n    image = add_JPEG_noise(image)\n    image = util.single2uint(image)\n    example = {\"image\":image}\n    return example\n\n\n# TODO incase there is a pickle error one needs to replace a += x with a = a + x in add_speckle_noise etc...\ndef degradation_bsrgan_plus(img, sf=4, shuffle_prob=0.5, use_sharp=True, lq_patchsize=64, isp_model=None):\n    \"\"\"\n    This is an extended degradation model by combining\n    the degradation models of BSRGAN and Real-ESRGAN\n    ----------\n    img: HXWXC, [0, 1], its size should be large than (lq_patchsizexsf)x(lq_patchsizexsf)\n    sf: scale factor\n    use_shuffle: the degradation shuffle\n    use_sharp: sharpening the img\n    Returns\n    -------\n    img: low-quality patch, size: lq_patchsizeXlq_patchsizeXC, range: [0, 1]\n    hq: corresponding high-quality patch, size: (lq_patchsizexsf)X(lq_patchsizexsf)XC, range: [0, 1]\n    \"\"\"\n\n    h1, w1 = img.shape[:2]\n    img = img.copy()[:w1 - w1 % sf, :h1 - h1 % sf, ...]  # mod crop\n    h, w = img.shape[:2]\n\n    if h < lq_patchsize * sf or w < lq_patchsize * sf:\n        raise ValueError(f'img size ({h1}X{w1}) is too small!')\n\n    if use_sharp:\n        img = add_sharpening(img)\n    hq = img.copy()\n\n    if random.random() < shuffle_prob:\n        shuffle_order = random.sample(range(13), 13)\n    else:\n        shuffle_order = list(range(13))\n        # local shuffle for noise, JPEG is always the last one\n        shuffle_order[2:6] = random.sample(shuffle_order[2:6], len(range(2, 6)))\n        shuffle_order[9:13] = random.sample(shuffle_order[9:13], len(range(9, 13)))\n\n    poisson_prob, speckle_prob, isp_prob = 0.1, 0.1, 0.1\n\n    for i in shuffle_order:\n        if i == 0:\n            img = add_blur(img, sf=sf)\n        elif i == 1:\n            img = add_resize(img, sf=sf)\n        elif i == 2:\n            img = add_Gaussian_noise(img, noise_level1=2, noise_level2=25)\n        elif i == 3:\n            if random.random() < poisson_prob:\n                img = add_Poisson_noise(img)\n        elif i == 4:\n            if random.random() < speckle_prob:\n                img = add_speckle_noise(img)\n        elif i == 5:\n            if random.random() < isp_prob and isp_model is not None:\n                with torch.no_grad():\n                    img, hq = isp_model.forward(img.copy(), hq)\n        elif i == 6:\n            img = add_JPEG_noise(img)\n        elif i == 7:\n            img = add_blur(img, sf=sf)\n        elif i == 8:\n            img = add_resize(img, sf=sf)\n        elif i == 9:\n            img = add_Gaussian_noise(img, noise_level1=2, noise_level2=25)\n        elif i == 10:\n            if random.random() < poisson_prob:\n                img = add_Poisson_noise(img)\n        elif i == 11:\n            if random.random() < speckle_prob:\n                img = add_speckle_noise(img)\n        elif i == 12:\n            if random.random() < isp_prob and isp_model is not None:\n                with torch.no_grad():\n                    img, hq = isp_model.forward(img.copy(), hq)\n        else:\n            print('check the shuffle!')\n\n    # resize to desired size\n    img = cv2.resize(img, (int(1 / sf * hq.shape[1]), int(1 / sf * hq.shape[0])),\n                     interpolation=random.choice([1, 2, 3]))\n\n    # add final JPEG compression noise\n    img = add_JPEG_noise(img)\n\n    # random crop\n    img, hq = random_crop(img, hq, sf, lq_patchsize)\n\n    return img, hq\n\n\nif __name__ == '__main__':\n\tprint(\"hey\")\n\timg = util.imread_uint('utils/test.png', 3)\n\tprint(img)\n\timg = util.uint2single(img)\n\tprint(img)\n\timg = img[:448, :448]\n\th = img.shape[0] // 4\n\tprint(\"resizing to\", h)\n\tsf = 4\n\tdeg_fn = partial(degradation_bsrgan_variant, sf=sf)\n\tfor i in range(20):\n\t\tprint(i)\n\t\timg_lq = deg_fn(img)\n\t\tprint(img_lq)\n\t\timg_lq_bicubic = albumentations.SmallestMaxSize(max_size=h, interpolation=cv2.INTER_CUBIC)(image=img)[\"image\"]\n\t\tprint(img_lq.shape)\n\t\tprint(\"bicubic\", img_lq_bicubic.shape)\n\t\tprint(img_hq.shape)\n\t\tlq_nearest = cv2.resize(util.single2uint(img_lq), (int(sf * img_lq.shape[1]), int(sf * img_lq.shape[0])),\n\t\t                        interpolation=0)\n\t\tlq_bicubic_nearest = cv2.resize(util.single2uint(img_lq_bicubic), (int(sf * img_lq.shape[1]), int(sf * img_lq.shape[0])),\n\t\t                        interpolation=0)\n\t\timg_concat = np.concatenate([lq_bicubic_nearest, lq_nearest, util.single2uint(img_hq)], axis=1)\n\t\tutil.imsave(img_concat, str(i) + '.png')\n\n\n", "ldm/modules/image_degradation/utils_image.py": "import os\nimport math\nimport random\nimport numpy as np\nimport torch\nimport cv2\nfrom torchvision.utils import make_grid\nfrom datetime import datetime\n#import matplotlib.pyplot as plt   # TODO: check with Dominik, also bsrgan.py vs bsrgan_light.py\n\n\nos.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n\n\n'''\n# --------------------------------------------\n# Kai Zhang (github: https://github.com/cszn)\n# 03/Mar/2019\n# --------------------------------------------\n# https://github.com/twhui/SRGAN-pyTorch\n# https://github.com/xinntao/BasicSR\n# --------------------------------------------\n'''\n\n\nIMG_EXTENSIONS = ['.jpg', '.JPG', '.jpeg', '.JPEG', '.png', '.PNG', '.ppm', '.PPM', '.bmp', '.BMP', '.tif']\n\n\ndef is_image_file(filename):\n    return any(filename.endswith(extension) for extension in IMG_EXTENSIONS)\n\n\ndef get_timestamp():\n    return datetime.now().strftime('%y%m%d-%H%M%S')\n\n\ndef imshow(x, title=None, cbar=False, figsize=None):\n    plt.figure(figsize=figsize)\n    plt.imshow(np.squeeze(x), interpolation='nearest', cmap='gray')\n    if title:\n        plt.title(title)\n    if cbar:\n        plt.colorbar()\n    plt.show()\n\n\ndef surf(Z, cmap='rainbow', figsize=None):\n    plt.figure(figsize=figsize)\n    ax3 = plt.axes(projection='3d')\n\n    w, h = Z.shape[:2]\n    xx = np.arange(0,w,1)\n    yy = np.arange(0,h,1)\n    X, Y = np.meshgrid(xx, yy)\n    ax3.plot_surface(X,Y,Z,cmap=cmap)\n    #ax3.contour(X,Y,Z, zdim='z',offset=-2\uff0ccmap=cmap)\n    plt.show()\n\n\n'''\n# --------------------------------------------\n# get image pathes\n# --------------------------------------------\n'''\n\n\ndef get_image_paths(dataroot):\n    paths = None  # return None if dataroot is None\n    if dataroot is not None:\n        paths = sorted(_get_paths_from_images(dataroot))\n    return paths\n\n\ndef _get_paths_from_images(path):\n    assert os.path.isdir(path), '{:s} is not a valid directory'.format(path)\n    images = []\n    for dirpath, _, fnames in sorted(os.walk(path)):\n        for fname in sorted(fnames):\n            if is_image_file(fname):\n                img_path = os.path.join(dirpath, fname)\n                images.append(img_path)\n    assert images, '{:s} has no valid image file'.format(path)\n    return images\n\n\n'''\n# --------------------------------------------\n# split large images into small images \n# --------------------------------------------\n'''\n\n\ndef patches_from_image(img, p_size=512, p_overlap=64, p_max=800):\n    w, h = img.shape[:2]\n    patches = []\n    if w > p_max and h > p_max:\n        w1 = list(np.arange(0, w-p_size, p_size-p_overlap, dtype=np.int))\n        h1 = list(np.arange(0, h-p_size, p_size-p_overlap, dtype=np.int))\n        w1.append(w-p_size)\n        h1.append(h-p_size)\n#        print(w1)\n#        print(h1)\n        for i in w1:\n            for j in h1:\n                patches.append(img[i:i+p_size, j:j+p_size,:])\n    else:\n        patches.append(img)\n\n    return patches\n\n\ndef imssave(imgs, img_path):\n    \"\"\"\n    imgs: list, N images of size WxHxC\n    \"\"\"\n    img_name, ext = os.path.splitext(os.path.basename(img_path))\n\n    for i, img in enumerate(imgs):\n        if img.ndim == 3:\n            img = img[:, :, [2, 1, 0]]\n        new_path = os.path.join(os.path.dirname(img_path), img_name+str('_s{:04d}'.format(i))+'.png')\n        cv2.imwrite(new_path, img)\n\n\ndef split_imageset(original_dataroot, taget_dataroot, n_channels=3, p_size=800, p_overlap=96, p_max=1000):\n    \"\"\"\n    split the large images from original_dataroot into small overlapped images with size (p_size)x(p_size),\n    and save them into taget_dataroot; only the images with larger size than (p_max)x(p_max)\n    will be splitted.\n    Args:\n        original_dataroot:\n        taget_dataroot:\n        p_size: size of small images\n        p_overlap: patch size in training is a good choice\n        p_max: images with smaller size than (p_max)x(p_max) keep unchanged.\n    \"\"\"\n    paths = get_image_paths(original_dataroot)\n    for img_path in paths:\n        # img_name, ext = os.path.splitext(os.path.basename(img_path))\n        img = imread_uint(img_path, n_channels=n_channels)\n        patches = patches_from_image(img, p_size, p_overlap, p_max)\n        imssave(patches, os.path.join(taget_dataroot,os.path.basename(img_path)))\n        #if original_dataroot == taget_dataroot:\n        #del img_path\n\n'''\n# --------------------------------------------\n# makedir\n# --------------------------------------------\n'''\n\n\ndef mkdir(path):\n    if not os.path.exists(path):\n        os.makedirs(path)\n\n\ndef mkdirs(paths):\n    if isinstance(paths, str):\n        mkdir(paths)\n    else:\n        for path in paths:\n            mkdir(path)\n\n\ndef mkdir_and_rename(path):\n    if os.path.exists(path):\n        new_name = path + '_archived_' + get_timestamp()\n        print('Path already exists. Rename it to [{:s}]'.format(new_name))\n        os.rename(path, new_name)\n    os.makedirs(path)\n\n\n'''\n# --------------------------------------------\n# read image from path\n# opencv is fast, but read BGR numpy image\n# --------------------------------------------\n'''\n\n\n# --------------------------------------------\n# get uint8 image of size HxWxn_channles (RGB)\n# --------------------------------------------\ndef imread_uint(path, n_channels=3):\n    #  input: path\n    # output: HxWx3(RGB or GGG), or HxWx1 (G)\n    if n_channels == 1:\n        img = cv2.imread(path, 0)  # cv2.IMREAD_GRAYSCALE\n        img = np.expand_dims(img, axis=2)  # HxWx1\n    elif n_channels == 3:\n        img = cv2.imread(path, cv2.IMREAD_UNCHANGED)  # BGR or G\n        if img.ndim == 2:\n            img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)  # GGG\n        else:\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # RGB\n    return img\n\n\n# --------------------------------------------\n# matlab's imwrite\n# --------------------------------------------\ndef imsave(img, img_path):\n    img = np.squeeze(img)\n    if img.ndim == 3:\n        img = img[:, :, [2, 1, 0]]\n    cv2.imwrite(img_path, img)\n\ndef imwrite(img, img_path):\n    img = np.squeeze(img)\n    if img.ndim == 3:\n        img = img[:, :, [2, 1, 0]]\n    cv2.imwrite(img_path, img)\n\n\n\n# --------------------------------------------\n# get single image of size HxWxn_channles (BGR)\n# --------------------------------------------\ndef read_img(path):\n    # read image by cv2\n    # return: Numpy float32, HWC, BGR, [0,1]\n    img = cv2.imread(path, cv2.IMREAD_UNCHANGED)  # cv2.IMREAD_GRAYSCALE\n    img = img.astype(np.float32) / 255.\n    if img.ndim == 2:\n        img = np.expand_dims(img, axis=2)\n    # some images have 4 channels\n    if img.shape[2] > 3:\n        img = img[:, :, :3]\n    return img\n\n\n'''\n# --------------------------------------------\n# image format conversion\n# --------------------------------------------\n# numpy(single) <--->  numpy(unit)\n# numpy(single) <--->  tensor\n# numpy(unit)   <--->  tensor\n# --------------------------------------------\n'''\n\n\n# --------------------------------------------\n# numpy(single) [0, 1] <--->  numpy(unit)\n# --------------------------------------------\n\n\ndef uint2single(img):\n\n    return np.float32(img/255.)\n\n\ndef single2uint(img):\n\n    return np.uint8((img.clip(0, 1)*255.).round())\n\n\ndef uint162single(img):\n\n    return np.float32(img/65535.)\n\n\ndef single2uint16(img):\n\n    return np.uint16((img.clip(0, 1)*65535.).round())\n\n\n# --------------------------------------------\n# numpy(unit) (HxWxC or HxW) <--->  tensor\n# --------------------------------------------\n\n\n# convert uint to 4-dimensional torch tensor\ndef uint2tensor4(img):\n    if img.ndim == 2:\n        img = np.expand_dims(img, axis=2)\n    return torch.from_numpy(np.ascontiguousarray(img)).permute(2, 0, 1).float().div(255.).unsqueeze(0)\n\n\n# convert uint to 3-dimensional torch tensor\ndef uint2tensor3(img):\n    if img.ndim == 2:\n        img = np.expand_dims(img, axis=2)\n    return torch.from_numpy(np.ascontiguousarray(img)).permute(2, 0, 1).float().div(255.)\n\n\n# convert 2/3/4-dimensional torch tensor to uint\ndef tensor2uint(img):\n    img = img.data.squeeze().float().clamp_(0, 1).cpu().numpy()\n    if img.ndim == 3:\n        img = np.transpose(img, (1, 2, 0))\n    return np.uint8((img*255.0).round())\n\n\n# --------------------------------------------\n# numpy(single) (HxWxC) <--->  tensor\n# --------------------------------------------\n\n\n# convert single (HxWxC) to 3-dimensional torch tensor\ndef single2tensor3(img):\n    return torch.from_numpy(np.ascontiguousarray(img)).permute(2, 0, 1).float()\n\n\n# convert single (HxWxC) to 4-dimensional torch tensor\ndef single2tensor4(img):\n    return torch.from_numpy(np.ascontiguousarray(img)).permute(2, 0, 1).float().unsqueeze(0)\n\n\n# convert torch tensor to single\ndef tensor2single(img):\n    img = img.data.squeeze().float().cpu().numpy()\n    if img.ndim == 3:\n        img = np.transpose(img, (1, 2, 0))\n\n    return img\n\n# convert torch tensor to single\ndef tensor2single3(img):\n    img = img.data.squeeze().float().cpu().numpy()\n    if img.ndim == 3:\n        img = np.transpose(img, (1, 2, 0))\n    elif img.ndim == 2:\n        img = np.expand_dims(img, axis=2)\n    return img\n\n\ndef single2tensor5(img):\n    return torch.from_numpy(np.ascontiguousarray(img)).permute(2, 0, 1, 3).float().unsqueeze(0)\n\n\ndef single32tensor5(img):\n    return torch.from_numpy(np.ascontiguousarray(img)).float().unsqueeze(0).unsqueeze(0)\n\n\ndef single42tensor4(img):\n    return torch.from_numpy(np.ascontiguousarray(img)).permute(2, 0, 1, 3).float()\n\n\n# from skimage.io import imread, imsave\ndef tensor2img(tensor, out_type=np.uint8, min_max=(0, 1)):\n    '''\n    Converts a torch Tensor into an image Numpy array of BGR channel order\n    Input: 4D(B,(3/1),H,W), 3D(C,H,W), or 2D(H,W), any range, RGB channel order\n    Output: 3D(H,W,C) or 2D(H,W), [0,255], np.uint8 (default)\n    '''\n    tensor = tensor.squeeze().float().cpu().clamp_(*min_max)  # squeeze first, then clamp\n    tensor = (tensor - min_max[0]) / (min_max[1] - min_max[0])  # to range [0,1]\n    n_dim = tensor.dim()\n    if n_dim == 4:\n        n_img = len(tensor)\n        img_np = make_grid(tensor, nrow=int(math.sqrt(n_img)), normalize=False).numpy()\n        img_np = np.transpose(img_np[[2, 1, 0], :, :], (1, 2, 0))  # HWC, BGR\n    elif n_dim == 3:\n        img_np = tensor.numpy()\n        img_np = np.transpose(img_np[[2, 1, 0], :, :], (1, 2, 0))  # HWC, BGR\n    elif n_dim == 2:\n        img_np = tensor.numpy()\n    else:\n        raise TypeError(\n            'Only support 4D, 3D and 2D tensor. But received with dimension: {:d}'.format(n_dim))\n    if out_type == np.uint8:\n        img_np = (img_np * 255.0).round()\n        # Important. Unlike matlab, numpy.unit8() WILL NOT round by default.\n    return img_np.astype(out_type)\n\n\n'''\n# --------------------------------------------\n# Augmentation, flipe and/or rotate\n# --------------------------------------------\n# The following two are enough.\n# (1) augmet_img: numpy image of WxHxC or WxH\n# (2) augment_img_tensor4: tensor image 1xCxWxH\n# --------------------------------------------\n'''\n\n\ndef augment_img(img, mode=0):\n    '''Kai Zhang (github: https://github.com/cszn)\n    '''\n    if mode == 0:\n        return img\n    elif mode == 1:\n        return np.flipud(np.rot90(img))\n    elif mode == 2:\n        return np.flipud(img)\n    elif mode == 3:\n        return np.rot90(img, k=3)\n    elif mode == 4:\n        return np.flipud(np.rot90(img, k=2))\n    elif mode == 5:\n        return np.rot90(img)\n    elif mode == 6:\n        return np.rot90(img, k=2)\n    elif mode == 7:\n        return np.flipud(np.rot90(img, k=3))\n\n\ndef augment_img_tensor4(img, mode=0):\n    '''Kai Zhang (github: https://github.com/cszn)\n    '''\n    if mode == 0:\n        return img\n    elif mode == 1:\n        return img.rot90(1, [2, 3]).flip([2])\n    elif mode == 2:\n        return img.flip([2])\n    elif mode == 3:\n        return img.rot90(3, [2, 3])\n    elif mode == 4:\n        return img.rot90(2, [2, 3]).flip([2])\n    elif mode == 5:\n        return img.rot90(1, [2, 3])\n    elif mode == 6:\n        return img.rot90(2, [2, 3])\n    elif mode == 7:\n        return img.rot90(3, [2, 3]).flip([2])\n\n\ndef augment_img_tensor(img, mode=0):\n    '''Kai Zhang (github: https://github.com/cszn)\n    '''\n    img_size = img.size()\n    img_np = img.data.cpu().numpy()\n    if len(img_size) == 3:\n        img_np = np.transpose(img_np, (1, 2, 0))\n    elif len(img_size) == 4:\n        img_np = np.transpose(img_np, (2, 3, 1, 0))\n    img_np = augment_img(img_np, mode=mode)\n    img_tensor = torch.from_numpy(np.ascontiguousarray(img_np))\n    if len(img_size) == 3:\n        img_tensor = img_tensor.permute(2, 0, 1)\n    elif len(img_size) == 4:\n        img_tensor = img_tensor.permute(3, 2, 0, 1)\n\n    return img_tensor.type_as(img)\n\n\ndef augment_img_np3(img, mode=0):\n    if mode == 0:\n        return img\n    elif mode == 1:\n        return img.transpose(1, 0, 2)\n    elif mode == 2:\n        return img[::-1, :, :]\n    elif mode == 3:\n        img = img[::-1, :, :]\n        img = img.transpose(1, 0, 2)\n        return img\n    elif mode == 4:\n        return img[:, ::-1, :]\n    elif mode == 5:\n        img = img[:, ::-1, :]\n        img = img.transpose(1, 0, 2)\n        return img\n    elif mode == 6:\n        img = img[:, ::-1, :]\n        img = img[::-1, :, :]\n        return img\n    elif mode == 7:\n        img = img[:, ::-1, :]\n        img = img[::-1, :, :]\n        img = img.transpose(1, 0, 2)\n        return img\n\n\ndef augment_imgs(img_list, hflip=True, rot=True):\n    # horizontal flip OR rotate\n    hflip = hflip and random.random() < 0.5\n    vflip = rot and random.random() < 0.5\n    rot90 = rot and random.random() < 0.5\n\n    def _augment(img):\n        if hflip:\n            img = img[:, ::-1, :]\n        if vflip:\n            img = img[::-1, :, :]\n        if rot90:\n            img = img.transpose(1, 0, 2)\n        return img\n\n    return [_augment(img) for img in img_list]\n\n\n'''\n# --------------------------------------------\n# modcrop and shave\n# --------------------------------------------\n'''\n\n\ndef modcrop(img_in, scale):\n    # img_in: Numpy, HWC or HW\n    img = np.copy(img_in)\n    if img.ndim == 2:\n        H, W = img.shape\n        H_r, W_r = H % scale, W % scale\n        img = img[:H - H_r, :W - W_r]\n    elif img.ndim == 3:\n        H, W, C = img.shape\n        H_r, W_r = H % scale, W % scale\n        img = img[:H - H_r, :W - W_r, :]\n    else:\n        raise ValueError('Wrong img ndim: [{:d}].'.format(img.ndim))\n    return img\n\n\ndef shave(img_in, border=0):\n    # img_in: Numpy, HWC or HW\n    img = np.copy(img_in)\n    h, w = img.shape[:2]\n    img = img[border:h-border, border:w-border]\n    return img\n\n\n'''\n# --------------------------------------------\n# image processing process on numpy image\n# channel_convert(in_c, tar_type, img_list):\n# rgb2ycbcr(img, only_y=True):\n# bgr2ycbcr(img, only_y=True):\n# ycbcr2rgb(img):\n# --------------------------------------------\n'''\n\n\ndef rgb2ycbcr(img, only_y=True):\n    '''same as matlab rgb2ycbcr\n    only_y: only return Y channel\n    Input:\n        uint8, [0, 255]\n        float, [0, 1]\n    '''\n    in_img_type = img.dtype\n    img.astype(np.float32)\n    if in_img_type != np.uint8:\n        img *= 255.\n    # convert\n    if only_y:\n        rlt = np.dot(img, [65.481, 128.553, 24.966]) / 255.0 + 16.0\n    else:\n        rlt = np.matmul(img, [[65.481, -37.797, 112.0], [128.553, -74.203, -93.786],\n                              [24.966, 112.0, -18.214]]) / 255.0 + [16, 128, 128]\n    if in_img_type == np.uint8:\n        rlt = rlt.round()\n    else:\n        rlt /= 255.\n    return rlt.astype(in_img_type)\n\n\ndef ycbcr2rgb(img):\n    '''same as matlab ycbcr2rgb\n    Input:\n        uint8, [0, 255]\n        float, [0, 1]\n    '''\n    in_img_type = img.dtype\n    img.astype(np.float32)\n    if in_img_type != np.uint8:\n        img *= 255.\n    # convert\n    rlt = np.matmul(img, [[0.00456621, 0.00456621, 0.00456621], [0, -0.00153632, 0.00791071],\n                          [0.00625893, -0.00318811, 0]]) * 255.0 + [-222.921, 135.576, -276.836]\n    if in_img_type == np.uint8:\n        rlt = rlt.round()\n    else:\n        rlt /= 255.\n    return rlt.astype(in_img_type)\n\n\ndef bgr2ycbcr(img, only_y=True):\n    '''bgr version of rgb2ycbcr\n    only_y: only return Y channel\n    Input:\n        uint8, [0, 255]\n        float, [0, 1]\n    '''\n    in_img_type = img.dtype\n    img.astype(np.float32)\n    if in_img_type != np.uint8:\n        img *= 255.\n    # convert\n    if only_y:\n        rlt = np.dot(img, [24.966, 128.553, 65.481]) / 255.0 + 16.0\n    else:\n        rlt = np.matmul(img, [[24.966, 112.0, -18.214], [128.553, -74.203, -93.786],\n                              [65.481, -37.797, 112.0]]) / 255.0 + [16, 128, 128]\n    if in_img_type == np.uint8:\n        rlt = rlt.round()\n    else:\n        rlt /= 255.\n    return rlt.astype(in_img_type)\n\n\ndef channel_convert(in_c, tar_type, img_list):\n    # conversion among BGR, gray and y\n    if in_c == 3 and tar_type == 'gray':  # BGR to gray\n        gray_list = [cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) for img in img_list]\n        return [np.expand_dims(img, axis=2) for img in gray_list]\n    elif in_c == 3 and tar_type == 'y':  # BGR to y\n        y_list = [bgr2ycbcr(img, only_y=True) for img in img_list]\n        return [np.expand_dims(img, axis=2) for img in y_list]\n    elif in_c == 1 and tar_type == 'RGB':  # gray/y to BGR\n        return [cv2.cvtColor(img, cv2.COLOR_GRAY2BGR) for img in img_list]\n    else:\n        return img_list\n\n\n'''\n# --------------------------------------------\n# metric, PSNR and SSIM\n# --------------------------------------------\n'''\n\n\n# --------------------------------------------\n# PSNR\n# --------------------------------------------\ndef calculate_psnr(img1, img2, border=0):\n    # img1 and img2 have range [0, 255]\n    #img1 = img1.squeeze()\n    #img2 = img2.squeeze()\n    if not img1.shape == img2.shape:\n        raise ValueError('Input images must have the same dimensions.')\n    h, w = img1.shape[:2]\n    img1 = img1[border:h-border, border:w-border]\n    img2 = img2[border:h-border, border:w-border]\n\n    img1 = img1.astype(np.float64)\n    img2 = img2.astype(np.float64)\n    mse = np.mean((img1 - img2)**2)\n    if mse == 0:\n        return float('inf')\n    return 20 * math.log10(255.0 / math.sqrt(mse))\n\n\n# --------------------------------------------\n# SSIM\n# --------------------------------------------\ndef calculate_ssim(img1, img2, border=0):\n    '''calculate SSIM\n    the same outputs as MATLAB's\n    img1, img2: [0, 255]\n    '''\n    #img1 = img1.squeeze()\n    #img2 = img2.squeeze()\n    if not img1.shape == img2.shape:\n        raise ValueError('Input images must have the same dimensions.')\n    h, w = img1.shape[:2]\n    img1 = img1[border:h-border, border:w-border]\n    img2 = img2[border:h-border, border:w-border]\n\n    if img1.ndim == 2:\n        return ssim(img1, img2)\n    elif img1.ndim == 3:\n        if img1.shape[2] == 3:\n            ssims = []\n            for i in range(3):\n                ssims.append(ssim(img1[:,:,i], img2[:,:,i]))\n            return np.array(ssims).mean()\n        elif img1.shape[2] == 1:\n            return ssim(np.squeeze(img1), np.squeeze(img2))\n    else:\n        raise ValueError('Wrong input image dimensions.')\n\n\ndef ssim(img1, img2):\n    C1 = (0.01 * 255)**2\n    C2 = (0.03 * 255)**2\n\n    img1 = img1.astype(np.float64)\n    img2 = img2.astype(np.float64)\n    kernel = cv2.getGaussianKernel(11, 1.5)\n    window = np.outer(kernel, kernel.transpose())\n\n    mu1 = cv2.filter2D(img1, -1, window)[5:-5, 5:-5]  # valid\n    mu2 = cv2.filter2D(img2, -1, window)[5:-5, 5:-5]\n    mu1_sq = mu1**2\n    mu2_sq = mu2**2\n    mu1_mu2 = mu1 * mu2\n    sigma1_sq = cv2.filter2D(img1**2, -1, window)[5:-5, 5:-5] - mu1_sq\n    sigma2_sq = cv2.filter2D(img2**2, -1, window)[5:-5, 5:-5] - mu2_sq\n    sigma12 = cv2.filter2D(img1 * img2, -1, window)[5:-5, 5:-5] - mu1_mu2\n\n    ssim_map = ((2 * mu1_mu2 + C1) * (2 * sigma12 + C2)) / ((mu1_sq + mu2_sq + C1) *\n                                                            (sigma1_sq + sigma2_sq + C2))\n    return ssim_map.mean()\n\n\n'''\n# --------------------------------------------\n# matlab's bicubic imresize (numpy and torch) [0, 1]\n# --------------------------------------------\n'''\n\n\n# matlab 'imresize' function, now only support 'bicubic'\ndef cubic(x):\n    absx = torch.abs(x)\n    absx2 = absx**2\n    absx3 = absx**3\n    return (1.5*absx3 - 2.5*absx2 + 1) * ((absx <= 1).type_as(absx)) + \\\n        (-0.5*absx3 + 2.5*absx2 - 4*absx + 2) * (((absx > 1)*(absx <= 2)).type_as(absx))\n\n\ndef calculate_weights_indices(in_length, out_length, scale, kernel, kernel_width, antialiasing):\n    if (scale < 1) and (antialiasing):\n        # Use a modified kernel to simultaneously interpolate and antialias- larger kernel width\n        kernel_width = kernel_width / scale\n\n    # Output-space coordinates\n    x = torch.linspace(1, out_length, out_length)\n\n    # Input-space coordinates. Calculate the inverse mapping such that 0.5\n    # in output space maps to 0.5 in input space, and 0.5+scale in output\n    # space maps to 1.5 in input space.\n    u = x / scale + 0.5 * (1 - 1 / scale)\n\n    # What is the left-most pixel that can be involved in the computation?\n    left = torch.floor(u - kernel_width / 2)\n\n    # What is the maximum number of pixels that can be involved in the\n    # computation?  Note: it's OK to use an extra pixel here; if the\n    # corresponding weights are all zero, it will be eliminated at the end\n    # of this function.\n    P = math.ceil(kernel_width) + 2\n\n    # The indices of the input pixels involved in computing the k-th output\n    # pixel are in row k of the indices matrix.\n    indices = left.view(out_length, 1).expand(out_length, P) + torch.linspace(0, P - 1, P).view(\n        1, P).expand(out_length, P)\n\n    # The weights used to compute the k-th output pixel are in row k of the\n    # weights matrix.\n    distance_to_center = u.view(out_length, 1).expand(out_length, P) - indices\n    # apply cubic kernel\n    if (scale < 1) and (antialiasing):\n        weights = scale * cubic(distance_to_center * scale)\n    else:\n        weights = cubic(distance_to_center)\n    # Normalize the weights matrix so that each row sums to 1.\n    weights_sum = torch.sum(weights, 1).view(out_length, 1)\n    weights = weights / weights_sum.expand(out_length, P)\n\n    # If a column in weights is all zero, get rid of it. only consider the first and last column.\n    weights_zero_tmp = torch.sum((weights == 0), 0)\n    if not math.isclose(weights_zero_tmp[0], 0, rel_tol=1e-6):\n        indices = indices.narrow(1, 1, P - 2)\n        weights = weights.narrow(1, 1, P - 2)\n    if not math.isclose(weights_zero_tmp[-1], 0, rel_tol=1e-6):\n        indices = indices.narrow(1, 0, P - 2)\n        weights = weights.narrow(1, 0, P - 2)\n    weights = weights.contiguous()\n    indices = indices.contiguous()\n    sym_len_s = -indices.min() + 1\n    sym_len_e = indices.max() - in_length\n    indices = indices + sym_len_s - 1\n    return weights, indices, int(sym_len_s), int(sym_len_e)\n\n\n# --------------------------------------------\n# imresize for tensor image [0, 1]\n# --------------------------------------------\ndef imresize(img, scale, antialiasing=True):\n    # Now the scale should be the same for H and W\n    # input: img: pytorch tensor, CHW or HW [0,1]\n    # output: CHW or HW [0,1] w/o round\n    need_squeeze = True if img.dim() == 2 else False\n    if need_squeeze:\n        img.unsqueeze_(0)\n    in_C, in_H, in_W = img.size()\n    out_C, out_H, out_W = in_C, math.ceil(in_H * scale), math.ceil(in_W * scale)\n    kernel_width = 4\n    kernel = 'cubic'\n\n    # Return the desired dimension order for performing the resize.  The\n    # strategy is to perform the resize first along the dimension with the\n    # smallest scale factor.\n    # Now we do not support this.\n\n    # get weights and indices\n    weights_H, indices_H, sym_len_Hs, sym_len_He = calculate_weights_indices(\n        in_H, out_H, scale, kernel, kernel_width, antialiasing)\n    weights_W, indices_W, sym_len_Ws, sym_len_We = calculate_weights_indices(\n        in_W, out_W, scale, kernel, kernel_width, antialiasing)\n    # process H dimension\n    # symmetric copying\n    img_aug = torch.FloatTensor(in_C, in_H + sym_len_Hs + sym_len_He, in_W)\n    img_aug.narrow(1, sym_len_Hs, in_H).copy_(img)\n\n    sym_patch = img[:, :sym_len_Hs, :]\n    inv_idx = torch.arange(sym_patch.size(1) - 1, -1, -1).long()\n    sym_patch_inv = sym_patch.index_select(1, inv_idx)\n    img_aug.narrow(1, 0, sym_len_Hs).copy_(sym_patch_inv)\n\n    sym_patch = img[:, -sym_len_He:, :]\n    inv_idx = torch.arange(sym_patch.size(1) - 1, -1, -1).long()\n    sym_patch_inv = sym_patch.index_select(1, inv_idx)\n    img_aug.narrow(1, sym_len_Hs + in_H, sym_len_He).copy_(sym_patch_inv)\n\n    out_1 = torch.FloatTensor(in_C, out_H, in_W)\n    kernel_width = weights_H.size(1)\n    for i in range(out_H):\n        idx = int(indices_H[i][0])\n        for j in range(out_C):\n            out_1[j, i, :] = img_aug[j, idx:idx + kernel_width, :].transpose(0, 1).mv(weights_H[i])\n\n    # process W dimension\n    # symmetric copying\n    out_1_aug = torch.FloatTensor(in_C, out_H, in_W + sym_len_Ws + sym_len_We)\n    out_1_aug.narrow(2, sym_len_Ws, in_W).copy_(out_1)\n\n    sym_patch = out_1[:, :, :sym_len_Ws]\n    inv_idx = torch.arange(sym_patch.size(2) - 1, -1, -1).long()\n    sym_patch_inv = sym_patch.index_select(2, inv_idx)\n    out_1_aug.narrow(2, 0, sym_len_Ws).copy_(sym_patch_inv)\n\n    sym_patch = out_1[:, :, -sym_len_We:]\n    inv_idx = torch.arange(sym_patch.size(2) - 1, -1, -1).long()\n    sym_patch_inv = sym_patch.index_select(2, inv_idx)\n    out_1_aug.narrow(2, sym_len_Ws + in_W, sym_len_We).copy_(sym_patch_inv)\n\n    out_2 = torch.FloatTensor(in_C, out_H, out_W)\n    kernel_width = weights_W.size(1)\n    for i in range(out_W):\n        idx = int(indices_W[i][0])\n        for j in range(out_C):\n            out_2[j, :, i] = out_1_aug[j, :, idx:idx + kernel_width].mv(weights_W[i])\n    if need_squeeze:\n        out_2.squeeze_()\n    return out_2\n\n\n# --------------------------------------------\n# imresize for numpy image [0, 1]\n# --------------------------------------------\ndef imresize_np(img, scale, antialiasing=True):\n    # Now the scale should be the same for H and W\n    # input: img: Numpy, HWC or HW [0,1]\n    # output: HWC or HW [0,1] w/o round\n    img = torch.from_numpy(img)\n    need_squeeze = True if img.dim() == 2 else False\n    if need_squeeze:\n        img.unsqueeze_(2)\n\n    in_H, in_W, in_C = img.size()\n    out_C, out_H, out_W = in_C, math.ceil(in_H * scale), math.ceil(in_W * scale)\n    kernel_width = 4\n    kernel = 'cubic'\n\n    # Return the desired dimension order for performing the resize.  The\n    # strategy is to perform the resize first along the dimension with the\n    # smallest scale factor.\n    # Now we do not support this.\n\n    # get weights and indices\n    weights_H, indices_H, sym_len_Hs, sym_len_He = calculate_weights_indices(\n        in_H, out_H, scale, kernel, kernel_width, antialiasing)\n    weights_W, indices_W, sym_len_Ws, sym_len_We = calculate_weights_indices(\n        in_W, out_W, scale, kernel, kernel_width, antialiasing)\n    # process H dimension\n    # symmetric copying\n    img_aug = torch.FloatTensor(in_H + sym_len_Hs + sym_len_He, in_W, in_C)\n    img_aug.narrow(0, sym_len_Hs, in_H).copy_(img)\n\n    sym_patch = img[:sym_len_Hs, :, :]\n    inv_idx = torch.arange(sym_patch.size(0) - 1, -1, -1).long()\n    sym_patch_inv = sym_patch.index_select(0, inv_idx)\n    img_aug.narrow(0, 0, sym_len_Hs).copy_(sym_patch_inv)\n\n    sym_patch = img[-sym_len_He:, :, :]\n    inv_idx = torch.arange(sym_patch.size(0) - 1, -1, -1).long()\n    sym_patch_inv = sym_patch.index_select(0, inv_idx)\n    img_aug.narrow(0, sym_len_Hs + in_H, sym_len_He).copy_(sym_patch_inv)\n\n    out_1 = torch.FloatTensor(out_H, in_W, in_C)\n    kernel_width = weights_H.size(1)\n    for i in range(out_H):\n        idx = int(indices_H[i][0])\n        for j in range(out_C):\n            out_1[i, :, j] = img_aug[idx:idx + kernel_width, :, j].transpose(0, 1).mv(weights_H[i])\n\n    # process W dimension\n    # symmetric copying\n    out_1_aug = torch.FloatTensor(out_H, in_W + sym_len_Ws + sym_len_We, in_C)\n    out_1_aug.narrow(1, sym_len_Ws, in_W).copy_(out_1)\n\n    sym_patch = out_1[:, :sym_len_Ws, :]\n    inv_idx = torch.arange(sym_patch.size(1) - 1, -1, -1).long()\n    sym_patch_inv = sym_patch.index_select(1, inv_idx)\n    out_1_aug.narrow(1, 0, sym_len_Ws).copy_(sym_patch_inv)\n\n    sym_patch = out_1[:, -sym_len_We:, :]\n    inv_idx = torch.arange(sym_patch.size(1) - 1, -1, -1).long()\n    sym_patch_inv = sym_patch.index_select(1, inv_idx)\n    out_1_aug.narrow(1, sym_len_Ws + in_W, sym_len_We).copy_(sym_patch_inv)\n\n    out_2 = torch.FloatTensor(out_H, out_W, in_C)\n    kernel_width = weights_W.size(1)\n    for i in range(out_W):\n        idx = int(indices_W[i][0])\n        for j in range(out_C):\n            out_2[:, i, j] = out_1_aug[:, idx:idx + kernel_width, j].mv(weights_W[i])\n    if need_squeeze:\n        out_2.squeeze_()\n\n    return out_2.numpy()\n\n\nif __name__ == '__main__':\n    print('---')\n#    img = imread_uint('test.bmp', 3)\n#    img = uint2single(img)\n#    img_bicubic = imresize_np(img, 1/4)", "ldm/modules/image_degradation/__init__.py": "from ldm.modules.image_degradation.bsrgan import degradation_bsrgan_variant as degradation_fn_bsr\nfrom ldm.modules.image_degradation.bsrgan_light import degradation_bsrgan_variant as degradation_fn_bsr_light\n", "ldm/modules/distributions/distributions.py": "import torch\nimport numpy as np\n\n\nclass AbstractDistribution:\n    def sample(self):\n        raise NotImplementedError()\n\n    def mode(self):\n        raise NotImplementedError()\n\n\nclass DiracDistribution(AbstractDistribution):\n    def __init__(self, value):\n        self.value = value\n\n    def sample(self):\n        return self.value\n\n    def mode(self):\n        return self.value\n\n\nclass DiagonalGaussianDistribution(object):\n    def __init__(self, parameters, deterministic=False):\n        self.parameters = parameters\n        self.mean, self.logvar = torch.chunk(parameters, 2, dim=1)\n        self.logvar = torch.clamp(self.logvar, -30.0, 20.0)\n        self.deterministic = deterministic\n        self.std = torch.exp(0.5 * self.logvar)\n        self.var = torch.exp(self.logvar)\n        if self.deterministic:\n            self.var = self.std = torch.zeros_like(self.mean).to(device=self.parameters.device)\n\n    def sample(self):\n        x = self.mean + self.std * torch.randn(self.mean.shape).to(device=self.parameters.device)\n        return x\n\n    def kl(self, other=None):\n        if self.deterministic:\n            return torch.Tensor([0.])\n        else:\n            if other is None:\n                return 0.5 * torch.sum(torch.pow(self.mean, 2)\n                                       + self.var - 1.0 - self.logvar,\n                                       dim=[1, 2, 3])\n            else:\n                return 0.5 * torch.sum(\n                    torch.pow(self.mean - other.mean, 2) / other.var\n                    + self.var / other.var - 1.0 - self.logvar + other.logvar,\n                    dim=[1, 2, 3])\n\n    def nll(self, sample, dims=[1,2,3]):\n        if self.deterministic:\n            return torch.Tensor([0.])\n        logtwopi = np.log(2.0 * np.pi)\n        return 0.5 * torch.sum(\n            logtwopi + self.logvar + torch.pow(sample - self.mean, 2) / self.var,\n            dim=dims)\n\n    def mode(self):\n        return self.mean\n\n\ndef normal_kl(mean1, logvar1, mean2, logvar2):\n    \"\"\"\n    source: https://github.com/openai/guided-diffusion/blob/27c20a8fab9cb472df5d6bdd6c8d11c8f430b924/guided_diffusion/losses.py#L12\n    Compute the KL divergence between two gaussians.\n    Shapes are automatically broadcasted, so batches can be compared to\n    scalars, among other use cases.\n    \"\"\"\n    tensor = None\n    for obj in (mean1, logvar1, mean2, logvar2):\n        if isinstance(obj, torch.Tensor):\n            tensor = obj\n            break\n    assert tensor is not None, \"at least one argument must be a Tensor\"\n\n    # Force variances to be Tensors. Broadcasting helps convert scalars to\n    # Tensors, but it does not work for torch.exp().\n    logvar1, logvar2 = [\n        x if isinstance(x, torch.Tensor) else torch.tensor(x).to(tensor)\n        for x in (logvar1, logvar2)\n    ]\n\n    return 0.5 * (\n        -1.0\n        + logvar2\n        - logvar1\n        + torch.exp(logvar1 - logvar2)\n        + ((mean1 - mean2) ** 2) * torch.exp(-logvar2)\n    )\n", "ldm/modules/distributions/__init__.py": "", "ldm/modules/midas/api.py": "# based on https://github.com/isl-org/MiDaS\n\nimport cv2\nimport torch\nimport torch.nn as nn\nfrom torchvision.transforms import Compose\n\nfrom ldm.modules.midas.midas.dpt_depth import DPTDepthModel\nfrom ldm.modules.midas.midas.midas_net import MidasNet\nfrom ldm.modules.midas.midas.midas_net_custom import MidasNet_small\nfrom ldm.modules.midas.midas.transforms import Resize, NormalizeImage, PrepareForNet\n\n\nISL_PATHS = {\n    \"dpt_large\": \"midas_models/dpt_large-midas-2f21e586.pt\",\n    \"dpt_hybrid\": \"midas_models/dpt_hybrid-midas-501f0c75.pt\",\n    \"midas_v21\": \"\",\n    \"midas_v21_small\": \"\",\n}\n\n\ndef disabled_train(self, mode=True):\n    \"\"\"Overwrite model.train with this function to make sure train/eval mode\n    does not change anymore.\"\"\"\n    return self\n\n\ndef load_midas_transform(model_type):\n    # https://github.com/isl-org/MiDaS/blob/master/run.py\n    # load transform only\n    if model_type == \"dpt_large\":  # DPT-Large\n        net_w, net_h = 384, 384\n        resize_mode = \"minimal\"\n        normalization = NormalizeImage(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n\n    elif model_type == \"dpt_hybrid\":  # DPT-Hybrid\n        net_w, net_h = 384, 384\n        resize_mode = \"minimal\"\n        normalization = NormalizeImage(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n\n    elif model_type == \"midas_v21\":\n        net_w, net_h = 384, 384\n        resize_mode = \"upper_bound\"\n        normalization = NormalizeImage(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n\n    elif model_type == \"midas_v21_small\":\n        net_w, net_h = 256, 256\n        resize_mode = \"upper_bound\"\n        normalization = NormalizeImage(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n\n    else:\n        assert False, f\"model_type '{model_type}' not implemented, use: --model_type large\"\n\n    transform = Compose(\n        [\n            Resize(\n                net_w,\n                net_h,\n                resize_target=None,\n                keep_aspect_ratio=True,\n                ensure_multiple_of=32,\n                resize_method=resize_mode,\n                image_interpolation_method=cv2.INTER_CUBIC,\n            ),\n            normalization,\n            PrepareForNet(),\n        ]\n    )\n\n    return transform\n\n\ndef load_model(model_type):\n    # https://github.com/isl-org/MiDaS/blob/master/run.py\n    # load network\n    model_path = ISL_PATHS[model_type]\n    if model_type == \"dpt_large\":  # DPT-Large\n        model = DPTDepthModel(\n            path=model_path,\n            backbone=\"vitl16_384\",\n            non_negative=True,\n        )\n        net_w, net_h = 384, 384\n        resize_mode = \"minimal\"\n        normalization = NormalizeImage(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n\n    elif model_type == \"dpt_hybrid\":  # DPT-Hybrid\n        model = DPTDepthModel(\n            path=model_path,\n            backbone=\"vitb_rn50_384\",\n            non_negative=True,\n        )\n        net_w, net_h = 384, 384\n        resize_mode = \"minimal\"\n        normalization = NormalizeImage(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n\n    elif model_type == \"midas_v21\":\n        model = MidasNet(model_path, non_negative=True)\n        net_w, net_h = 384, 384\n        resize_mode = \"upper_bound\"\n        normalization = NormalizeImage(\n            mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n        )\n\n    elif model_type == \"midas_v21_small\":\n        model = MidasNet_small(model_path, features=64, backbone=\"efficientnet_lite3\", exportable=True,\n                               non_negative=True, blocks={'expand': True})\n        net_w, net_h = 256, 256\n        resize_mode = \"upper_bound\"\n        normalization = NormalizeImage(\n            mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n        )\n\n    else:\n        print(f\"model_type '{model_type}' not implemented, use: --model_type large\")\n        assert False\n\n    transform = Compose(\n        [\n            Resize(\n                net_w,\n                net_h,\n                resize_target=None,\n                keep_aspect_ratio=True,\n                ensure_multiple_of=32,\n                resize_method=resize_mode,\n                image_interpolation_method=cv2.INTER_CUBIC,\n            ),\n            normalization,\n            PrepareForNet(),\n        ]\n    )\n\n    return model.eval(), transform\n\n\nclass MiDaSInference(nn.Module):\n    MODEL_TYPES_TORCH_HUB = [\n        \"DPT_Large\",\n        \"DPT_Hybrid\",\n        \"MiDaS_small\"\n    ]\n    MODEL_TYPES_ISL = [\n        \"dpt_large\",\n        \"dpt_hybrid\",\n        \"midas_v21\",\n        \"midas_v21_small\",\n    ]\n\n    def __init__(self, model_type):\n        super().__init__()\n        assert (model_type in self.MODEL_TYPES_ISL)\n        model, _ = load_model(model_type)\n        self.model = model\n        self.model.train = disabled_train\n\n    def forward(self, x):\n        # x in 0..1 as produced by calling self.transform on a 0..1 float64 numpy array\n        # NOTE: we expect that the correct transform has been called during dataloading.\n        with torch.no_grad():\n            prediction = self.model(x)\n            prediction = torch.nn.functional.interpolate(\n                prediction.unsqueeze(1),\n                size=x.shape[2:],\n                mode=\"bicubic\",\n                align_corners=False,\n            )\n        assert prediction.shape == (x.shape[0], 1, x.shape[2], x.shape[3])\n        return prediction\n\n", "ldm/modules/midas/utils.py": "\"\"\"Utils for monoDepth.\"\"\"\nimport sys\nimport re\nimport numpy as np\nimport cv2\nimport torch\n\n\ndef read_pfm(path):\n    \"\"\"Read pfm file.\n\n    Args:\n        path (str): path to file\n\n    Returns:\n        tuple: (data, scale)\n    \"\"\"\n    with open(path, \"rb\") as file:\n\n        color = None\n        width = None\n        height = None\n        scale = None\n        endian = None\n\n        header = file.readline().rstrip()\n        if header.decode(\"ascii\") == \"PF\":\n            color = True\n        elif header.decode(\"ascii\") == \"Pf\":\n            color = False\n        else:\n            raise Exception(\"Not a PFM file: \" + path)\n\n        dim_match = re.match(r\"^(\\d+)\\s(\\d+)\\s$\", file.readline().decode(\"ascii\"))\n        if dim_match:\n            width, height = list(map(int, dim_match.groups()))\n        else:\n            raise Exception(\"Malformed PFM header.\")\n\n        scale = float(file.readline().decode(\"ascii\").rstrip())\n        if scale < 0:\n            # little-endian\n            endian = \"<\"\n            scale = -scale\n        else:\n            # big-endian\n            endian = \">\"\n\n        data = np.fromfile(file, endian + \"f\")\n        shape = (height, width, 3) if color else (height, width)\n\n        data = np.reshape(data, shape)\n        data = np.flipud(data)\n\n        return data, scale\n\n\ndef write_pfm(path, image, scale=1):\n    \"\"\"Write pfm file.\n\n    Args:\n        path (str): pathto file\n        image (array): data\n        scale (int, optional): Scale. Defaults to 1.\n    \"\"\"\n\n    with open(path, \"wb\") as file:\n        color = None\n\n        if image.dtype.name != \"float32\":\n            raise Exception(\"Image dtype must be float32.\")\n\n        image = np.flipud(image)\n\n        if len(image.shape) == 3 and image.shape[2] == 3:  # color image\n            color = True\n        elif (\n            len(image.shape) == 2 or len(image.shape) == 3 and image.shape[2] == 1\n        ):  # greyscale\n            color = False\n        else:\n            raise Exception(\"Image must have H x W x 3, H x W x 1 or H x W dimensions.\")\n\n        file.write(\"PF\\n\" if color else \"Pf\\n\".encode())\n        file.write(\"%d %d\\n\".encode() % (image.shape[1], image.shape[0]))\n\n        endian = image.dtype.byteorder\n\n        if endian == \"<\" or endian == \"=\" and sys.byteorder == \"little\":\n            scale = -scale\n\n        file.write(\"%f\\n\".encode() % scale)\n\n        image.tofile(file)\n\n\ndef read_image(path):\n    \"\"\"Read image and output RGB image (0-1).\n\n    Args:\n        path (str): path to file\n\n    Returns:\n        array: RGB image (0-1)\n    \"\"\"\n    img = cv2.imread(path)\n\n    if img.ndim == 2:\n        img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) / 255.0\n\n    return img\n\n\ndef resize_image(img):\n    \"\"\"Resize image and make it fit for network.\n\n    Args:\n        img (array): image\n\n    Returns:\n        tensor: data ready for network\n    \"\"\"\n    height_orig = img.shape[0]\n    width_orig = img.shape[1]\n\n    if width_orig > height_orig:\n        scale = width_orig / 384\n    else:\n        scale = height_orig / 384\n\n    height = (np.ceil(height_orig / scale / 32) * 32).astype(int)\n    width = (np.ceil(width_orig / scale / 32) * 32).astype(int)\n\n    img_resized = cv2.resize(img, (width, height), interpolation=cv2.INTER_AREA)\n\n    img_resized = (\n        torch.from_numpy(np.transpose(img_resized, (2, 0, 1))).contiguous().float()\n    )\n    img_resized = img_resized.unsqueeze(0)\n\n    return img_resized\n\n\ndef resize_depth(depth, width, height):\n    \"\"\"Resize depth map and bring to CPU (numpy).\n\n    Args:\n        depth (tensor): depth\n        width (int): image width\n        height (int): image height\n\n    Returns:\n        array: processed depth\n    \"\"\"\n    depth = torch.squeeze(depth[0, :, :, :]).to(\"cpu\")\n\n    depth_resized = cv2.resize(\n        depth.numpy(), (width, height), interpolation=cv2.INTER_CUBIC\n    )\n\n    return depth_resized\n\ndef write_depth(path, depth, bits=1):\n    \"\"\"Write depth map to pfm and png file.\n\n    Args:\n        path (str): filepath without extension\n        depth (array): depth\n    \"\"\"\n    write_pfm(path + \".pfm\", depth.astype(np.float32))\n\n    depth_min = depth.min()\n    depth_max = depth.max()\n\n    max_val = (2**(8*bits))-1\n\n    if depth_max - depth_min > np.finfo(\"float\").eps:\n        out = max_val * (depth - depth_min) / (depth_max - depth_min)\n    else:\n        out = np.zeros(depth.shape, dtype=depth.type)\n\n    if bits == 1:\n        cv2.imwrite(path + \".png\", out.astype(\"uint8\"))\n    elif bits == 2:\n        cv2.imwrite(path + \".png\", out.astype(\"uint16\"))\n\n    return\n", "ldm/modules/midas/__init__.py": "", "ldm/modules/midas/midas/midas_net.py": "\"\"\"MidashNet: Network for monocular depth estimation trained by mixing several datasets.\nThis file contains code that is adapted from\nhttps://github.com/thomasjpfan/pytorch_refinenet/blob/master/pytorch_refinenet/refinenet/refinenet_4cascade.py\n\"\"\"\nimport torch\nimport torch.nn as nn\n\nfrom .base_model import BaseModel\nfrom .blocks import FeatureFusionBlock, Interpolate, _make_encoder\n\n\nclass MidasNet(BaseModel):\n    \"\"\"Network for monocular depth estimation.\n    \"\"\"\n\n    def __init__(self, path=None, features=256, non_negative=True):\n        \"\"\"Init.\n\n        Args:\n            path (str, optional): Path to saved model. Defaults to None.\n            features (int, optional): Number of features. Defaults to 256.\n            backbone (str, optional): Backbone network for encoder. Defaults to resnet50\n        \"\"\"\n        print(\"Loading weights: \", path)\n\n        super(MidasNet, self).__init__()\n\n        use_pretrained = False if path is None else True\n\n        self.pretrained, self.scratch = _make_encoder(backbone=\"resnext101_wsl\", features=features, use_pretrained=use_pretrained)\n\n        self.scratch.refinenet4 = FeatureFusionBlock(features)\n        self.scratch.refinenet3 = FeatureFusionBlock(features)\n        self.scratch.refinenet2 = FeatureFusionBlock(features)\n        self.scratch.refinenet1 = FeatureFusionBlock(features)\n\n        self.scratch.output_conv = nn.Sequential(\n            nn.Conv2d(features, 128, kernel_size=3, stride=1, padding=1),\n            Interpolate(scale_factor=2, mode=\"bilinear\"),\n            nn.Conv2d(128, 32, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(True),\n            nn.Conv2d(32, 1, kernel_size=1, stride=1, padding=0),\n            nn.ReLU(True) if non_negative else nn.Identity(),\n        )\n\n        if path:\n            self.load(path)\n\n    def forward(self, x):\n        \"\"\"Forward pass.\n\n        Args:\n            x (tensor): input data (image)\n\n        Returns:\n            tensor: depth\n        \"\"\"\n\n        layer_1 = self.pretrained.layer1(x)\n        layer_2 = self.pretrained.layer2(layer_1)\n        layer_3 = self.pretrained.layer3(layer_2)\n        layer_4 = self.pretrained.layer4(layer_3)\n\n        layer_1_rn = self.scratch.layer1_rn(layer_1)\n        layer_2_rn = self.scratch.layer2_rn(layer_2)\n        layer_3_rn = self.scratch.layer3_rn(layer_3)\n        layer_4_rn = self.scratch.layer4_rn(layer_4)\n\n        path_4 = self.scratch.refinenet4(layer_4_rn)\n        path_3 = self.scratch.refinenet3(path_4, layer_3_rn)\n        path_2 = self.scratch.refinenet2(path_3, layer_2_rn)\n        path_1 = self.scratch.refinenet1(path_2, layer_1_rn)\n\n        out = self.scratch.output_conv(path_1)\n\n        return torch.squeeze(out, dim=1)\n", "ldm/modules/midas/midas/base_model.py": "import torch\n\n\nclass BaseModel(torch.nn.Module):\n    def load(self, path):\n        \"\"\"Load model from file.\n\n        Args:\n            path (str): file path\n        \"\"\"\n        parameters = torch.load(path, map_location=torch.device('cpu'))\n\n        if \"optimizer\" in parameters:\n            parameters = parameters[\"model\"]\n\n        self.load_state_dict(parameters)\n", "ldm/modules/midas/midas/midas_net_custom.py": "\"\"\"MidashNet: Network for monocular depth estimation trained by mixing several datasets.\nThis file contains code that is adapted from\nhttps://github.com/thomasjpfan/pytorch_refinenet/blob/master/pytorch_refinenet/refinenet/refinenet_4cascade.py\n\"\"\"\nimport torch\nimport torch.nn as nn\n\nfrom .base_model import BaseModel\nfrom .blocks import FeatureFusionBlock, FeatureFusionBlock_custom, Interpolate, _make_encoder\n\n\nclass MidasNet_small(BaseModel):\n    \"\"\"Network for monocular depth estimation.\n    \"\"\"\n\n    def __init__(self, path=None, features=64, backbone=\"efficientnet_lite3\", non_negative=True, exportable=True, channels_last=False, align_corners=True,\n        blocks={'expand': True}):\n        \"\"\"Init.\n\n        Args:\n            path (str, optional): Path to saved model. Defaults to None.\n            features (int, optional): Number of features. Defaults to 256.\n            backbone (str, optional): Backbone network for encoder. Defaults to resnet50\n        \"\"\"\n        print(\"Loading weights: \", path)\n\n        super(MidasNet_small, self).__init__()\n\n        use_pretrained = False if path else True\n                \n        self.channels_last = channels_last\n        self.blocks = blocks\n        self.backbone = backbone\n\n        self.groups = 1\n\n        features1=features\n        features2=features\n        features3=features\n        features4=features\n        self.expand = False\n        if \"expand\" in self.blocks and self.blocks['expand'] == True:\n            self.expand = True\n            features1=features\n            features2=features*2\n            features3=features*4\n            features4=features*8\n\n        self.pretrained, self.scratch = _make_encoder(self.backbone, features, use_pretrained, groups=self.groups, expand=self.expand, exportable=exportable)\n  \n        self.scratch.activation = nn.ReLU(False)    \n\n        self.scratch.refinenet4 = FeatureFusionBlock_custom(features4, self.scratch.activation, deconv=False, bn=False, expand=self.expand, align_corners=align_corners)\n        self.scratch.refinenet3 = FeatureFusionBlock_custom(features3, self.scratch.activation, deconv=False, bn=False, expand=self.expand, align_corners=align_corners)\n        self.scratch.refinenet2 = FeatureFusionBlock_custom(features2, self.scratch.activation, deconv=False, bn=False, expand=self.expand, align_corners=align_corners)\n        self.scratch.refinenet1 = FeatureFusionBlock_custom(features1, self.scratch.activation, deconv=False, bn=False, align_corners=align_corners)\n\n        \n        self.scratch.output_conv = nn.Sequential(\n            nn.Conv2d(features, features//2, kernel_size=3, stride=1, padding=1, groups=self.groups),\n            Interpolate(scale_factor=2, mode=\"bilinear\"),\n            nn.Conv2d(features//2, 32, kernel_size=3, stride=1, padding=1),\n            self.scratch.activation,\n            nn.Conv2d(32, 1, kernel_size=1, stride=1, padding=0),\n            nn.ReLU(True) if non_negative else nn.Identity(),\n            nn.Identity(),\n        )\n        \n        if path:\n            self.load(path)\n\n\n    def forward(self, x):\n        \"\"\"Forward pass.\n\n        Args:\n            x (tensor): input data (image)\n\n        Returns:\n            tensor: depth\n        \"\"\"\n        if self.channels_last==True:\n            print(\"self.channels_last = \", self.channels_last)\n            x.contiguous(memory_format=torch.channels_last)\n\n\n        layer_1 = self.pretrained.layer1(x)\n        layer_2 = self.pretrained.layer2(layer_1)\n        layer_3 = self.pretrained.layer3(layer_2)\n        layer_4 = self.pretrained.layer4(layer_3)\n        \n        layer_1_rn = self.scratch.layer1_rn(layer_1)\n        layer_2_rn = self.scratch.layer2_rn(layer_2)\n        layer_3_rn = self.scratch.layer3_rn(layer_3)\n        layer_4_rn = self.scratch.layer4_rn(layer_4)\n\n\n        path_4 = self.scratch.refinenet4(layer_4_rn)\n        path_3 = self.scratch.refinenet3(path_4, layer_3_rn)\n        path_2 = self.scratch.refinenet2(path_3, layer_2_rn)\n        path_1 = self.scratch.refinenet1(path_2, layer_1_rn)\n        \n        out = self.scratch.output_conv(path_1)\n\n        return torch.squeeze(out, dim=1)\n\n\n\ndef fuse_model(m):\n    prev_previous_type = nn.Identity()\n    prev_previous_name = ''\n    previous_type = nn.Identity()\n    previous_name = ''\n    for name, module in m.named_modules():\n        if prev_previous_type == nn.Conv2d and previous_type == nn.BatchNorm2d and type(module) == nn.ReLU:\n            # print(\"FUSED \", prev_previous_name, previous_name, name)\n            torch.quantization.fuse_modules(m, [prev_previous_name, previous_name, name], inplace=True)\n        elif prev_previous_type == nn.Conv2d and previous_type == nn.BatchNorm2d:\n            # print(\"FUSED \", prev_previous_name, previous_name)\n            torch.quantization.fuse_modules(m, [prev_previous_name, previous_name], inplace=True)\n        # elif previous_type == nn.Conv2d and type(module) == nn.ReLU:\n        #    print(\"FUSED \", previous_name, name)\n        #    torch.quantization.fuse_modules(m, [previous_name, name], inplace=True)\n\n        prev_previous_type = previous_type\n        prev_previous_name = previous_name\n        previous_type = type(module)\n        previous_name = name", "ldm/modules/midas/midas/blocks.py": "import torch\nimport torch.nn as nn\n\nfrom .vit import (\n    _make_pretrained_vitb_rn50_384,\n    _make_pretrained_vitl16_384,\n    _make_pretrained_vitb16_384,\n    forward_vit,\n)\n\ndef _make_encoder(backbone, features, use_pretrained, groups=1, expand=False, exportable=True, hooks=None, use_vit_only=False, use_readout=\"ignore\",):\n    if backbone == \"vitl16_384\":\n        pretrained = _make_pretrained_vitl16_384(\n            use_pretrained, hooks=hooks, use_readout=use_readout\n        )\n        scratch = _make_scratch(\n            [256, 512, 1024, 1024], features, groups=groups, expand=expand\n        )  # ViT-L/16 - 85.0% Top1 (backbone)\n    elif backbone == \"vitb_rn50_384\":\n        pretrained = _make_pretrained_vitb_rn50_384(\n            use_pretrained,\n            hooks=hooks,\n            use_vit_only=use_vit_only,\n            use_readout=use_readout,\n        )\n        scratch = _make_scratch(\n            [256, 512, 768, 768], features, groups=groups, expand=expand\n        )  # ViT-H/16 - 85.0% Top1 (backbone)\n    elif backbone == \"vitb16_384\":\n        pretrained = _make_pretrained_vitb16_384(\n            use_pretrained, hooks=hooks, use_readout=use_readout\n        )\n        scratch = _make_scratch(\n            [96, 192, 384, 768], features, groups=groups, expand=expand\n        )  # ViT-B/16 - 84.6% Top1 (backbone)\n    elif backbone == \"resnext101_wsl\":\n        pretrained = _make_pretrained_resnext101_wsl(use_pretrained)\n        scratch = _make_scratch([256, 512, 1024, 2048], features, groups=groups, expand=expand)     # efficientnet_lite3  \n    elif backbone == \"efficientnet_lite3\":\n        pretrained = _make_pretrained_efficientnet_lite3(use_pretrained, exportable=exportable)\n        scratch = _make_scratch([32, 48, 136, 384], features, groups=groups, expand=expand)  # efficientnet_lite3     \n    else:\n        print(f\"Backbone '{backbone}' not implemented\")\n        assert False\n        \n    return pretrained, scratch\n\n\ndef _make_scratch(in_shape, out_shape, groups=1, expand=False):\n    scratch = nn.Module()\n\n    out_shape1 = out_shape\n    out_shape2 = out_shape\n    out_shape3 = out_shape\n    out_shape4 = out_shape\n    if expand==True:\n        out_shape1 = out_shape\n        out_shape2 = out_shape*2\n        out_shape3 = out_shape*4\n        out_shape4 = out_shape*8\n\n    scratch.layer1_rn = nn.Conv2d(\n        in_shape[0], out_shape1, kernel_size=3, stride=1, padding=1, bias=False, groups=groups\n    )\n    scratch.layer2_rn = nn.Conv2d(\n        in_shape[1], out_shape2, kernel_size=3, stride=1, padding=1, bias=False, groups=groups\n    )\n    scratch.layer3_rn = nn.Conv2d(\n        in_shape[2], out_shape3, kernel_size=3, stride=1, padding=1, bias=False, groups=groups\n    )\n    scratch.layer4_rn = nn.Conv2d(\n        in_shape[3], out_shape4, kernel_size=3, stride=1, padding=1, bias=False, groups=groups\n    )\n\n    return scratch\n\n\ndef _make_pretrained_efficientnet_lite3(use_pretrained, exportable=False):\n    efficientnet = torch.hub.load(\n        \"rwightman/gen-efficientnet-pytorch\",\n        \"tf_efficientnet_lite3\",\n        pretrained=use_pretrained,\n        exportable=exportable\n    )\n    return _make_efficientnet_backbone(efficientnet)\n\n\ndef _make_efficientnet_backbone(effnet):\n    pretrained = nn.Module()\n\n    pretrained.layer1 = nn.Sequential(\n        effnet.conv_stem, effnet.bn1, effnet.act1, *effnet.blocks[0:2]\n    )\n    pretrained.layer2 = nn.Sequential(*effnet.blocks[2:3])\n    pretrained.layer3 = nn.Sequential(*effnet.blocks[3:5])\n    pretrained.layer4 = nn.Sequential(*effnet.blocks[5:9])\n\n    return pretrained\n    \n\ndef _make_resnet_backbone(resnet):\n    pretrained = nn.Module()\n    pretrained.layer1 = nn.Sequential(\n        resnet.conv1, resnet.bn1, resnet.relu, resnet.maxpool, resnet.layer1\n    )\n\n    pretrained.layer2 = resnet.layer2\n    pretrained.layer3 = resnet.layer3\n    pretrained.layer4 = resnet.layer4\n\n    return pretrained\n\n\ndef _make_pretrained_resnext101_wsl(use_pretrained):\n    resnet = torch.hub.load(\"facebookresearch/WSL-Images\", \"resnext101_32x8d_wsl\")\n    return _make_resnet_backbone(resnet)\n\n\n\nclass Interpolate(nn.Module):\n    \"\"\"Interpolation module.\n    \"\"\"\n\n    def __init__(self, scale_factor, mode, align_corners=False):\n        \"\"\"Init.\n\n        Args:\n            scale_factor (float): scaling\n            mode (str): interpolation mode\n        \"\"\"\n        super(Interpolate, self).__init__()\n\n        self.interp = nn.functional.interpolate\n        self.scale_factor = scale_factor\n        self.mode = mode\n        self.align_corners = align_corners\n\n    def forward(self, x):\n        \"\"\"Forward pass.\n\n        Args:\n            x (tensor): input\n\n        Returns:\n            tensor: interpolated data\n        \"\"\"\n\n        x = self.interp(\n            x, scale_factor=self.scale_factor, mode=self.mode, align_corners=self.align_corners\n        )\n\n        return x\n\n\nclass ResidualConvUnit(nn.Module):\n    \"\"\"Residual convolution module.\n    \"\"\"\n\n    def __init__(self, features):\n        \"\"\"Init.\n\n        Args:\n            features (int): number of features\n        \"\"\"\n        super().__init__()\n\n        self.conv1 = nn.Conv2d(\n            features, features, kernel_size=3, stride=1, padding=1, bias=True\n        )\n\n        self.conv2 = nn.Conv2d(\n            features, features, kernel_size=3, stride=1, padding=1, bias=True\n        )\n\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        \"\"\"Forward pass.\n\n        Args:\n            x (tensor): input\n\n        Returns:\n            tensor: output\n        \"\"\"\n        out = self.relu(x)\n        out = self.conv1(out)\n        out = self.relu(out)\n        out = self.conv2(out)\n\n        return out + x\n\n\nclass FeatureFusionBlock(nn.Module):\n    \"\"\"Feature fusion block.\n    \"\"\"\n\n    def __init__(self, features):\n        \"\"\"Init.\n\n        Args:\n            features (int): number of features\n        \"\"\"\n        super(FeatureFusionBlock, self).__init__()\n\n        self.resConfUnit1 = ResidualConvUnit(features)\n        self.resConfUnit2 = ResidualConvUnit(features)\n\n    def forward(self, *xs):\n        \"\"\"Forward pass.\n\n        Returns:\n            tensor: output\n        \"\"\"\n        output = xs[0]\n\n        if len(xs) == 2:\n            output += self.resConfUnit1(xs[1])\n\n        output = self.resConfUnit2(output)\n\n        output = nn.functional.interpolate(\n            output, scale_factor=2, mode=\"bilinear\", align_corners=True\n        )\n\n        return output\n\n\n\n\nclass ResidualConvUnit_custom(nn.Module):\n    \"\"\"Residual convolution module.\n    \"\"\"\n\n    def __init__(self, features, activation, bn):\n        \"\"\"Init.\n\n        Args:\n            features (int): number of features\n        \"\"\"\n        super().__init__()\n\n        self.bn = bn\n\n        self.groups=1\n\n        self.conv1 = nn.Conv2d(\n            features, features, kernel_size=3, stride=1, padding=1, bias=True, groups=self.groups\n        )\n        \n        self.conv2 = nn.Conv2d(\n            features, features, kernel_size=3, stride=1, padding=1, bias=True, groups=self.groups\n        )\n\n        if self.bn==True:\n            self.bn1 = nn.BatchNorm2d(features)\n            self.bn2 = nn.BatchNorm2d(features)\n\n        self.activation = activation\n\n        self.skip_add = nn.quantized.FloatFunctional()\n\n    def forward(self, x):\n        \"\"\"Forward pass.\n\n        Args:\n            x (tensor): input\n\n        Returns:\n            tensor: output\n        \"\"\"\n        \n        out = self.activation(x)\n        out = self.conv1(out)\n        if self.bn==True:\n            out = self.bn1(out)\n       \n        out = self.activation(out)\n        out = self.conv2(out)\n        if self.bn==True:\n            out = self.bn2(out)\n\n        if self.groups > 1:\n            out = self.conv_merge(out)\n\n        return self.skip_add.add(out, x)\n\n        # return out + x\n\n\nclass FeatureFusionBlock_custom(nn.Module):\n    \"\"\"Feature fusion block.\n    \"\"\"\n\n    def __init__(self, features, activation, deconv=False, bn=False, expand=False, align_corners=True):\n        \"\"\"Init.\n\n        Args:\n            features (int): number of features\n        \"\"\"\n        super(FeatureFusionBlock_custom, self).__init__()\n\n        self.deconv = deconv\n        self.align_corners = align_corners\n\n        self.groups=1\n\n        self.expand = expand\n        out_features = features\n        if self.expand==True:\n            out_features = features//2\n        \n        self.out_conv = nn.Conv2d(features, out_features, kernel_size=1, stride=1, padding=0, bias=True, groups=1)\n\n        self.resConfUnit1 = ResidualConvUnit_custom(features, activation, bn)\n        self.resConfUnit2 = ResidualConvUnit_custom(features, activation, bn)\n        \n        self.skip_add = nn.quantized.FloatFunctional()\n\n    def forward(self, *xs):\n        \"\"\"Forward pass.\n\n        Returns:\n            tensor: output\n        \"\"\"\n        output = xs[0]\n\n        if len(xs) == 2:\n            res = self.resConfUnit1(xs[1])\n            output = self.skip_add.add(output, res)\n            # output += res\n\n        output = self.resConfUnit2(output)\n\n        output = nn.functional.interpolate(\n            output, scale_factor=2, mode=\"bilinear\", align_corners=self.align_corners\n        )\n\n        output = self.out_conv(output)\n\n        return output\n\n", "ldm/modules/midas/midas/transforms.py": "import numpy as np\nimport cv2\nimport math\n\n\ndef apply_min_size(sample, size, image_interpolation_method=cv2.INTER_AREA):\n    \"\"\"Rezise the sample to ensure the given size. Keeps aspect ratio.\n\n    Args:\n        sample (dict): sample\n        size (tuple): image size\n\n    Returns:\n        tuple: new size\n    \"\"\"\n    shape = list(sample[\"disparity\"].shape)\n\n    if shape[0] >= size[0] and shape[1] >= size[1]:\n        return sample\n\n    scale = [0, 0]\n    scale[0] = size[0] / shape[0]\n    scale[1] = size[1] / shape[1]\n\n    scale = max(scale)\n\n    shape[0] = math.ceil(scale * shape[0])\n    shape[1] = math.ceil(scale * shape[1])\n\n    # resize\n    sample[\"image\"] = cv2.resize(\n        sample[\"image\"], tuple(shape[::-1]), interpolation=image_interpolation_method\n    )\n\n    sample[\"disparity\"] = cv2.resize(\n        sample[\"disparity\"], tuple(shape[::-1]), interpolation=cv2.INTER_NEAREST\n    )\n    sample[\"mask\"] = cv2.resize(\n        sample[\"mask\"].astype(np.float32),\n        tuple(shape[::-1]),\n        interpolation=cv2.INTER_NEAREST,\n    )\n    sample[\"mask\"] = sample[\"mask\"].astype(bool)\n\n    return tuple(shape)\n\n\nclass Resize(object):\n    \"\"\"Resize sample to given size (width, height).\n    \"\"\"\n\n    def __init__(\n        self,\n        width,\n        height,\n        resize_target=True,\n        keep_aspect_ratio=False,\n        ensure_multiple_of=1,\n        resize_method=\"lower_bound\",\n        image_interpolation_method=cv2.INTER_AREA,\n    ):\n        \"\"\"Init.\n\n        Args:\n            width (int): desired output width\n            height (int): desired output height\n            resize_target (bool, optional):\n                True: Resize the full sample (image, mask, target).\n                False: Resize image only.\n                Defaults to True.\n            keep_aspect_ratio (bool, optional):\n                True: Keep the aspect ratio of the input sample.\n                Output sample might not have the given width and height, and\n                resize behaviour depends on the parameter 'resize_method'.\n                Defaults to False.\n            ensure_multiple_of (int, optional):\n                Output width and height is constrained to be multiple of this parameter.\n                Defaults to 1.\n            resize_method (str, optional):\n                \"lower_bound\": Output will be at least as large as the given size.\n                \"upper_bound\": Output will be at max as large as the given size. (Output size might be smaller than given size.)\n                \"minimal\": Scale as least as possible.  (Output size might be smaller than given size.)\n                Defaults to \"lower_bound\".\n        \"\"\"\n        self.__width = width\n        self.__height = height\n\n        self.__resize_target = resize_target\n        self.__keep_aspect_ratio = keep_aspect_ratio\n        self.__multiple_of = ensure_multiple_of\n        self.__resize_method = resize_method\n        self.__image_interpolation_method = image_interpolation_method\n\n    def constrain_to_multiple_of(self, x, min_val=0, max_val=None):\n        y = (np.round(x / self.__multiple_of) * self.__multiple_of).astype(int)\n\n        if max_val is not None and y > max_val:\n            y = (np.floor(x / self.__multiple_of) * self.__multiple_of).astype(int)\n\n        if y < min_val:\n            y = (np.ceil(x / self.__multiple_of) * self.__multiple_of).astype(int)\n\n        return y\n\n    def get_size(self, width, height):\n        # determine new height and width\n        scale_height = self.__height / height\n        scale_width = self.__width / width\n\n        if self.__keep_aspect_ratio:\n            if self.__resize_method == \"lower_bound\":\n                # scale such that output size is lower bound\n                if scale_width > scale_height:\n                    # fit width\n                    scale_height = scale_width\n                else:\n                    # fit height\n                    scale_width = scale_height\n            elif self.__resize_method == \"upper_bound\":\n                # scale such that output size is upper bound\n                if scale_width < scale_height:\n                    # fit width\n                    scale_height = scale_width\n                else:\n                    # fit height\n                    scale_width = scale_height\n            elif self.__resize_method == \"minimal\":\n                # scale as least as possbile\n                if abs(1 - scale_width) < abs(1 - scale_height):\n                    # fit width\n                    scale_height = scale_width\n                else:\n                    # fit height\n                    scale_width = scale_height\n            else:\n                raise ValueError(\n                    f\"resize_method {self.__resize_method} not implemented\"\n                )\n\n        if self.__resize_method == \"lower_bound\":\n            new_height = self.constrain_to_multiple_of(\n                scale_height * height, min_val=self.__height\n            )\n            new_width = self.constrain_to_multiple_of(\n                scale_width * width, min_val=self.__width\n            )\n        elif self.__resize_method == \"upper_bound\":\n            new_height = self.constrain_to_multiple_of(\n                scale_height * height, max_val=self.__height\n            )\n            new_width = self.constrain_to_multiple_of(\n                scale_width * width, max_val=self.__width\n            )\n        elif self.__resize_method == \"minimal\":\n            new_height = self.constrain_to_multiple_of(scale_height * height)\n            new_width = self.constrain_to_multiple_of(scale_width * width)\n        else:\n            raise ValueError(f\"resize_method {self.__resize_method} not implemented\")\n\n        return (new_width, new_height)\n\n    def __call__(self, sample):\n        width, height = self.get_size(\n            sample[\"image\"].shape[1], sample[\"image\"].shape[0]\n        )\n\n        # resize sample\n        sample[\"image\"] = cv2.resize(\n            sample[\"image\"],\n            (width, height),\n            interpolation=self.__image_interpolation_method,\n        )\n\n        if self.__resize_target:\n            if \"disparity\" in sample:\n                sample[\"disparity\"] = cv2.resize(\n                    sample[\"disparity\"],\n                    (width, height),\n                    interpolation=cv2.INTER_NEAREST,\n                )\n\n            if \"depth\" in sample:\n                sample[\"depth\"] = cv2.resize(\n                    sample[\"depth\"], (width, height), interpolation=cv2.INTER_NEAREST\n                )\n\n            sample[\"mask\"] = cv2.resize(\n                sample[\"mask\"].astype(np.float32),\n                (width, height),\n                interpolation=cv2.INTER_NEAREST,\n            )\n            sample[\"mask\"] = sample[\"mask\"].astype(bool)\n\n        return sample\n\n\nclass NormalizeImage(object):\n    \"\"\"Normlize image by given mean and std.\n    \"\"\"\n\n    def __init__(self, mean, std):\n        self.__mean = mean\n        self.__std = std\n\n    def __call__(self, sample):\n        sample[\"image\"] = (sample[\"image\"] - self.__mean) / self.__std\n\n        return sample\n\n\nclass PrepareForNet(object):\n    \"\"\"Prepare sample for usage as network input.\n    \"\"\"\n\n    def __init__(self):\n        pass\n\n    def __call__(self, sample):\n        image = np.transpose(sample[\"image\"], (2, 0, 1))\n        sample[\"image\"] = np.ascontiguousarray(image).astype(np.float32)\n\n        if \"mask\" in sample:\n            sample[\"mask\"] = sample[\"mask\"].astype(np.float32)\n            sample[\"mask\"] = np.ascontiguousarray(sample[\"mask\"])\n\n        if \"disparity\" in sample:\n            disparity = sample[\"disparity\"].astype(np.float32)\n            sample[\"disparity\"] = np.ascontiguousarray(disparity)\n\n        if \"depth\" in sample:\n            depth = sample[\"depth\"].astype(np.float32)\n            sample[\"depth\"] = np.ascontiguousarray(depth)\n\n        return sample\n", "ldm/modules/midas/midas/vit.py": "import torch\nimport torch.nn as nn\nimport timm\nimport types\nimport math\nimport torch.nn.functional as F\n\n\nclass Slice(nn.Module):\n    def __init__(self, start_index=1):\n        super(Slice, self).__init__()\n        self.start_index = start_index\n\n    def forward(self, x):\n        return x[:, self.start_index :]\n\n\nclass AddReadout(nn.Module):\n    def __init__(self, start_index=1):\n        super(AddReadout, self).__init__()\n        self.start_index = start_index\n\n    def forward(self, x):\n        if self.start_index == 2:\n            readout = (x[:, 0] + x[:, 1]) / 2\n        else:\n            readout = x[:, 0]\n        return x[:, self.start_index :] + readout.unsqueeze(1)\n\n\nclass ProjectReadout(nn.Module):\n    def __init__(self, in_features, start_index=1):\n        super(ProjectReadout, self).__init__()\n        self.start_index = start_index\n\n        self.project = nn.Sequential(nn.Linear(2 * in_features, in_features), nn.GELU())\n\n    def forward(self, x):\n        readout = x[:, 0].unsqueeze(1).expand_as(x[:, self.start_index :])\n        features = torch.cat((x[:, self.start_index :], readout), -1)\n\n        return self.project(features)\n\n\nclass Transpose(nn.Module):\n    def __init__(self, dim0, dim1):\n        super(Transpose, self).__init__()\n        self.dim0 = dim0\n        self.dim1 = dim1\n\n    def forward(self, x):\n        x = x.transpose(self.dim0, self.dim1)\n        return x\n\n\ndef forward_vit(pretrained, x):\n    b, c, h, w = x.shape\n\n    glob = pretrained.model.forward_flex(x)\n\n    layer_1 = pretrained.activations[\"1\"]\n    layer_2 = pretrained.activations[\"2\"]\n    layer_3 = pretrained.activations[\"3\"]\n    layer_4 = pretrained.activations[\"4\"]\n\n    layer_1 = pretrained.act_postprocess1[0:2](layer_1)\n    layer_2 = pretrained.act_postprocess2[0:2](layer_2)\n    layer_3 = pretrained.act_postprocess3[0:2](layer_3)\n    layer_4 = pretrained.act_postprocess4[0:2](layer_4)\n\n    unflatten = nn.Sequential(\n        nn.Unflatten(\n            2,\n            torch.Size(\n                [\n                    h // pretrained.model.patch_size[1],\n                    w // pretrained.model.patch_size[0],\n                ]\n            ),\n        )\n    )\n\n    if layer_1.ndim == 3:\n        layer_1 = unflatten(layer_1)\n    if layer_2.ndim == 3:\n        layer_2 = unflatten(layer_2)\n    if layer_3.ndim == 3:\n        layer_3 = unflatten(layer_3)\n    if layer_4.ndim == 3:\n        layer_4 = unflatten(layer_4)\n\n    layer_1 = pretrained.act_postprocess1[3 : len(pretrained.act_postprocess1)](layer_1)\n    layer_2 = pretrained.act_postprocess2[3 : len(pretrained.act_postprocess2)](layer_2)\n    layer_3 = pretrained.act_postprocess3[3 : len(pretrained.act_postprocess3)](layer_3)\n    layer_4 = pretrained.act_postprocess4[3 : len(pretrained.act_postprocess4)](layer_4)\n\n    return layer_1, layer_2, layer_3, layer_4\n\n\ndef _resize_pos_embed(self, posemb, gs_h, gs_w):\n    posemb_tok, posemb_grid = (\n        posemb[:, : self.start_index],\n        posemb[0, self.start_index :],\n    )\n\n    gs_old = int(math.sqrt(len(posemb_grid)))\n\n    posemb_grid = posemb_grid.reshape(1, gs_old, gs_old, -1).permute(0, 3, 1, 2)\n    posemb_grid = F.interpolate(posemb_grid, size=(gs_h, gs_w), mode=\"bilinear\")\n    posemb_grid = posemb_grid.permute(0, 2, 3, 1).reshape(1, gs_h * gs_w, -1)\n\n    posemb = torch.cat([posemb_tok, posemb_grid], dim=1)\n\n    return posemb\n\n\ndef forward_flex(self, x):\n    b, c, h, w = x.shape\n\n    pos_embed = self._resize_pos_embed(\n        self.pos_embed, h // self.patch_size[1], w // self.patch_size[0]\n    )\n\n    B = x.shape[0]\n\n    if hasattr(self.patch_embed, \"backbone\"):\n        x = self.patch_embed.backbone(x)\n        if isinstance(x, (list, tuple)):\n            x = x[-1]  # last feature if backbone outputs list/tuple of features\n\n    x = self.patch_embed.proj(x).flatten(2).transpose(1, 2)\n\n    if getattr(self, \"dist_token\", None) is not None:\n        cls_tokens = self.cls_token.expand(\n            B, -1, -1\n        )  # stole cls_tokens impl from Phil Wang, thanks\n        dist_token = self.dist_token.expand(B, -1, -1)\n        x = torch.cat((cls_tokens, dist_token, x), dim=1)\n    else:\n        cls_tokens = self.cls_token.expand(\n            B, -1, -1\n        )  # stole cls_tokens impl from Phil Wang, thanks\n        x = torch.cat((cls_tokens, x), dim=1)\n\n    x = x + pos_embed\n    x = self.pos_drop(x)\n\n    for blk in self.blocks:\n        x = blk(x)\n\n    x = self.norm(x)\n\n    return x\n\n\nactivations = {}\n\n\ndef get_activation(name):\n    def hook(model, input, output):\n        activations[name] = output\n\n    return hook\n\n\ndef get_readout_oper(vit_features, features, use_readout, start_index=1):\n    if use_readout == \"ignore\":\n        readout_oper = [Slice(start_index)] * len(features)\n    elif use_readout == \"add\":\n        readout_oper = [AddReadout(start_index)] * len(features)\n    elif use_readout == \"project\":\n        readout_oper = [\n            ProjectReadout(vit_features, start_index) for out_feat in features\n        ]\n    else:\n        assert (\n            False\n        ), \"wrong operation for readout token, use_readout can be 'ignore', 'add', or 'project'\"\n\n    return readout_oper\n\n\ndef _make_vit_b16_backbone(\n    model,\n    features=[96, 192, 384, 768],\n    size=[384, 384],\n    hooks=[2, 5, 8, 11],\n    vit_features=768,\n    use_readout=\"ignore\",\n    start_index=1,\n):\n    pretrained = nn.Module()\n\n    pretrained.model = model\n    pretrained.model.blocks[hooks[0]].register_forward_hook(get_activation(\"1\"))\n    pretrained.model.blocks[hooks[1]].register_forward_hook(get_activation(\"2\"))\n    pretrained.model.blocks[hooks[2]].register_forward_hook(get_activation(\"3\"))\n    pretrained.model.blocks[hooks[3]].register_forward_hook(get_activation(\"4\"))\n\n    pretrained.activations = activations\n\n    readout_oper = get_readout_oper(vit_features, features, use_readout, start_index)\n\n    # 32, 48, 136, 384\n    pretrained.act_postprocess1 = nn.Sequential(\n        readout_oper[0],\n        Transpose(1, 2),\n        nn.Unflatten(2, torch.Size([size[0] // 16, size[1] // 16])),\n        nn.Conv2d(\n            in_channels=vit_features,\n            out_channels=features[0],\n            kernel_size=1,\n            stride=1,\n            padding=0,\n        ),\n        nn.ConvTranspose2d(\n            in_channels=features[0],\n            out_channels=features[0],\n            kernel_size=4,\n            stride=4,\n            padding=0,\n            bias=True,\n            dilation=1,\n            groups=1,\n        ),\n    )\n\n    pretrained.act_postprocess2 = nn.Sequential(\n        readout_oper[1],\n        Transpose(1, 2),\n        nn.Unflatten(2, torch.Size([size[0] // 16, size[1] // 16])),\n        nn.Conv2d(\n            in_channels=vit_features,\n            out_channels=features[1],\n            kernel_size=1,\n            stride=1,\n            padding=0,\n        ),\n        nn.ConvTranspose2d(\n            in_channels=features[1],\n            out_channels=features[1],\n            kernel_size=2,\n            stride=2,\n            padding=0,\n            bias=True,\n            dilation=1,\n            groups=1,\n        ),\n    )\n\n    pretrained.act_postprocess3 = nn.Sequential(\n        readout_oper[2],\n        Transpose(1, 2),\n        nn.Unflatten(2, torch.Size([size[0] // 16, size[1] // 16])),\n        nn.Conv2d(\n            in_channels=vit_features,\n            out_channels=features[2],\n            kernel_size=1,\n            stride=1,\n            padding=0,\n        ),\n    )\n\n    pretrained.act_postprocess4 = nn.Sequential(\n        readout_oper[3],\n        Transpose(1, 2),\n        nn.Unflatten(2, torch.Size([size[0] // 16, size[1] // 16])),\n        nn.Conv2d(\n            in_channels=vit_features,\n            out_channels=features[3],\n            kernel_size=1,\n            stride=1,\n            padding=0,\n        ),\n        nn.Conv2d(\n            in_channels=features[3],\n            out_channels=features[3],\n            kernel_size=3,\n            stride=2,\n            padding=1,\n        ),\n    )\n\n    pretrained.model.start_index = start_index\n    pretrained.model.patch_size = [16, 16]\n\n    # We inject this function into the VisionTransformer instances so that\n    # we can use it with interpolated position embeddings without modifying the library source.\n    pretrained.model.forward_flex = types.MethodType(forward_flex, pretrained.model)\n    pretrained.model._resize_pos_embed = types.MethodType(\n        _resize_pos_embed, pretrained.model\n    )\n\n    return pretrained\n\n\ndef _make_pretrained_vitl16_384(pretrained, use_readout=\"ignore\", hooks=None):\n    model = timm.create_model(\"vit_large_patch16_384\", pretrained=pretrained)\n\n    hooks = [5, 11, 17, 23] if hooks == None else hooks\n    return _make_vit_b16_backbone(\n        model,\n        features=[256, 512, 1024, 1024],\n        hooks=hooks,\n        vit_features=1024,\n        use_readout=use_readout,\n    )\n\n\ndef _make_pretrained_vitb16_384(pretrained, use_readout=\"ignore\", hooks=None):\n    model = timm.create_model(\"vit_base_patch16_384\", pretrained=pretrained)\n\n    hooks = [2, 5, 8, 11] if hooks == None else hooks\n    return _make_vit_b16_backbone(\n        model, features=[96, 192, 384, 768], hooks=hooks, use_readout=use_readout\n    )\n\n\ndef _make_pretrained_deitb16_384(pretrained, use_readout=\"ignore\", hooks=None):\n    model = timm.create_model(\"vit_deit_base_patch16_384\", pretrained=pretrained)\n\n    hooks = [2, 5, 8, 11] if hooks == None else hooks\n    return _make_vit_b16_backbone(\n        model, features=[96, 192, 384, 768], hooks=hooks, use_readout=use_readout\n    )\n\n\ndef _make_pretrained_deitb16_distil_384(pretrained, use_readout=\"ignore\", hooks=None):\n    model = timm.create_model(\n        \"vit_deit_base_distilled_patch16_384\", pretrained=pretrained\n    )\n\n    hooks = [2, 5, 8, 11] if hooks == None else hooks\n    return _make_vit_b16_backbone(\n        model,\n        features=[96, 192, 384, 768],\n        hooks=hooks,\n        use_readout=use_readout,\n        start_index=2,\n    )\n\n\ndef _make_vit_b_rn50_backbone(\n    model,\n    features=[256, 512, 768, 768],\n    size=[384, 384],\n    hooks=[0, 1, 8, 11],\n    vit_features=768,\n    use_vit_only=False,\n    use_readout=\"ignore\",\n    start_index=1,\n):\n    pretrained = nn.Module()\n\n    pretrained.model = model\n\n    if use_vit_only == True:\n        pretrained.model.blocks[hooks[0]].register_forward_hook(get_activation(\"1\"))\n        pretrained.model.blocks[hooks[1]].register_forward_hook(get_activation(\"2\"))\n    else:\n        pretrained.model.patch_embed.backbone.stages[0].register_forward_hook(\n            get_activation(\"1\")\n        )\n        pretrained.model.patch_embed.backbone.stages[1].register_forward_hook(\n            get_activation(\"2\")\n        )\n\n    pretrained.model.blocks[hooks[2]].register_forward_hook(get_activation(\"3\"))\n    pretrained.model.blocks[hooks[3]].register_forward_hook(get_activation(\"4\"))\n\n    pretrained.activations = activations\n\n    readout_oper = get_readout_oper(vit_features, features, use_readout, start_index)\n\n    if use_vit_only == True:\n        pretrained.act_postprocess1 = nn.Sequential(\n            readout_oper[0],\n            Transpose(1, 2),\n            nn.Unflatten(2, torch.Size([size[0] // 16, size[1] // 16])),\n            nn.Conv2d(\n                in_channels=vit_features,\n                out_channels=features[0],\n                kernel_size=1,\n                stride=1,\n                padding=0,\n            ),\n            nn.ConvTranspose2d(\n                in_channels=features[0],\n                out_channels=features[0],\n                kernel_size=4,\n                stride=4,\n                padding=0,\n                bias=True,\n                dilation=1,\n                groups=1,\n            ),\n        )\n\n        pretrained.act_postprocess2 = nn.Sequential(\n            readout_oper[1],\n            Transpose(1, 2),\n            nn.Unflatten(2, torch.Size([size[0] // 16, size[1] // 16])),\n            nn.Conv2d(\n                in_channels=vit_features,\n                out_channels=features[1],\n                kernel_size=1,\n                stride=1,\n                padding=0,\n            ),\n            nn.ConvTranspose2d(\n                in_channels=features[1],\n                out_channels=features[1],\n                kernel_size=2,\n                stride=2,\n                padding=0,\n                bias=True,\n                dilation=1,\n                groups=1,\n            ),\n        )\n    else:\n        pretrained.act_postprocess1 = nn.Sequential(\n            nn.Identity(), nn.Identity(), nn.Identity()\n        )\n        pretrained.act_postprocess2 = nn.Sequential(\n            nn.Identity(), nn.Identity(), nn.Identity()\n        )\n\n    pretrained.act_postprocess3 = nn.Sequential(\n        readout_oper[2],\n        Transpose(1, 2),\n        nn.Unflatten(2, torch.Size([size[0] // 16, size[1] // 16])),\n        nn.Conv2d(\n            in_channels=vit_features,\n            out_channels=features[2],\n            kernel_size=1,\n            stride=1,\n            padding=0,\n        ),\n    )\n\n    pretrained.act_postprocess4 = nn.Sequential(\n        readout_oper[3],\n        Transpose(1, 2),\n        nn.Unflatten(2, torch.Size([size[0] // 16, size[1] // 16])),\n        nn.Conv2d(\n            in_channels=vit_features,\n            out_channels=features[3],\n            kernel_size=1,\n            stride=1,\n            padding=0,\n        ),\n        nn.Conv2d(\n            in_channels=features[3],\n            out_channels=features[3],\n            kernel_size=3,\n            stride=2,\n            padding=1,\n        ),\n    )\n\n    pretrained.model.start_index = start_index\n    pretrained.model.patch_size = [16, 16]\n\n    # We inject this function into the VisionTransformer instances so that\n    # we can use it with interpolated position embeddings without modifying the library source.\n    pretrained.model.forward_flex = types.MethodType(forward_flex, pretrained.model)\n\n    # We inject this function into the VisionTransformer instances so that\n    # we can use it with interpolated position embeddings without modifying the library source.\n    pretrained.model._resize_pos_embed = types.MethodType(\n        _resize_pos_embed, pretrained.model\n    )\n\n    return pretrained\n\n\ndef _make_pretrained_vitb_rn50_384(\n    pretrained, use_readout=\"ignore\", hooks=None, use_vit_only=False\n):\n    model = timm.create_model(\"vit_base_resnet50_384\", pretrained=pretrained)\n\n    hooks = [0, 1, 8, 11] if hooks == None else hooks\n    return _make_vit_b_rn50_backbone(\n        model,\n        features=[256, 512, 768, 768],\n        size=[384, 384],\n        hooks=hooks,\n        use_vit_only=use_vit_only,\n        use_readout=use_readout,\n    )\n", "ldm/modules/midas/midas/__init__.py": "", "ldm/modules/midas/midas/dpt_depth.py": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .base_model import BaseModel\nfrom .blocks import (\n    FeatureFusionBlock,\n    FeatureFusionBlock_custom,\n    Interpolate,\n    _make_encoder,\n    forward_vit,\n)\n\n\ndef _make_fusion_block(features, use_bn):\n    return FeatureFusionBlock_custom(\n        features,\n        nn.ReLU(False),\n        deconv=False,\n        bn=use_bn,\n        expand=False,\n        align_corners=True,\n    )\n\n\nclass DPT(BaseModel):\n    def __init__(\n        self,\n        head,\n        features=256,\n        backbone=\"vitb_rn50_384\",\n        readout=\"project\",\n        channels_last=False,\n        use_bn=False,\n    ):\n\n        super(DPT, self).__init__()\n\n        self.channels_last = channels_last\n\n        hooks = {\n            \"vitb_rn50_384\": [0, 1, 8, 11],\n            \"vitb16_384\": [2, 5, 8, 11],\n            \"vitl16_384\": [5, 11, 17, 23],\n        }\n\n        # Instantiate backbone and reassemble blocks\n        self.pretrained, self.scratch = _make_encoder(\n            backbone,\n            features,\n            False, # Set to true of you want to train from scratch, uses ImageNet weights\n            groups=1,\n            expand=False,\n            exportable=False,\n            hooks=hooks[backbone],\n            use_readout=readout,\n        )\n\n        self.scratch.refinenet1 = _make_fusion_block(features, use_bn)\n        self.scratch.refinenet2 = _make_fusion_block(features, use_bn)\n        self.scratch.refinenet3 = _make_fusion_block(features, use_bn)\n        self.scratch.refinenet4 = _make_fusion_block(features, use_bn)\n\n        self.scratch.output_conv = head\n\n\n    def forward(self, x):\n        if self.channels_last == True:\n            x.contiguous(memory_format=torch.channels_last)\n\n        layer_1, layer_2, layer_3, layer_4 = forward_vit(self.pretrained, x)\n\n        layer_1_rn = self.scratch.layer1_rn(layer_1)\n        layer_2_rn = self.scratch.layer2_rn(layer_2)\n        layer_3_rn = self.scratch.layer3_rn(layer_3)\n        layer_4_rn = self.scratch.layer4_rn(layer_4)\n\n        path_4 = self.scratch.refinenet4(layer_4_rn)\n        path_3 = self.scratch.refinenet3(path_4, layer_3_rn)\n        path_2 = self.scratch.refinenet2(path_3, layer_2_rn)\n        path_1 = self.scratch.refinenet1(path_2, layer_1_rn)\n\n        out = self.scratch.output_conv(path_1)\n\n        return out\n\n\nclass DPTDepthModel(DPT):\n    def __init__(self, path=None, non_negative=True, **kwargs):\n        features = kwargs[\"features\"] if \"features\" in kwargs else 256\n\n        head = nn.Sequential(\n            nn.Conv2d(features, features // 2, kernel_size=3, stride=1, padding=1),\n            Interpolate(scale_factor=2, mode=\"bilinear\", align_corners=True),\n            nn.Conv2d(features // 2, 32, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(True),\n            nn.Conv2d(32, 1, kernel_size=1, stride=1, padding=0),\n            nn.ReLU(True) if non_negative else nn.Identity(),\n            nn.Identity(),\n        )\n\n        super().__init__(head, **kwargs)\n\n        if path is not None:\n           self.load(path)\n\n    def forward(self, x):\n        return super().forward(x).squeeze(dim=1)\n\n", "ldm/modules/karlo/diffusers_pipeline.py": "# Copyright 2022 Kakao Brain and The HuggingFace Team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport inspect\nfrom typing import List, Optional, Tuple, Union\n\nimport torch\nfrom torch.nn import functional as F\n\nfrom transformers import CLIPTextModelWithProjection, CLIPTokenizer\nfrom transformers.models.clip.modeling_clip import CLIPTextModelOutput\n\nfrom ...models import PriorTransformer, UNet2DConditionModel, UNet2DModel\nfrom ...pipelines import DiffusionPipeline, ImagePipelineOutput\nfrom ...schedulers import UnCLIPScheduler\nfrom ...utils import is_accelerate_available, logging, randn_tensor\nfrom .text_proj import UnCLIPTextProjModel\n\n\nlogger = logging.get_logger(__name__)  # pylint: disable=invalid-name\n\n\nclass UnCLIPPipeline(DiffusionPipeline):\n    \"\"\"\n    Pipeline for text-to-image generation using unCLIP\n    This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods the\n    library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)\n    Args:\n        text_encoder ([`CLIPTextModelWithProjection`]):\n            Frozen text-encoder.\n        tokenizer (`CLIPTokenizer`):\n            Tokenizer of class\n            [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer).\n        prior ([`PriorTransformer`]):\n            The canonincal unCLIP prior to approximate the image embedding from the text embedding.\n        text_proj ([`UnCLIPTextProjModel`]):\n            Utility class to prepare and combine the embeddings before they are passed to the decoder.\n        decoder ([`UNet2DConditionModel`]):\n            The decoder to invert the image embedding into an image.\n        super_res_first ([`UNet2DModel`]):\n            Super resolution unet. Used in all but the last step of the super resolution diffusion process.\n        super_res_last ([`UNet2DModel`]):\n            Super resolution unet. Used in the last step of the super resolution diffusion process.\n        prior_scheduler ([`UnCLIPScheduler`]):\n            Scheduler used in the prior denoising process. Just a modified DDPMScheduler.\n        decoder_scheduler ([`UnCLIPScheduler`]):\n            Scheduler used in the decoder denoising process. Just a modified DDPMScheduler.\n        super_res_scheduler ([`UnCLIPScheduler`]):\n            Scheduler used in the super resolution denoising process. Just a modified DDPMScheduler.\n    \"\"\"\n\n    prior: PriorTransformer\n    decoder: UNet2DConditionModel\n    text_proj: UnCLIPTextProjModel\n    text_encoder: CLIPTextModelWithProjection\n    tokenizer: CLIPTokenizer\n    super_res_first: UNet2DModel\n    super_res_last: UNet2DModel\n\n    prior_scheduler: UnCLIPScheduler\n    decoder_scheduler: UnCLIPScheduler\n    super_res_scheduler: UnCLIPScheduler\n\n    def __init__(\n        self,\n        prior: PriorTransformer,\n        decoder: UNet2DConditionModel,\n        text_encoder: CLIPTextModelWithProjection,\n        tokenizer: CLIPTokenizer,\n        text_proj: UnCLIPTextProjModel,\n        super_res_first: UNet2DModel,\n        super_res_last: UNet2DModel,\n        prior_scheduler: UnCLIPScheduler,\n        decoder_scheduler: UnCLIPScheduler,\n        super_res_scheduler: UnCLIPScheduler,\n    ):\n        super().__init__()\n\n        self.register_modules(\n            prior=prior,\n            decoder=decoder,\n            text_encoder=text_encoder,\n            tokenizer=tokenizer,\n            text_proj=text_proj,\n            super_res_first=super_res_first,\n            super_res_last=super_res_last,\n            prior_scheduler=prior_scheduler,\n            decoder_scheduler=decoder_scheduler,\n            super_res_scheduler=super_res_scheduler,\n        )\n\n    def prepare_latents(self, shape, dtype, device, generator, latents, scheduler):\n        if latents is None:\n            latents = randn_tensor(shape, generator=generator, device=device, dtype=dtype)\n        else:\n            if latents.shape != shape:\n                raise ValueError(f\"Unexpected latents shape, got {latents.shape}, expected {shape}\")\n            latents = latents.to(device)\n\n        latents = latents * scheduler.init_noise_sigma\n        return latents\n\n    def _encode_prompt(\n        self,\n        prompt,\n        device,\n        num_images_per_prompt,\n        do_classifier_free_guidance,\n        text_model_output: Optional[Union[CLIPTextModelOutput, Tuple]] = None,\n        text_attention_mask: Optional[torch.Tensor] = None,\n    ):\n        if text_model_output is None:\n            batch_size = len(prompt) if isinstance(prompt, list) else 1\n            # get prompt text embeddings\n            text_inputs = self.tokenizer(\n                prompt,\n                padding=\"max_length\",\n                max_length=self.tokenizer.model_max_length,\n                return_tensors=\"pt\",\n            )\n            text_input_ids = text_inputs.input_ids\n            text_mask = text_inputs.attention_mask.bool().to(device)\n\n            if text_input_ids.shape[-1] > self.tokenizer.model_max_length:\n                removed_text = self.tokenizer.batch_decode(text_input_ids[:, self.tokenizer.model_max_length :])\n                logger.warning(\n                    \"The following part of your input was truncated because CLIP can only handle sequences up to\"\n                    f\" {self.tokenizer.model_max_length} tokens: {removed_text}\"\n                )\n                text_input_ids = text_input_ids[:, : self.tokenizer.model_max_length]\n\n            text_encoder_output = self.text_encoder(text_input_ids.to(device))\n\n            text_embeddings = text_encoder_output.text_embeds\n            text_encoder_hidden_states = text_encoder_output.last_hidden_state\n\n        else:\n            batch_size = text_model_output[0].shape[0]\n            text_embeddings, text_encoder_hidden_states = text_model_output[0], text_model_output[1]\n            text_mask = text_attention_mask\n\n        text_embeddings = text_embeddings.repeat_interleave(num_images_per_prompt, dim=0)\n        text_encoder_hidden_states = text_encoder_hidden_states.repeat_interleave(num_images_per_prompt, dim=0)\n        text_mask = text_mask.repeat_interleave(num_images_per_prompt, dim=0)\n\n        if do_classifier_free_guidance:\n            uncond_tokens = [\"\"] * batch_size\n\n            uncond_input = self.tokenizer(\n                uncond_tokens,\n                padding=\"max_length\",\n                max_length=self.tokenizer.model_max_length,\n                truncation=True,\n                return_tensors=\"pt\",\n            )\n            uncond_text_mask = uncond_input.attention_mask.bool().to(device)\n            uncond_embeddings_text_encoder_output = self.text_encoder(uncond_input.input_ids.to(device))\n\n            uncond_embeddings = uncond_embeddings_text_encoder_output.text_embeds\n            uncond_text_encoder_hidden_states = uncond_embeddings_text_encoder_output.last_hidden_state\n\n            # duplicate unconditional embeddings for each generation per prompt, using mps friendly method\n\n            seq_len = uncond_embeddings.shape[1]\n            uncond_embeddings = uncond_embeddings.repeat(1, num_images_per_prompt)\n            uncond_embeddings = uncond_embeddings.view(batch_size * num_images_per_prompt, seq_len)\n\n            seq_len = uncond_text_encoder_hidden_states.shape[1]\n            uncond_text_encoder_hidden_states = uncond_text_encoder_hidden_states.repeat(1, num_images_per_prompt, 1)\n            uncond_text_encoder_hidden_states = uncond_text_encoder_hidden_states.view(\n                batch_size * num_images_per_prompt, seq_len, -1\n            )\n            uncond_text_mask = uncond_text_mask.repeat_interleave(num_images_per_prompt, dim=0)\n\n            # done duplicates\n\n            # For classifier free guidance, we need to do two forward passes.\n            # Here we concatenate the unconditional and text embeddings into a single batch\n            # to avoid doing two forward passes\n            text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n            text_encoder_hidden_states = torch.cat([uncond_text_encoder_hidden_states, text_encoder_hidden_states])\n\n            text_mask = torch.cat([uncond_text_mask, text_mask])\n\n        return text_embeddings, text_encoder_hidden_states, text_mask\n\n    def enable_sequential_cpu_offload(self, gpu_id=0):\n        r\"\"\"\n        Offloads all models to CPU using accelerate, significantly reducing memory usage. When called, the pipeline's\n        models have their state dicts saved to CPU and then are moved to a `torch.device('meta') and loaded to GPU only\n        when their specific submodule has its `forward` method called.\n        \"\"\"\n        if is_accelerate_available():\n            from accelerate import cpu_offload\n        else:\n            raise ImportError(\"Please install accelerate via `pip install accelerate`\")\n\n        device = torch.device(f\"cuda:{gpu_id}\")\n\n        # TODO: self.prior.post_process_latents is not covered by the offload hooks, so it fails if added to the list\n        models = [\n            self.decoder,\n            self.text_proj,\n            self.text_encoder,\n            self.super_res_first,\n            self.super_res_last,\n        ]\n        for cpu_offloaded_model in models:\n            if cpu_offloaded_model is not None:\n                cpu_offload(cpu_offloaded_model, device)\n\n    @property\n    def _execution_device(self):\n        r\"\"\"\n        Returns the device on which the pipeline's models will be executed. After calling\n        `pipeline.enable_sequential_cpu_offload()` the execution device can only be inferred from Accelerate's module\n        hooks.\n        \"\"\"\n        if self.device != torch.device(\"meta\") or not hasattr(self.decoder, \"_hf_hook\"):\n            return self.device\n        for module in self.decoder.modules():\n            if (\n                hasattr(module, \"_hf_hook\")\n                and hasattr(module._hf_hook, \"execution_device\")\n                and module._hf_hook.execution_device is not None\n            ):\n                return torch.device(module._hf_hook.execution_device)\n        return self.device\n\n    @torch.no_grad()\n    def __call__(\n        self,\n        prompt: Optional[Union[str, List[str]]] = None,\n        num_images_per_prompt: int = 1,\n        prior_num_inference_steps: int = 25,\n        decoder_num_inference_steps: int = 25,\n        super_res_num_inference_steps: int = 7,\n        generator: Optional[torch.Generator] = None,\n        prior_latents: Optional[torch.FloatTensor] = None,\n        decoder_latents: Optional[torch.FloatTensor] = None,\n        super_res_latents: Optional[torch.FloatTensor] = None,\n        text_model_output: Optional[Union[CLIPTextModelOutput, Tuple]] = None,\n        text_attention_mask: Optional[torch.Tensor] = None,\n        prior_guidance_scale: float = 4.0,\n        decoder_guidance_scale: float = 8.0,\n        output_type: Optional[str] = \"pil\",\n        return_dict: bool = True,\n    ):\n        \"\"\"\n        Function invoked when calling the pipeline for generation.\n        Args:\n            prompt (`str` or `List[str]`):\n                The prompt or prompts to guide the image generation. This can only be left undefined if\n                `text_model_output` and `text_attention_mask` is passed.\n            num_images_per_prompt (`int`, *optional*, defaults to 1):\n                The number of images to generate per prompt.\n            prior_num_inference_steps (`int`, *optional*, defaults to 25):\n                The number of denoising steps for the prior. More denoising steps usually lead to a higher quality\n                image at the expense of slower inference.\n            decoder_num_inference_steps (`int`, *optional*, defaults to 25):\n                The number of denoising steps for the decoder. More denoising steps usually lead to a higher quality\n                image at the expense of slower inference.\n            super_res_num_inference_steps (`int`, *optional*, defaults to 7):\n                The number of denoising steps for super resolution. More denoising steps usually lead to a higher\n                quality image at the expense of slower inference.\n            generator (`torch.Generator`, *optional*):\n                One or a list of [torch generator(s)](https://pytorch.org/docs/stable/generated/torch.Generator.html)\n                to make generation deterministic.\n            prior_latents (`torch.FloatTensor` of shape (batch size, embeddings dimension), *optional*):\n                Pre-generated noisy latents to be used as inputs for the prior.\n            decoder_latents (`torch.FloatTensor` of shape (batch size, channels, height, width), *optional*):\n                Pre-generated noisy latents to be used as inputs for the decoder.\n            super_res_latents (`torch.FloatTensor` of shape (batch size, channels, super res height, super res width), *optional*):\n                Pre-generated noisy latents to be used as inputs for the decoder.\n            prior_guidance_scale (`float`, *optional*, defaults to 4.0):\n                Guidance scale as defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).\n                `guidance_scale` is defined as `w` of equation 2. of [Imagen\n                Paper](https://arxiv.org/pdf/2205.11487.pdf). Guidance scale is enabled by setting `guidance_scale >\n                1`. Higher guidance scale encourages to generate images that are closely linked to the text `prompt`,\n                usually at the expense of lower image quality.\n            decoder_guidance_scale (`float`, *optional*, defaults to 4.0):\n                Guidance scale as defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).\n                `guidance_scale` is defined as `w` of equation 2. of [Imagen\n                Paper](https://arxiv.org/pdf/2205.11487.pdf). Guidance scale is enabled by setting `guidance_scale >\n                1`. Higher guidance scale encourages to generate images that are closely linked to the text `prompt`,\n                usually at the expense of lower image quality.\n            text_model_output (`CLIPTextModelOutput`, *optional*):\n                Pre-defined CLIPTextModel outputs that can be derived from the text encoder. Pre-defined text outputs\n                can be passed for tasks like text embedding interpolations. Make sure to also pass\n                `text_attention_mask` in this case. `prompt` can the be left to `None`.\n            text_attention_mask (`torch.Tensor`, *optional*):\n                Pre-defined CLIP text attention mask that can be derived from the tokenizer. Pre-defined text attention\n                masks are necessary when passing `text_model_output`.\n            output_type (`str`, *optional*, defaults to `\"pil\"`):\n                The output format of the generated image. Choose between\n                [PIL](https://pillow.readthedocs.io/en/stable/): `PIL.Image.Image` or `np.array`.\n            return_dict (`bool`, *optional*, defaults to `True`):\n                Whether or not to return a [`~pipelines.ImagePipelineOutput`] instead of a plain tuple.\n        \"\"\"\n        if prompt is not None:\n            if isinstance(prompt, str):\n                batch_size = 1\n            elif isinstance(prompt, list):\n                batch_size = len(prompt)\n            else:\n                raise ValueError(f\"`prompt` has to be of type `str` or `list` but is {type(prompt)}\")\n        else:\n            batch_size = text_model_output[0].shape[0]\n\n        device = self._execution_device\n\n        batch_size = batch_size * num_images_per_prompt\n\n        do_classifier_free_guidance = prior_guidance_scale > 1.0 or decoder_guidance_scale > 1.0\n\n        text_embeddings, text_encoder_hidden_states, text_mask = self._encode_prompt(\n            prompt, device, num_images_per_prompt, do_classifier_free_guidance, text_model_output, text_attention_mask\n        )\n\n        # prior\n\n        self.prior_scheduler.set_timesteps(prior_num_inference_steps, device=device)\n        prior_timesteps_tensor = self.prior_scheduler.timesteps\n\n        embedding_dim = self.prior.config.embedding_dim\n\n        prior_latents = self.prepare_latents(\n            (batch_size, embedding_dim),\n            text_embeddings.dtype,\n            device,\n            generator,\n            prior_latents,\n            self.prior_scheduler,\n        )\n\n        for i, t in enumerate(self.progress_bar(prior_timesteps_tensor)):\n            # expand the latents if we are doing classifier free guidance\n            latent_model_input = torch.cat([prior_latents] * 2) if do_classifier_free_guidance else prior_latents\n\n            predicted_image_embedding = self.prior(\n                latent_model_input,\n                timestep=t,\n                proj_embedding=text_embeddings,\n                encoder_hidden_states=text_encoder_hidden_states,\n                attention_mask=text_mask,\n            ).predicted_image_embedding\n\n            if do_classifier_free_guidance:\n                predicted_image_embedding_uncond, predicted_image_embedding_text = predicted_image_embedding.chunk(2)\n                predicted_image_embedding = predicted_image_embedding_uncond + prior_guidance_scale * (\n                    predicted_image_embedding_text - predicted_image_embedding_uncond\n                )\n\n            if i + 1 == prior_timesteps_tensor.shape[0]:\n                prev_timestep = None\n            else:\n                prev_timestep = prior_timesteps_tensor[i + 1]\n\n            prior_latents = self.prior_scheduler.step(\n                predicted_image_embedding,\n                timestep=t,\n                sample=prior_latents,\n                generator=generator,\n                prev_timestep=prev_timestep,\n            ).prev_sample\n\n        prior_latents = self.prior.post_process_latents(prior_latents)\n\n        image_embeddings = prior_latents\n\n        # done prior\n\n        # decoder\n\n        text_encoder_hidden_states, additive_clip_time_embeddings = self.text_proj(\n            image_embeddings=image_embeddings,\n            text_embeddings=text_embeddings,\n            text_encoder_hidden_states=text_encoder_hidden_states,\n            do_classifier_free_guidance=do_classifier_free_guidance,\n        )\n\n        decoder_text_mask = F.pad(text_mask, (self.text_proj.clip_extra_context_tokens, 0), value=1)\n\n        self.decoder_scheduler.set_timesteps(decoder_num_inference_steps, device=device)\n        decoder_timesteps_tensor = self.decoder_scheduler.timesteps\n\n        num_channels_latents = self.decoder.in_channels\n        height = self.decoder.sample_size\n        width = self.decoder.sample_size\n\n        decoder_latents = self.prepare_latents(\n            (batch_size, num_channels_latents, height, width),\n            text_encoder_hidden_states.dtype,\n            device,\n            generator,\n            decoder_latents,\n            self.decoder_scheduler,\n        )\n\n        for i, t in enumerate(self.progress_bar(decoder_timesteps_tensor)):\n            # expand the latents if we are doing classifier free guidance\n            latent_model_input = torch.cat([decoder_latents] * 2) if do_classifier_free_guidance else decoder_latents\n\n            noise_pred = self.decoder(\n                sample=latent_model_input,\n                timestep=t,\n                encoder_hidden_states=text_encoder_hidden_states,\n                class_labels=additive_clip_time_embeddings,\n                attention_mask=decoder_text_mask,\n            ).sample\n\n            if do_classifier_free_guidance:\n                noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n                noise_pred_uncond, _ = noise_pred_uncond.split(latent_model_input.shape[1], dim=1)\n                noise_pred_text, predicted_variance = noise_pred_text.split(latent_model_input.shape[1], dim=1)\n                noise_pred = noise_pred_uncond + decoder_guidance_scale * (noise_pred_text - noise_pred_uncond)\n                noise_pred = torch.cat([noise_pred, predicted_variance], dim=1)\n\n            if i + 1 == decoder_timesteps_tensor.shape[0]:\n                prev_timestep = None\n            else:\n                prev_timestep = decoder_timesteps_tensor[i + 1]\n\n            # compute the previous noisy sample x_t -> x_t-1\n            decoder_latents = self.decoder_scheduler.step(\n                noise_pred, t, decoder_latents, prev_timestep=prev_timestep, generator=generator\n            ).prev_sample\n\n        decoder_latents = decoder_latents.clamp(-1, 1)\n\n        image_small = decoder_latents\n\n        # done decoder\n\n        # super res\n\n        self.super_res_scheduler.set_timesteps(super_res_num_inference_steps, device=device)\n        super_res_timesteps_tensor = self.super_res_scheduler.timesteps\n\n        channels = self.super_res_first.in_channels // 2\n        height = self.super_res_first.sample_size\n        width = self.super_res_first.sample_size\n\n        super_res_latents = self.prepare_latents(\n            (batch_size, channels, height, width),\n            image_small.dtype,\n            device,\n            generator,\n            super_res_latents,\n            self.super_res_scheduler,\n        )\n\n        interpolate_antialias = {}\n        if \"antialias\" in inspect.signature(F.interpolate).parameters:\n            interpolate_antialias[\"antialias\"] = True\n\n        image_upscaled = F.interpolate(\n            image_small, size=[height, width], mode=\"bicubic\", align_corners=False, **interpolate_antialias\n        )\n\n        for i, t in enumerate(self.progress_bar(super_res_timesteps_tensor)):\n            # no classifier free guidance\n\n            if i == super_res_timesteps_tensor.shape[0] - 1:\n                unet = self.super_res_last\n            else:\n                unet = self.super_res_first\n\n            latent_model_input = torch.cat([super_res_latents, image_upscaled], dim=1)\n\n            noise_pred = unet(\n                sample=latent_model_input,\n                timestep=t,\n            ).sample\n\n            if i + 1 == super_res_timesteps_tensor.shape[0]:\n                prev_timestep = None\n            else:\n                prev_timestep = super_res_timesteps_tensor[i + 1]\n\n            # compute the previous noisy sample x_t -> x_t-1\n            super_res_latents = self.super_res_scheduler.step(\n                noise_pred, t, super_res_latents, prev_timestep=prev_timestep, generator=generator\n            ).prev_sample\n\n        image = super_res_latents\n        # done super res\n\n        # post processing\n\n        image = image * 0.5 + 0.5\n        image = image.clamp(0, 1)\n        image = image.cpu().permute(0, 2, 3, 1).float().numpy()\n\n        if output_type == \"pil\":\n            image = self.numpy_to_pil(image)\n\n        if not return_dict:\n            return (image,)\n\n        return ImagePipelineOutput(images=image)", "ldm/modules/karlo/__init__.py": "", "ldm/modules/karlo/kakao/sampler.py": "# ------------------------------------------------------------------------------------\n# Karlo-v1.0.alpha\n# Copyright (c) 2022 KakaoBrain. All Rights Reserved.\n\n# source: https://github.com/kakaobrain/karlo/blob/3c68a50a16d76b48a15c181d1c5a5e0879a90f85/karlo/sampler/t2i.py#L15\n# ------------------------------------------------------------------------------------\n\nfrom typing import Iterator\n\nimport torch\nimport torchvision.transforms.functional as TVF\nfrom torchvision.transforms import InterpolationMode\n\nfrom .template import BaseSampler, CKPT_PATH\n\n\nclass T2ISampler(BaseSampler):\n    \"\"\"\n    A sampler for text-to-image generation.\n    :param root_dir: directory for model checkpoints.\n    :param sampling_type: [\"default\", \"fast\"]\n    \"\"\"\n\n    def __init__(\n            self,\n            root_dir: str,\n            sampling_type: str = \"default\",\n    ):\n        super().__init__(root_dir, sampling_type)\n\n    @classmethod\n    def from_pretrained(\n            cls,\n            root_dir: str,\n            clip_model_path: str,\n            clip_stat_path: str,\n            sampling_type: str = \"default\",\n    ):\n\n        model = cls(\n            root_dir=root_dir,\n            sampling_type=sampling_type,\n        )\n        model.load_clip(clip_model_path)\n        model.load_prior(\n            f\"{CKPT_PATH['prior']}\",\n            clip_stat_path=clip_stat_path,\n            prior_config=\"configs/karlo/prior_1B_vit_l.yaml\"\n        )\n        model.load_decoder(f\"{CKPT_PATH['decoder']}\", decoder_config=\"configs/karlo/decoder_900M_vit_l.yaml\")\n        model.load_sr_64_256(CKPT_PATH[\"sr_256\"], sr_config=\"configs/karlo/improved_sr_64_256_1.4B.yaml\")\n        return model\n\n    def preprocess(\n            self,\n            prompt: str,\n            bsz: int,\n    ):\n        \"\"\"Setup prompts & cfg scales\"\"\"\n        prompts_batch = [prompt for _ in range(bsz)]\n\n        prior_cf_scales_batch = [self._prior_cf_scale] * len(prompts_batch)\n        prior_cf_scales_batch = torch.tensor(prior_cf_scales_batch, device=\"cuda\")\n\n        decoder_cf_scales_batch = [self._decoder_cf_scale] * len(prompts_batch)\n        decoder_cf_scales_batch = torch.tensor(decoder_cf_scales_batch, device=\"cuda\")\n\n        \"\"\" Get CLIP text feature \"\"\"\n        clip_model = self._clip\n        tokenizer = self._tokenizer\n        max_txt_length = self._prior.model.text_ctx\n\n        tok, mask = tokenizer.padded_tokens_and_mask(prompts_batch, max_txt_length)\n        cf_token, cf_mask = tokenizer.padded_tokens_and_mask([\"\"], max_txt_length)\n        if not (cf_token.shape == tok.shape):\n            cf_token = cf_token.expand(tok.shape[0], -1)\n            cf_mask = cf_mask.expand(tok.shape[0], -1)\n\n        tok = torch.cat([tok, cf_token], dim=0)\n        mask = torch.cat([mask, cf_mask], dim=0)\n\n        tok, mask = tok.to(device=\"cuda\"), mask.to(device=\"cuda\")\n        txt_feat, txt_feat_seq = clip_model.encode_text(tok)\n\n        return (\n            prompts_batch,\n            prior_cf_scales_batch,\n            decoder_cf_scales_batch,\n            txt_feat,\n            txt_feat_seq,\n            tok,\n            mask,\n        )\n\n    def __call__(\n            self,\n            prompt: str,\n            bsz: int,\n            progressive_mode=None,\n    ) -> Iterator[torch.Tensor]:\n        assert progressive_mode in (\"loop\", \"stage\", \"final\")\n        with torch.no_grad(), torch.cuda.amp.autocast():\n            (\n                prompts_batch,\n                prior_cf_scales_batch,\n                decoder_cf_scales_batch,\n                txt_feat,\n                txt_feat_seq,\n                tok,\n                mask,\n            ) = self.preprocess(\n                prompt,\n                bsz,\n            )\n\n            \"\"\" Transform CLIP text feature into image feature \"\"\"\n            img_feat = self._prior(\n                txt_feat,\n                txt_feat_seq,\n                mask,\n                prior_cf_scales_batch,\n                timestep_respacing=self._prior_sm,\n            )\n\n            \"\"\" Generate 64x64px images \"\"\"\n            images_64_outputs = self._decoder(\n                txt_feat,\n                txt_feat_seq,\n                tok,\n                mask,\n                img_feat,\n                cf_guidance_scales=decoder_cf_scales_batch,\n                timestep_respacing=self._decoder_sm,\n            )\n\n            images_64 = None\n            for k, out in enumerate(images_64_outputs):\n                images_64 = out\n                if progressive_mode == \"loop\":\n                    yield torch.clamp(out * 0.5 + 0.5, 0.0, 1.0)\n            if progressive_mode == \"stage\":\n                yield torch.clamp(out * 0.5 + 0.5, 0.0, 1.0)\n\n            images_64 = torch.clamp(images_64, -1, 1)\n\n            \"\"\" Upsample 64x64 to 256x256 \"\"\"\n            images_256 = TVF.resize(\n                images_64,\n                [256, 256],\n                interpolation=InterpolationMode.BICUBIC,\n                antialias=True,\n            )\n            images_256_outputs = self._sr_64_256(\n                images_256, timestep_respacing=self._sr_sm\n            )\n\n            for k, out in enumerate(images_256_outputs):\n                images_256 = out\n                if progressive_mode == \"loop\":\n                    yield torch.clamp(out * 0.5 + 0.5, 0.0, 1.0)\n            if progressive_mode == \"stage\":\n                yield torch.clamp(out * 0.5 + 0.5, 0.0, 1.0)\n\n        yield torch.clamp(images_256 * 0.5 + 0.5, 0.0, 1.0)\n\n\nclass PriorSampler(BaseSampler):\n    \"\"\"\n    A sampler for text-to-image generation, but only the prior.\n    :param root_dir: directory for model checkpoints.\n    :param sampling_type: [\"default\", \"fast\"]\n    \"\"\"\n\n    def __init__(\n            self,\n            root_dir: str,\n            sampling_type: str = \"default\",\n    ):\n        super().__init__(root_dir, sampling_type)\n\n    @classmethod\n    def from_pretrained(\n            cls,\n            root_dir: str,\n            clip_model_path: str,\n            clip_stat_path: str,\n            sampling_type: str = \"default\",\n    ):\n        model = cls(\n            root_dir=root_dir,\n            sampling_type=sampling_type,\n        )\n        model.load_clip(clip_model_path)\n        model.load_prior(\n            f\"{CKPT_PATH['prior']}\",\n            clip_stat_path=clip_stat_path,\n            prior_config=\"configs/karlo/prior_1B_vit_l.yaml\"\n        )\n        return model\n\n    def preprocess(\n            self,\n            prompt: str,\n            bsz: int,\n    ):\n        \"\"\"Setup prompts & cfg scales\"\"\"\n        prompts_batch = [prompt for _ in range(bsz)]\n\n        prior_cf_scales_batch = [self._prior_cf_scale] * len(prompts_batch)\n        prior_cf_scales_batch = torch.tensor(prior_cf_scales_batch, device=\"cuda\")\n\n        decoder_cf_scales_batch = [self._decoder_cf_scale] * len(prompts_batch)\n        decoder_cf_scales_batch = torch.tensor(decoder_cf_scales_batch, device=\"cuda\")\n\n        \"\"\" Get CLIP text feature \"\"\"\n        clip_model = self._clip\n        tokenizer = self._tokenizer\n        max_txt_length = self._prior.model.text_ctx\n\n        tok, mask = tokenizer.padded_tokens_and_mask(prompts_batch, max_txt_length)\n        cf_token, cf_mask = tokenizer.padded_tokens_and_mask([\"\"], max_txt_length)\n        if not (cf_token.shape == tok.shape):\n            cf_token = cf_token.expand(tok.shape[0], -1)\n            cf_mask = cf_mask.expand(tok.shape[0], -1)\n\n        tok = torch.cat([tok, cf_token], dim=0)\n        mask = torch.cat([mask, cf_mask], dim=0)\n\n        tok, mask = tok.to(device=\"cuda\"), mask.to(device=\"cuda\")\n        txt_feat, txt_feat_seq = clip_model.encode_text(tok)\n\n        return (\n            prompts_batch,\n            prior_cf_scales_batch,\n            decoder_cf_scales_batch,\n            txt_feat,\n            txt_feat_seq,\n            tok,\n            mask,\n        )\n\n    def __call__(\n            self,\n            prompt: str,\n            bsz: int,\n            progressive_mode=None,\n    ) -> Iterator[torch.Tensor]:\n        assert progressive_mode in (\"loop\", \"stage\", \"final\")\n        with torch.no_grad(), torch.cuda.amp.autocast():\n            (\n                prompts_batch,\n                prior_cf_scales_batch,\n                decoder_cf_scales_batch,\n                txt_feat,\n                txt_feat_seq,\n                tok,\n                mask,\n            ) = self.preprocess(\n                prompt,\n                bsz,\n            )\n\n            \"\"\" Transform CLIP text feature into image feature \"\"\"\n            img_feat = self._prior(\n                txt_feat,\n                txt_feat_seq,\n                mask,\n                prior_cf_scales_batch,\n                timestep_respacing=self._prior_sm,\n            )\n\n            yield img_feat\n", "ldm/modules/karlo/kakao/template.py": "# ------------------------------------------------------------------------------------\n# Karlo-v1.0.alpha\n# Copyright (c) 2022 KakaoBrain. All Rights Reserved.\n# ------------------------------------------------------------------------------------\n\nimport os\nimport logging\nimport torch\n\nfrom omegaconf import OmegaConf\n\nfrom ldm.modules.karlo.kakao.models.clip import CustomizedCLIP, CustomizedTokenizer\nfrom ldm.modules.karlo.kakao.models.prior_model import PriorDiffusionModel\nfrom ldm.modules.karlo.kakao.models.decoder_model import Text2ImProgressiveModel\nfrom ldm.modules.karlo.kakao.models.sr_64_256 import ImprovedSupRes64to256ProgressiveModel\n\n\nSAMPLING_CONF = {\n    \"default\": {\n        \"prior_sm\": \"25\",\n        \"prior_n_samples\": 1,\n        \"prior_cf_scale\": 4.0,\n        \"decoder_sm\": \"50\",\n        \"decoder_cf_scale\": 8.0,\n        \"sr_sm\": \"7\",\n    },\n    \"fast\": {\n        \"prior_sm\": \"25\",\n        \"prior_n_samples\": 1,\n        \"prior_cf_scale\": 4.0,\n        \"decoder_sm\": \"25\",\n        \"decoder_cf_scale\": 8.0,\n        \"sr_sm\": \"7\",\n    },\n}\n\nCKPT_PATH = {\n    \"prior\": \"prior-ckpt-step=01000000-of-01000000.ckpt\",\n    \"decoder\": \"decoder-ckpt-step=01000000-of-01000000.ckpt\",\n    \"sr_256\": \"improved-sr-ckpt-step=1.2M.ckpt\",\n}\n\n\nclass BaseSampler:\n    _PRIOR_CLASS = PriorDiffusionModel\n    _DECODER_CLASS = Text2ImProgressiveModel\n    _SR256_CLASS = ImprovedSupRes64to256ProgressiveModel\n\n    def __init__(\n        self,\n        root_dir: str,\n        sampling_type: str = \"fast\",\n    ):\n        self._root_dir = root_dir\n\n        sampling_type = SAMPLING_CONF[sampling_type]\n        self._prior_sm = sampling_type[\"prior_sm\"]\n        self._prior_n_samples = sampling_type[\"prior_n_samples\"]\n        self._prior_cf_scale = sampling_type[\"prior_cf_scale\"]\n\n        assert self._prior_n_samples == 1\n\n        self._decoder_sm = sampling_type[\"decoder_sm\"]\n        self._decoder_cf_scale = sampling_type[\"decoder_cf_scale\"]\n\n        self._sr_sm = sampling_type[\"sr_sm\"]\n\n    def __repr__(self):\n        line = \"\"\n        line += f\"Prior, sampling method: {self._prior_sm}, cf_scale: {self._prior_cf_scale}\\n\"\n        line += f\"Decoder, sampling method: {self._decoder_sm}, cf_scale: {self._decoder_cf_scale}\\n\"\n        line += f\"SR(64->256), sampling method: {self._sr_sm}\"\n\n        return line\n\n    def load_clip(self, clip_path: str):\n        clip = CustomizedCLIP.load_from_checkpoint(\n            os.path.join(self._root_dir, clip_path)\n        )\n        clip = torch.jit.script(clip)\n        clip.cuda()\n        clip.eval()\n\n        self._clip = clip\n        self._tokenizer = CustomizedTokenizer()\n\n    def load_prior(\n        self,\n        ckpt_path: str,\n        clip_stat_path: str,\n        prior_config: str = \"configs/prior_1B_vit_l.yaml\"\n    ):\n        logging.info(f\"Loading prior: {ckpt_path}\")\n\n        config = OmegaConf.load(prior_config)\n        clip_mean, clip_std = torch.load(\n            os.path.join(self._root_dir, clip_stat_path), map_location=\"cpu\"\n        )\n\n        prior = self._PRIOR_CLASS.load_from_checkpoint(\n            config,\n            self._tokenizer,\n            clip_mean,\n            clip_std,\n            os.path.join(self._root_dir, ckpt_path),\n            strict=True,\n        )\n        prior.cuda()\n        prior.eval()\n        logging.info(\"done.\")\n\n        self._prior = prior\n\n    def load_decoder(self, ckpt_path: str, decoder_config: str = \"configs/decoder_900M_vit_l.yaml\"):\n        logging.info(f\"Loading decoder: {ckpt_path}\")\n\n        config = OmegaConf.load(decoder_config)\n        decoder = self._DECODER_CLASS.load_from_checkpoint(\n            config,\n            self._tokenizer,\n            os.path.join(self._root_dir, ckpt_path),\n            strict=True,\n        )\n        decoder.cuda()\n        decoder.eval()\n        logging.info(\"done.\")\n\n        self._decoder = decoder\n\n    def load_sr_64_256(self, ckpt_path: str, sr_config: str = \"configs/improved_sr_64_256_1.4B.yaml\"):\n        logging.info(f\"Loading SR(64->256): {ckpt_path}\")\n\n        config = OmegaConf.load(sr_config)\n        sr = self._SR256_CLASS.load_from_checkpoint(\n            config, os.path.join(self._root_dir, ckpt_path), strict=True\n        )\n        sr.cuda()\n        sr.eval()\n        logging.info(\"done.\")\n\n        self._sr_64_256 = sr", "ldm/modules/karlo/kakao/__init__.py": "", "ldm/modules/karlo/kakao/models/prior_model.py": "# ------------------------------------------------------------------------------------\n# Karlo-v1.0.alpha\n# Copyright (c) 2022 KakaoBrain. All Rights Reserved.\n# ------------------------------------------------------------------------------------\n\nimport copy\nimport torch\n\nfrom ldm.modules.karlo.kakao.modules import create_gaussian_diffusion\nfrom ldm.modules.karlo.kakao.modules.xf import PriorTransformer\n\n\nclass PriorDiffusionModel(torch.nn.Module):\n    \"\"\"\n    A prior that generates clip image feature based on the text prompt.\n\n    :param config: yaml config to define the decoder.\n    :param tokenizer: tokenizer used in clip.\n    :param clip_mean: mean to normalize the clip image feature (zero-mean, unit variance).\n    :param clip_std: std to noramlize the clip image feature (zero-mean, unit variance).\n    \"\"\"\n\n    def __init__(self, config, tokenizer, clip_mean, clip_std):\n        super().__init__()\n\n        self._conf = config\n        self._model_conf = config.model.hparams\n        self._diffusion_kwargs = dict(\n            steps=config.diffusion.steps,\n            learn_sigma=config.diffusion.learn_sigma,\n            sigma_small=config.diffusion.sigma_small,\n            noise_schedule=config.diffusion.noise_schedule,\n            use_kl=config.diffusion.use_kl,\n            predict_xstart=config.diffusion.predict_xstart,\n            rescale_learned_sigmas=config.diffusion.rescale_learned_sigmas,\n            timestep_respacing=config.diffusion.timestep_respacing,\n        )\n        self._tokenizer = tokenizer\n\n        self.register_buffer(\"clip_mean\", clip_mean[None, :], persistent=False)\n        self.register_buffer(\"clip_std\", clip_std[None, :], persistent=False)\n\n        causal_mask = self.get_causal_mask()\n        self.register_buffer(\"causal_mask\", causal_mask, persistent=False)\n\n        self.model = PriorTransformer(\n            text_ctx=self._model_conf.text_ctx,\n            xf_width=self._model_conf.xf_width,\n            xf_layers=self._model_conf.xf_layers,\n            xf_heads=self._model_conf.xf_heads,\n            xf_final_ln=self._model_conf.xf_final_ln,\n            clip_dim=self._model_conf.clip_dim,\n        )\n\n        cf_token, cf_mask = self.set_cf_text_tensor()\n        self.register_buffer(\"cf_token\", cf_token, persistent=False)\n        self.register_buffer(\"cf_mask\", cf_mask, persistent=False)\n\n    @classmethod\n    def load_from_checkpoint(\n        cls, config, tokenizer, clip_mean, clip_std, ckpt_path, strict: bool = True\n    ):\n        ckpt = torch.load(ckpt_path, map_location=\"cpu\")[\"state_dict\"]\n\n        model = cls(config, tokenizer, clip_mean, clip_std)\n        model.load_state_dict(ckpt, strict=strict)\n        return model\n\n    def set_cf_text_tensor(self):\n        return self._tokenizer.padded_tokens_and_mask([\"\"], self.model.text_ctx)\n\n    def get_sample_fn(self, timestep_respacing):\n        use_ddim = timestep_respacing.startswith((\"ddim\", \"fast\"))\n\n        diffusion_kwargs = copy.deepcopy(self._diffusion_kwargs)\n        diffusion_kwargs.update(timestep_respacing=timestep_respacing)\n        diffusion = create_gaussian_diffusion(**diffusion_kwargs)\n        sample_fn = diffusion.ddim_sample_loop if use_ddim else diffusion.p_sample_loop\n\n        return sample_fn\n\n    def get_causal_mask(self):\n        seq_len = self._model_conf.text_ctx + 4\n        mask = torch.empty(seq_len, seq_len)\n        mask.fill_(float(\"-inf\"))\n        mask.triu_(1)\n        mask = mask[None, ...]\n        return mask\n\n    def forward(\n        self,\n        txt_feat,\n        txt_feat_seq,\n        mask,\n        cf_guidance_scales=None,\n        timestep_respacing=None,\n        denoised_fn=True,\n    ):\n        # cfg should be enabled in inference\n        assert cf_guidance_scales is not None and all(cf_guidance_scales > 0.0)\n\n        bsz_ = txt_feat.shape[0]\n        bsz = bsz_ // 2\n\n        def guided_model_fn(x_t, ts, **kwargs):\n            half = x_t[: len(x_t) // 2]\n            combined = torch.cat([half, half], dim=0)\n            model_out = self.model(combined, ts, **kwargs)\n            eps, rest = (\n                model_out[:, : int(x_t.shape[1])],\n                model_out[:, int(x_t.shape[1]) :],\n            )\n            cond_eps, uncond_eps = torch.split(eps, len(eps) // 2, dim=0)\n            half_eps = uncond_eps + cf_guidance_scales.view(-1, 1) * (\n                cond_eps - uncond_eps\n            )\n            eps = torch.cat([half_eps, half_eps], dim=0)\n            return torch.cat([eps, rest], dim=1)\n\n        cond = {\n            \"text_emb\": txt_feat,\n            \"text_enc\": txt_feat_seq,\n            \"mask\": mask,\n            \"causal_mask\": self.causal_mask,\n        }\n        sample_fn = self.get_sample_fn(timestep_respacing)\n        sample = sample_fn(\n            guided_model_fn,\n            (bsz_, self.model.clip_dim),\n            noise=None,\n            device=txt_feat.device,\n            clip_denoised=False,\n            denoised_fn=lambda x: torch.clamp(x, -10, 10),\n            model_kwargs=cond,\n        )\n        sample = (sample * self.clip_std) + self.clip_mean\n\n        return sample[:bsz]\n", "ldm/modules/karlo/kakao/models/sr_256_1k.py": "# ------------------------------------------------------------------------------------\n# Karlo-v1.0.alpha\n# Copyright (c) 2022 KakaoBrain. All Rights Reserved.\n# ------------------------------------------------------------------------------------\n\nfrom ldm.modules.karlo.kakao.models.sr_64_256 import SupRes64to256Progressive\n\n\nclass SupRes256to1kProgressive(SupRes64to256Progressive):\n    pass  # no difference currently\n", "ldm/modules/karlo/kakao/models/decoder_model.py": "# ------------------------------------------------------------------------------------\n# Karlo-v1.0.alpha\n# Copyright (c) 2022 KakaoBrain. All Rights Reserved.\n# ------------------------------------------------------------------------------------\n\nimport copy\nimport torch\n\nfrom ldm.modules.karlo.kakao.modules import create_gaussian_diffusion\nfrom ldm.modules.karlo.kakao.modules.unet import PLMImUNet\n\n\nclass Text2ImProgressiveModel(torch.nn.Module):\n    \"\"\"\n    A decoder that generates 64x64px images based on the text prompt.\n\n    :param config: yaml config to define the decoder.\n    :param tokenizer: tokenizer used in clip.\n    \"\"\"\n\n    def __init__(\n        self,\n        config,\n        tokenizer,\n    ):\n        super().__init__()\n\n        self._conf = config\n        self._model_conf = config.model.hparams\n        self._diffusion_kwargs = dict(\n            steps=config.diffusion.steps,\n            learn_sigma=config.diffusion.learn_sigma,\n            sigma_small=config.diffusion.sigma_small,\n            noise_schedule=config.diffusion.noise_schedule,\n            use_kl=config.diffusion.use_kl,\n            predict_xstart=config.diffusion.predict_xstart,\n            rescale_learned_sigmas=config.diffusion.rescale_learned_sigmas,\n            timestep_respacing=config.diffusion.timestep_respacing,\n        )\n        self._tokenizer = tokenizer\n\n        self.model = self.create_plm_dec_model()\n\n        cf_token, cf_mask = self.set_cf_text_tensor()\n        self.register_buffer(\"cf_token\", cf_token, persistent=False)\n        self.register_buffer(\"cf_mask\", cf_mask, persistent=False)\n\n    @classmethod\n    def load_from_checkpoint(cls, config, tokenizer, ckpt_path, strict: bool = True):\n        ckpt = torch.load(ckpt_path, map_location=\"cpu\")[\"state_dict\"]\n\n        model = cls(config, tokenizer)\n        model.load_state_dict(ckpt, strict=strict)\n        return model\n\n    def create_plm_dec_model(self):\n        image_size = self._model_conf.image_size\n        if self._model_conf.channel_mult == \"\":\n            if image_size == 256:\n                channel_mult = (1, 1, 2, 2, 4, 4)\n            elif image_size == 128:\n                channel_mult = (1, 1, 2, 3, 4)\n            elif image_size == 64:\n                channel_mult = (1, 2, 3, 4)\n            else:\n                raise ValueError(f\"unsupported image size: {image_size}\")\n        else:\n            channel_mult = tuple(\n                int(ch_mult) for ch_mult in self._model_conf.channel_mult.split(\",\")\n            )\n            assert 2 ** (len(channel_mult) + 2) == image_size\n\n        attention_ds = []\n        for res in self._model_conf.attention_resolutions.split(\",\"):\n            attention_ds.append(image_size // int(res))\n\n        return PLMImUNet(\n            text_ctx=self._model_conf.text_ctx,\n            xf_width=self._model_conf.xf_width,\n            in_channels=3,\n            model_channels=self._model_conf.num_channels,\n            out_channels=6 if self._model_conf.learn_sigma else 3,\n            num_res_blocks=self._model_conf.num_res_blocks,\n            attention_resolutions=tuple(attention_ds),\n            dropout=self._model_conf.dropout,\n            channel_mult=channel_mult,\n            num_heads=self._model_conf.num_heads,\n            num_head_channels=self._model_conf.num_head_channels,\n            num_heads_upsample=self._model_conf.num_heads_upsample,\n            use_scale_shift_norm=self._model_conf.use_scale_shift_norm,\n            resblock_updown=self._model_conf.resblock_updown,\n            clip_dim=self._model_conf.clip_dim,\n            clip_emb_mult=self._model_conf.clip_emb_mult,\n            clip_emb_type=self._model_conf.clip_emb_type,\n            clip_emb_drop=self._model_conf.clip_emb_drop,\n        )\n\n    def set_cf_text_tensor(self):\n        return self._tokenizer.padded_tokens_and_mask([\"\"], self.model.text_ctx)\n\n    def get_sample_fn(self, timestep_respacing):\n        use_ddim = timestep_respacing.startswith((\"ddim\", \"fast\"))\n\n        diffusion_kwargs = copy.deepcopy(self._diffusion_kwargs)\n        diffusion_kwargs.update(timestep_respacing=timestep_respacing)\n        diffusion = create_gaussian_diffusion(**diffusion_kwargs)\n        sample_fn = (\n            diffusion.ddim_sample_loop_progressive\n            if use_ddim\n            else diffusion.p_sample_loop_progressive\n        )\n\n        return sample_fn\n\n    def forward(\n        self,\n        txt_feat,\n        txt_feat_seq,\n        tok,\n        mask,\n        img_feat=None,\n        cf_guidance_scales=None,\n        timestep_respacing=None,\n    ):\n        # cfg should be enabled in inference\n        assert cf_guidance_scales is not None and all(cf_guidance_scales > 0.0)\n        assert img_feat is not None\n\n        bsz = txt_feat.shape[0]\n        img_sz = self._model_conf.image_size\n\n        def guided_model_fn(x_t, ts, **kwargs):\n            half = x_t[: len(x_t) // 2]\n            combined = torch.cat([half, half], dim=0)\n            model_out = self.model(combined, ts, **kwargs)\n            eps, rest = model_out[:, :3], model_out[:, 3:]\n            cond_eps, uncond_eps = torch.split(eps, len(eps) // 2, dim=0)\n            half_eps = uncond_eps + cf_guidance_scales.view(-1, 1, 1, 1) * (\n                cond_eps - uncond_eps\n            )\n            eps = torch.cat([half_eps, half_eps], dim=0)\n            return torch.cat([eps, rest], dim=1)\n\n        cf_feat = self.model.cf_param.unsqueeze(0)\n        cf_feat = cf_feat.expand(bsz // 2, -1)\n        feat = torch.cat([img_feat, cf_feat.to(txt_feat.device)], dim=0)\n\n        cond = {\n            \"y\": feat,\n            \"txt_feat\": txt_feat,\n            \"txt_feat_seq\": txt_feat_seq,\n            \"mask\": mask,\n        }\n        sample_fn = self.get_sample_fn(timestep_respacing)\n        sample_outputs = sample_fn(\n            guided_model_fn,\n            (bsz, 3, img_sz, img_sz),\n            noise=None,\n            device=txt_feat.device,\n            clip_denoised=True,\n            model_kwargs=cond,\n        )\n\n        for out in sample_outputs:\n            sample = out[\"sample\"]\n            yield sample if cf_guidance_scales is None else sample[\n                : sample.shape[0] // 2\n            ]\n\n\nclass Text2ImModel(Text2ImProgressiveModel):\n    def forward(\n        self,\n        txt_feat,\n        txt_feat_seq,\n        tok,\n        mask,\n        img_feat=None,\n        cf_guidance_scales=None,\n        timestep_respacing=None,\n    ):\n        last_out = None\n        for out in super().forward(\n            txt_feat,\n            txt_feat_seq,\n            tok,\n            mask,\n            img_feat,\n            cf_guidance_scales,\n            timestep_respacing,\n        ):\n            last_out = out\n        return last_out\n", "ldm/modules/karlo/kakao/models/__init__.py": "", "ldm/modules/karlo/kakao/models/clip.py": "# ------------------------------------------------------------------------------------\n# Karlo-v1.0.alpha\n# Copyright (c) 2022 KakaoBrain. All Rights Reserved.\n# ------------------------------------------------------------------------------------\n# ------------------------------------------------------------------------------------\n# Adapted from OpenAI's CLIP (https://github.com/openai/CLIP/)\n# ------------------------------------------------------------------------------------\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport clip\n\nfrom clip.model import CLIP, convert_weights\nfrom clip.simple_tokenizer import SimpleTokenizer, default_bpe\n\n\n\"\"\"===== Monkey-Patching original CLIP for JIT compile =====\"\"\"\n\n\nclass LayerNorm(nn.LayerNorm):\n    \"\"\"Subclass torch's LayerNorm to handle fp16.\"\"\"\n\n    def forward(self, x: torch.Tensor):\n        orig_type = x.dtype\n        ret = F.layer_norm(\n            x.type(torch.float32),\n            self.normalized_shape,\n            self.weight,\n            self.bias,\n            self.eps,\n        )\n        return ret.type(orig_type)\n\n\nclip.model.LayerNorm = LayerNorm\ndelattr(clip.model.CLIP, \"forward\")\n\n\"\"\"===== End of Monkey-Patching =====\"\"\"\n\n\nclass CustomizedCLIP(CLIP):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n    @torch.jit.export\n    def encode_image(self, image):\n        return self.visual(image)\n\n    @torch.jit.export\n    def encode_text(self, text):\n        # re-define this function to return unpooled text features\n\n        x = self.token_embedding(text).type(self.dtype)  # [batch_size, n_ctx, d_model]\n\n        x = x + self.positional_embedding.type(self.dtype)\n        x = x.permute(1, 0, 2)  # NLD -> LND\n        x = self.transformer(x)\n        x = x.permute(1, 0, 2)  # LND -> NLD\n        x = self.ln_final(x).type(self.dtype)\n\n        x_seq = x\n        # x.shape = [batch_size, n_ctx, transformer.width]\n        # take features from the eot embedding (eot_token is the highest number in each sequence)\n        x_out = x[torch.arange(x.shape[0]), text.argmax(dim=-1)] @ self.text_projection\n\n        return x_out, x_seq\n\n    @torch.jit.ignore\n    def forward(self, image, text):\n        super().forward(image, text)\n\n    @classmethod\n    def load_from_checkpoint(cls, ckpt_path: str):\n        state_dict = torch.load(ckpt_path, map_location=\"cpu\").state_dict()\n\n        vit = \"visual.proj\" in state_dict\n        if vit:\n            vision_width = state_dict[\"visual.conv1.weight\"].shape[0]\n            vision_layers = len(\n                [\n                    k\n                    for k in state_dict.keys()\n                    if k.startswith(\"visual.\") and k.endswith(\".attn.in_proj_weight\")\n                ]\n            )\n            vision_patch_size = state_dict[\"visual.conv1.weight\"].shape[-1]\n            grid_size = round(\n                (state_dict[\"visual.positional_embedding\"].shape[0] - 1) ** 0.5\n            )\n            image_resolution = vision_patch_size * grid_size\n        else:\n            counts: list = [\n                len(\n                    set(\n                        k.split(\".\")[2]\n                        for k in state_dict\n                        if k.startswith(f\"visual.layer{b}\")\n                    )\n                )\n                for b in [1, 2, 3, 4]\n            ]\n            vision_layers = tuple(counts)\n            vision_width = state_dict[\"visual.layer1.0.conv1.weight\"].shape[0]\n            output_width = round(\n                (state_dict[\"visual.attnpool.positional_embedding\"].shape[0] - 1) ** 0.5\n            )\n            vision_patch_size = None\n            assert (\n                output_width**2 + 1\n                == state_dict[\"visual.attnpool.positional_embedding\"].shape[0]\n            )\n            image_resolution = output_width * 32\n\n        embed_dim = state_dict[\"text_projection\"].shape[1]\n        context_length = state_dict[\"positional_embedding\"].shape[0]\n        vocab_size = state_dict[\"token_embedding.weight\"].shape[0]\n        transformer_width = state_dict[\"ln_final.weight\"].shape[0]\n        transformer_heads = transformer_width // 64\n        transformer_layers = len(\n            set(\n                k.split(\".\")[2]\n                for k in state_dict\n                if k.startswith(\"transformer.resblocks\")\n            )\n        )\n\n        model = cls(\n            embed_dim,\n            image_resolution,\n            vision_layers,\n            vision_width,\n            vision_patch_size,\n            context_length,\n            vocab_size,\n            transformer_width,\n            transformer_heads,\n            transformer_layers,\n        )\n\n        for key in [\"input_resolution\", \"context_length\", \"vocab_size\"]:\n            if key in state_dict:\n                del state_dict[key]\n\n        convert_weights(model)\n        model.load_state_dict(state_dict)\n        model.eval()\n        model.float()\n        return model\n\n\nclass CustomizedTokenizer(SimpleTokenizer):\n    def __init__(self):\n        super().__init__(bpe_path=default_bpe())\n\n        self.sot_token = self.encoder[\"<|startoftext|>\"]\n        self.eot_token = self.encoder[\"<|endoftext|>\"]\n\n    def padded_tokens_and_mask(self, texts, text_ctx):\n        assert isinstance(texts, list) and all(\n            isinstance(elem, str) for elem in texts\n        ), \"texts should be a list of strings\"\n\n        all_tokens = [\n            [self.sot_token] + self.encode(text) + [self.eot_token] for text in texts\n        ]\n\n        mask = [\n            [True] * min(text_ctx, len(tokens))\n            + [False] * max(text_ctx - len(tokens), 0)\n            for tokens in all_tokens\n        ]\n        mask = torch.tensor(mask, dtype=torch.bool)\n        result = torch.zeros(len(all_tokens), text_ctx, dtype=torch.int)\n        for i, tokens in enumerate(all_tokens):\n            if len(tokens) > text_ctx:\n                tokens = tokens[:text_ctx]\n                tokens[-1] = self.eot_token\n            result[i, : len(tokens)] = torch.tensor(tokens)\n\n        return result, mask\n", "ldm/modules/karlo/kakao/models/sr_64_256.py": "# ------------------------------------------------------------------------------------\n# Karlo-v1.0.alpha\n# Copyright (c) 2022 KakaoBrain. All Rights Reserved.\n# ------------------------------------------------------------------------------------\n\nimport copy\nimport torch\n\nfrom ldm.modules.karlo.kakao.modules.unet import SuperResUNetModel\nfrom ldm.modules.karlo.kakao.modules import create_gaussian_diffusion\n\n\nclass ImprovedSupRes64to256ProgressiveModel(torch.nn.Module):\n    \"\"\"\n    ImprovedSR model fine-tunes the pretrained DDPM-based SR model by using adversarial and perceptual losses.\n    In specific, the low-resolution sample is iteratively recovered by 6 steps with the frozen pretrained SR model.\n    In the following additional one step, a seperate fine-tuned model recovers high-frequency details.\n    This approach greatly improves the fidelity of images of 256x256px, even with small number of reverse steps.\n    \"\"\"\n\n    def __init__(self, config):\n        super().__init__()\n\n        self._config = config\n        self._diffusion_kwargs = dict(\n            steps=config.diffusion.steps,\n            learn_sigma=config.diffusion.learn_sigma,\n            sigma_small=config.diffusion.sigma_small,\n            noise_schedule=config.diffusion.noise_schedule,\n            use_kl=config.diffusion.use_kl,\n            predict_xstart=config.diffusion.predict_xstart,\n            rescale_learned_sigmas=config.diffusion.rescale_learned_sigmas,\n        )\n\n        self.model_first_steps = SuperResUNetModel(\n            in_channels=3,  # auto-changed to 6 inside the model\n            model_channels=config.model.hparams.channels,\n            out_channels=3,\n            num_res_blocks=config.model.hparams.depth,\n            attention_resolutions=(),  # no attention\n            dropout=config.model.hparams.dropout,\n            channel_mult=config.model.hparams.channels_multiple,\n            resblock_updown=True,\n            use_middle_attention=False,\n        )\n        self.model_last_step = SuperResUNetModel(\n            in_channels=3,  # auto-changed to 6 inside the model\n            model_channels=config.model.hparams.channels,\n            out_channels=3,\n            num_res_blocks=config.model.hparams.depth,\n            attention_resolutions=(),  # no attention\n            dropout=config.model.hparams.dropout,\n            channel_mult=config.model.hparams.channels_multiple,\n            resblock_updown=True,\n            use_middle_attention=False,\n        )\n\n    @classmethod\n    def load_from_checkpoint(cls, config, ckpt_path, strict: bool = True):\n        ckpt = torch.load(ckpt_path, map_location=\"cpu\")[\"state_dict\"]\n\n        model = cls(config)\n        model.load_state_dict(ckpt, strict=strict)\n        return model\n\n    def get_sample_fn(self, timestep_respacing):\n        diffusion_kwargs = copy.deepcopy(self._diffusion_kwargs)\n        diffusion_kwargs.update(timestep_respacing=timestep_respacing)\n        diffusion = create_gaussian_diffusion(**diffusion_kwargs)\n        return diffusion.p_sample_loop_progressive_for_improved_sr\n\n    def forward(self, low_res, timestep_respacing=\"7\", **kwargs):\n        assert (\n            timestep_respacing == \"7\"\n        ), \"different respacing method may work, but no guaranteed\"\n\n        sample_fn = self.get_sample_fn(timestep_respacing)\n        sample_outputs = sample_fn(\n            self.model_first_steps,\n            self.model_last_step,\n            shape=low_res.shape,\n            clip_denoised=True,\n            model_kwargs=dict(low_res=low_res),\n            **kwargs,\n        )\n        for x in sample_outputs:\n            sample = x[\"sample\"]\n            yield sample\n", "ldm/modules/karlo/kakao/modules/resample.py": "# ------------------------------------------------------------------------------------\n# Modified from Guided-Diffusion (https://github.com/openai/guided-diffusion)\n# ------------------------------------------------------------------------------------\n\nfrom abc import abstractmethod\n\nimport torch as th\n\n\ndef create_named_schedule_sampler(name, diffusion):\n    \"\"\"\n    Create a ScheduleSampler from a library of pre-defined samplers.\n\n    :param name: the name of the sampler.\n    :param diffusion: the diffusion object to sample for.\n    \"\"\"\n    if name == \"uniform\":\n        return UniformSampler(diffusion)\n    else:\n        raise NotImplementedError(f\"unknown schedule sampler: {name}\")\n\n\nclass ScheduleSampler(th.nn.Module):\n    \"\"\"\n    A distribution over timesteps in the diffusion process, intended to reduce\n    variance of the objective.\n\n    By default, samplers perform unbiased importance sampling, in which the\n    objective's mean is unchanged.\n    However, subclasses may override sample() to change how the resampled\n    terms are reweighted, allowing for actual changes in the objective.\n    \"\"\"\n\n    @abstractmethod\n    def weights(self):\n        \"\"\"\n        Get a numpy array of weights, one per diffusion step.\n\n        The weights needn't be normalized, but must be positive.\n        \"\"\"\n\n    def sample(self, batch_size, device):\n        \"\"\"\n        Importance-sample timesteps for a batch.\n\n        :param batch_size: the number of timesteps.\n        :param device: the torch device to save to.\n        :return: a tuple (timesteps, weights):\n                 - timesteps: a tensor of timestep indices.\n                 - weights: a tensor of weights to scale the resulting losses.\n        \"\"\"\n        w = self.weights()\n        p = w / th.sum(w)\n        indices = p.multinomial(batch_size, replacement=True)\n        weights = 1 / (len(p) * p[indices])\n        return indices, weights\n\n\nclass UniformSampler(ScheduleSampler):\n    def __init__(self, diffusion):\n        super(UniformSampler, self).__init__()\n        self.diffusion = diffusion\n        self.register_buffer(\n            \"_weights\", th.ones([diffusion.num_timesteps]), persistent=False\n        )\n\n    def weights(self):\n        return self._weights\n", "ldm/modules/karlo/kakao/modules/nn.py": "# ------------------------------------------------------------------------------------\n# Adapted from Guided-Diffusion repo (https://github.com/openai/guided-diffusion)\n# ------------------------------------------------------------------------------------\n\nimport math\n\nimport torch as th\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass GroupNorm32(nn.GroupNorm):\n    def __init__(self, num_groups, num_channels, swish, eps=1e-5):\n        super().__init__(num_groups=num_groups, num_channels=num_channels, eps=eps)\n        self.swish = swish\n\n    def forward(self, x):\n        y = super().forward(x.float()).to(x.dtype)\n        if self.swish == 1.0:\n            y = F.silu(y)\n        elif self.swish:\n            y = y * F.sigmoid(y * float(self.swish))\n        return y\n\n\ndef conv_nd(dims, *args, **kwargs):\n    \"\"\"\n    Create a 1D, 2D, or 3D convolution module.\n    \"\"\"\n    if dims == 1:\n        return nn.Conv1d(*args, **kwargs)\n    elif dims == 2:\n        return nn.Conv2d(*args, **kwargs)\n    elif dims == 3:\n        return nn.Conv3d(*args, **kwargs)\n    raise ValueError(f\"unsupported dimensions: {dims}\")\n\n\ndef linear(*args, **kwargs):\n    \"\"\"\n    Create a linear module.\n    \"\"\"\n    return nn.Linear(*args, **kwargs)\n\n\ndef avg_pool_nd(dims, *args, **kwargs):\n    \"\"\"\n    Create a 1D, 2D, or 3D average pooling module.\n    \"\"\"\n    if dims == 1:\n        return nn.AvgPool1d(*args, **kwargs)\n    elif dims == 2:\n        return nn.AvgPool2d(*args, **kwargs)\n    elif dims == 3:\n        return nn.AvgPool3d(*args, **kwargs)\n    raise ValueError(f\"unsupported dimensions: {dims}\")\n\n\ndef zero_module(module):\n    \"\"\"\n    Zero out the parameters of a module and return it.\n    \"\"\"\n    for p in module.parameters():\n        p.detach().zero_()\n    return module\n\n\ndef scale_module(module, scale):\n    \"\"\"\n    Scale the parameters of a module and return it.\n    \"\"\"\n    for p in module.parameters():\n        p.detach().mul_(scale)\n    return module\n\n\ndef normalization(channels, swish=0.0):\n    \"\"\"\n    Make a standard normalization layer, with an optional swish activation.\n\n    :param channels: number of input channels.\n    :return: an nn.Module for normalization.\n    \"\"\"\n    return GroupNorm32(num_channels=channels, num_groups=32, swish=swish)\n\n\ndef timestep_embedding(timesteps, dim, max_period=10000):\n    \"\"\"\n    Create sinusoidal timestep embeddings.\n\n    :param timesteps: a 1-D Tensor of N indices, one per batch element.\n                      These may be fractional.\n    :param dim: the dimension of the output.\n    :param max_period: controls the minimum frequency of the embeddings.\n    :return: an [N x dim] Tensor of positional embeddings.\n    \"\"\"\n    half = dim // 2\n    freqs = th.exp(\n        -math.log(max_period)\n        * th.arange(start=0, end=half, dtype=th.float32, device=timesteps.device)\n        / half\n    )\n    args = timesteps[:, None].float() * freqs[None]\n    embedding = th.cat([th.cos(args), th.sin(args)], dim=-1)\n    if dim % 2:\n        embedding = th.cat([embedding, th.zeros_like(embedding[:, :1])], dim=-1)\n    return embedding\n\n\ndef mean_flat(tensor):\n    \"\"\"\n    Take the mean over all non-batch dimensions.\n    \"\"\"\n    return tensor.mean(dim=list(range(1, len(tensor.shape))))\n", "ldm/modules/karlo/kakao/modules/xf.py": "# ------------------------------------------------------------------------------------\n# Adapted from the repos below:\n# (a) Guided-Diffusion (https://github.com/openai/guided-diffusion)\n# (b) CLIP ViT (https://github.com/openai/CLIP/)\n# ------------------------------------------------------------------------------------\n\nimport math\n\nimport torch as th\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .nn import timestep_embedding\n\n\ndef convert_module_to_f16(param):\n    \"\"\"\n    Convert primitive modules to float16.\n    \"\"\"\n    if isinstance(param, (nn.Linear, nn.Conv2d, nn.ConvTranspose2d)):\n        param.weight.data = param.weight.data.half()\n        if param.bias is not None:\n            param.bias.data = param.bias.data.half()\n\n\nclass LayerNorm(nn.LayerNorm):\n    \"\"\"\n    Implementation that supports fp16 inputs but fp32 gains/biases.\n    \"\"\"\n\n    def forward(self, x: th.Tensor):\n        return super().forward(x.float()).to(x.dtype)\n\n\nclass MultiheadAttention(nn.Module):\n    def __init__(self, n_ctx, width, heads):\n        super().__init__()\n        self.n_ctx = n_ctx\n        self.width = width\n        self.heads = heads\n        self.c_qkv = nn.Linear(width, width * 3)\n        self.c_proj = nn.Linear(width, width)\n        self.attention = QKVMultiheadAttention(heads, n_ctx)\n\n    def forward(self, x, mask=None):\n        x = self.c_qkv(x)\n        x = self.attention(x, mask=mask)\n        x = self.c_proj(x)\n        return x\n\n\nclass MLP(nn.Module):\n    def __init__(self, width):\n        super().__init__()\n        self.width = width\n        self.c_fc = nn.Linear(width, width * 4)\n        self.c_proj = nn.Linear(width * 4, width)\n        self.gelu = nn.GELU()\n\n    def forward(self, x):\n        return self.c_proj(self.gelu(self.c_fc(x)))\n\n\nclass QKVMultiheadAttention(nn.Module):\n    def __init__(self, n_heads: int, n_ctx: int):\n        super().__init__()\n        self.n_heads = n_heads\n        self.n_ctx = n_ctx\n\n    def forward(self, qkv, mask=None):\n        bs, n_ctx, width = qkv.shape\n        attn_ch = width // self.n_heads // 3\n        scale = 1 / math.sqrt(math.sqrt(attn_ch))\n        qkv = qkv.view(bs, n_ctx, self.n_heads, -1)\n        q, k, v = th.split(qkv, attn_ch, dim=-1)\n        weight = th.einsum(\"bthc,bshc->bhts\", q * scale, k * scale)\n        wdtype = weight.dtype\n        if mask is not None:\n            weight = weight + mask[:, None, ...]\n        weight = th.softmax(weight, dim=-1).type(wdtype)\n        return th.einsum(\"bhts,bshc->bthc\", weight, v).reshape(bs, n_ctx, -1)\n\n\nclass ResidualAttentionBlock(nn.Module):\n    def __init__(\n        self,\n        n_ctx: int,\n        width: int,\n        heads: int,\n    ):\n        super().__init__()\n\n        self.attn = MultiheadAttention(\n            n_ctx,\n            width,\n            heads,\n        )\n        self.ln_1 = LayerNorm(width)\n        self.mlp = MLP(width)\n        self.ln_2 = LayerNorm(width)\n\n    def forward(self, x, mask=None):\n        x = x + self.attn(self.ln_1(x), mask=mask)\n        x = x + self.mlp(self.ln_2(x))\n        return x\n\n\nclass Transformer(nn.Module):\n    def __init__(\n        self,\n        n_ctx: int,\n        width: int,\n        layers: int,\n        heads: int,\n    ):\n        super().__init__()\n        self.n_ctx = n_ctx\n        self.width = width\n        self.layers = layers\n        self.resblocks = nn.ModuleList(\n            [\n                ResidualAttentionBlock(\n                    n_ctx,\n                    width,\n                    heads,\n                )\n                for _ in range(layers)\n            ]\n        )\n\n    def forward(self, x, mask=None):\n        for block in self.resblocks:\n            x = block(x, mask=mask)\n        return x\n\n\nclass PriorTransformer(nn.Module):\n    \"\"\"\n    A Causal Transformer that conditions on CLIP text embedding, text.\n\n    :param text_ctx: number of text tokens to expect.\n    :param xf_width: width of the transformer.\n    :param xf_layers: depth of the transformer.\n    :param xf_heads: heads in the transformer.\n    :param xf_final_ln: use a LayerNorm after the output layer.\n    :param clip_dim: dimension of clip feature.\n    \"\"\"\n\n    def __init__(\n        self,\n        text_ctx,\n        xf_width,\n        xf_layers,\n        xf_heads,\n        xf_final_ln,\n        clip_dim,\n    ):\n        super().__init__()\n\n        self.text_ctx = text_ctx\n        self.xf_width = xf_width\n        self.xf_layers = xf_layers\n        self.xf_heads = xf_heads\n        self.clip_dim = clip_dim\n        self.ext_len = 4\n\n        self.time_embed = nn.Sequential(\n            nn.Linear(xf_width, xf_width),\n            nn.SiLU(),\n            nn.Linear(xf_width, xf_width),\n        )\n        self.text_enc_proj = nn.Linear(clip_dim, xf_width)\n        self.text_emb_proj = nn.Linear(clip_dim, xf_width)\n        self.clip_img_proj = nn.Linear(clip_dim, xf_width)\n        self.out_proj = nn.Linear(xf_width, clip_dim)\n        self.transformer = Transformer(\n            text_ctx + self.ext_len,\n            xf_width,\n            xf_layers,\n            xf_heads,\n        )\n        if xf_final_ln:\n            self.final_ln = LayerNorm(xf_width)\n        else:\n            self.final_ln = None\n\n        self.positional_embedding = nn.Parameter(\n            th.empty(1, text_ctx + self.ext_len, xf_width)\n        )\n        self.prd_emb = nn.Parameter(th.randn((1, 1, xf_width)))\n\n        nn.init.normal_(self.prd_emb, std=0.01)\n        nn.init.normal_(self.positional_embedding, std=0.01)\n\n    def forward(\n        self,\n        x,\n        timesteps,\n        text_emb=None,\n        text_enc=None,\n        mask=None,\n        causal_mask=None,\n    ):\n        bsz = x.shape[0]\n        mask = F.pad(mask, (0, self.ext_len), value=True)\n\n        t_emb = self.time_embed(timestep_embedding(timesteps, self.xf_width))\n        text_enc = self.text_enc_proj(text_enc)\n        text_emb = self.text_emb_proj(text_emb)\n        x = self.clip_img_proj(x)\n\n        input_seq = [\n            text_enc,\n            text_emb[:, None, :],\n            t_emb[:, None, :],\n            x[:, None, :],\n            self.prd_emb.to(x.dtype).expand(bsz, -1, -1),\n        ]\n        input = th.cat(input_seq, dim=1)\n        input = input + self.positional_embedding.to(input.dtype)\n\n        mask = th.where(mask, 0.0, float(\"-inf\"))\n        mask = (mask[:, None, :] + causal_mask).to(input.dtype)\n\n        out = self.transformer(input, mask=mask)\n        if self.final_ln is not None:\n            out = self.final_ln(out)\n\n        out = self.out_proj(out[:, -1])\n\n        return out\n", "ldm/modules/karlo/kakao/modules/__init__.py": "# ------------------------------------------------------------------------------------\n# Adapted from Guided-Diffusion repo (https://github.com/openai/guided-diffusion)\n# ------------------------------------------------------------------------------------\n\n\nfrom .diffusion import gaussian_diffusion as gd\nfrom .diffusion.respace import (\n    SpacedDiffusion,\n    space_timesteps,\n)\n\n\ndef create_gaussian_diffusion(\n    steps,\n    learn_sigma,\n    sigma_small,\n    noise_schedule,\n    use_kl,\n    predict_xstart,\n    rescale_learned_sigmas,\n    timestep_respacing,\n):\n    betas = gd.get_named_beta_schedule(noise_schedule, steps)\n    if use_kl:\n        loss_type = gd.LossType.RESCALED_KL\n    elif rescale_learned_sigmas:\n        loss_type = gd.LossType.RESCALED_MSE\n    else:\n        loss_type = gd.LossType.MSE\n    if not timestep_respacing:\n        timestep_respacing = [steps]\n\n    return SpacedDiffusion(\n        use_timesteps=space_timesteps(steps, timestep_respacing),\n        betas=betas,\n        model_mean_type=(\n            gd.ModelMeanType.EPSILON if not predict_xstart else gd.ModelMeanType.START_X\n        ),\n        model_var_type=(\n            (\n                gd.ModelVarType.FIXED_LARGE\n                if not sigma_small\n                else gd.ModelVarType.FIXED_SMALL\n            )\n            if not learn_sigma\n            else gd.ModelVarType.LEARNED_RANGE\n        ),\n        loss_type=loss_type,\n    )\n", "ldm/modules/karlo/kakao/modules/unet.py": "# ------------------------------------------------------------------------------------\n# Modified from Guided-Diffusion (https://github.com/openai/guided-diffusion)\n# ------------------------------------------------------------------------------------\n\nimport math\nfrom abc import abstractmethod\n\nimport torch as th\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .nn import (\n    avg_pool_nd,\n    conv_nd,\n    linear,\n    normalization,\n    timestep_embedding,\n    zero_module,\n)\nfrom .xf import LayerNorm\n\n\nclass TimestepBlock(nn.Module):\n    \"\"\"\n    Any module where forward() takes timestep embeddings as a second argument.\n    \"\"\"\n\n    @abstractmethod\n    def forward(self, x, emb):\n        \"\"\"\n        Apply the module to `x` given `emb` timestep embeddings.\n        \"\"\"\n\n\nclass TimestepEmbedSequential(nn.Sequential, TimestepBlock):\n    \"\"\"\n    A sequential module that passes timestep embeddings to the children that\n    support it as an extra input.\n    \"\"\"\n\n    def forward(self, x, emb, encoder_out=None, mask=None):\n        for layer in self:\n            if isinstance(layer, TimestepBlock):\n                x = layer(x, emb)\n            elif isinstance(layer, AttentionBlock):\n                x = layer(x, encoder_out, mask=mask)\n            else:\n                x = layer(x)\n        return x\n\n\nclass Upsample(nn.Module):\n    \"\"\"\n    An upsampling layer with an optional convolution.\n\n    :param channels: channels in the inputs and outputs.\n    :param use_conv: a bool determining if a convolution is applied.\n    :param dims: determines if the signal is 1D, 2D, or 3D. If 3D, then\n                 upsampling occurs in the inner-two dimensions.\n    \"\"\"\n\n    def __init__(self, channels, use_conv, dims=2, out_channels=None):\n        super().__init__()\n        self.channels = channels\n        self.out_channels = out_channels or channels\n        self.use_conv = use_conv\n        self.dims = dims\n        if use_conv:\n            self.conv = conv_nd(dims, self.channels, self.out_channels, 3, padding=1)\n\n    def forward(self, x):\n        assert x.shape[1] == self.channels\n        if self.dims == 3:\n            x = F.interpolate(\n                x, (x.shape[2], x.shape[3] * 2, x.shape[4] * 2), mode=\"nearest\"\n            )\n        else:\n            x = F.interpolate(x, scale_factor=2, mode=\"nearest\")\n        if self.use_conv:\n            x = self.conv(x)\n        return x\n\n\nclass Downsample(nn.Module):\n    \"\"\"\n    A downsampling layer with an optional convolution.\n\n    :param channels: channels in the inputs and outputs.\n    :param use_conv: a bool determining if a convolution is applied.\n    :param dims: determines if the signal is 1D, 2D, or 3D. If 3D, then\n                 downsampling occurs in the inner-two dimensions.\n    \"\"\"\n\n    def __init__(self, channels, use_conv, dims=2, out_channels=None):\n        super().__init__()\n        self.channels = channels\n        self.out_channels = out_channels or channels\n        self.use_conv = use_conv\n        self.dims = dims\n        stride = 2 if dims != 3 else (1, 2, 2)\n        if use_conv:\n            self.op = conv_nd(\n                dims, self.channels, self.out_channels, 3, stride=stride, padding=1\n            )\n        else:\n            assert self.channels == self.out_channels\n            self.op = avg_pool_nd(dims, kernel_size=stride, stride=stride)\n\n    def forward(self, x):\n        assert x.shape[1] == self.channels\n        return self.op(x)\n\n\nclass ResBlock(TimestepBlock):\n    \"\"\"\n    A residual block that can optionally change the number of channels.\n\n    :param channels: the number of input channels.\n    :param emb_channels: the number of timestep embedding channels.\n    :param dropout: the rate of dropout.\n    :param out_channels: if specified, the number of out channels.\n    :param use_conv: if True and out_channels is specified, use a spatial\n        convolution instead of a smaller 1x1 convolution to change the\n        channels in the skip connection.\n    :param dims: determines if the signal is 1D, 2D, or 3D.\n    :param use_checkpoint: if True, use gradient checkpointing on this module.\n    :param up: if True, use this block for upsampling.\n    :param down: if True, use this block for downsampling.\n    \"\"\"\n\n    def __init__(\n        self,\n        channels,\n        emb_channels,\n        dropout,\n        out_channels=None,\n        use_conv=False,\n        use_scale_shift_norm=False,\n        dims=2,\n        use_checkpoint=False,\n        up=False,\n        down=False,\n    ):\n        super().__init__()\n        self.channels = channels\n        self.emb_channels = emb_channels\n        self.dropout = dropout\n        self.out_channels = out_channels or channels\n        self.use_conv = use_conv\n        self.use_checkpoint = use_checkpoint\n        self.use_scale_shift_norm = use_scale_shift_norm\n\n        self.in_layers = nn.Sequential(\n            normalization(channels, swish=1.0),\n            nn.Identity(),\n            conv_nd(dims, channels, self.out_channels, 3, padding=1),\n        )\n\n        self.updown = up or down\n\n        if up:\n            self.h_upd = Upsample(channels, False, dims)\n            self.x_upd = Upsample(channels, False, dims)\n        elif down:\n            self.h_upd = Downsample(channels, False, dims)\n            self.x_upd = Downsample(channels, False, dims)\n        else:\n            self.h_upd = self.x_upd = nn.Identity()\n\n        self.emb_layers = nn.Sequential(\n            nn.SiLU(),\n            linear(\n                emb_channels,\n                2 * self.out_channels if use_scale_shift_norm else self.out_channels,\n            ),\n        )\n        self.out_layers = nn.Sequential(\n            normalization(\n                self.out_channels, swish=0.0 if use_scale_shift_norm else 1.0\n            ),\n            nn.SiLU() if use_scale_shift_norm else nn.Identity(),\n            nn.Dropout(p=dropout),\n            zero_module(\n                conv_nd(dims, self.out_channels, self.out_channels, 3, padding=1)\n            ),\n        )\n\n        if self.out_channels == channels:\n            self.skip_connection = nn.Identity()\n        elif use_conv:\n            self.skip_connection = conv_nd(\n                dims, channels, self.out_channels, 3, padding=1\n            )\n        else:\n            self.skip_connection = conv_nd(dims, channels, self.out_channels, 1)\n\n    def forward(self, x, emb):\n        \"\"\"\n        Apply the block to a Tensor, conditioned on a timestep embedding.\n\n        :param x: an [N x C x ...] Tensor of features.\n        :param emb: an [N x emb_channels] Tensor of timestep embeddings.\n        :return: an [N x C x ...] Tensor of outputs.\n        \"\"\"\n        if self.updown:\n            in_rest, in_conv = self.in_layers[:-1], self.in_layers[-1]\n            h = in_rest(x)\n            h = self.h_upd(h)\n            x = self.x_upd(x)\n            h = in_conv(h)\n        else:\n            h = self.in_layers(x)\n        emb_out = self.emb_layers(emb)\n        while len(emb_out.shape) < len(h.shape):\n            emb_out = emb_out[..., None]\n        if self.use_scale_shift_norm:\n            out_norm, out_rest = self.out_layers[0], self.out_layers[1:]\n            scale, shift = th.chunk(emb_out, 2, dim=1)\n            h = out_norm(h) * (1 + scale) + shift\n            h = out_rest(h)\n        else:\n            h = h + emb_out\n            h = self.out_layers(h)\n        return self.skip_connection(x) + h\n\n\nclass ResBlockNoTimeEmbedding(nn.Module):\n    \"\"\"\n    A residual block without time embedding\n\n    :param channels: the number of input channels.\n    :param emb_channels: the number of timestep embedding channels.\n    :param dropout: the rate of dropout.\n    :param out_channels: if specified, the number of out channels.\n    :param use_conv: if True and out_channels is specified, use a spatial\n        convolution instead of a smaller 1x1 convolution to change the\n        channels in the skip connection.\n    :param dims: determines if the signal is 1D, 2D, or 3D.\n    :param use_checkpoint: if True, use gradient checkpointing on this module.\n    :param up: if True, use this block for upsampling.\n    :param down: if True, use this block for downsampling.\n    \"\"\"\n\n    def __init__(\n        self,\n        channels,\n        emb_channels,\n        dropout,\n        out_channels=None,\n        use_conv=False,\n        dims=2,\n        use_checkpoint=False,\n        up=False,\n        down=False,\n        **kwargs,\n    ):\n        super().__init__()\n        self.channels = channels\n        self.emb_channels = emb_channels\n        self.dropout = dropout\n        self.out_channels = out_channels or channels\n        self.use_conv = use_conv\n        self.use_checkpoint = use_checkpoint\n\n        self.in_layers = nn.Sequential(\n            normalization(channels, swish=1.0),\n            nn.Identity(),\n            conv_nd(dims, channels, self.out_channels, 3, padding=1),\n        )\n\n        self.updown = up or down\n\n        if up:\n            self.h_upd = Upsample(channels, False, dims)\n            self.x_upd = Upsample(channels, False, dims)\n        elif down:\n            self.h_upd = Downsample(channels, False, dims)\n            self.x_upd = Downsample(channels, False, dims)\n        else:\n            self.h_upd = self.x_upd = nn.Identity()\n\n        self.out_layers = nn.Sequential(\n            normalization(self.out_channels, swish=1.0),\n            nn.Dropout(p=dropout),\n            zero_module(\n                conv_nd(dims, self.out_channels, self.out_channels, 3, padding=1)\n            ),\n        )\n\n        if self.out_channels == channels:\n            self.skip_connection = nn.Identity()\n        elif use_conv:\n            self.skip_connection = conv_nd(\n                dims, channels, self.out_channels, 3, padding=1\n            )\n        else:\n            self.skip_connection = conv_nd(dims, channels, self.out_channels, 1)\n\n    def forward(self, x, emb=None):\n        \"\"\"\n        Apply the block to a Tensor, NOT conditioned on a timestep embedding.\n\n        :param x: an [N x C x ...] Tensor of features.\n        :param emb: an [N x emb_channels] Tensor of timestep embeddings.\n        :return: an [N x C x ...] Tensor of outputs.\n        \"\"\"\n        assert emb is None\n\n        if self.updown:\n            in_rest, in_conv = self.in_layers[:-1], self.in_layers[-1]\n            h = in_rest(x)\n            h = self.h_upd(h)\n            x = self.x_upd(x)\n            h = in_conv(h)\n        else:\n            h = self.in_layers(x)\n        h = self.out_layers(h)\n        return self.skip_connection(x) + h\n\n\nclass AttentionBlock(nn.Module):\n    \"\"\"\n    An attention block that allows spatial positions to attend to each other.\n\n    Originally ported from here, but adapted to the N-d case.\n    https://github.com/hojonathanho/diffusion/blob/1e0dceb3b3495bbe19116a5e1b3596cd0706c543/diffusion_tf/models/unet.py#L66.\n    \"\"\"\n\n    def __init__(\n        self,\n        channels,\n        num_heads=1,\n        num_head_channels=-1,\n        use_checkpoint=False,\n        encoder_channels=None,\n    ):\n        super().__init__()\n        self.channels = channels\n        if num_head_channels == -1:\n            self.num_heads = num_heads\n        else:\n            assert (\n                channels % num_head_channels == 0\n            ), f\"q,k,v channels {channels} is not divisible by num_head_channels {num_head_channels}\"\n            self.num_heads = channels // num_head_channels\n        self.use_checkpoint = use_checkpoint\n        self.norm = normalization(channels, swish=0.0)\n        self.qkv = conv_nd(1, channels, channels * 3, 1)\n        self.attention = QKVAttention(self.num_heads)\n\n        if encoder_channels is not None:\n            self.encoder_kv = conv_nd(1, encoder_channels, channels * 2, 1)\n        self.proj_out = zero_module(conv_nd(1, channels, channels, 1))\n\n    def forward(self, x, encoder_out=None, mask=None):\n        b, c, *spatial = x.shape\n        qkv = self.qkv(self.norm(x).view(b, c, -1))\n        if encoder_out is not None:\n            encoder_out = self.encoder_kv(encoder_out)\n            h = self.attention(qkv, encoder_out, mask=mask)\n        else:\n            h = self.attention(qkv)\n        h = self.proj_out(h)\n        return x + h.reshape(b, c, *spatial)\n\n\nclass QKVAttention(nn.Module):\n    \"\"\"\n    A module which performs QKV attention. Matches legacy QKVAttention + input/ouput heads shaping\n    \"\"\"\n\n    def __init__(self, n_heads):\n        super().__init__()\n        self.n_heads = n_heads\n\n    def forward(self, qkv, encoder_kv=None, mask=None):\n        \"\"\"\n        Apply QKV attention.\n\n        :param qkv: an [N x (H * 3 * C) x T] tensor of Qs, Ks, and Vs.\n        :return: an [N x (H * C) x T] tensor after attention.\n        \"\"\"\n        bs, width, length = qkv.shape\n        assert width % (3 * self.n_heads) == 0\n        ch = width // (3 * self.n_heads)\n        q, k, v = qkv.reshape(bs * self.n_heads, ch * 3, length).split(ch, dim=1)\n        if encoder_kv is not None:\n            assert encoder_kv.shape[1] == self.n_heads * ch * 2\n            ek, ev = encoder_kv.reshape(bs * self.n_heads, ch * 2, -1).split(ch, dim=1)\n            k = th.cat([ek, k], dim=-1)\n            v = th.cat([ev, v], dim=-1)\n        scale = 1 / math.sqrt(math.sqrt(ch))\n        weight = th.einsum(\"bct,bcs->bts\", q * scale, k * scale)\n        if mask is not None:\n            mask = F.pad(mask, (0, length), value=0.0)\n            mask = (\n                mask.unsqueeze(1)\n                .expand(-1, self.n_heads, -1)\n                .reshape(bs * self.n_heads, 1, -1)\n            )\n            weight = weight + mask\n        weight = th.softmax(weight, dim=-1)\n        a = th.einsum(\"bts,bcs->bct\", weight, v)\n        return a.reshape(bs, -1, length)\n\n\nclass UNetModel(nn.Module):\n    \"\"\"\n    The full UNet model with attention and timestep embedding.\n\n    :param in_channels: channels in the input Tensor.\n    :param model_channels: base channel count for the model.\n    :param out_channels: channels in the output Tensor.\n    :param num_res_blocks: number of residual blocks per downsample.\n    :param attention_resolutions: a collection of downsample rates at which\n        attention will take place. May be a set, list, or tuple.\n        For example, if this contains 4, then at 4x downsampling, attention\n        will be used.\n    :param dropout: the dropout probability.\n    :param channel_mult: channel multiplier for each level of the UNet.\n    :param conv_resample: if True, use learned convolutions for upsampling and\n        downsampling.\n    :param dims: determines if the signal is 1D, 2D, or 3D.\n    :param clip_dim: dimension of clip feature.\n    :param num_classes: if specified (as an int), then this model will be\n        class-conditional with `num_classes` classes.\n    :param use_checkpoint: use gradient checkpointing to reduce memory usage.\n    :param num_heads: the number of attention heads in each attention layer.\n    :param num_heads_channels: if specified, ignore num_heads and instead use\n                               a fixed channel width per attention head.\n    :param num_heads_upsample: works with num_heads to set a different number\n                               of heads for upsampling. Deprecated.\n    :param use_scale_shift_norm: use a FiLM-like conditioning mechanism.\n    :param resblock_updown: use residual blocks for up/downsampling.\n    :param encoder_channels: use to make the dimension of query and kv same in AttentionBlock.\n    :param use_time_embedding: use time embedding for condition.\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels,\n        model_channels,\n        out_channels,\n        num_res_blocks,\n        attention_resolutions,\n        dropout=0,\n        channel_mult=(1, 2, 4, 8),\n        conv_resample=True,\n        dims=2,\n        clip_dim=None,\n        use_checkpoint=False,\n        num_heads=1,\n        num_head_channels=-1,\n        num_heads_upsample=-1,\n        use_scale_shift_norm=False,\n        use_middle_attention=True,\n        resblock_updown=False,\n        encoder_channels=None,\n        use_time_embedding=True,\n    ):\n        super().__init__()\n\n        if num_heads_upsample == -1:\n            num_heads_upsample = num_heads\n\n        self.in_channels = in_channels\n        self.model_channels = model_channels\n        self.out_channels = out_channels\n        self.num_res_blocks = num_res_blocks\n        self.attention_resolutions = attention_resolutions\n        self.dropout = dropout\n        self.channel_mult = channel_mult\n        self.conv_resample = conv_resample\n        self.clip_dim = clip_dim\n        self.use_checkpoint = use_checkpoint\n        self.num_heads = num_heads\n        self.num_head_channels = num_head_channels\n        self.num_heads_upsample = num_heads_upsample\n        self.use_middle_attention = use_middle_attention\n        self.use_time_embedding = use_time_embedding\n\n        if self.use_time_embedding:\n            time_embed_dim = model_channels * 4\n            self.time_embed = nn.Sequential(\n                linear(model_channels, time_embed_dim),\n                nn.SiLU(),\n                linear(time_embed_dim, time_embed_dim),\n            )\n\n            if self.clip_dim is not None:\n                self.clip_emb = nn.Linear(clip_dim, time_embed_dim)\n        else:\n            time_embed_dim = None\n\n        CustomResidualBlock = (\n            ResBlock if self.use_time_embedding else ResBlockNoTimeEmbedding\n        )\n        ch = input_ch = int(channel_mult[0] * model_channels)\n        self.input_blocks = nn.ModuleList(\n            [TimestepEmbedSequential(conv_nd(dims, in_channels, ch, 3, padding=1))]\n        )\n        self._feature_size = ch\n        input_block_chans = [ch]\n        ds = 1\n        for level, mult in enumerate(channel_mult):\n            for _ in range(num_res_blocks):\n                layers = [\n                    CustomResidualBlock(\n                        ch,\n                        time_embed_dim,\n                        dropout,\n                        out_channels=int(mult * model_channels),\n                        dims=dims,\n                        use_checkpoint=use_checkpoint,\n                        use_scale_shift_norm=use_scale_shift_norm,\n                    )\n                ]\n                ch = int(mult * model_channels)\n                if ds in attention_resolutions:\n                    layers.append(\n                        AttentionBlock(\n                            ch,\n                            use_checkpoint=use_checkpoint,\n                            num_heads=num_heads,\n                            num_head_channels=num_head_channels,\n                            encoder_channels=encoder_channels,\n                        )\n                    )\n                self.input_blocks.append(TimestepEmbedSequential(*layers))\n                self._feature_size += ch\n                input_block_chans.append(ch)\n            if level != len(channel_mult) - 1:\n                out_ch = ch\n                self.input_blocks.append(\n                    TimestepEmbedSequential(\n                        CustomResidualBlock(\n                            ch,\n                            time_embed_dim,\n                            dropout,\n                            out_channels=out_ch,\n                            dims=dims,\n                            use_checkpoint=use_checkpoint,\n                            use_scale_shift_norm=use_scale_shift_norm,\n                            down=True,\n                        )\n                        if resblock_updown\n                        else Downsample(\n                            ch, conv_resample, dims=dims, out_channels=out_ch\n                        )\n                    )\n                )\n                ch = out_ch\n                input_block_chans.append(ch)\n                ds *= 2\n                self._feature_size += ch\n\n        self.middle_block = TimestepEmbedSequential(\n            CustomResidualBlock(\n                ch,\n                time_embed_dim,\n                dropout,\n                dims=dims,\n                use_checkpoint=use_checkpoint,\n                use_scale_shift_norm=use_scale_shift_norm,\n            ),\n            *(\n                AttentionBlock(\n                    ch,\n                    use_checkpoint=use_checkpoint,\n                    num_heads=num_heads,\n                    num_head_channels=num_head_channels,\n                    encoder_channels=encoder_channels,\n                ),\n            )\n            if self.use_middle_attention\n            else tuple(),  # add AttentionBlock or not\n            CustomResidualBlock(\n                ch,\n                time_embed_dim,\n                dropout,\n                dims=dims,\n                use_checkpoint=use_checkpoint,\n                use_scale_shift_norm=use_scale_shift_norm,\n            ),\n        )\n        self._feature_size += ch\n\n        self.output_blocks = nn.ModuleList([])\n        for level, mult in list(enumerate(channel_mult))[::-1]:\n            for i in range(num_res_blocks + 1):\n                ich = input_block_chans.pop()\n                layers = [\n                    CustomResidualBlock(\n                        ch + ich,\n                        time_embed_dim,\n                        dropout,\n                        out_channels=int(model_channels * mult),\n                        dims=dims,\n                        use_checkpoint=use_checkpoint,\n                        use_scale_shift_norm=use_scale_shift_norm,\n                    )\n                ]\n                ch = int(model_channels * mult)\n                if ds in attention_resolutions:\n                    layers.append(\n                        AttentionBlock(\n                            ch,\n                            use_checkpoint=use_checkpoint,\n                            num_heads=num_heads_upsample,\n                            num_head_channels=num_head_channels,\n                            encoder_channels=encoder_channels,\n                        )\n                    )\n                if level and i == num_res_blocks:\n                    out_ch = ch\n                    layers.append(\n                        CustomResidualBlock(\n                            ch,\n                            time_embed_dim,\n                            dropout,\n                            out_channels=out_ch,\n                            dims=dims,\n                            use_checkpoint=use_checkpoint,\n                            use_scale_shift_norm=use_scale_shift_norm,\n                            up=True,\n                        )\n                        if resblock_updown\n                        else Upsample(ch, conv_resample, dims=dims, out_channels=out_ch)\n                    )\n                    ds //= 2\n                self.output_blocks.append(TimestepEmbedSequential(*layers))\n                self._feature_size += ch\n\n        self.out = nn.Sequential(\n            normalization(ch, swish=1.0),\n            nn.Identity(),\n            zero_module(conv_nd(dims, input_ch, out_channels, 3, padding=1)),\n        )\n\n    def forward(self, x, timesteps, y=None):\n        \"\"\"\n        Apply the model to an input batch.\n\n        :param x: an [N x C x ...] Tensor of inputs.\n        :param timesteps: a 1-D batch of timesteps.\n        :param y: an [N] Tensor of labels, if class-conditional.\n        :return: an [N x C x ...] Tensor of outputs.\n        \"\"\"\n        assert (y is not None) == (\n            self.clip_dim is not None\n        ), \"must specify y if and only if the model is clip-rep-conditional\"\n\n        hs = []\n        if self.use_time_embedding:\n            emb = self.time_embed(timestep_embedding(timesteps, self.model_channels))\n            if self.clip_dim is not None:\n                emb = emb + self.clip_emb(y)\n        else:\n            emb = None\n\n        h = x\n        for module in self.input_blocks:\n            h = module(h, emb)\n            hs.append(h)\n        h = self.middle_block(h, emb)\n        for module in self.output_blocks:\n            h = th.cat([h, hs.pop()], dim=1)\n            h = module(h, emb)\n\n        return self.out(h)\n\n\nclass SuperResUNetModel(UNetModel):\n    \"\"\"\n    A UNetModel that performs super-resolution.\n\n    Expects an extra kwarg `low_res` to condition on a low-resolution image.\n    Assumes that the shape of low-resolution and the input should be the same.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        if \"in_channels\" in kwargs:\n            kwargs = dict(kwargs)\n            kwargs[\"in_channels\"] = kwargs[\"in_channels\"] * 2\n        else:\n            # Curse you, Python. Or really, just curse positional arguments :|.\n            args = list(args)\n            args[1] = args[1] * 2\n        super().__init__(*args, **kwargs)\n\n    def forward(self, x, timesteps, low_res=None, **kwargs):\n        _, _, new_height, new_width = x.shape\n        assert new_height == low_res.shape[2] and new_width == low_res.shape[3]\n\n        x = th.cat([x, low_res], dim=1)\n        return super().forward(x, timesteps, **kwargs)\n\n\nclass PLMImUNet(UNetModel):\n    \"\"\"\n    A UNetModel that conditions on text with a pretrained text encoder in CLIP.\n\n    :param text_ctx: number of text tokens to expect.\n    :param xf_width: width of the transformer.\n    :param clip_emb_mult: #extra tokens by projecting clip text feature.\n    :param clip_emb_type: type of condition (here, we fix clip image feature).\n    :param clip_emb_drop: dropout rato of clip image feature for cfg.\n    \"\"\"\n\n    def __init__(\n        self,\n        text_ctx,\n        xf_width,\n        *args,\n        clip_emb_mult=None,\n        clip_emb_type=\"image\",\n        clip_emb_drop=0.0,\n        **kwargs,\n    ):\n        self.text_ctx = text_ctx\n        self.xf_width = xf_width\n        self.clip_emb_mult = clip_emb_mult\n        self.clip_emb_type = clip_emb_type\n        self.clip_emb_drop = clip_emb_drop\n\n        if not xf_width:\n            super().__init__(*args, **kwargs, encoder_channels=None)\n        else:\n            super().__init__(*args, **kwargs, encoder_channels=xf_width)\n\n        # Project text encoded feat seq from pre-trained text encoder in CLIP\n        self.text_seq_proj = nn.Sequential(\n            nn.Linear(self.clip_dim, xf_width),\n            LayerNorm(xf_width),\n        )\n        # Project CLIP text feat\n        self.text_feat_proj = nn.Linear(self.clip_dim, self.model_channels * 4)\n\n        assert clip_emb_mult is not None\n        assert clip_emb_type == \"image\"\n        assert self.clip_dim is not None, \"CLIP representation dim should be specified\"\n\n        self.clip_tok_proj = nn.Linear(\n            self.clip_dim, self.xf_width * self.clip_emb_mult\n        )\n        if self.clip_emb_drop > 0:\n            self.cf_param = nn.Parameter(th.empty(self.clip_dim, dtype=th.float32))\n\n    def proc_clip_emb_drop(self, feat):\n        if self.clip_emb_drop > 0:\n            bsz, feat_dim = feat.shape\n            assert (\n                feat_dim == self.clip_dim\n            ), f\"CLIP input dim: {feat_dim}, model CLIP dim: {self.clip_dim}\"\n            drop_idx = th.rand((bsz,), device=feat.device) < self.clip_emb_drop\n            feat = th.where(\n                drop_idx[..., None], self.cf_param[None].type_as(feat), feat\n            )\n        return feat\n\n    def forward(\n        self, x, timesteps, txt_feat=None, txt_feat_seq=None, mask=None, y=None\n    ):\n        bsz = x.shape[0]\n        hs = []\n        emb = self.time_embed(timestep_embedding(timesteps, self.model_channels))\n        emb = emb + self.clip_emb(y)\n\n        xf_out = self.text_seq_proj(txt_feat_seq)\n        xf_out = xf_out.permute(0, 2, 1)\n        emb = emb + self.text_feat_proj(txt_feat)\n        xf_out = th.cat(\n            [\n                self.clip_tok_proj(y).reshape(bsz, -1, self.clip_emb_mult),\n                xf_out,\n            ],\n            dim=2,\n        )\n        mask = F.pad(mask, (self.clip_emb_mult, 0), value=True)\n        mask = th.where(mask, 0.0, float(\"-inf\"))\n\n        h = x\n        for module in self.input_blocks:\n            h = module(h, emb, xf_out, mask=mask)\n            hs.append(h)\n        h = self.middle_block(h, emb, xf_out, mask=mask)\n        for module in self.output_blocks:\n            h = th.cat([h, hs.pop()], dim=1)\n            h = module(h, emb, xf_out, mask=mask)\n        h = self.out(h)\n\n        return h\n", "ldm/modules/karlo/kakao/modules/diffusion/gaussian_diffusion.py": "# ------------------------------------------------------------------------------------\n# Adapted from Guided-Diffusion repo (https://github.com/openai/guided-diffusion)\n# ------------------------------------------------------------------------------------\n\nimport enum\nimport math\n\nimport numpy as np\nimport torch as th\n\n\ndef _warmup_beta(beta_start, beta_end, num_diffusion_timesteps, warmup_frac):\n    betas = beta_end * np.ones(num_diffusion_timesteps, dtype=np.float64)\n    warmup_time = int(num_diffusion_timesteps * warmup_frac)\n    betas[:warmup_time] = np.linspace(\n        beta_start, beta_end, warmup_time, dtype=np.float64\n    )\n    return betas\n\n\ndef get_beta_schedule(beta_schedule, *, beta_start, beta_end, num_diffusion_timesteps):\n    \"\"\"\n    This is the deprecated API for creating beta schedules.\n    See get_named_beta_schedule() for the new library of schedules.\n    \"\"\"\n    if beta_schedule == \"quad\":\n        betas = (\n            np.linspace(\n                beta_start**0.5,\n                beta_end**0.5,\n                num_diffusion_timesteps,\n                dtype=np.float64,\n            )\n            ** 2\n        )\n    elif beta_schedule == \"linear\":\n        betas = np.linspace(\n            beta_start, beta_end, num_diffusion_timesteps, dtype=np.float64\n        )\n    elif beta_schedule == \"warmup10\":\n        betas = _warmup_beta(beta_start, beta_end, num_diffusion_timesteps, 0.1)\n    elif beta_schedule == \"warmup50\":\n        betas = _warmup_beta(beta_start, beta_end, num_diffusion_timesteps, 0.5)\n    elif beta_schedule == \"const\":\n        betas = beta_end * np.ones(num_diffusion_timesteps, dtype=np.float64)\n    elif beta_schedule == \"jsd\":  # 1/T, 1/(T-1), 1/(T-2), ..., 1\n        betas = 1.0 / np.linspace(\n            num_diffusion_timesteps, 1, num_diffusion_timesteps, dtype=np.float64\n        )\n    else:\n        raise NotImplementedError(beta_schedule)\n    assert betas.shape == (num_diffusion_timesteps,)\n    return betas\n\n\ndef get_named_beta_schedule(schedule_name, num_diffusion_timesteps):\n    \"\"\"\n    Get a pre-defined beta schedule for the given name.\n    The beta schedule library consists of beta schedules which remain similar\n    in the limit of num_diffusion_timesteps.\n    Beta schedules may be added, but should not be removed or changed once\n    they are committed to maintain backwards compatibility.\n    \"\"\"\n    if schedule_name == \"linear\":\n        # Linear schedule from Ho et al, extended to work for any number of\n        # diffusion steps.\n        scale = 1000 / num_diffusion_timesteps\n        return get_beta_schedule(\n            \"linear\",\n            beta_start=scale * 0.0001,\n            beta_end=scale * 0.02,\n            num_diffusion_timesteps=num_diffusion_timesteps,\n        )\n    elif schedule_name == \"squaredcos_cap_v2\":\n        return betas_for_alpha_bar(\n            num_diffusion_timesteps,\n            lambda t: math.cos((t + 0.008) / 1.008 * math.pi / 2) ** 2,\n        )\n    else:\n        raise NotImplementedError(f\"unknown beta schedule: {schedule_name}\")\n\n\ndef betas_for_alpha_bar(num_diffusion_timesteps, alpha_bar, max_beta=0.999):\n    \"\"\"\n    Create a beta schedule that discretizes the given alpha_t_bar function,\n    which defines the cumulative product of (1-beta) over time from t = [0,1].\n    :param num_diffusion_timesteps: the number of betas to produce.\n    :param alpha_bar: a lambda that takes an argument t from 0 to 1 and\n                      produces the cumulative product of (1-beta) up to that\n                      part of the diffusion process.\n    :param max_beta: the maximum beta to use; use values lower than 1 to\n                     prevent singularities.\n    \"\"\"\n    betas = []\n    for i in range(num_diffusion_timesteps):\n        t1 = i / num_diffusion_timesteps\n        t2 = (i + 1) / num_diffusion_timesteps\n        betas.append(min(1 - alpha_bar(t2) / alpha_bar(t1), max_beta))\n    return np.array(betas)\n\n\nclass ModelMeanType(enum.Enum):\n    \"\"\"\n    Which type of output the model predicts.\n    \"\"\"\n\n    PREVIOUS_X = enum.auto()  # the model predicts x_{t-1}\n    START_X = enum.auto()  # the model predicts x_0\n    EPSILON = enum.auto()  # the model predicts epsilon\n\n\nclass ModelVarType(enum.Enum):\n    \"\"\"\n    What is used as the model's output variance.\n    The LEARNED_RANGE option has been added to allow the model to predict\n    values between FIXED_SMALL and FIXED_LARGE, making its job easier.\n    \"\"\"\n\n    LEARNED = enum.auto()\n    FIXED_SMALL = enum.auto()\n    FIXED_LARGE = enum.auto()\n    LEARNED_RANGE = enum.auto()\n\n\nclass LossType(enum.Enum):\n    MSE = enum.auto()  # use raw MSE loss (and KL when learning variances)\n    RESCALED_MSE = (\n        enum.auto()\n    )  # use raw MSE loss (with RESCALED_KL when learning variances)\n    KL = enum.auto()  # use the variational lower-bound\n    RESCALED_KL = enum.auto()  # like KL, but rescale to estimate the full VLB\n\n    def is_vb(self):\n        return self == LossType.KL or self == LossType.RESCALED_KL\n\n\nclass GaussianDiffusion(th.nn.Module):\n    \"\"\"\n    Utilities for training and sampling diffusion models.\n    Original ported from this codebase:\n    https://github.com/hojonathanho/diffusion/blob/1e0dceb3b3495bbe19116a5e1b3596cd0706c543/diffusion_tf/diffusion_utils_2.py#L42\n    :param betas: a 1-D numpy array of betas for each diffusion timestep,\n                  starting at T and going to 1.\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        betas,\n        model_mean_type,\n        model_var_type,\n        loss_type,\n    ):\n        super(GaussianDiffusion, self).__init__()\n        self.model_mean_type = model_mean_type\n        self.model_var_type = model_var_type\n        self.loss_type = loss_type\n\n        # Use float64 for accuracy.\n        betas = np.array(betas, dtype=np.float64)\n        assert len(betas.shape) == 1, \"betas must be 1-D\"\n        assert (betas > 0).all() and (betas <= 1).all()\n\n        self.num_timesteps = int(betas.shape[0])\n\n        alphas = 1.0 - betas\n        alphas_cumprod = np.cumprod(alphas, axis=0)\n        alphas_cumprod_prev = np.append(1.0, alphas_cumprod[:-1])\n        alphas_cumprod_next = np.append(alphas_cumprod[1:], 0.0)\n        assert alphas_cumprod_prev.shape == (self.num_timesteps,)\n\n        # calculations for diffusion q(x_t | x_{t-1}) and others\n        sqrt_alphas_cumprod = np.sqrt(alphas_cumprod)\n        sqrt_one_minus_alphas_cumprod = np.sqrt(1.0 - alphas_cumprod)\n        log_one_minus_alphas_cumprod = np.log(1.0 - alphas_cumprod)\n        sqrt_recip_alphas_cumprod = np.sqrt(1.0 / alphas_cumprod)\n        sqrt_recipm1_alphas_cumprod = np.sqrt(1.0 / alphas_cumprod - 1)\n\n        # calculations for posterior q(x_{t-1} | x_t, x_0)\n        posterior_variance = (\n            betas * (1.0 - alphas_cumprod_prev) / (1.0 - alphas_cumprod)\n        )\n        # below: log calculation clipped because the posterior variance is 0 at the beginning of the diffusion chain\n        posterior_log_variance_clipped = np.log(\n            np.append(posterior_variance[1], posterior_variance[1:])\n        )\n        posterior_mean_coef1 = (\n            betas * np.sqrt(alphas_cumprod_prev) / (1.0 - alphas_cumprod)\n        )\n        posterior_mean_coef2 = (\n            (1.0 - alphas_cumprod_prev) * np.sqrt(alphas) / (1.0 - alphas_cumprod)\n        )\n\n        self.register_buffer(\"betas\", th.from_numpy(betas), persistent=False)\n        self.register_buffer(\n            \"alphas_cumprod\", th.from_numpy(alphas_cumprod), persistent=False\n        )\n        self.register_buffer(\n            \"alphas_cumprod_prev\", th.from_numpy(alphas_cumprod_prev), persistent=False\n        )\n        self.register_buffer(\n            \"alphas_cumprod_next\", th.from_numpy(alphas_cumprod_next), persistent=False\n        )\n\n        self.register_buffer(\n            \"sqrt_alphas_cumprod\", th.from_numpy(sqrt_alphas_cumprod), persistent=False\n        )\n        self.register_buffer(\n            \"sqrt_one_minus_alphas_cumprod\",\n            th.from_numpy(sqrt_one_minus_alphas_cumprod),\n            persistent=False,\n        )\n        self.register_buffer(\n            \"log_one_minus_alphas_cumprod\",\n            th.from_numpy(log_one_minus_alphas_cumprod),\n            persistent=False,\n        )\n        self.register_buffer(\n            \"sqrt_recip_alphas_cumprod\",\n            th.from_numpy(sqrt_recip_alphas_cumprod),\n            persistent=False,\n        )\n        self.register_buffer(\n            \"sqrt_recipm1_alphas_cumprod\",\n            th.from_numpy(sqrt_recipm1_alphas_cumprod),\n            persistent=False,\n        )\n\n        self.register_buffer(\n            \"posterior_variance\", th.from_numpy(posterior_variance), persistent=False\n        )\n        self.register_buffer(\n            \"posterior_log_variance_clipped\",\n            th.from_numpy(posterior_log_variance_clipped),\n            persistent=False,\n        )\n        self.register_buffer(\n            \"posterior_mean_coef1\",\n            th.from_numpy(posterior_mean_coef1),\n            persistent=False,\n        )\n        self.register_buffer(\n            \"posterior_mean_coef2\",\n            th.from_numpy(posterior_mean_coef2),\n            persistent=False,\n        )\n\n    def q_mean_variance(self, x_start, t):\n        \"\"\"\n        Get the distribution q(x_t | x_0).\n        :param x_start: the [N x C x ...] tensor of noiseless inputs.\n        :param t: the number of diffusion steps (minus 1). Here, 0 means one step.\n        :return: A tuple (mean, variance, log_variance), all of x_start's shape.\n        \"\"\"\n        mean = (\n            _extract_into_tensor(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start\n        )\n        variance = _extract_into_tensor(1.0 - self.alphas_cumprod, t, x_start.shape)\n        log_variance = _extract_into_tensor(\n            self.log_one_minus_alphas_cumprod, t, x_start.shape\n        )\n        return mean, variance, log_variance\n\n    def q_sample(self, x_start, t, noise=None):\n        \"\"\"\n        Diffuse the data for a given number of diffusion steps.\n        In other words, sample from q(x_t | x_0).\n        :param x_start: the initial data batch.\n        :param t: the number of diffusion steps (minus 1). Here, 0 means one step.\n        :param noise: if specified, the split-out normal noise.\n        :return: A noisy version of x_start.\n        \"\"\"\n        if noise is None:\n            noise = th.randn_like(x_start)\n        assert noise.shape == x_start.shape\n        return (\n            _extract_into_tensor(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start\n            + _extract_into_tensor(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape)\n            * noise\n        )\n\n    def q_posterior_mean_variance(self, x_start, x_t, t):\n        \"\"\"\n        Compute the mean and variance of the diffusion posterior:\n            q(x_{t-1} | x_t, x_0)\n        \"\"\"\n        assert x_start.shape == x_t.shape\n        posterior_mean = (\n            _extract_into_tensor(self.posterior_mean_coef1, t, x_t.shape) * x_start\n            + _extract_into_tensor(self.posterior_mean_coef2, t, x_t.shape) * x_t\n        )\n        posterior_variance = _extract_into_tensor(self.posterior_variance, t, x_t.shape)\n        posterior_log_variance_clipped = _extract_into_tensor(\n            self.posterior_log_variance_clipped, t, x_t.shape\n        )\n        assert (\n            posterior_mean.shape[0]\n            == posterior_variance.shape[0]\n            == posterior_log_variance_clipped.shape[0]\n            == x_start.shape[0]\n        )\n        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n\n    def p_mean_variance(\n        self,\n        model,\n        x,\n        t,\n        clip_denoised=True,\n        denoised_fn=None,\n        model_kwargs=None,\n        **ignore_kwargs,\n    ):\n        \"\"\"\n        Apply the model to get p(x_{t-1} | x_t), as well as a prediction of\n        the initial x, x_0.\n        :param model: the model, which takes a signal and a batch of timesteps\n                      as input.\n        :param x: the [N x C x ...] tensor at time t.\n        :param t: a 1-D Tensor of timesteps.\n        :param clip_denoised: if True, clip the denoised signal into [-1, 1].\n        :param denoised_fn: if not None, a function which applies to the\n            x_start prediction before it is used to sample. Applies before\n            clip_denoised.\n        :param model_kwargs: if not None, a dict of extra keyword arguments to\n            pass to the model. This can be used for conditioning.\n        :return: a dict with the following keys:\n                 - 'mean': the model mean output.\n                 - 'variance': the model variance output.\n                 - 'log_variance': the log of 'variance'.\n                 - 'pred_xstart': the prediction for x_0.\n        \"\"\"\n        if model_kwargs is None:\n            model_kwargs = {}\n\n        B, C = x.shape[:2]\n        assert t.shape == (B,)\n        model_output = model(x, t, **model_kwargs)\n        if isinstance(model_output, tuple):\n            model_output, extra = model_output\n        else:\n            extra = None\n\n        if self.model_var_type in [ModelVarType.LEARNED, ModelVarType.LEARNED_RANGE]:\n            assert model_output.shape == (B, C * 2, *x.shape[2:])\n            model_output, model_var_values = th.split(model_output, C, dim=1)\n            if self.model_var_type == ModelVarType.LEARNED:\n                model_log_variance = model_var_values\n                model_variance = th.exp(model_log_variance)\n            else:\n                min_log = _extract_into_tensor(\n                    self.posterior_log_variance_clipped, t, x.shape\n                )\n                max_log = _extract_into_tensor(th.log(self.betas), t, x.shape)\n                # The model_var_values is [-1, 1] for [min_var, max_var].\n                frac = (model_var_values + 1) / 2\n                model_log_variance = frac * max_log + (1 - frac) * min_log\n                model_variance = th.exp(model_log_variance)\n        else:\n            model_variance, model_log_variance = {\n                # for fixedlarge, we set the initial (log-)variance like so\n                # to get a better decoder log likelihood.\n                ModelVarType.FIXED_LARGE: (\n                    th.cat([self.posterior_variance[1][None], self.betas[1:]]),\n                    th.log(th.cat([self.posterior_variance[1][None], self.betas[1:]])),\n                ),\n                ModelVarType.FIXED_SMALL: (\n                    self.posterior_variance,\n                    self.posterior_log_variance_clipped,\n                ),\n            }[self.model_var_type]\n            model_variance = _extract_into_tensor(model_variance, t, x.shape)\n            model_log_variance = _extract_into_tensor(model_log_variance, t, x.shape)\n\n        def process_xstart(x):\n            if denoised_fn is not None:\n                x = denoised_fn(x)\n            if clip_denoised:\n                return x.clamp(-1, 1)\n            return x\n\n        if self.model_mean_type == ModelMeanType.PREVIOUS_X:\n            pred_xstart = process_xstart(\n                self._predict_xstart_from_xprev(x_t=x, t=t, xprev=model_output)\n            )\n            model_mean = model_output\n        elif self.model_mean_type in [ModelMeanType.START_X, ModelMeanType.EPSILON]:\n            if self.model_mean_type == ModelMeanType.START_X:\n                pred_xstart = process_xstart(model_output)\n            else:\n                pred_xstart = process_xstart(\n                    self._predict_xstart_from_eps(x_t=x, t=t, eps=model_output)\n                )\n            model_mean, _, _ = self.q_posterior_mean_variance(\n                x_start=pred_xstart, x_t=x, t=t\n            )\n        else:\n            raise NotImplementedError(self.model_mean_type)\n\n        assert (\n            model_mean.shape == model_log_variance.shape == pred_xstart.shape == x.shape\n        )\n        return {\n            \"mean\": model_mean,\n            \"variance\": model_variance,\n            \"log_variance\": model_log_variance,\n            \"pred_xstart\": pred_xstart,\n        }\n\n    def _predict_xstart_from_eps(self, x_t, t, eps):\n        assert x_t.shape == eps.shape\n        return (\n            _extract_into_tensor(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t\n            - _extract_into_tensor(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape) * eps\n        )\n\n    def _predict_eps_from_xstart(self, x_t, t, pred_xstart):\n        return (\n            _extract_into_tensor(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t\n            - pred_xstart\n        ) / _extract_into_tensor(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape)\n\n    def condition_mean(self, cond_fn, p_mean_var, x, t, model_kwargs=None):\n        \"\"\"\n        Compute the mean for the previous step, given a function cond_fn that\n        computes the gradient of a conditional log probability with respect to\n        x. In particular, cond_fn computes grad(log(p(y|x))), and we want to\n        condition on y.\n        This uses the conditioning strategy from Sohl-Dickstein et al. (2015).\n        \"\"\"\n        gradient = cond_fn(x, t, **model_kwargs)\n        new_mean = (\n            p_mean_var[\"mean\"].float() + p_mean_var[\"variance\"] * gradient.float()\n        )\n        return new_mean\n\n    def condition_score(self, cond_fn, p_mean_var, x, t, model_kwargs=None):\n        \"\"\"\n        Compute what the p_mean_variance output would have been, should the\n        model's score function be conditioned by cond_fn.\n        See condition_mean() for details on cond_fn.\n        Unlike condition_mean(), this instead uses the conditioning strategy\n        from Song et al (2020).\n        \"\"\"\n        alpha_bar = _extract_into_tensor(self.alphas_cumprod, t, x.shape)\n\n        eps = self._predict_eps_from_xstart(x, t, p_mean_var[\"pred_xstart\"])\n        eps = eps - (1 - alpha_bar).sqrt() * cond_fn(x, t, **model_kwargs)\n\n        out = p_mean_var.copy()\n        out[\"pred_xstart\"] = self._predict_xstart_from_eps(x, t, eps)\n        out[\"mean\"], _, _ = self.q_posterior_mean_variance(\n            x_start=out[\"pred_xstart\"], x_t=x, t=t\n        )\n        return out\n\n    def p_sample(\n        self,\n        model,\n        x,\n        t,\n        clip_denoised=True,\n        denoised_fn=None,\n        cond_fn=None,\n        model_kwargs=None,\n    ):\n        \"\"\"\n        Sample x_{t-1} from the model at the given timestep.\n        :param model: the model to sample from.\n        :param x: the current tensor at x_{t-1}.\n        :param t: the value of t, starting at 0 for the first diffusion step.\n        :param clip_denoised: if True, clip the x_start prediction to [-1, 1].\n        :param denoised_fn: if not None, a function which applies to the\n            x_start prediction before it is used to sample.\n        :param cond_fn: if not None, this is a gradient function that acts\n                        similarly to the model.\n        :param model_kwargs: if not None, a dict of extra keyword arguments to\n            pass to the model. This can be used for conditioning.\n        :return: a dict containing the following keys:\n                 - 'sample': a random sample from the model.\n                 - 'pred_xstart': a prediction of x_0.\n        \"\"\"\n        out = self.p_mean_variance(\n            model,\n            x,\n            t,\n            clip_denoised=clip_denoised,\n            denoised_fn=denoised_fn,\n            model_kwargs=model_kwargs,\n        )\n        noise = th.randn_like(x)\n        nonzero_mask = (\n            (t != 0).float().view(-1, *([1] * (len(x.shape) - 1)))\n        )  # no noise when t == 0\n        if cond_fn is not None:\n            out[\"mean\"] = self.condition_mean(\n                cond_fn, out, x, t, model_kwargs=model_kwargs\n            )\n        sample = out[\"mean\"] + nonzero_mask * th.exp(0.5 * out[\"log_variance\"]) * noise\n        return {\"sample\": sample, \"pred_xstart\": out[\"pred_xstart\"]}\n\n    def p_sample_loop(\n        self,\n        model,\n        shape,\n        noise=None,\n        clip_denoised=True,\n        denoised_fn=None,\n        cond_fn=None,\n        model_kwargs=None,\n        device=None,\n        progress=False,\n    ):\n        \"\"\"\n        Generate samples from the model.\n        :param model: the model module.\n        :param shape: the shape of the samples, (N, C, H, W).\n        :param noise: if specified, the noise from the encoder to sample.\n                      Should be of the same shape as `shape`.\n        :param clip_denoised: if True, clip x_start predictions to [-1, 1].\n        :param denoised_fn: if not None, a function which applies to the\n            x_start prediction before it is used to sample.\n        :param cond_fn: if not None, this is a gradient function that acts\n                        similarly to the model.\n        :param model_kwargs: if not None, a dict of extra keyword arguments to\n            pass to the model. This can be used for conditioning.\n        :param device: if specified, the device to create the samples on.\n                       If not specified, use a model parameter's device.\n        :param progress: if True, show a tqdm progress bar.\n        :return: a non-differentiable batch of samples.\n        \"\"\"\n        final = None\n        for sample in self.p_sample_loop_progressive(\n            model,\n            shape,\n            noise=noise,\n            clip_denoised=clip_denoised,\n            denoised_fn=denoised_fn,\n            cond_fn=cond_fn,\n            model_kwargs=model_kwargs,\n            device=device,\n            progress=progress,\n        ):\n            final = sample\n        return final[\"sample\"]\n\n    def p_sample_loop_progressive(\n        self,\n        model,\n        shape,\n        noise=None,\n        clip_denoised=True,\n        denoised_fn=None,\n        cond_fn=None,\n        model_kwargs=None,\n        device=None,\n        progress=False,\n    ):\n        \"\"\"\n        Generate samples from the model and yield intermediate samples from\n        each timestep of diffusion.\n        Arguments are the same as p_sample_loop().\n        Returns a generator over dicts, where each dict is the return value of\n        p_sample().\n        \"\"\"\n        if device is None:\n            device = next(model.parameters()).device\n        assert isinstance(shape, (tuple, list))\n        if noise is not None:\n            img = noise\n        else:\n            img = th.randn(*shape, device=device)\n        indices = list(range(self.num_timesteps))[::-1]\n\n        if progress:\n            # Lazy import so that we don't depend on tqdm.\n            from tqdm.auto import tqdm\n\n            indices = tqdm(indices)\n\n        for idx, i in enumerate(indices):\n            t = th.tensor([i] * shape[0], device=device)\n            with th.no_grad():\n                out = self.p_sample(\n                    model,\n                    img,\n                    t,\n                    clip_denoised=clip_denoised,\n                    denoised_fn=denoised_fn,\n                    cond_fn=cond_fn,\n                    model_kwargs=model_kwargs,\n                )\n                yield out\n                img = out[\"sample\"]\n\n    def p_sample_loop_progressive_for_improved_sr(\n        self,\n        model,\n        model_aux,\n        shape,\n        noise=None,\n        clip_denoised=True,\n        denoised_fn=None,\n        cond_fn=None,\n        model_kwargs=None,\n        device=None,\n        progress=False,\n    ):\n        \"\"\"\n        Modified version of p_sample_loop_progressive for sampling from the improved sr model\n        \"\"\"\n\n        if device is None:\n            device = next(model.parameters()).device\n        assert isinstance(shape, (tuple, list))\n        if noise is not None:\n            img = noise\n        else:\n            img = th.randn(*shape, device=device)\n        indices = list(range(self.num_timesteps))[::-1]\n\n        if progress:\n            # Lazy import so that we don't depend on tqdm.\n            from tqdm.auto import tqdm\n\n            indices = tqdm(indices)\n\n        for idx, i in enumerate(indices):\n            t = th.tensor([i] * shape[0], device=device)\n            with th.no_grad():\n                out = self.p_sample(\n                    model_aux if len(indices) - 1 == idx else model,\n                    img,\n                    t,\n                    clip_denoised=clip_denoised,\n                    denoised_fn=denoised_fn,\n                    cond_fn=cond_fn,\n                    model_kwargs=model_kwargs,\n                )\n                yield out\n                img = out[\"sample\"]\n\n    def ddim_sample(\n        self,\n        model,\n        x,\n        t,\n        clip_denoised=True,\n        denoised_fn=None,\n        cond_fn=None,\n        model_kwargs=None,\n        eta=0.0,\n    ):\n        \"\"\"\n        Sample x_{t-1} from the model using DDIM.\n        Same usage as p_sample().\n        \"\"\"\n        out = self.p_mean_variance(\n            model,\n            x,\n            t,\n            clip_denoised=clip_denoised,\n            denoised_fn=denoised_fn,\n            model_kwargs=model_kwargs,\n        )\n        if cond_fn is not None:\n            out = self.condition_score(cond_fn, out, x, t, model_kwargs=model_kwargs)\n\n        # Usually our model outputs epsilon, but we re-derive it\n        # in case we used x_start or x_prev prediction.\n        eps = self._predict_eps_from_xstart(x, t, out[\"pred_xstart\"])\n\n        alpha_bar = _extract_into_tensor(self.alphas_cumprod, t, x.shape)\n        alpha_bar_prev = _extract_into_tensor(self.alphas_cumprod_prev, t, x.shape)\n        sigma = (\n            eta\n            * th.sqrt((1 - alpha_bar_prev) / (1 - alpha_bar))\n            * th.sqrt(1 - alpha_bar / alpha_bar_prev)\n        )\n        # Equation 12.\n        noise = th.randn_like(x)\n        mean_pred = (\n            out[\"pred_xstart\"] * th.sqrt(alpha_bar_prev)\n            + th.sqrt(1 - alpha_bar_prev - sigma**2) * eps\n        )\n        nonzero_mask = (\n            (t != 0).float().view(-1, *([1] * (len(x.shape) - 1)))\n        )  # no noise when t == 0\n        sample = mean_pred + nonzero_mask * sigma * noise\n        return {\"sample\": sample, \"pred_xstart\": out[\"pred_xstart\"]}\n\n    def ddim_reverse_sample(\n        self,\n        model,\n        x,\n        t,\n        clip_denoised=True,\n        denoised_fn=None,\n        cond_fn=None,\n        model_kwargs=None,\n        eta=0.0,\n    ):\n        \"\"\"\n        Sample x_{t+1} from the model using DDIM reverse ODE.\n        \"\"\"\n        assert eta == 0.0, \"Reverse ODE only for deterministic path\"\n        out = self.p_mean_variance(\n            model,\n            x,\n            t,\n            clip_denoised=clip_denoised,\n            denoised_fn=denoised_fn,\n            model_kwargs=model_kwargs,\n        )\n        if cond_fn is not None:\n            out = self.condition_score(cond_fn, out, x, t, model_kwargs=model_kwargs)\n        # Usually our model outputs epsilon, but we re-derive it\n        # in case we used x_start or x_prev prediction.\n        eps = (\n            _extract_into_tensor(self.sqrt_recip_alphas_cumprod, t, x.shape) * x\n            - out[\"pred_xstart\"]\n        ) / _extract_into_tensor(self.sqrt_recipm1_alphas_cumprod, t, x.shape)\n        alpha_bar_next = _extract_into_tensor(self.alphas_cumprod_next, t, x.shape)\n\n        # Equation 12. reversed\n        mean_pred = (\n            out[\"pred_xstart\"] * th.sqrt(alpha_bar_next)\n            + th.sqrt(1 - alpha_bar_next) * eps\n        )\n\n        return {\"sample\": mean_pred, \"pred_xstart\": out[\"pred_xstart\"]}\n\n    def ddim_sample_loop(\n        self,\n        model,\n        shape,\n        noise=None,\n        clip_denoised=True,\n        denoised_fn=None,\n        cond_fn=None,\n        model_kwargs=None,\n        device=None,\n        progress=False,\n        eta=0.0,\n    ):\n        \"\"\"\n        Generate samples from the model using DDIM.\n        Same usage as p_sample_loop().\n        \"\"\"\n        final = None\n        for sample in self.ddim_sample_loop_progressive(\n            model,\n            shape,\n            noise=noise,\n            clip_denoised=clip_denoised,\n            denoised_fn=denoised_fn,\n            cond_fn=cond_fn,\n            model_kwargs=model_kwargs,\n            device=device,\n            progress=progress,\n            eta=eta,\n        ):\n            final = sample\n        return final[\"sample\"]\n\n    def ddim_sample_loop_progressive(\n        self,\n        model,\n        shape,\n        noise=None,\n        clip_denoised=True,\n        denoised_fn=None,\n        cond_fn=None,\n        model_kwargs=None,\n        device=None,\n        progress=False,\n        eta=0.0,\n    ):\n        \"\"\"\n        Use DDIM to sample from the model and yield intermediate samples from\n        each timestep of DDIM.\n        Same usage as p_sample_loop_progressive().\n        \"\"\"\n        if device is None:\n            device = next(model.parameters()).device\n        assert isinstance(shape, (tuple, list))\n        if noise is not None:\n            img = noise\n        else:\n            img = th.randn(*shape, device=device)\n        indices = list(range(self.num_timesteps))[::-1]\n\n        if progress:\n            # Lazy import so that we don't depend on tqdm.\n            from tqdm.auto import tqdm\n\n            indices = tqdm(indices)\n\n        for i in indices:\n            t = th.tensor([i] * shape[0], device=device)\n            with th.no_grad():\n                out = self.ddim_sample(\n                    model,\n                    img,\n                    t,\n                    clip_denoised=clip_denoised,\n                    denoised_fn=denoised_fn,\n                    cond_fn=cond_fn,\n                    model_kwargs=model_kwargs,\n                    eta=eta,\n                )\n                yield out\n                img = out[\"sample\"]\n\n\ndef _extract_into_tensor(arr, timesteps, broadcast_shape):\n    \"\"\"\n    Extract values from a 1-D numpy array for a batch of indices.\n    :param arr: the 1-D numpy array.\n    :param timesteps: a tensor of indices into the array to extract.\n    :param broadcast_shape: a larger shape of K dimensions with the batch\n                            dimension equal to the length of timesteps.\n    :return: a tensor of shape [batch_size, 1, ...] where the shape has K dims.\n    \"\"\"\n    res = arr.to(device=timesteps.device)[timesteps].float()\n    while len(res.shape) < len(broadcast_shape):\n        res = res[..., None]\n    return res + th.zeros(broadcast_shape, device=timesteps.device)\n", "ldm/modules/karlo/kakao/modules/diffusion/respace.py": "# ------------------------------------------------------------------------------------\n# Adapted from Guided-Diffusion repo (https://github.com/openai/guided-diffusion)\n# ------------------------------------------------------------------------------------\n\n\nimport torch as th\n\nfrom .gaussian_diffusion import GaussianDiffusion\n\n\ndef space_timesteps(num_timesteps, section_counts):\n    \"\"\"\n    Create a list of timesteps to use from an original diffusion process,\n    given the number of timesteps we want to take from equally-sized portions\n    of the original process.\n\n    For example, if there's 300 timesteps and the section counts are [10,15,20]\n    then the first 100 timesteps are strided to be 10 timesteps, the second 100\n    are strided to be 15 timesteps, and the final 100 are strided to be 20.\n\n    :param num_timesteps: the number of diffusion steps in the original\n                          process to divide up.\n    :param section_counts: either a list of numbers, or a string containing\n                           comma-separated numbers, indicating the step count\n                           per section. As a special case, use \"ddimN\" where N\n                           is a number of steps to use the striding from the\n                           DDIM paper.\n    :return: a set of diffusion steps from the original process to use.\n    \"\"\"\n    if isinstance(section_counts, str):\n        if section_counts.startswith(\"ddim\"):\n            desired_count = int(section_counts[len(\"ddim\") :])\n            for i in range(1, num_timesteps):\n                if len(range(0, num_timesteps, i)) == desired_count:\n                    return set(range(0, num_timesteps, i))\n            raise ValueError(\n                f\"cannot create exactly {num_timesteps} steps with an integer stride\"\n            )\n        elif section_counts == \"fast27\":\n            steps = space_timesteps(num_timesteps, \"10,10,3,2,2\")\n            # Help reduce DDIM artifacts from noisiest timesteps.\n            steps.remove(num_timesteps - 1)\n            steps.add(num_timesteps - 3)\n            return steps\n        section_counts = [int(x) for x in section_counts.split(\",\")]\n    size_per = num_timesteps // len(section_counts)\n    extra = num_timesteps % len(section_counts)\n    start_idx = 0\n    all_steps = []\n    for i, section_count in enumerate(section_counts):\n        size = size_per + (1 if i < extra else 0)\n        if size < section_count:\n            raise ValueError(\n                f\"cannot divide section of {size} steps into {section_count}\"\n            )\n        if section_count <= 1:\n            frac_stride = 1\n        else:\n            frac_stride = (size - 1) / (section_count - 1)\n        cur_idx = 0.0\n        taken_steps = []\n        for _ in range(section_count):\n            taken_steps.append(start_idx + round(cur_idx))\n            cur_idx += frac_stride\n        all_steps += taken_steps\n        start_idx += size\n    return set(all_steps)\n\n\nclass SpacedDiffusion(GaussianDiffusion):\n    \"\"\"\n    A diffusion process which can skip steps in a base diffusion process.\n\n    :param use_timesteps: a collection (sequence or set) of timesteps from the\n                          original diffusion process to retain.\n    :param kwargs: the kwargs to create the base diffusion process.\n    \"\"\"\n\n    def __init__(self, use_timesteps, **kwargs):\n        self.use_timesteps = set(use_timesteps)\n        self.original_num_steps = len(kwargs[\"betas\"])\n\n        base_diffusion = GaussianDiffusion(**kwargs)  # pylint: disable=missing-kwoa\n        last_alpha_cumprod = 1.0\n        new_betas = []\n        timestep_map = []\n        for i, alpha_cumprod in enumerate(base_diffusion.alphas_cumprod):\n            if i in self.use_timesteps:\n                new_betas.append(1 - alpha_cumprod / last_alpha_cumprod)\n                last_alpha_cumprod = alpha_cumprod\n                timestep_map.append(i)\n        kwargs[\"betas\"] = th.tensor(new_betas).numpy()\n        super().__init__(**kwargs)\n        self.register_buffer(\"timestep_map\", th.tensor(timestep_map), persistent=False)\n\n    def p_mean_variance(self, model, *args, **kwargs):\n        return super().p_mean_variance(self._wrap_model(model), *args, **kwargs)\n\n    def condition_mean(self, cond_fn, *args, **kwargs):\n        return super().condition_mean(self._wrap_model(cond_fn), *args, **kwargs)\n\n    def condition_score(self, cond_fn, *args, **kwargs):\n        return super().condition_score(self._wrap_model(cond_fn), *args, **kwargs)\n\n    def _wrap_model(self, model):\n        def wrapped(x, ts, **kwargs):\n            ts_cpu = ts.detach().to(\"cpu\")\n            return model(\n                x, self.timestep_map[ts_cpu].to(device=ts.device, dtype=ts.dtype), **kwargs\n            )\n\n        return wrapped\n", "scripts/img2img.py": "\"\"\"make variations of input image\"\"\"\n\nimport argparse, os\nimport PIL\nimport torch\nimport numpy as np\nfrom omegaconf import OmegaConf\nfrom PIL import Image\nfrom tqdm import tqdm, trange\nfrom itertools import islice\nfrom einops import rearrange, repeat\nfrom torchvision.utils import make_grid\nfrom torch import autocast\nfrom contextlib import nullcontext\nfrom pytorch_lightning import seed_everything\nfrom imwatermark import WatermarkEncoder\n\n\nfrom scripts.txt2img import put_watermark\nfrom ldm.util import instantiate_from_config\nfrom ldm.models.diffusion.ddim import DDIMSampler\n\n\ndef chunk(it, size):\n    it = iter(it)\n    return iter(lambda: tuple(islice(it, size)), ())\n\n\ndef load_model_from_config(config, ckpt, verbose=False):\n    print(f\"Loading model from {ckpt}\")\n    pl_sd = torch.load(ckpt, map_location=\"cpu\")\n    if \"global_step\" in pl_sd:\n        print(f\"Global Step: {pl_sd['global_step']}\")\n    sd = pl_sd[\"state_dict\"]\n    model = instantiate_from_config(config.model)\n    m, u = model.load_state_dict(sd, strict=False)\n    if len(m) > 0 and verbose:\n        print(\"missing keys:\")\n        print(m)\n    if len(u) > 0 and verbose:\n        print(\"unexpected keys:\")\n        print(u)\n\n    model.cuda()\n    model.eval()\n    return model\n\n\ndef load_img(path):\n    image = Image.open(path).convert(\"RGB\")\n    w, h = image.size\n    print(f\"loaded input image of size ({w}, {h}) from {path}\")\n    w, h = map(lambda x: x - x % 64, (w, h))  # resize to integer multiple of 64\n    image = image.resize((w, h), resample=PIL.Image.LANCZOS)\n    image = np.array(image).astype(np.float32) / 255.0\n    image = image[None].transpose(0, 3, 1, 2)\n    image = torch.from_numpy(image)\n    return 2. * image - 1.\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(\n        \"--prompt\",\n        type=str,\n        nargs=\"?\",\n        default=\"a painting of a virus monster playing guitar\",\n        help=\"the prompt to render\"\n    )\n\n    parser.add_argument(\n        \"--init-img\",\n        type=str,\n        nargs=\"?\",\n        help=\"path to the input image\"\n    )\n\n    parser.add_argument(\n        \"--outdir\",\n        type=str,\n        nargs=\"?\",\n        help=\"dir to write results to\",\n        default=\"outputs/img2img-samples\"\n    )\n\n    parser.add_argument(\n        \"--ddim_steps\",\n        type=int,\n        default=50,\n        help=\"number of ddim sampling steps\",\n    )\n\n    parser.add_argument(\n        \"--fixed_code\",\n        action='store_true',\n        help=\"if enabled, uses the same starting code across all samples \",\n    )\n\n    parser.add_argument(\n        \"--ddim_eta\",\n        type=float,\n        default=0.0,\n        help=\"ddim eta (eta=0.0 corresponds to deterministic sampling\",\n    )\n    parser.add_argument(\n        \"--n_iter\",\n        type=int,\n        default=1,\n        help=\"sample this often\",\n    )\n\n    parser.add_argument(\n        \"--C\",\n        type=int,\n        default=4,\n        help=\"latent channels\",\n    )\n    parser.add_argument(\n        \"--f\",\n        type=int,\n        default=8,\n        help=\"downsampling factor, most often 8 or 16\",\n    )\n\n    parser.add_argument(\n        \"--n_samples\",\n        type=int,\n        default=2,\n        help=\"how many samples to produce for each given prompt. A.k.a batch size\",\n    )\n\n    parser.add_argument(\n        \"--n_rows\",\n        type=int,\n        default=0,\n        help=\"rows in the grid (default: n_samples)\",\n    )\n\n    parser.add_argument(\n        \"--scale\",\n        type=float,\n        default=9.0,\n        help=\"unconditional guidance scale: eps = eps(x, empty) + scale * (eps(x, cond) - eps(x, empty))\",\n    )\n\n    parser.add_argument(\n        \"--strength\",\n        type=float,\n        default=0.8,\n        help=\"strength for noising/unnoising. 1.0 corresponds to full destruction of information in init image\",\n    )\n\n    parser.add_argument(\n        \"--from-file\",\n        type=str,\n        help=\"if specified, load prompts from this file\",\n    )\n    parser.add_argument(\n        \"--config\",\n        type=str,\n        default=\"configs/stable-diffusion/v2-inference.yaml\",\n        help=\"path to config which constructs model\",\n    )\n    parser.add_argument(\n        \"--ckpt\",\n        type=str,\n        help=\"path to checkpoint of model\",\n    )\n    parser.add_argument(\n        \"--seed\",\n        type=int,\n        default=42,\n        help=\"the seed (for reproducible sampling)\",\n    )\n    parser.add_argument(\n        \"--precision\",\n        type=str,\n        help=\"evaluate at this precision\",\n        choices=[\"full\", \"autocast\"],\n        default=\"autocast\"\n    )\n\n    opt = parser.parse_args()\n    seed_everything(opt.seed)\n\n    config = OmegaConf.load(f\"{opt.config}\")\n    model = load_model_from_config(config, f\"{opt.ckpt}\")\n\n    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n    model = model.to(device)\n\n    sampler = DDIMSampler(model)\n\n    os.makedirs(opt.outdir, exist_ok=True)\n    outpath = opt.outdir\n\n    print(\"Creating invisible watermark encoder (see https://github.com/ShieldMnt/invisible-watermark)...\")\n    wm = \"SDV2\"\n    wm_encoder = WatermarkEncoder()\n    wm_encoder.set_watermark('bytes', wm.encode('utf-8'))\n\n    batch_size = opt.n_samples\n    n_rows = opt.n_rows if opt.n_rows > 0 else batch_size\n    if not opt.from_file:\n        prompt = opt.prompt\n        assert prompt is not None\n        data = [batch_size * [prompt]]\n\n    else:\n        print(f\"reading prompts from {opt.from_file}\")\n        with open(opt.from_file, \"r\") as f:\n            data = f.read().splitlines()\n            data = list(chunk(data, batch_size))\n\n    sample_path = os.path.join(outpath, \"samples\")\n    os.makedirs(sample_path, exist_ok=True)\n    base_count = len(os.listdir(sample_path))\n    grid_count = len(os.listdir(outpath)) - 1\n\n    assert os.path.isfile(opt.init_img)\n    init_image = load_img(opt.init_img).to(device)\n    init_image = repeat(init_image, '1 ... -> b ...', b=batch_size)\n    init_latent = model.get_first_stage_encoding(model.encode_first_stage(init_image))  # move to latent space\n\n    sampler.make_schedule(ddim_num_steps=opt.ddim_steps, ddim_eta=opt.ddim_eta, verbose=False)\n\n    assert 0. <= opt.strength <= 1., 'can only work with strength in [0.0, 1.0]'\n    t_enc = int(opt.strength * opt.ddim_steps)\n    print(f\"target t_enc is {t_enc} steps\")\n\n    precision_scope = autocast if opt.precision == \"autocast\" else nullcontext\n    with torch.no_grad():\n        with precision_scope(\"cuda\"):\n            with model.ema_scope():\n                all_samples = list()\n                for n in trange(opt.n_iter, desc=\"Sampling\"):\n                    for prompts in tqdm(data, desc=\"data\"):\n                        uc = None\n                        if opt.scale != 1.0:\n                            uc = model.get_learned_conditioning(batch_size * [\"\"])\n                        if isinstance(prompts, tuple):\n                            prompts = list(prompts)\n                        c = model.get_learned_conditioning(prompts)\n\n                        # encode (scaled latent)\n                        z_enc = sampler.stochastic_encode(init_latent, torch.tensor([t_enc] * batch_size).to(device))\n                        # decode it\n                        samples = sampler.decode(z_enc, c, t_enc, unconditional_guidance_scale=opt.scale,\n                                                 unconditional_conditioning=uc, )\n\n                        x_samples = model.decode_first_stage(samples)\n                        x_samples = torch.clamp((x_samples + 1.0) / 2.0, min=0.0, max=1.0)\n\n                        for x_sample in x_samples:\n                            x_sample = 255. * rearrange(x_sample.cpu().numpy(), 'c h w -> h w c')\n                            img = Image.fromarray(x_sample.astype(np.uint8))\n                            img = put_watermark(img, wm_encoder)\n                            img.save(os.path.join(sample_path, f\"{base_count:05}.png\"))\n                            base_count += 1\n                        all_samples.append(x_samples)\n\n                # additionally, save as grid\n                grid = torch.stack(all_samples, 0)\n                grid = rearrange(grid, 'n b c h w -> (n b) c h w')\n                grid = make_grid(grid, nrow=n_rows)\n\n                # to image\n                grid = 255. * rearrange(grid, 'c h w -> h w c').cpu().numpy()\n                grid = Image.fromarray(grid.astype(np.uint8))\n                grid = put_watermark(grid, wm_encoder)\n                grid.save(os.path.join(outpath, f'grid-{grid_count:04}.png'))\n                grid_count += 1\n\n    print(f\"Your samples are ready and waiting for you here: \\n{outpath} \\nEnjoy.\")\n\n\nif __name__ == \"__main__\":\n    main()\n", "scripts/txt2img.py": "import argparse, os\nimport cv2\nimport torch\nimport numpy as np\nfrom omegaconf import OmegaConf\nfrom PIL import Image\nfrom tqdm import tqdm, trange\nfrom itertools import islice\nfrom einops import rearrange\nfrom torchvision.utils import make_grid\nfrom pytorch_lightning import seed_everything\nfrom torch import autocast\nfrom contextlib import nullcontext\nfrom imwatermark import WatermarkEncoder\n\nfrom ldm.util import instantiate_from_config\nfrom ldm.models.diffusion.ddim import DDIMSampler\nfrom ldm.models.diffusion.plms import PLMSSampler\nfrom ldm.models.diffusion.dpm_solver import DPMSolverSampler\n\ntorch.set_grad_enabled(False)\n\ndef chunk(it, size):\n    it = iter(it)\n    return iter(lambda: tuple(islice(it, size)), ())\n\n\ndef load_model_from_config(config, ckpt, device=torch.device(\"cuda\"), verbose=False):\n    print(f\"Loading model from {ckpt}\")\n    pl_sd = torch.load(ckpt, map_location=\"cpu\")\n    if \"global_step\" in pl_sd:\n        print(f\"Global Step: {pl_sd['global_step']}\")\n    sd = pl_sd[\"state_dict\"]\n    model = instantiate_from_config(config.model)\n    m, u = model.load_state_dict(sd, strict=False)\n    if len(m) > 0 and verbose:\n        print(\"missing keys:\")\n        print(m)\n    if len(u) > 0 and verbose:\n        print(\"unexpected keys:\")\n        print(u)\n\n    if device == torch.device(\"cuda\"):\n        model.cuda()\n    elif device == torch.device(\"cpu\"):\n        model.cpu()\n        model.cond_stage_model.device = \"cpu\"\n    else:\n        raise ValueError(f\"Incorrect device name. Received: {device}\")\n    model.eval()\n    return model\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--prompt\",\n        type=str,\n        nargs=\"?\",\n        default=\"a professional photograph of an astronaut riding a triceratops\",\n        help=\"the prompt to render\"\n    )\n    parser.add_argument(\n        \"--outdir\",\n        type=str,\n        nargs=\"?\",\n        help=\"dir to write results to\",\n        default=\"outputs/txt2img-samples\"\n    )\n    parser.add_argument(\n        \"--steps\",\n        type=int,\n        default=50,\n        help=\"number of ddim sampling steps\",\n    )\n    parser.add_argument(\n        \"--plms\",\n        action='store_true',\n        help=\"use plms sampling\",\n    )\n    parser.add_argument(\n        \"--dpm\",\n        action='store_true',\n        help=\"use DPM (2) sampler\",\n    )\n    parser.add_argument(\n        \"--fixed_code\",\n        action='store_true',\n        help=\"if enabled, uses the same starting code across all samples \",\n    )\n    parser.add_argument(\n        \"--ddim_eta\",\n        type=float,\n        default=0.0,\n        help=\"ddim eta (eta=0.0 corresponds to deterministic sampling\",\n    )\n    parser.add_argument(\n        \"--n_iter\",\n        type=int,\n        default=3,\n        help=\"sample this often\",\n    )\n    parser.add_argument(\n        \"--H\",\n        type=int,\n        default=512,\n        help=\"image height, in pixel space\",\n    )\n    parser.add_argument(\n        \"--W\",\n        type=int,\n        default=512,\n        help=\"image width, in pixel space\",\n    )\n    parser.add_argument(\n        \"--C\",\n        type=int,\n        default=4,\n        help=\"latent channels\",\n    )\n    parser.add_argument(\n        \"--f\",\n        type=int,\n        default=8,\n        help=\"downsampling factor, most often 8 or 16\",\n    )\n    parser.add_argument(\n        \"--n_samples\",\n        type=int,\n        default=3,\n        help=\"how many samples to produce for each given prompt. A.k.a batch size\",\n    )\n    parser.add_argument(\n        \"--n_rows\",\n        type=int,\n        default=0,\n        help=\"rows in the grid (default: n_samples)\",\n    )\n    parser.add_argument(\n        \"--scale\",\n        type=float,\n        default=9.0,\n        help=\"unconditional guidance scale: eps = eps(x, empty) + scale * (eps(x, cond) - eps(x, empty))\",\n    )\n    parser.add_argument(\n        \"--from-file\",\n        type=str,\n        help=\"if specified, load prompts from this file, separated by newlines\",\n    )\n    parser.add_argument(\n        \"--config\",\n        type=str,\n        default=\"configs/stable-diffusion/v2-inference.yaml\",\n        help=\"path to config which constructs model\",\n    )\n    parser.add_argument(\n        \"--ckpt\",\n        type=str,\n        help=\"path to checkpoint of model\",\n    )\n    parser.add_argument(\n        \"--seed\",\n        type=int,\n        default=42,\n        help=\"the seed (for reproducible sampling)\",\n    )\n    parser.add_argument(\n        \"--precision\",\n        type=str,\n        help=\"evaluate at this precision\",\n        choices=[\"full\", \"autocast\"],\n        default=\"autocast\"\n    )\n    parser.add_argument(\n        \"--repeat\",\n        type=int,\n        default=1,\n        help=\"repeat each prompt in file this often\",\n    )\n    parser.add_argument(\n        \"--device\",\n        type=str,\n        help=\"Device on which Stable Diffusion will be run\",\n        choices=[\"cpu\", \"cuda\"],\n        default=\"cpu\"\n    )\n    parser.add_argument(\n        \"--torchscript\",\n        action='store_true',\n        help=\"Use TorchScript\",\n    )\n    parser.add_argument(\n        \"--ipex\",\n        action='store_true',\n        help=\"Use Intel\u00ae Extension for PyTorch*\",\n    )\n    parser.add_argument(\n        \"--bf16\",\n        action='store_true',\n        help=\"Use bfloat16\",\n    )\n    opt = parser.parse_args()\n    return opt\n\n\ndef put_watermark(img, wm_encoder=None):\n    if wm_encoder is not None:\n        img = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)\n        img = wm_encoder.encode(img, 'dwtDct')\n        img = Image.fromarray(img[:, :, ::-1])\n    return img\n\n\ndef main(opt):\n    seed_everything(opt.seed)\n\n    config = OmegaConf.load(f\"{opt.config}\")\n    device = torch.device(\"cuda\") if opt.device == \"cuda\" else torch.device(\"cpu\")\n    model = load_model_from_config(config, f\"{opt.ckpt}\", device)\n\n    if opt.plms:\n        sampler = PLMSSampler(model, device=device)\n    elif opt.dpm:\n        sampler = DPMSolverSampler(model, device=device)\n    else:\n        sampler = DDIMSampler(model, device=device)\n\n    os.makedirs(opt.outdir, exist_ok=True)\n    outpath = opt.outdir\n\n    print(\"Creating invisible watermark encoder (see https://github.com/ShieldMnt/invisible-watermark)...\")\n    wm = \"SDV2\"\n    wm_encoder = WatermarkEncoder()\n    wm_encoder.set_watermark('bytes', wm.encode('utf-8'))\n\n    batch_size = opt.n_samples\n    n_rows = opt.n_rows if opt.n_rows > 0 else batch_size\n    if not opt.from_file:\n        prompt = opt.prompt\n        assert prompt is not None\n        data = [batch_size * [prompt]]\n\n    else:\n        print(f\"reading prompts from {opt.from_file}\")\n        with open(opt.from_file, \"r\") as f:\n            data = f.read().splitlines()\n            data = [p for p in data for i in range(opt.repeat)]\n            data = list(chunk(data, batch_size))\n\n    sample_path = os.path.join(outpath, \"samples\")\n    os.makedirs(sample_path, exist_ok=True)\n    sample_count = 0\n    base_count = len(os.listdir(sample_path))\n    grid_count = len(os.listdir(outpath)) - 1\n\n    start_code = None\n    if opt.fixed_code:\n        start_code = torch.randn([opt.n_samples, opt.C, opt.H // opt.f, opt.W // opt.f], device=device)\n\n    if opt.torchscript or opt.ipex:\n        transformer = model.cond_stage_model.model\n        unet = model.model.diffusion_model\n        decoder = model.first_stage_model.decoder\n        additional_context = torch.cpu.amp.autocast() if opt.bf16 else nullcontext()\n        shape = [opt.C, opt.H // opt.f, opt.W // opt.f]\n\n        if opt.bf16 and not opt.torchscript and not opt.ipex:\n            raise ValueError('Bfloat16 is supported only for torchscript+ipex')\n        if opt.bf16 and unet.dtype != torch.bfloat16:\n            raise ValueError(\"Use configs/stable-diffusion/intel/ configs with bf16 enabled if \" +\n                             \"you'd like to use bfloat16 with CPU.\")\n        if unet.dtype == torch.float16 and device == torch.device(\"cpu\"):\n            raise ValueError(\"Use configs/stable-diffusion/intel/ configs for your model if you'd like to run it on CPU.\")\n\n        if opt.ipex:\n            import intel_extension_for_pytorch as ipex\n            bf16_dtype = torch.bfloat16 if opt.bf16 else None\n            transformer = transformer.to(memory_format=torch.channels_last)\n            transformer = ipex.optimize(transformer, level=\"O1\", inplace=True)\n\n            unet = unet.to(memory_format=torch.channels_last)\n            unet = ipex.optimize(unet, level=\"O1\", auto_kernel_selection=True, inplace=True, dtype=bf16_dtype)\n\n            decoder = decoder.to(memory_format=torch.channels_last)\n            decoder = ipex.optimize(decoder, level=\"O1\", auto_kernel_selection=True, inplace=True, dtype=bf16_dtype)\n\n        if opt.torchscript:\n            with torch.no_grad(), additional_context:\n                # get UNET scripted\n                if unet.use_checkpoint:\n                    raise ValueError(\"Gradient checkpoint won't work with tracing. \" +\n                    \"Use configs/stable-diffusion/intel/ configs for your model or disable checkpoint in your config.\")\n\n                img_in = torch.ones(2, 4, 96, 96, dtype=torch.float32)\n                t_in = torch.ones(2, dtype=torch.int64)\n                context = torch.ones(2, 77, 1024, dtype=torch.float32)\n                scripted_unet = torch.jit.trace(unet, (img_in, t_in, context))\n                scripted_unet = torch.jit.optimize_for_inference(scripted_unet)\n                print(type(scripted_unet))\n                model.model.scripted_diffusion_model = scripted_unet\n\n                # get Decoder for first stage model scripted\n                samples_ddim = torch.ones(1, 4, 96, 96, dtype=torch.float32)\n                scripted_decoder = torch.jit.trace(decoder, (samples_ddim))\n                scripted_decoder = torch.jit.optimize_for_inference(scripted_decoder)\n                print(type(scripted_decoder))\n                model.first_stage_model.decoder = scripted_decoder\n\n        prompts = data[0]\n        print(\"Running a forward pass to initialize optimizations\")\n        uc = None\n        if opt.scale != 1.0:\n            uc = model.get_learned_conditioning(batch_size * [\"\"])\n        if isinstance(prompts, tuple):\n            prompts = list(prompts)\n\n        with torch.no_grad(), additional_context:\n            for _ in range(3):\n                c = model.get_learned_conditioning(prompts)\n            samples_ddim, _ = sampler.sample(S=5,\n                                             conditioning=c,\n                                             batch_size=batch_size,\n                                             shape=shape,\n                                             verbose=False,\n                                             unconditional_guidance_scale=opt.scale,\n                                             unconditional_conditioning=uc,\n                                             eta=opt.ddim_eta,\n                                             x_T=start_code)\n            print(\"Running a forward pass for decoder\")\n            for _ in range(3):\n                x_samples_ddim = model.decode_first_stage(samples_ddim)\n\n    precision_scope = autocast if opt.precision==\"autocast\" or opt.bf16 else nullcontext\n    with torch.no_grad(), \\\n        precision_scope(opt.device), \\\n        model.ema_scope():\n            all_samples = list()\n            for n in trange(opt.n_iter, desc=\"Sampling\"):\n                for prompts in tqdm(data, desc=\"data\"):\n                    uc = None\n                    if opt.scale != 1.0:\n                        uc = model.get_learned_conditioning(batch_size * [\"\"])\n                    if isinstance(prompts, tuple):\n                        prompts = list(prompts)\n                    c = model.get_learned_conditioning(prompts)\n                    shape = [opt.C, opt.H // opt.f, opt.W // opt.f]\n                    samples, _ = sampler.sample(S=opt.steps,\n                                                     conditioning=c,\n                                                     batch_size=opt.n_samples,\n                                                     shape=shape,\n                                                     verbose=False,\n                                                     unconditional_guidance_scale=opt.scale,\n                                                     unconditional_conditioning=uc,\n                                                     eta=opt.ddim_eta,\n                                                     x_T=start_code)\n\n                    x_samples = model.decode_first_stage(samples)\n                    x_samples = torch.clamp((x_samples + 1.0) / 2.0, min=0.0, max=1.0)\n\n                    for x_sample in x_samples:\n                        x_sample = 255. * rearrange(x_sample.cpu().numpy(), 'c h w -> h w c')\n                        img = Image.fromarray(x_sample.astype(np.uint8))\n                        img = put_watermark(img, wm_encoder)\n                        img.save(os.path.join(sample_path, f\"{base_count:05}.png\"))\n                        base_count += 1\n                        sample_count += 1\n\n                    all_samples.append(x_samples)\n\n            # additionally, save as grid\n            grid = torch.stack(all_samples, 0)\n            grid = rearrange(grid, 'n b c h w -> (n b) c h w')\n            grid = make_grid(grid, nrow=n_rows)\n\n            # to image\n            grid = 255. * rearrange(grid, 'c h w -> h w c').cpu().numpy()\n            grid = Image.fromarray(grid.astype(np.uint8))\n            grid = put_watermark(grid, wm_encoder)\n            grid.save(os.path.join(outpath, f'grid-{grid_count:04}.png'))\n            grid_count += 1\n\n    print(f\"Your samples are ready and waiting for you here: \\n{outpath} \\n\"\n          f\" \\nEnjoy.\")\n\n\nif __name__ == \"__main__\":\n    opt = parse_args()\n    main(opt)\n", "scripts/streamlit/superresolution.py": "import sys\nimport torch\nimport numpy as np\nimport streamlit as st\nfrom PIL import Image\nfrom omegaconf import OmegaConf\nfrom einops import repeat, rearrange\nfrom pytorch_lightning import seed_everything\nfrom imwatermark import WatermarkEncoder\n\nfrom scripts.txt2img import put_watermark\nfrom ldm.models.diffusion.ddim import DDIMSampler\nfrom ldm.models.diffusion.ddpm import LatentUpscaleDiffusion, LatentUpscaleFinetuneDiffusion\nfrom ldm.util import exists, instantiate_from_config\n\n\ntorch.set_grad_enabled(False)\n\n\n@st.cache(allow_output_mutation=True)\ndef initialize_model(config, ckpt):\n    config = OmegaConf.load(config)\n    model = instantiate_from_config(config.model)\n    model.load_state_dict(torch.load(ckpt)[\"state_dict\"], strict=False)\n\n    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n    model = model.to(device)\n    sampler = DDIMSampler(model)\n    return sampler\n\n\ndef make_batch_sd(\n        image,\n        txt,\n        device,\n        num_samples=1,\n):\n    image = np.array(image.convert(\"RGB\"))\n    image = torch.from_numpy(image).to(dtype=torch.float32) / 127.5 - 1.0\n    batch = {\n        \"lr\": rearrange(image, 'h w c -> 1 c h w'),\n        \"txt\": num_samples * [txt],\n    }\n    batch[\"lr\"] = repeat(batch[\"lr\"].to(device=device), \"1 ... -> n ...\", n=num_samples)\n    return batch\n\n\ndef make_noise_augmentation(model, batch, noise_level=None):\n    x_low = batch[model.low_scale_key]\n    x_low = x_low.to(memory_format=torch.contiguous_format).float()\n    x_aug, noise_level = model.low_scale_model(x_low, noise_level)\n    return x_aug, noise_level\n\n\ndef paint(sampler, image, prompt, seed, scale, h, w, steps, num_samples=1, callback=None, eta=0., noise_level=None):\n    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n    model = sampler.model\n    seed_everything(seed)\n    prng = np.random.RandomState(seed)\n    start_code = prng.randn(num_samples, model.channels, h , w)\n    start_code = torch.from_numpy(start_code).to(device=device, dtype=torch.float32)\n\n    print(\"Creating invisible watermark encoder (see https://github.com/ShieldMnt/invisible-watermark)...\")\n    wm = \"SDV2\"\n    wm_encoder = WatermarkEncoder()\n    wm_encoder.set_watermark('bytes', wm.encode('utf-8'))\n    with torch.no_grad(),\\\n            torch.autocast(\"cuda\"):\n        batch = make_batch_sd(image, txt=prompt, device=device, num_samples=num_samples)\n        c = model.cond_stage_model.encode(batch[\"txt\"])\n        c_cat = list()\n        if isinstance(model, LatentUpscaleFinetuneDiffusion):\n            for ck in model.concat_keys:\n                cc = batch[ck]\n                if exists(model.reshuffle_patch_size):\n                    assert isinstance(model.reshuffle_patch_size, int)\n                    cc = rearrange(cc, 'b c (p1 h) (p2 w) -> b (p1 p2 c) h w',\n                                   p1=model.reshuffle_patch_size, p2=model.reshuffle_patch_size)\n                c_cat.append(cc)\n            c_cat = torch.cat(c_cat, dim=1)\n            # cond\n            cond = {\"c_concat\": [c_cat], \"c_crossattn\": [c]}\n            # uncond cond\n            uc_cross = model.get_unconditional_conditioning(num_samples, \"\")\n            uc_full = {\"c_concat\": [c_cat], \"c_crossattn\": [uc_cross]}\n        elif isinstance(model, LatentUpscaleDiffusion):\n            x_augment, noise_level = make_noise_augmentation(model, batch, noise_level)\n            cond = {\"c_concat\": [x_augment], \"c_crossattn\": [c], \"c_adm\": noise_level}\n            # uncond cond\n            uc_cross = model.get_unconditional_conditioning(num_samples, \"\")\n            uc_full = {\"c_concat\": [x_augment], \"c_crossattn\": [uc_cross], \"c_adm\": noise_level}\n        else:\n            raise NotImplementedError()\n\n        shape = [model.channels, h, w]\n        samples, intermediates = sampler.sample(\n            steps,\n            num_samples,\n            shape,\n            cond,\n            verbose=False,\n            eta=eta,\n            unconditional_guidance_scale=scale,\n            unconditional_conditioning=uc_full,\n            x_T=start_code,\n            callback=callback\n        )\n    with torch.no_grad():\n        x_samples_ddim = model.decode_first_stage(samples)\n    result = torch.clamp((x_samples_ddim + 1.0) / 2.0, min=0.0, max=1.0)\n    result = result.cpu().numpy().transpose(0, 2, 3, 1) * 255\n    st.text(f\"upscaled image shape: {result.shape}\")\n    return [put_watermark(Image.fromarray(img.astype(np.uint8)), wm_encoder) for img in result]\n\n\ndef run():\n    st.title(\"Stable Diffusion Upscaling\")\n    # run via streamlit run scripts/demo/depth2img.py <path-tp-config> <path-to-ckpt>\n    sampler = initialize_model(sys.argv[1], sys.argv[2])\n\n    image = st.file_uploader(\"Image\", [\"jpg\", \"png\"])\n    if image:\n        image = Image.open(image)\n        w, h = image.size\n        st.text(f\"loaded input image of size ({w}, {h})\")\n        width, height = map(lambda x: x - x % 64, (w, h))  # resize to integer multiple of 64\n        image = image.resize((width, height))\n        st.text(f\"resized input image to size ({width}, {height} (w, h))\")\n        st.image(image)\n\n        st.write(f\"\\n Tip: Add a description of the object that should be upscaled, e.g.: 'a professional photograph of a cat'\")\n        prompt = st.text_input(\"Prompt\", \"a high quality professional photograph\")\n\n        seed = st.number_input(\"Seed\", min_value=0, max_value=1000000, value=0)\n        num_samples = st.number_input(\"Number of Samples\", min_value=1, max_value=64, value=1)\n        scale = st.slider(\"Scale\", min_value=0.1, max_value=30.0, value=9.0, step=0.1)\n        steps = st.slider(\"DDIM Steps\", min_value=2, max_value=250, value=50, step=1)\n        eta = st.sidebar.number_input(\"eta (DDIM)\", value=0., min_value=0., max_value=1.)\n\n        noise_level = None\n        if isinstance(sampler.model, LatentUpscaleDiffusion):\n            # TODO: make this work for all models\n            noise_level = st.sidebar.number_input(\"Noise Augmentation\", min_value=0, max_value=350, value=20)\n            noise_level = torch.Tensor(num_samples * [noise_level]).to(sampler.model.device).long()\n\n        t_progress = st.progress(0)\n        def t_callback(t):\n            t_progress.progress(min((t + 1) / steps, 1.))\n\n        sampler.make_schedule(steps, ddim_eta=eta, verbose=True)\n        if st.button(\"Sample\"):\n            result = paint(\n                sampler=sampler,\n                image=image,\n                prompt=prompt,\n                seed=seed,\n                scale=scale,\n                h=height, w=width, steps=steps,\n                num_samples=num_samples,\n                callback=t_callback,\n                noise_level=noise_level,\n                eta=eta\n            )\n            st.write(\"Result\")\n            for image in result:\n                st.image(image, output_format='PNG')\n\n\nif __name__ == \"__main__\":\n    run()\n", "scripts/streamlit/inpainting.py": "import sys\nimport cv2\nimport torch\nimport numpy as np\nimport streamlit as st\nfrom PIL import Image\nfrom omegaconf import OmegaConf\nfrom einops import repeat\nfrom streamlit_drawable_canvas import st_canvas\nfrom imwatermark import WatermarkEncoder\n\nfrom ldm.models.diffusion.ddim import DDIMSampler\nfrom ldm.util import instantiate_from_config\n\n\ntorch.set_grad_enabled(False)\n\n\ndef put_watermark(img, wm_encoder=None):\n    if wm_encoder is not None:\n        img = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)\n        img = wm_encoder.encode(img, 'dwtDct')\n        img = Image.fromarray(img[:, :, ::-1])\n    return img\n\n\n@st.cache(allow_output_mutation=True)\ndef initialize_model(config, ckpt):\n    config = OmegaConf.load(config)\n    model = instantiate_from_config(config.model)\n\n    model.load_state_dict(torch.load(ckpt)[\"state_dict\"], strict=False)\n\n    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n    model = model.to(device)\n    sampler = DDIMSampler(model)\n\n    return sampler\n\n\ndef make_batch_sd(\n        image,\n        mask,\n        txt,\n        device,\n        num_samples=1):\n    image = np.array(image.convert(\"RGB\"))\n    image = image[None].transpose(0, 3, 1, 2)\n    image = torch.from_numpy(image).to(dtype=torch.float32) / 127.5 - 1.0\n\n    mask = np.array(mask.convert(\"L\"))\n    mask = mask.astype(np.float32) / 255.0\n    mask = mask[None, None]\n    mask[mask < 0.5] = 0\n    mask[mask >= 0.5] = 1\n    mask = torch.from_numpy(mask)\n\n    masked_image = image * (mask < 0.5)\n\n    batch = {\n        \"image\": repeat(image.to(device=device), \"1 ... -> n ...\", n=num_samples),\n        \"txt\": num_samples * [txt],\n        \"mask\": repeat(mask.to(device=device), \"1 ... -> n ...\", n=num_samples),\n        \"masked_image\": repeat(masked_image.to(device=device), \"1 ... -> n ...\", n=num_samples),\n    }\n    return batch\n\n\ndef inpaint(sampler, image, mask, prompt, seed, scale, ddim_steps, num_samples=1, w=512, h=512, eta=1.):\n    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n    model = sampler.model\n\n    print(\"Creating invisible watermark encoder (see https://github.com/ShieldMnt/invisible-watermark)...\")\n    wm = \"SDV2\"\n    wm_encoder = WatermarkEncoder()\n    wm_encoder.set_watermark('bytes', wm.encode('utf-8'))\n\n    prng = np.random.RandomState(seed)\n    start_code = prng.randn(num_samples, 4, h // 8, w // 8)\n    start_code = torch.from_numpy(start_code).to(device=device, dtype=torch.float32)\n\n    with torch.no_grad(), \\\n            torch.autocast(\"cuda\"):\n            batch = make_batch_sd(image, mask, txt=prompt, device=device, num_samples=num_samples)\n\n            c = model.cond_stage_model.encode(batch[\"txt\"])\n\n            c_cat = list()\n            for ck in model.concat_keys:\n                cc = batch[ck].float()\n                if ck != model.masked_image_key:\n                    bchw = [num_samples, 4, h // 8, w // 8]\n                    cc = torch.nn.functional.interpolate(cc, size=bchw[-2:])\n                else:\n                    cc = model.get_first_stage_encoding(model.encode_first_stage(cc))\n                c_cat.append(cc)\n            c_cat = torch.cat(c_cat, dim=1)\n\n            # cond\n            cond = {\"c_concat\": [c_cat], \"c_crossattn\": [c]}\n\n            # uncond cond\n            uc_cross = model.get_unconditional_conditioning(num_samples, \"\")\n            uc_full = {\"c_concat\": [c_cat], \"c_crossattn\": [uc_cross]}\n\n            shape = [model.channels, h // 8, w // 8]\n            samples_cfg, intermediates = sampler.sample(\n                ddim_steps,\n                num_samples,\n                shape,\n                cond,\n                verbose=False,\n                eta=eta,\n                unconditional_guidance_scale=scale,\n                unconditional_conditioning=uc_full,\n                x_T=start_code,\n            )\n            x_samples_ddim = model.decode_first_stage(samples_cfg)\n\n            result = torch.clamp((x_samples_ddim + 1.0) / 2.0,\n                                 min=0.0, max=1.0)\n\n            result = result.cpu().numpy().transpose(0, 2, 3, 1) * 255\n    return [put_watermark(Image.fromarray(img.astype(np.uint8)), wm_encoder) for img in result]\n\n\ndef run():\n    st.title(\"Stable Diffusion Inpainting\")\n\n    sampler = initialize_model(sys.argv[1], sys.argv[2])\n\n    image = st.file_uploader(\"Image\", [\"jpg\", \"png\"])\n    if image:\n        image = Image.open(image)\n        w, h = image.size\n        print(f\"loaded input image of size ({w}, {h})\")\n        width, height = map(lambda x: x - x % 64, (w, h))  # resize to integer multiple of 32\n        image = image.resize((width, height))\n\n        prompt = st.text_input(\"Prompt\")\n\n        seed = st.number_input(\"Seed\", min_value=0, max_value=1000000, value=0)\n        num_samples = st.number_input(\"Number of Samples\", min_value=1, max_value=64, value=1)\n        scale = st.slider(\"Scale\", min_value=0.1, max_value=30.0, value=10., step=0.1)\n        ddim_steps = st.slider(\"DDIM Steps\", min_value=0, max_value=50, value=50, step=1)\n        eta = st.sidebar.number_input(\"eta (DDIM)\", value=0., min_value=0., max_value=1.)\n\n        fill_color = \"rgba(255, 255, 255, 0.0)\"\n        stroke_width = st.number_input(\"Brush Size\",\n                                       value=64,\n                                       min_value=1,\n                                       max_value=100)\n        stroke_color = \"rgba(255, 255, 255, 1.0)\"\n        bg_color = \"rgba(0, 0, 0, 1.0)\"\n        drawing_mode = \"freedraw\"\n\n        st.write(\"Canvas\")\n        st.caption(\n            \"Draw a mask to inpaint, then click the 'Send to Streamlit' button (bottom left, with an arrow on it).\")\n        canvas_result = st_canvas(\n            fill_color=fill_color,\n            stroke_width=stroke_width,\n            stroke_color=stroke_color,\n            background_color=bg_color,\n            background_image=image,\n            update_streamlit=False,\n            height=height,\n            width=width,\n            drawing_mode=drawing_mode,\n            key=\"canvas\",\n        )\n        if canvas_result:\n            mask = canvas_result.image_data\n            mask = mask[:, :, -1] > 0\n            if mask.sum() > 0:\n                mask = Image.fromarray(mask)\n\n                result = inpaint(\n                    sampler=sampler,\n                    image=image,\n                    mask=mask,\n                    prompt=prompt,\n                    seed=seed,\n                    scale=scale,\n                    ddim_steps=ddim_steps,\n                    num_samples=num_samples,\n                    h=height, w=width, eta=eta\n                )\n                st.write(\"Inpainted\")\n                for image in result:\n                    st.image(image, output_format='PNG')\n\n\nif __name__ == \"__main__\":\n    run()", "scripts/streamlit/stableunclip.py": "import importlib\nimport streamlit as st\nimport torch\nimport cv2\nimport numpy as np\nimport PIL\nfrom omegaconf import OmegaConf\nfrom PIL import Image\nfrom tqdm import trange\nimport io, os\nfrom torch import autocast\nfrom einops import rearrange, repeat\nfrom torchvision.utils import make_grid\nfrom pytorch_lightning import seed_everything\nfrom contextlib import nullcontext\n\nfrom ldm.models.diffusion.ddim import DDIMSampler\nfrom ldm.models.diffusion.plms import PLMSSampler\nfrom ldm.models.diffusion.dpm_solver import DPMSolverSampler\n\ntorch.set_grad_enabled(False)\n\nPROMPTS_ROOT = \"scripts/prompts/\"\nSAVE_PATH = \"outputs/demo/stable-unclip/\"\n\nVERSION2SPECS = {\n    \"Stable unCLIP-L\": {\"H\": 768, \"W\": 768, \"C\": 4, \"f\": 8},\n    \"Stable unOpenCLIP-H\": {\"H\": 768, \"W\": 768, \"C\": 4, \"f\": 8},\n    \"Full Karlo\": {}\n}\n\n\ndef get_obj_from_str(string, reload=False):\n    module, cls = string.rsplit(\".\", 1)\n    importlib.invalidate_caches()\n    if reload:\n        module_imp = importlib.import_module(module)\n        importlib.reload(module_imp)\n    return getattr(importlib.import_module(module, package=None), cls)\n\n\ndef instantiate_from_config(config):\n    if not \"target\" in config:\n        raise KeyError(\"Expected key `target` to instantiate.\")\n    return get_obj_from_str(config[\"target\"])(**config.get(\"params\", dict()))\n\n\ndef get_interactive_image(key=None):\n    image = st.file_uploader(\"Input\", type=[\"jpg\", \"JPEG\", \"png\"], key=key)\n    if image is not None:\n        image = Image.open(image)\n        if not image.mode == \"RGB\":\n            image = image.convert(\"RGB\")\n        return image\n\n\ndef load_img(display=True, key=None):\n    image = get_interactive_image(key=key)\n    if display:\n        st.image(image)\n    w, h = image.size\n    print(f\"loaded input image of size ({w}, {h})\")\n    w, h = map(lambda x: x - x % 64, (w, h))\n    image = image.resize((w, h), resample=PIL.Image.LANCZOS)\n    image = np.array(image).astype(np.float32) / 255.0\n    image = image[None].transpose(0, 3, 1, 2)\n    image = torch.from_numpy(image)\n    return 2. * image - 1.\n\n\ndef get_init_img(batch_size=1, key=None):\n    init_image = load_img(key=key).cuda()\n    init_image = repeat(init_image, '1 ... -> b ...', b=batch_size)\n    return init_image\n\n\ndef sample(\n        model,\n        prompt,\n        n_runs=3,\n        n_samples=2,\n        H=512,\n        W=512,\n        C=4,\n        f=8,\n        scale=10.0,\n        ddim_steps=50,\n        ddim_eta=0.0,\n        callback=None,\n        skip_single_save=False,\n        save_grid=True,\n        ucg_schedule=None,\n        negative_prompt=\"\",\n        adm_cond=None,\n        adm_uc=None,\n        use_full_precision=False,\n        only_adm_cond=False\n):\n    batch_size = n_samples\n    precision_scope = autocast if not use_full_precision else nullcontext\n    # decoderscope = autocast if not use_full_precision else nullcontext\n    if use_full_precision: st.warning(f\"Running {model.__class__.__name__} at full precision.\")\n    if isinstance(prompt, str):\n        prompt = [prompt]\n    prompts = batch_size * prompt\n\n    outputs = st.empty()\n\n    with precision_scope(\"cuda\"):\n        with model.ema_scope():\n            all_samples = list()\n            for n in trange(n_runs, desc=\"Sampling\"):\n                shape = [C, H // f, W // f]\n                if not only_adm_cond:\n                    uc = None\n                    if scale != 1.0:\n                        uc = model.get_learned_conditioning(batch_size * [negative_prompt])\n                    if isinstance(prompts, tuple):\n                        prompts = list(prompts)\n                    c = model.get_learned_conditioning(prompts)\n\n                if adm_cond is not None:\n                    if adm_cond.shape[0] == 1:\n                        adm_cond = repeat(adm_cond, '1 ... -> b ...', b=batch_size)\n                    if adm_uc is None:\n                        st.warning(\"Not guiding via c_adm\")\n                        adm_uc = adm_cond\n                    else:\n                        if adm_uc.shape[0] == 1:\n                            adm_uc = repeat(adm_uc, '1 ... -> b ...', b=batch_size)\n                    if not only_adm_cond:\n                        c = {\"c_crossattn\": [c], \"c_adm\": adm_cond}\n                        uc = {\"c_crossattn\": [uc], \"c_adm\": adm_uc}\n                    else:\n                        c = adm_cond\n                        uc = adm_uc\n                samples_ddim, _ = sampler.sample(S=ddim_steps,\n                                                 conditioning=c,\n                                                 batch_size=batch_size,\n                                                 shape=shape,\n                                                 verbose=False,\n                                                 unconditional_guidance_scale=scale,\n                                                 unconditional_conditioning=uc,\n                                                 eta=ddim_eta,\n                                                 x_T=None,\n                                                 callback=callback,\n                                                 ucg_schedule=ucg_schedule\n                                                 )\n                x_samples = model.decode_first_stage(samples_ddim)\n                x_samples = torch.clamp((x_samples + 1.0) / 2.0, min=0.0, max=1.0)\n\n                if not skip_single_save:\n                    base_count = len(os.listdir(os.path.join(SAVE_PATH, \"samples\")))\n                    for x_sample in x_samples:\n                        x_sample = 255. * rearrange(x_sample.cpu().numpy(), 'c h w -> h w c')\n                        Image.fromarray(x_sample.astype(np.uint8)).save(\n                            os.path.join(SAVE_PATH, \"samples\", f\"{base_count:09}.png\"))\n                        base_count += 1\n\n                all_samples.append(x_samples)\n\n                # get grid of all samples\n                grid = torch.stack(all_samples, 0)\n                grid = rearrange(grid, 'n b c h w -> (n h) (b w) c')\n                outputs.image(grid.cpu().numpy())\n\n            # additionally, save grid\n            grid = Image.fromarray((255. * grid.cpu().numpy()).astype(np.uint8))\n            if save_grid:\n                grid_count = len(os.listdir(SAVE_PATH)) - 1\n                grid.save(os.path.join(SAVE_PATH, f'grid-{grid_count:06}.png'))\n\n    return x_samples\n\n\ndef make_oscillating_guidance_schedule(num_steps, max_weight=15., min_weight=1.):\n    schedule = list()\n    for i in range(num_steps):\n        if float(i / num_steps) < 0.1:\n            schedule.append(max_weight)\n        elif i % 2 == 0:\n            schedule.append(min_weight)\n        else:\n            schedule.append(max_weight)\n    print(f\"OSCILLATING GUIDANCE SCHEDULE: \\n {schedule}\")\n    return schedule\n\n\ndef torch2np(x):\n    x = ((x + 1.0) * 127.5).clamp(0, 255).to(dtype=torch.uint8)\n    x = x.permute(0, 2, 3, 1).detach().cpu().numpy()\n    return x\n\n\n@st.cache(allow_output_mutation=True, suppress_st_warning=True)\ndef init(version=\"Stable unCLIP-L\", load_karlo_prior=False):\n    state = dict()\n    if not \"model\" in state:\n        if version == \"Stable unCLIP-L\":\n            config = \"configs/stable-diffusion/v2-1-stable-unclip-l-inference.yaml\"\n            ckpt = \"checkpoints/sd21-unclip-l.ckpt\"\n\n        elif version == \"Stable unOpenCLIP-H\":\n            config = \"configs/stable-diffusion/v2-1-stable-unclip-h-inference.yaml\"\n            ckpt = \"checkpoints/sd21-unclip-h.ckpt\"\n\n        elif version == \"Full Karlo\":\n            from ldm.modules.karlo.kakao.sampler import T2ISampler\n            st.info(\"Loading full KARLO..\")\n            karlo = T2ISampler.from_pretrained(\n                root_dir=\"checkpoints/karlo_models\",\n                clip_model_path=\"ViT-L-14.pt\",\n                clip_stat_path=\"ViT-L-14_stats.th\",\n                sampling_type=\"default\",\n            )\n            state[\"karlo_prior\"] = karlo\n            state[\"msg\"] = \"loaded full Karlo\"\n            return state\n        else:\n            raise ValueError(f\"version {version} unknown!\")\n\n        config = OmegaConf.load(config)\n        model, msg = load_model_from_config(config, ckpt, vae_sd=None)\n        state[\"msg\"] = msg\n\n        if load_karlo_prior:\n            from ldm.modules.karlo.kakao.sampler import PriorSampler\n            st.info(\"Loading KARLO CLIP prior...\")\n            karlo_prior = PriorSampler.from_pretrained(\n                root_dir=\"checkpoints/karlo_models\",\n                clip_model_path=\"ViT-L-14.pt\",\n                clip_stat_path=\"ViT-L-14_stats.th\",\n                sampling_type=\"default\",\n            )\n            state[\"karlo_prior\"] = karlo_prior\n        state[\"model\"] = model\n        state[\"ckpt\"] = ckpt\n        state[\"config\"] = config\n    return state\n\n\ndef load_model_from_config(config, ckpt, verbose=False, vae_sd=None):\n    print(f\"Loading model from {ckpt}\")\n    pl_sd = torch.load(ckpt, map_location=\"cpu\")\n    msg = None\n    if \"global_step\" in pl_sd:\n        msg = f\"This is global step {pl_sd['global_step']}. \"\n    if \"model_ema.num_updates\" in pl_sd[\"state_dict\"]:\n        msg += f\"And we got {pl_sd['state_dict']['model_ema.num_updates']} EMA updates.\"\n    global_step = pl_sd.get(\"global_step\", \"?\")\n    sd = pl_sd[\"state_dict\"]\n    if vae_sd is not None:\n        for k in sd.keys():\n            if \"first_stage\" in k:\n                sd[k] = vae_sd[k[len(\"first_stage_model.\"):]]\n\n    model = instantiate_from_config(config.model)\n    m, u = model.load_state_dict(sd, strict=False)\n    if len(m) > 0 and verbose:\n        print(\"missing keys:\")\n        print(m)\n    if len(u) > 0 and verbose:\n        print(\"unexpected keys:\")\n        print(u)\n\n    model.cuda()\n    model.eval()\n    print(f\"Loaded global step {global_step}\")\n    return model, msg\n\n\nif __name__ == \"__main__\":\n    st.title(\"Stable unCLIP\")\n    mode = \"txt2img\"\n    version = st.selectbox(\"Model Version\", list(VERSION2SPECS.keys()), 0)\n    use_karlo_prior = version in [\"Stable unCLIP-L\"] and st.checkbox(\"Use KARLO prior\", False)\n    state = init(version=version, load_karlo_prior=use_karlo_prior)\n    prompt = st.text_input(\"Prompt\", \"a professional photograph\")\n    negative_prompt = st.text_input(\"Negative Prompt\", \"\")\n    scale = st.number_input(\"cfg-scale\", value=10., min_value=-100., max_value=100.)\n    number_rows = st.number_input(\"num rows\", value=2, min_value=1, max_value=10)\n    number_cols = st.number_input(\"num cols\", value=2, min_value=1, max_value=10)\n    steps = st.sidebar.number_input(\"steps\", value=20, min_value=1, max_value=1000)\n    eta = st.sidebar.number_input(\"eta (DDIM)\", value=0., min_value=0., max_value=1.)\n    force_full_precision = st.sidebar.checkbox(\"Force FP32\", False)  # TODO: check if/where things break.\n    if version != \"Full Karlo\":\n        H = st.sidebar.number_input(\"H\", value=VERSION2SPECS[version][\"H\"], min_value=64, max_value=2048)\n        W = st.sidebar.number_input(\"W\", value=VERSION2SPECS[version][\"W\"], min_value=64, max_value=2048)\n        C = VERSION2SPECS[version][\"C\"]\n        f = VERSION2SPECS[version][\"f\"]\n\n    SAVE_PATH = os.path.join(SAVE_PATH, version)\n    os.makedirs(os.path.join(SAVE_PATH, \"samples\"), exist_ok=True)\n\n    seed = st.sidebar.number_input(\"seed\", value=42, min_value=0, max_value=int(1e9))\n    seed_everything(seed)\n\n    ucg_schedule = None\n    sampler = st.sidebar.selectbox(\"Sampler\", [\"DDIM\", \"DPM\"], 0)\n    if version == \"Full Karlo\":\n        pass\n    else:\n        if sampler == \"DPM\":\n            sampler = DPMSolverSampler(state[\"model\"])\n        elif sampler == \"DDIM\":\n            sampler = DDIMSampler(state[\"model\"])\n        else:\n            raise ValueError(f\"unknown sampler {sampler}!\")\n\n    adm_cond, adm_uc = None, None\n    if use_karlo_prior:\n        # uses the prior\n        karlo_sampler = state[\"karlo_prior\"]\n        noise_level = None\n        if state[\"model\"].noise_augmentor is not None:\n            noise_level = st.number_input(\"Noise Augmentation for CLIP embeddings\", min_value=0,\n                                          max_value=state[\"model\"].noise_augmentor.max_noise_level - 1, value=0)\n        with torch.no_grad():\n            karlo_prediction = iter(\n                karlo_sampler(\n                    prompt=prompt,\n                    bsz=number_cols,\n                    progressive_mode=\"final\",\n                )\n            ).__next__()\n            adm_cond = karlo_prediction\n            if noise_level is not None:\n                c_adm, noise_level_emb = state[\"model\"].noise_augmentor(adm_cond, noise_level=repeat(\n                    torch.tensor([noise_level]).to(state[\"model\"].device), '1 -> b', b=number_cols))\n                adm_cond = torch.cat((c_adm, noise_level_emb), 1)\n            adm_uc = torch.zeros_like(adm_cond)\n    elif version == \"Full Karlo\":\n        pass\n    else:\n        num_inputs = st.number_input(\"Number of Input Images\", 1)\n\n\n        def make_conditionings_from_input(num=1, key=None):\n            init_img = get_init_img(batch_size=number_cols, key=key)\n            with torch.no_grad():\n                adm_cond = state[\"model\"].embedder(init_img)\n                weight = st.slider(f\"Weight for Input {num}\", min_value=-10., max_value=10., value=1.)\n                if state[\"model\"].noise_augmentor is not None:\n                    noise_level = st.number_input(f\"Noise Augmentation for CLIP embedding of input #{num}\", min_value=0,\n                                                  max_value=state[\"model\"].noise_augmentor.max_noise_level - 1,\n                                                  value=0, )\n                    c_adm, noise_level_emb = state[\"model\"].noise_augmentor(adm_cond, noise_level=repeat(\n                        torch.tensor([noise_level]).to(state[\"model\"].device), '1 -> b', b=number_cols))\n                    adm_cond = torch.cat((c_adm, noise_level_emb), 1) * weight\n                adm_uc = torch.zeros_like(adm_cond)\n            return adm_cond, adm_uc, weight\n\n\n        adm_inputs = list()\n        weights = list()\n        for n in range(num_inputs):\n            adm_cond, adm_uc, w = make_conditionings_from_input(num=n + 1, key=n)\n            weights.append(w)\n            adm_inputs.append(adm_cond)\n        adm_cond = torch.stack(adm_inputs).sum(0) / sum(weights)\n        if num_inputs > 1:\n            if st.checkbox(\"Apply Noise to Embedding Mix\", True):\n                noise_level = st.number_input(f\"Noise Augmentation for averaged CLIP embeddings\", min_value=0,\n                                              max_value=state[\"model\"].noise_augmentor.max_noise_level - 1, value=50, )\n                c_adm, noise_level_emb = state[\"model\"].noise_augmentor(\n                    adm_cond[:, :state[\"model\"].noise_augmentor.time_embed.dim],\n                    noise_level=repeat(\n                        torch.tensor([noise_level]).to(state[\"model\"].device), '1 -> b', b=number_cols))\n                adm_cond = torch.cat((c_adm, noise_level_emb), 1)\n\n    if st.button(\"Sample\"):\n        print(\"running prompt:\", prompt)\n        st.text(\"Sampling\")\n        t_progress = st.progress(0)\n        result = st.empty()\n\n\n        def t_callback(t):\n            t_progress.progress(min((t + 1) / steps, 1.))\n\n\n        if version == \"Full Karlo\":\n            outputs = st.empty()\n            karlo_sampler = state[\"karlo_prior\"]\n            all_samples = list()\n            with torch.no_grad():\n                for _ in range(number_rows):\n                    karlo_prediction = iter(\n                        karlo_sampler(\n                            prompt=prompt,\n                            bsz=number_cols,\n                            progressive_mode=\"final\",\n                        )\n                    ).__next__()\n                    all_samples.append(karlo_prediction)\n            grid = torch.stack(all_samples, 0)\n            grid = rearrange(grid, 'n b c h w -> (n h) (b w) c')\n            outputs.image(grid.cpu().numpy())\n\n        else:\n            samples = sample(\n                state[\"model\"],\n                prompt,\n                n_runs=number_rows,\n                n_samples=number_cols,\n                H=H, W=W, C=C, f=f,\n                scale=scale,\n                ddim_steps=steps,\n                ddim_eta=eta,\n                callback=t_callback,\n                ucg_schedule=ucg_schedule,\n                negative_prompt=negative_prompt,\n                adm_cond=adm_cond, adm_uc=adm_uc,\n                use_full_precision=force_full_precision,\n                only_adm_cond=False\n            )\n", "scripts/streamlit/depth2img.py": "import sys\nimport torch\nimport numpy as np\nimport streamlit as st\nfrom PIL import Image\nfrom omegaconf import OmegaConf\nfrom einops import repeat, rearrange\nfrom pytorch_lightning import seed_everything\nfrom imwatermark import WatermarkEncoder\n\nfrom scripts.txt2img import put_watermark\nfrom ldm.util import instantiate_from_config\nfrom ldm.models.diffusion.ddim import DDIMSampler\nfrom ldm.data.util import AddMiDaS\n\ntorch.set_grad_enabled(False)\n\n\n@st.cache(allow_output_mutation=True)\ndef initialize_model(config, ckpt):\n    config = OmegaConf.load(config)\n    model = instantiate_from_config(config.model)\n    model.load_state_dict(torch.load(ckpt)[\"state_dict\"], strict=False)\n\n    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n    model = model.to(device)\n    sampler = DDIMSampler(model)\n    return sampler\n\n\ndef make_batch_sd(\n        image,\n        txt,\n        device,\n        num_samples=1,\n        model_type=\"dpt_hybrid\"\n):\n    image = np.array(image.convert(\"RGB\"))\n    image = torch.from_numpy(image).to(dtype=torch.float32) / 127.5 - 1.0\n    # sample['jpg'] is tensor hwc in [-1, 1] at this point\n    midas_trafo = AddMiDaS(model_type=model_type)\n    batch = {\n        \"jpg\": image,\n        \"txt\": num_samples * [txt],\n    }\n    batch = midas_trafo(batch)\n    batch[\"jpg\"] = rearrange(batch[\"jpg\"], 'h w c -> 1 c h w')\n    batch[\"jpg\"] = repeat(batch[\"jpg\"].to(device=device), \"1 ... -> n ...\", n=num_samples)\n    batch[\"midas_in\"] = repeat(torch.from_numpy(batch[\"midas_in\"][None, ...]).to(device=device), \"1 ... -> n ...\", n=num_samples)\n    return batch\n\n\ndef paint(sampler, image, prompt, t_enc, seed, scale, num_samples=1, callback=None,\n          do_full_sample=False):\n    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n    model = sampler.model\n    seed_everything(seed)\n\n    print(\"Creating invisible watermark encoder (see https://github.com/ShieldMnt/invisible-watermark)...\")\n    wm = \"SDV2\"\n    wm_encoder = WatermarkEncoder()\n    wm_encoder.set_watermark('bytes', wm.encode('utf-8'))\n\n    with torch.no_grad(),\\\n            torch.autocast(\"cuda\"):\n        batch = make_batch_sd(image, txt=prompt, device=device, num_samples=num_samples)\n        z = model.get_first_stage_encoding(model.encode_first_stage(batch[model.first_stage_key]))  # move to latent space\n        c = model.cond_stage_model.encode(batch[\"txt\"])\n        c_cat = list()\n        for ck in model.concat_keys:\n            cc = batch[ck]\n            cc = model.depth_model(cc)\n            depth_min, depth_max = torch.amin(cc, dim=[1, 2, 3], keepdim=True), torch.amax(cc, dim=[1, 2, 3],\n                                                                                           keepdim=True)\n            display_depth = (cc - depth_min) / (depth_max - depth_min)\n            st.image(Image.fromarray((display_depth[0, 0, ...].cpu().numpy() * 255.).astype(np.uint8)))\n            cc = torch.nn.functional.interpolate(\n                cc,\n                size=z.shape[2:],\n                mode=\"bicubic\",\n                align_corners=False,\n            )\n            depth_min, depth_max = torch.amin(cc, dim=[1, 2, 3], keepdim=True), torch.amax(cc, dim=[1, 2, 3],\n                                                                                           keepdim=True)\n            cc = 2. * (cc - depth_min) / (depth_max - depth_min) - 1.\n            c_cat.append(cc)\n        c_cat = torch.cat(c_cat, dim=1)\n        # cond\n        cond = {\"c_concat\": [c_cat], \"c_crossattn\": [c]}\n\n        # uncond cond\n        uc_cross = model.get_unconditional_conditioning(num_samples, \"\")\n        uc_full = {\"c_concat\": [c_cat], \"c_crossattn\": [uc_cross]}\n        if not do_full_sample:\n            # encode (scaled latent)\n            z_enc = sampler.stochastic_encode(z, torch.tensor([t_enc] * num_samples).to(model.device))\n        else:\n            z_enc = torch.randn_like(z)\n        # decode it\n        samples = sampler.decode(z_enc, cond, t_enc, unconditional_guidance_scale=scale,\n                                 unconditional_conditioning=uc_full, callback=callback)\n        x_samples_ddim = model.decode_first_stage(samples)\n        result = torch.clamp((x_samples_ddim + 1.0) / 2.0, min=0.0, max=1.0)\n        result = result.cpu().numpy().transpose(0, 2, 3, 1) * 255\n    return [put_watermark(Image.fromarray(img.astype(np.uint8)), wm_encoder) for img in result]\n\n\ndef run():\n    st.title(\"Stable Diffusion Depth2Img\")\n    # run via streamlit run scripts/demo/depth2img.py <path-tp-config> <path-to-ckpt>\n    sampler = initialize_model(sys.argv[1], sys.argv[2])\n\n    image = st.file_uploader(\"Image\", [\"jpg\", \"png\"])\n    if image:\n        image = Image.open(image)\n        w, h = image.size\n        st.text(f\"loaded input image of size ({w}, {h})\")\n        width, height = map(lambda x: x - x % 64, (w, h))  # resize to integer multiple of 64\n        image = image.resize((width, height))\n        st.text(f\"resized input image to size ({width}, {height} (w, h))\")\n        st.image(image)\n\n        prompt = st.text_input(\"Prompt\")\n\n        seed = st.number_input(\"Seed\", min_value=0, max_value=1000000, value=0)\n        num_samples = st.number_input(\"Number of Samples\", min_value=1, max_value=64, value=1)\n        scale = st.slider(\"Scale\", min_value=0.1, max_value=30.0, value=9.0, step=0.1)\n        steps = st.slider(\"DDIM Steps\", min_value=0, max_value=50, value=50, step=1)\n        strength = st.slider(\"Strength\", min_value=0., max_value=1., value=0.9)\n\n        t_progress = st.progress(0)\n        def t_callback(t):\n            t_progress.progress(min((t + 1) / t_enc, 1.))\n\n        assert 0. <= strength <= 1., 'can only work with strength in [0.0, 1.0]'\n        do_full_sample = strength == 1.\n        t_enc = min(int(strength * steps), steps-1)\n        sampler.make_schedule(steps, ddim_eta=0., verbose=True)\n        if st.button(\"Sample\"):\n            result = paint(\n                sampler=sampler,\n                image=image,\n                prompt=prompt,\n                t_enc=t_enc,\n                seed=seed,\n                scale=scale,\n                num_samples=num_samples,\n                callback=t_callback,\n                do_full_sample=do_full_sample,\n            )\n            st.write(\"Result\")\n            for image in result:\n                st.image(image, output_format='PNG')\n\n\nif __name__ == \"__main__\":\n    run()\n", "scripts/gradio/superresolution.py": "import sys\nimport torch\nimport numpy as np\nimport gradio as gr\nfrom PIL import Image\nfrom omegaconf import OmegaConf\nfrom einops import repeat, rearrange\nfrom pytorch_lightning import seed_everything\nfrom imwatermark import WatermarkEncoder\n\nfrom scripts.txt2img import put_watermark\nfrom ldm.models.diffusion.ddim import DDIMSampler\nfrom ldm.models.diffusion.ddpm import LatentUpscaleDiffusion, LatentUpscaleFinetuneDiffusion\nfrom ldm.util import exists, instantiate_from_config\n\n\ntorch.set_grad_enabled(False)\n\n\ndef initialize_model(config, ckpt):\n    config = OmegaConf.load(config)\n    model = instantiate_from_config(config.model)\n    model.load_state_dict(torch.load(ckpt)[\"state_dict\"], strict=False)\n\n    device = torch.device(\n        \"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n    model = model.to(device)\n    sampler = DDIMSampler(model)\n    return sampler\n\n\ndef make_batch_sd(\n        image,\n        txt,\n        device,\n        num_samples=1,\n):\n    image = np.array(image.convert(\"RGB\"))\n    image = torch.from_numpy(image).to(dtype=torch.float32) / 127.5 - 1.0\n    batch = {\n        \"lr\": rearrange(image, 'h w c -> 1 c h w'),\n        \"txt\": num_samples * [txt],\n    }\n    batch[\"lr\"] = repeat(batch[\"lr\"].to(device=device),\n                         \"1 ... -> n ...\", n=num_samples)\n    return batch\n\n\ndef make_noise_augmentation(model, batch, noise_level=None):\n    x_low = batch[model.low_scale_key]\n    x_low = x_low.to(memory_format=torch.contiguous_format).float()\n    x_aug, noise_level = model.low_scale_model(x_low, noise_level)\n    return x_aug, noise_level\n\n\ndef paint(sampler, image, prompt, seed, scale, h, w, steps, num_samples=1, callback=None, eta=0., noise_level=None):\n    device = torch.device(\n        \"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n    model = sampler.model\n    seed_everything(seed)\n    prng = np.random.RandomState(seed)\n    start_code = prng.randn(num_samples, model.channels, h, w)\n    start_code = torch.from_numpy(start_code).to(\n        device=device, dtype=torch.float32)\n\n    print(\"Creating invisible watermark encoder (see https://github.com/ShieldMnt/invisible-watermark)...\")\n    wm = \"SDV2\"\n    wm_encoder = WatermarkEncoder()\n    wm_encoder.set_watermark('bytes', wm.encode('utf-8'))\n    with torch.no_grad(),\\\n            torch.autocast(\"cuda\"):\n        batch = make_batch_sd(\n            image, txt=prompt, device=device, num_samples=num_samples)\n        c = model.cond_stage_model.encode(batch[\"txt\"])\n        c_cat = list()\n        if isinstance(model, LatentUpscaleFinetuneDiffusion):\n            for ck in model.concat_keys:\n                cc = batch[ck]\n                if exists(model.reshuffle_patch_size):\n                    assert isinstance(model.reshuffle_patch_size, int)\n                    cc = rearrange(cc, 'b c (p1 h) (p2 w) -> b (p1 p2 c) h w',\n                                   p1=model.reshuffle_patch_size, p2=model.reshuffle_patch_size)\n                c_cat.append(cc)\n            c_cat = torch.cat(c_cat, dim=1)\n            # cond\n            cond = {\"c_concat\": [c_cat], \"c_crossattn\": [c]}\n            # uncond cond\n            uc_cross = model.get_unconditional_conditioning(num_samples, \"\")\n            uc_full = {\"c_concat\": [c_cat], \"c_crossattn\": [uc_cross]}\n        elif isinstance(model, LatentUpscaleDiffusion):\n            x_augment, noise_level = make_noise_augmentation(\n                model, batch, noise_level)\n            cond = {\"c_concat\": [x_augment],\n                    \"c_crossattn\": [c], \"c_adm\": noise_level}\n            # uncond cond\n            uc_cross = model.get_unconditional_conditioning(num_samples, \"\")\n            uc_full = {\"c_concat\": [x_augment], \"c_crossattn\": [\n                uc_cross], \"c_adm\": noise_level}\n        else:\n            raise NotImplementedError()\n\n        shape = [model.channels, h, w]\n        samples, intermediates = sampler.sample(\n            steps,\n            num_samples,\n            shape,\n            cond,\n            verbose=False,\n            eta=eta,\n            unconditional_guidance_scale=scale,\n            unconditional_conditioning=uc_full,\n            x_T=start_code,\n            callback=callback\n        )\n    with torch.no_grad():\n        x_samples_ddim = model.decode_first_stage(samples)\n    result = torch.clamp((x_samples_ddim + 1.0) / 2.0, min=0.0, max=1.0)\n    result = result.cpu().numpy().transpose(0, 2, 3, 1) * 255\n    return [put_watermark(Image.fromarray(img.astype(np.uint8)), wm_encoder) for img in result]\n\n\ndef pad_image(input_image):\n    pad_w, pad_h = np.max(((2, 2), np.ceil(\n        np.array(input_image.size) / 64).astype(int)), axis=0) * 64 - input_image.size\n    im_padded = Image.fromarray(\n        np.pad(np.array(input_image), ((0, pad_h), (0, pad_w), (0, 0)), mode='edge'))\n    return im_padded\n\n\ndef predict(input_image, prompt, steps, num_samples, scale, seed, eta, noise_level):\n    init_image = input_image.convert(\"RGB\")\n    image = pad_image(init_image)  # resize to integer multiple of 32\n    width, height = image.size\n\n    noise_level = torch.Tensor(\n        num_samples * [noise_level]).to(sampler.model.device).long()\n    sampler.make_schedule(steps, ddim_eta=eta, verbose=True)\n    result = paint(\n        sampler=sampler,\n        image=image,\n        prompt=prompt,\n        seed=seed,\n        scale=scale,\n        h=height, w=width, steps=steps,\n        num_samples=num_samples,\n        callback=None,\n        noise_level=noise_level\n    )\n    return result\n\n\nsampler = initialize_model(sys.argv[1], sys.argv[2])\n\nblock = gr.Blocks().queue()\nwith block:\n    with gr.Row():\n        gr.Markdown(\"## Stable Diffusion Upscaling\")\n\n    with gr.Row():\n        with gr.Column():\n            input_image = gr.Image(source='upload', type=\"pil\")\n            gr.Markdown(\n                \"Tip: Add a description of the object that should be upscaled, e.g.: 'a professional photograph of a cat\")\n            prompt = gr.Textbox(label=\"Prompt\")\n            run_button = gr.Button(label=\"Run\")\n            with gr.Accordion(\"Advanced options\", open=False):\n                num_samples = gr.Slider(\n                    label=\"Number of Samples\", minimum=1, maximum=4, value=1, step=1)\n                steps = gr.Slider(label=\"DDIM Steps\", minimum=2,\n                                  maximum=200, value=75, step=1)\n                scale = gr.Slider(\n                    label=\"Scale\", minimum=0.1, maximum=30.0, value=10, step=0.1\n                )\n                seed = gr.Slider(\n                    label=\"Seed\",\n                    minimum=0,\n                    maximum=2147483647,\n                    step=1,\n                    randomize=True,\n                )\n                eta = gr.Number(label=\"eta (DDIM)\",\n                                value=0.0, min=0.0, max=1.0)\n                noise_level = None\n                if isinstance(sampler.model, LatentUpscaleDiffusion):\n                    # TODO: make this work for all models\n                    noise_level = gr.Number(\n                        label=\"Noise Augmentation\", min=0, max=350, value=20, step=1)\n\n        with gr.Column():\n            gallery = gr.Gallery(label=\"Generated images\", show_label=False).style(\n                grid=[2], height=\"auto\")\n\n    run_button.click(fn=predict, inputs=[\n                     input_image, prompt, steps, num_samples, scale, seed, eta, noise_level], outputs=[gallery])\n\n\nblock.launch()\n", "scripts/gradio/inpainting.py": "import sys\nimport cv2\nimport torch\nimport numpy as np\nimport gradio as gr\nfrom PIL import Image\nfrom omegaconf import OmegaConf\nfrom einops import repeat\nfrom imwatermark import WatermarkEncoder\nfrom pathlib import Path\n\nfrom ldm.models.diffusion.ddim import DDIMSampler\nfrom ldm.util import instantiate_from_config\n\n\ntorch.set_grad_enabled(False)\n\n\ndef put_watermark(img, wm_encoder=None):\n    if wm_encoder is not None:\n        img = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)\n        img = wm_encoder.encode(img, 'dwtDct')\n        img = Image.fromarray(img[:, :, ::-1])\n    return img\n\n\ndef initialize_model(config, ckpt):\n    config = OmegaConf.load(config)\n    model = instantiate_from_config(config.model)\n\n    model.load_state_dict(torch.load(ckpt)[\"state_dict\"], strict=False)\n\n    device = torch.device(\n        \"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n    model = model.to(device)\n    sampler = DDIMSampler(model)\n\n    return sampler\n\n\ndef make_batch_sd(\n        image,\n        mask,\n        txt,\n        device,\n        num_samples=1):\n    image = np.array(image.convert(\"RGB\"))\n    image = image[None].transpose(0, 3, 1, 2)\n    image = torch.from_numpy(image).to(dtype=torch.float32) / 127.5 - 1.0\n\n    mask = np.array(mask.convert(\"L\"))\n    mask = mask.astype(np.float32) / 255.0\n    mask = mask[None, None]\n    mask[mask < 0.5] = 0\n    mask[mask >= 0.5] = 1\n    mask = torch.from_numpy(mask)\n\n    masked_image = image * (mask < 0.5)\n\n    batch = {\n        \"image\": repeat(image.to(device=device), \"1 ... -> n ...\", n=num_samples),\n        \"txt\": num_samples * [txt],\n        \"mask\": repeat(mask.to(device=device), \"1 ... -> n ...\", n=num_samples),\n        \"masked_image\": repeat(masked_image.to(device=device), \"1 ... -> n ...\", n=num_samples),\n    }\n    return batch\n\n\ndef inpaint(sampler, image, mask, prompt, seed, scale, ddim_steps, num_samples=1, w=512, h=512):\n    device = torch.device(\n        \"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n    model = sampler.model\n\n    print(\"Creating invisible watermark encoder (see https://github.com/ShieldMnt/invisible-watermark)...\")\n    wm = \"SDV2\"\n    wm_encoder = WatermarkEncoder()\n    wm_encoder.set_watermark('bytes', wm.encode('utf-8'))\n\n    prng = np.random.RandomState(seed)\n    start_code = prng.randn(num_samples, 4, h // 8, w // 8)\n    start_code = torch.from_numpy(start_code).to(\n        device=device, dtype=torch.float32)\n\n    with torch.no_grad(), \\\n            torch.autocast(\"cuda\"):\n        batch = make_batch_sd(image, mask, txt=prompt,\n                              device=device, num_samples=num_samples)\n\n        c = model.cond_stage_model.encode(batch[\"txt\"])\n\n        c_cat = list()\n        for ck in model.concat_keys:\n            cc = batch[ck].float()\n            if ck != model.masked_image_key:\n                bchw = [num_samples, 4, h // 8, w // 8]\n                cc = torch.nn.functional.interpolate(cc, size=bchw[-2:])\n            else:\n                cc = model.get_first_stage_encoding(\n                    model.encode_first_stage(cc))\n            c_cat.append(cc)\n        c_cat = torch.cat(c_cat, dim=1)\n\n        # cond\n        cond = {\"c_concat\": [c_cat], \"c_crossattn\": [c]}\n\n        # uncond cond\n        uc_cross = model.get_unconditional_conditioning(num_samples, \"\")\n        uc_full = {\"c_concat\": [c_cat], \"c_crossattn\": [uc_cross]}\n\n        shape = [model.channels, h // 8, w // 8]\n        samples_cfg, intermediates = sampler.sample(\n            ddim_steps,\n            num_samples,\n            shape,\n            cond,\n            verbose=False,\n            eta=1.0,\n            unconditional_guidance_scale=scale,\n            unconditional_conditioning=uc_full,\n            x_T=start_code,\n        )\n        x_samples_ddim = model.decode_first_stage(samples_cfg)\n\n        result = torch.clamp((x_samples_ddim + 1.0) / 2.0,\n                             min=0.0, max=1.0)\n\n        result = result.cpu().numpy().transpose(0, 2, 3, 1) * 255\n    return [put_watermark(Image.fromarray(img.astype(np.uint8)), wm_encoder) for img in result]\n\ndef pad_image(input_image):\n    pad_w, pad_h = np.max(((2, 2), np.ceil(\n        np.array(input_image.size) / 64).astype(int)), axis=0) * 64 - input_image.size\n    im_padded = Image.fromarray(\n        np.pad(np.array(input_image), ((0, pad_h), (0, pad_w), (0, 0)), mode='edge'))\n    return im_padded\n\ndef predict(input_image, prompt, ddim_steps, num_samples, scale, seed):\n    init_image = input_image[\"image\"].convert(\"RGB\")\n    init_mask = input_image[\"mask\"].convert(\"RGB\")\n    image = pad_image(init_image) # resize to integer multiple of 32\n    mask = pad_image(init_mask) # resize to integer multiple of 32\n    width, height = image.size\n    print(\"Inpainting...\", width, height)\n\n    result = inpaint(\n        sampler=sampler,\n        image=image,\n        mask=mask,\n        prompt=prompt,\n        seed=seed,\n        scale=scale,\n        ddim_steps=ddim_steps,\n        num_samples=num_samples,\n        h=height, w=width\n    )\n\n    return result\n\n\nsampler = initialize_model(sys.argv[1], sys.argv[2])\n\nblock = gr.Blocks().queue()\nwith block:\n    with gr.Row():\n        gr.Markdown(\"## Stable Diffusion Inpainting\")\n\n    with gr.Row():\n        with gr.Column():\n            input_image = gr.Image(source='upload', tool='sketch', type=\"pil\")\n            prompt = gr.Textbox(label=\"Prompt\")\n            run_button = gr.Button(label=\"Run\")\n            with gr.Accordion(\"Advanced options\", open=False):\n                num_samples = gr.Slider(\n                    label=\"Images\", minimum=1, maximum=4, value=4, step=1)\n                ddim_steps = gr.Slider(label=\"Steps\", minimum=1,\n                                       maximum=50, value=45, step=1)\n                scale = gr.Slider(\n                    label=\"Guidance Scale\", minimum=0.1, maximum=30.0, value=10, step=0.1\n                )\n                seed = gr.Slider(\n                    label=\"Seed\",\n                    minimum=0,\n                    maximum=2147483647,\n                    step=1,\n                    randomize=True,\n                )\n        with gr.Column():\n            gallery = gr.Gallery(label=\"Generated images\", show_label=False).style(\n                grid=[2], height=\"auto\")\n\n    run_button.click(fn=predict, inputs=[\n                     input_image, prompt, ddim_steps, num_samples, scale, seed], outputs=[gallery])\n\n\nblock.launch()\n", "scripts/gradio/depth2img.py": "import sys\nimport torch\nimport numpy as np\nimport gradio as gr\nfrom PIL import Image\nfrom omegaconf import OmegaConf\nfrom einops import repeat, rearrange\nfrom pytorch_lightning import seed_everything\nfrom imwatermark import WatermarkEncoder\n\nfrom scripts.txt2img import put_watermark\nfrom ldm.util import instantiate_from_config\nfrom ldm.models.diffusion.ddim import DDIMSampler\nfrom ldm.data.util import AddMiDaS\n\ntorch.set_grad_enabled(False)\n\n\ndef initialize_model(config, ckpt):\n    config = OmegaConf.load(config)\n    model = instantiate_from_config(config.model)\n    model.load_state_dict(torch.load(ckpt)[\"state_dict\"], strict=False)\n\n    device = torch.device(\n        \"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n    model = model.to(device)\n    sampler = DDIMSampler(model)\n    return sampler\n\n\ndef make_batch_sd(\n        image,\n        txt,\n        device,\n        num_samples=1,\n        model_type=\"dpt_hybrid\"\n):\n    image = np.array(image.convert(\"RGB\"))\n    image = torch.from_numpy(image).to(dtype=torch.float32) / 127.5 - 1.0\n    # sample['jpg'] is tensor hwc in [-1, 1] at this point\n    midas_trafo = AddMiDaS(model_type=model_type)\n    batch = {\n        \"jpg\": image,\n        \"txt\": num_samples * [txt],\n    }\n    batch = midas_trafo(batch)\n    batch[\"jpg\"] = rearrange(batch[\"jpg\"], 'h w c -> 1 c h w')\n    batch[\"jpg\"] = repeat(batch[\"jpg\"].to(device=device),\n                          \"1 ... -> n ...\", n=num_samples)\n    batch[\"midas_in\"] = repeat(torch.from_numpy(batch[\"midas_in\"][None, ...]).to(\n        device=device), \"1 ... -> n ...\", n=num_samples)\n    return batch\n\n\ndef paint(sampler, image, prompt, t_enc, seed, scale, num_samples=1, callback=None,\n          do_full_sample=False):\n    device = torch.device(\n        \"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n    model = sampler.model\n    seed_everything(seed)\n\n    print(\"Creating invisible watermark encoder (see https://github.com/ShieldMnt/invisible-watermark)...\")\n    wm = \"SDV2\"\n    wm_encoder = WatermarkEncoder()\n    wm_encoder.set_watermark('bytes', wm.encode('utf-8'))\n\n    with torch.no_grad(),\\\n            torch.autocast(\"cuda\"):\n        batch = make_batch_sd(\n            image, txt=prompt, device=device, num_samples=num_samples)\n        z = model.get_first_stage_encoding(model.encode_first_stage(\n            batch[model.first_stage_key]))  # move to latent space\n        c = model.cond_stage_model.encode(batch[\"txt\"])\n        c_cat = list()\n        for ck in model.concat_keys:\n            cc = batch[ck]\n            cc = model.depth_model(cc)\n            depth_min, depth_max = torch.amin(cc, dim=[1, 2, 3], keepdim=True), torch.amax(cc, dim=[1, 2, 3],\n                                                                                           keepdim=True)\n            display_depth = (cc - depth_min) / (depth_max - depth_min)\n            depth_image = Image.fromarray(\n                (display_depth[0, 0, ...].cpu().numpy() * 255.).astype(np.uint8))\n            cc = torch.nn.functional.interpolate(\n                cc,\n                size=z.shape[2:],\n                mode=\"bicubic\",\n                align_corners=False,\n            )\n            depth_min, depth_max = torch.amin(cc, dim=[1, 2, 3], keepdim=True), torch.amax(cc, dim=[1, 2, 3],\n                                                                                           keepdim=True)\n            cc = 2. * (cc - depth_min) / (depth_max - depth_min) - 1.\n            c_cat.append(cc)\n        c_cat = torch.cat(c_cat, dim=1)\n        # cond\n        cond = {\"c_concat\": [c_cat], \"c_crossattn\": [c]}\n\n        # uncond cond\n        uc_cross = model.get_unconditional_conditioning(num_samples, \"\")\n        uc_full = {\"c_concat\": [c_cat], \"c_crossattn\": [uc_cross]}\n        if not do_full_sample:\n            # encode (scaled latent)\n            z_enc = sampler.stochastic_encode(\n                z, torch.tensor([t_enc] * num_samples).to(model.device))\n        else:\n            z_enc = torch.randn_like(z)\n        # decode it\n        samples = sampler.decode(z_enc, cond, t_enc, unconditional_guidance_scale=scale,\n                                 unconditional_conditioning=uc_full, callback=callback)\n        x_samples_ddim = model.decode_first_stage(samples)\n        result = torch.clamp((x_samples_ddim + 1.0) / 2.0, min=0.0, max=1.0)\n        result = result.cpu().numpy().transpose(0, 2, 3, 1) * 255\n    return [depth_image] + [put_watermark(Image.fromarray(img.astype(np.uint8)), wm_encoder) for img in result]\n\n\ndef pad_image(input_image):\n    pad_w, pad_h = np.max(((2, 2), np.ceil(\n        np.array(input_image.size) / 64).astype(int)), axis=0) * 64 - input_image.size\n    im_padded = Image.fromarray(\n        np.pad(np.array(input_image), ((0, pad_h), (0, pad_w), (0, 0)), mode='edge'))\n    return im_padded\n\n\ndef predict(input_image, prompt, steps, num_samples, scale, seed, eta, strength):\n    init_image = input_image.convert(\"RGB\")\n    image = pad_image(init_image)  # resize to integer multiple of 32\n\n    sampler.make_schedule(steps, ddim_eta=eta, verbose=True)\n    assert 0. <= strength <= 1., 'can only work with strength in [0.0, 1.0]'\n    do_full_sample = strength == 1.\n    t_enc = min(int(strength * steps), steps-1)\n    result = paint(\n        sampler=sampler,\n        image=image,\n        prompt=prompt,\n        t_enc=t_enc,\n        seed=seed,\n        scale=scale,\n        num_samples=num_samples,\n        callback=None,\n        do_full_sample=do_full_sample\n    )\n    return result\n\n\nsampler = initialize_model(sys.argv[1], sys.argv[2])\n\nblock = gr.Blocks().queue()\nwith block:\n    with gr.Row():\n        gr.Markdown(\"## Stable Diffusion Depth2Img\")\n\n    with gr.Row():\n        with gr.Column():\n            input_image = gr.Image(source='upload', type=\"pil\")\n            prompt = gr.Textbox(label=\"Prompt\")\n            run_button = gr.Button(label=\"Run\")\n            with gr.Accordion(\"Advanced options\", open=False):\n                num_samples = gr.Slider(\n                    label=\"Images\", minimum=1, maximum=4, value=1, step=1)\n                ddim_steps = gr.Slider(label=\"Steps\", minimum=1,\n                                       maximum=50, value=50, step=1)\n                scale = gr.Slider(\n                    label=\"Guidance Scale\", minimum=0.1, maximum=30.0, value=9.0, step=0.1\n                )\n                strength = gr.Slider(\n                    label=\"Strength\", minimum=0.0, maximum=1.0, value=0.9, step=0.01\n                )\n                seed = gr.Slider(\n                    label=\"Seed\",\n                    minimum=0,\n                    maximum=2147483647,\n                    step=1,\n                    randomize=True,\n                )\n                eta = gr.Number(label=\"eta (DDIM)\", value=0.0)\n        with gr.Column():\n            gallery = gr.Gallery(label=\"Generated images\", show_label=False).style(\n                grid=[2], height=\"auto\")\n\n    run_button.click(fn=predict, inputs=[\n                     input_image, prompt, ddim_steps, num_samples, scale, seed, eta, strength], outputs=[gallery])\n\n\nblock.launch()\n"}