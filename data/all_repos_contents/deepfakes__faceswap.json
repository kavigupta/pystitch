{"faceswap.py": "#!/usr/bin/env python3\n\"\"\" The master faceswap.py script \"\"\"\nimport gettext\nimport locale\nimport os\nimport sys\n\n# Translations don't work by default in Windows, so hack in environment variable\nif sys.platform.startswith(\"win\"):\n    os.environ[\"LANG\"], _ = locale.getdefaultlocale()\n\nfrom lib.cli import args as cli_args  # pylint:disable=wrong-import-position\nfrom lib.cli.args_train import TrainArgs  # pylint:disable=wrong-import-position\nfrom lib.cli.args_extract_convert import ConvertArgs, ExtractArgs  # noqa:E501 pylint:disable=wrong-import-position\nfrom lib.config import generate_configs  # pylint:disable=wrong-import-position\n\n# LOCALES\n_LANG = gettext.translation(\"faceswap\", localedir=\"locales\", fallback=True)\n_ = _LANG.gettext\n\nif sys.version_info < (3, 10):\n    raise ValueError(\"This program requires at least python 3.10\")\n\n_PARSER = cli_args.FullHelpArgumentParser()\n\n\ndef _bad_args(*args) -> None:  # pylint:disable=unused-argument\n    \"\"\" Print help to console when bad arguments are provided. \"\"\"\n    print(cli_args)\n    _PARSER.print_help()\n    sys.exit(0)\n\n\ndef _main() -> None:\n    \"\"\" The main entry point into Faceswap.\n\n    - Generates the config files, if they don't pre-exist.\n    - Compiles the :class:`~lib.cli.args.FullHelpArgumentParser` objects for each section of\n      Faceswap.\n    - Sets the default values and launches the relevant script.\n    - Outputs help if invalid parameters are provided.\n    \"\"\"\n    generate_configs()\n\n    subparser = _PARSER.add_subparsers()\n    ExtractArgs(subparser, \"extract\", _(\"Extract the faces from pictures or a video\"))\n    TrainArgs(subparser, \"train\", _(\"Train a model for the two faces A and B\"))\n    ConvertArgs(subparser,\n                \"convert\",\n                _(\"Convert source pictures or video to a new one with the face swapped\"))\n    cli_args.GuiArgs(subparser, \"gui\", _(\"Launch the Faceswap Graphical User Interface\"))\n    _PARSER.set_defaults(func=_bad_args)\n    arguments = _PARSER.parse_args()\n    arguments.func(arguments)\n\n\nif __name__ == \"__main__\":\n    _main()\n", "tools.py": "#!/usr/bin/env python3\n\"\"\" The master tools.py script \"\"\"\nimport gettext\nimport os\nimport sys\n\nfrom importlib import import_module\n\n# Importing the various tools\nfrom lib.cli.args import FullHelpArgumentParser\n\n# LOCALES\n_LANG = gettext.translation(\"tools\", localedir=\"locales\", fallback=True)\n_ = _LANG.gettext\n\n# Python version check\nif sys.version_info < (3, 10):\n    raise ValueError(\"This program requires at least python 3.10\")\n\n\ndef bad_args(*args):  # pylint:disable=unused-argument\n    \"\"\" Print help on bad arguments \"\"\"\n    PARSER.print_help()\n    sys.exit(0)\n\n\ndef _get_cli_opts():\n    \"\"\" Optain the subparsers and cli options for available tools \"\"\"\n    base_path = os.path.realpath(os.path.dirname(sys.argv[0]))\n    tools_dir = os.path.join(base_path, \"tools\")\n    for tool_name in sorted(os.listdir(tools_dir)):\n        cli_file = os.path.join(tools_dir, tool_name, \"cli.py\")\n        if os.path.exists(cli_file):\n            mod = \".\".join((\"tools\", tool_name, \"cli\"))\n            module = import_module(mod)\n            cliarg_class = getattr(module, f\"{tool_name.title()}Args\")\n            help_text = getattr(module, \"_HELPTEXT\")\n            yield tool_name, help_text, cliarg_class\n\n\nif __name__ == \"__main__\":\n    PARSER = FullHelpArgumentParser()\n    SUBPARSER = PARSER.add_subparsers()\n    for tool, helptext, cli_args in _get_cli_opts():\n        cli_args(SUBPARSER, tool, helptext)\n    PARSER.set_defaults(func=bad_args)\n    ARGUMENTS = PARSER.parse_args()\n    ARGUMENTS.func(ARGUMENTS)\n", "setup.py": "#!/usr/bin/env python3\n\"\"\" Install packages for faceswap.py \"\"\"\n# pylint:disable=too-many-lines\n\nimport logging\nimport ctypes\nimport json\nimport locale\nimport platform\nimport operator\nimport os\nimport re\nimport sys\nimport typing as T\nfrom shutil import which\nfrom subprocess import list2cmdline, PIPE, Popen, run, STDOUT\n\nfrom pkg_resources import parse_requirements\n\nfrom lib.logger import log_setup\n\nlogger = logging.getLogger(__name__)\nbackend_type: T.TypeAlias = T.Literal['nvidia', 'apple_silicon', 'directml', 'cpu', 'rocm', \"all\"]\n\n_INSTALL_FAILED = False\n# Packages that are explicitly required for setup.py\n_INSTALLER_REQUIREMENTS: list[tuple[str, str]] = [(\"pexpect>=4.8.0\", \"!Windows\"),\n                                                  (\"pywinpty==2.0.2\", \"Windows\")]\n# Conda packages that are required for a specific backend\n# TODO zlib-wapi is required on some Windows installs where cuDNN complains:\n# Could not locate zlibwapi.dll. Please make sure it is in your library path!\n# This only seems to occur on Anaconda cuDNN not conda-forge\n_BACKEND_SPECIFIC_CONDA: dict[backend_type, list[str]] = {\n    \"nvidia\": [\"cudatoolkit\", \"cudnn\", \"zlib-wapi\"],\n    \"apple_silicon\": [\"libblas\"]}\n# Packages that should only be installed through pip\n_FORCE_PIP: dict[backend_type, list[str]] = {\n    \"nvidia\": [\"tensorflow\"],\n    \"all\": [\n        \"tensorflow-cpu\",  # conda-forge leads to flatbuffer errors because of mixed sources\n        \"imageio-ffmpeg\"]}  # 17/11/23 Conda forge uses incorrect ffmpeg, so fallback to pip\n# Revisions of tensorflow GPU and cuda/cudnn requirements. These relate specifically to the\n# Tensorflow builds available from pypi\n_TENSORFLOW_REQUIREMENTS = {\">=2.10.0,<2.11.0\": [\">=11.2,<11.3\", \">=8.1,<8.2\"]}\n# ROCm min/max version requirements for Tensorflow\n_TENSORFLOW_ROCM_REQUIREMENTS = {\">=2.10.0,<2.11.0\": ((5, 2, 0), (5, 4, 0))}\n# TODO tensorflow-metal versioning\n\n# Mapping of Python packages to their conda names if different from pip or in non-default channel\n_CONDA_MAPPING: dict[str, tuple[str, str]] = {\n    \"cudatoolkit\": (\"cudatoolkit\", \"conda-forge\"),\n    \"cudnn\": (\"cudnn\", \"conda-forge\"),\n    \"fastcluster\": (\"fastcluster\", \"conda-forge\"),\n    \"ffmpy\": (\"ffmpy\", \"conda-forge\"),\n    # \"imageio-ffmpeg\": (\"imageio-ffmpeg\", \"conda-forge\"),\n    \"nvidia-ml-py\": (\"nvidia-ml-py\", \"conda-forge\"),\n    \"tensorflow-deps\": (\"tensorflow-deps\", \"apple\"),\n    \"libblas\": (\"libblas\", \"conda-forge\"),\n    \"zlib-wapi\": (\"zlib-wapi\", \"conda-forge\"),\n    \"xorg-libxft\": (\"xorg-libxft\", \"conda-forge\")}\n\n# Force output to utf-8\nsys.stdout.reconfigure(encoding=\"utf-8\", errors=\"replace\")  # type:ignore[attr-defined]\n\n\nclass Environment():\n    \"\"\" The current install environment\n\n    Parameters\n    ----------\n    updater: bool, Optional\n        ``True`` if the script is being called by Faceswap's internal updater. ``False`` if full\n        setup is running. Default: ``False``\n    \"\"\"\n\n    _backends = ((\"nvidia\", \"apple_silicon\", \"directml\", \"rocm\", \"cpu\"))\n\n    def __init__(self, updater: bool = False) -> None:\n        self.updater = updater\n        # Flag that setup is being run by installer so steps can be skipped\n        self.is_installer: bool = False\n        self.backend: backend_type | None = None\n        self.enable_docker: bool = False\n        self.cuda_cudnn = [\"\", \"\"]\n        self.rocm_version: tuple[int, ...] = (0, 0, 0)\n\n        self._process_arguments()\n        self._check_permission()\n        self._check_system()\n        self._check_python()\n        self._output_runtime_info()\n        self._check_pip()\n        self._upgrade_pip()\n        self._set_env_vars()\n\n    @property\n    def encoding(self) -> str:\n        \"\"\" Get system encoding \"\"\"\n        return locale.getpreferredencoding()\n\n    @property\n    def os_version(self) -> tuple[str, str]:\n        \"\"\" Get OS Version \"\"\"\n        return platform.system(), platform.release()\n\n    @property\n    def py_version(self) -> tuple[str, str]:\n        \"\"\" Get Python Version \"\"\"\n        return platform.python_version(), platform.architecture()[0]\n\n    @property\n    def is_conda(self) -> bool:\n        \"\"\" Check whether using Conda \"\"\"\n        return (\"conda\" in sys.version.lower() or\n                os.path.exists(os.path.join(sys.prefix, 'conda-meta')))\n\n    @property\n    def is_admin(self) -> bool:\n        \"\"\" Check whether user is admin \"\"\"\n        try:\n            retval = os.getuid() == 0  # type: ignore\n        except AttributeError:\n            retval = ctypes.windll.shell32.IsUserAnAdmin() != 0  # type: ignore\n        return retval\n\n    @property\n    def cuda_version(self) -> str:\n        \"\"\" str: The detected globally installed Cuda Version \"\"\"\n        return self.cuda_cudnn[0]\n\n    @property\n    def cudnn_version(self) -> str:\n        \"\"\" str: The detected globally installed cuDNN Version \"\"\"\n        return self.cuda_cudnn[1]\n\n    @property\n    def is_virtualenv(self) -> bool:\n        \"\"\" Check whether this is a virtual environment \"\"\"\n        if not self.is_conda:\n            retval = (hasattr(sys, \"real_prefix\") or\n                      (hasattr(sys, \"base_prefix\") and sys.base_prefix != sys.prefix))\n        else:\n            prefix = os.path.dirname(sys.prefix)\n            retval = os.path.basename(prefix) == \"envs\"\n        return retval\n\n    def _process_arguments(self) -> None:\n        \"\"\" Process any cli arguments and dummy in cli arguments if calling from updater. \"\"\"\n        args = [arg for arg in sys.argv]  # pylint:disable=unnecessary-comprehension\n        if self.updater:\n            from lib.utils import get_backend  # pylint:disable=import-outside-toplevel\n            args.append(f\"--{get_backend()}\")\n\n        logger.debug(args)\n        for arg in args:\n            if arg == \"--installer\":\n                self.is_installer = True\n            if not self.backend and (arg.startswith(\"--\") and\n                                     arg.replace(\"--\", \"\") in self._backends):\n                self.backend = arg.replace(\"--\", \"\").lower()  # type:ignore\n\n    def _check_permission(self) -> None:\n        \"\"\" Check for Admin permissions \"\"\"\n        if self.updater:\n            return\n        if self.is_admin:\n            logger.info(\"Running as Root/Admin\")\n        else:\n            logger.info(\"Running without root/admin privileges\")\n\n    def _check_system(self) -> None:\n        \"\"\" Check the system \"\"\"\n        if not self.updater:\n            logger.info(\"The tool provides tips for installation and installs required python \"\n                        \"packages\")\n        logger.info(\"Setup in %s %s\", self.os_version[0], self.os_version[1])\n        if not self.updater and not self.os_version[0] in [\"Windows\", \"Linux\", \"Darwin\"]:\n            logger.error(\"Your system %s is not supported!\", self.os_version[0])\n            sys.exit(1)\n        if self.os_version[0].lower() == \"darwin\" and platform.machine() == \"arm64\":\n            self.backend = \"apple_silicon\"\n\n            if not self.updater and not self.is_conda:\n                logger.error(\"Setting up Faceswap for Apple Silicon outside of a Conda \"\n                             \"environment is unsupported\")\n                sys.exit(1)\n\n    def _check_python(self) -> None:\n        \"\"\" Check python and virtual environment status \"\"\"\n        logger.info(\"Installed Python: %s %s\", self.py_version[0], self.py_version[1])\n\n        if self.updater:\n            return\n\n        if not ((3, 10) <= sys.version_info < (3, 11) and self.py_version[1] == \"64bit\"):\n            logger.error(\"Please run this script with Python version 3.10 64bit and try \"\n                         \"again.\")\n            sys.exit(1)\n\n    def _output_runtime_info(self) -> None:\n        \"\"\" Output run time info \"\"\"\n        if self.is_conda:\n            logger.info(\"Running in Conda\")\n        if self.is_virtualenv:\n            logger.info(\"Running in a Virtual Environment\")\n        logger.info(\"Encoding: %s\", self.encoding)\n\n    def _check_pip(self) -> None:\n        \"\"\" Check installed pip version \"\"\"\n        if self.updater:\n            return\n        try:\n            import pip  # noqa pylint:disable=unused-import,import-outside-toplevel\n        except ImportError:\n            logger.error(\"Import pip failed. Please Install python3-pip and try again\")\n            sys.exit(1)\n\n    def _upgrade_pip(self) -> None:\n        \"\"\" Upgrade pip to latest version \"\"\"\n        if not self.is_conda:\n            # Don't do this with Conda, as we must use Conda version of pip\n            logger.info(\"Upgrading pip...\")\n            pipexe = [sys.executable, \"-m\", \"pip\"]\n            pipexe.extend([\"install\", \"--no-cache-dir\", \"-qq\", \"--upgrade\"])\n            if not self.is_admin and not self.is_virtualenv:\n                pipexe.append(\"--user\")\n            pipexe.append(\"pip\")\n            run(pipexe, check=True)\n        import pip  # pylint:disable=import-outside-toplevel\n        pip_version = pip.__version__\n        logger.info(\"Installed pip: %s\", pip_version)\n\n    def set_config(self) -> None:\n        \"\"\" Set the backend in the faceswap config file \"\"\"\n        config = {\"backend\": self.backend}\n        pypath = os.path.dirname(os.path.realpath(__file__))\n        config_file = os.path.join(pypath, \"config\", \".faceswap\")\n        with open(config_file, \"w\", encoding=\"utf8\") as cnf:\n            json.dump(config, cnf)\n        logger.info(\"Faceswap config written to: %s\", config_file)\n\n    def _set_env_vars(self) -> None:\n        \"\"\" There are some foibles under Conda which need to be worked around in different\n        situations.\n\n        Linux:\n        Update the LD_LIBRARY_PATH environment variable when activating a conda environment\n        and revert it when deactivating.\n\n        Notes\n        -----\n        From Tensorflow 2.7, installing Cuda Toolkit from conda-forge and tensorflow from pip\n        causes tensorflow to not be able to locate shared libs and hence not use the GPU.\n        We update the environment variable for all instances using Conda as it shouldn't hurt\n        anything and may help avoid conflicts with globally installed Cuda\n        \"\"\"\n        if not self.is_conda:\n            return\n\n        linux_update = self.os_version[0].lower() == \"linux\" and self.backend == \"nvidia\"\n\n        if not linux_update:\n            return\n\n        conda_prefix = os.environ[\"CONDA_PREFIX\"]\n        activate_folder = os.path.join(conda_prefix, \"etc\", \"conda\", \"activate.d\")\n        deactivate_folder = os.path.join(conda_prefix, \"etc\", \"conda\", \"deactivate.d\")\n        os.makedirs(activate_folder, exist_ok=True)\n        os.makedirs(deactivate_folder, exist_ok=True)\n\n        activate_script = os.path.join(conda_prefix, activate_folder, \"env_vars.sh\")\n        deactivate_script = os.path.join(conda_prefix, deactivate_folder, \"env_vars.sh\")\n\n        if os.path.isfile(activate_script):\n            # Only create file if it does not already exist. There may be instances where people\n            # have created their own scripts, but these should be few and far between and those\n            # people should already know what they are doing.\n            return\n\n        conda_libs = os.path.join(conda_prefix, \"lib\")\n        activate = [\"#!/bin/sh\\n\\n\",\n                    \"export OLD_LD_LIBRARY_PATH=${LD_LIBRARY_PATH}\\n\",\n                    f\"export LD_LIBRARY_PATH='{conda_libs}':${{LD_LIBRARY_PATH}}\\n\"]\n        deactivate = [\"#!/bin/sh\\n\\n\",\n                      \"export LD_LIBRARY_PATH=${OLD_LD_LIBRARY_PATH}\\n\",\n                      \"unset OLD_LD_LIBRARY_PATH\\n\"]\n        logger.info(\"Cuda search path set to '%s'\", conda_libs)\n\n        with open(activate_script, \"w\", encoding=\"utf8\") as afile:\n            afile.writelines(activate)\n        with open(deactivate_script, \"w\", encoding=\"utf8\") as afile:\n            afile.writelines(deactivate)\n\n\nclass Packages():\n    \"\"\" Holds information about installed and required packages.\n    Handles updating dependencies based on running platform/backend\n\n    Parameters\n    ----------\n    environment: :class:`Environment`\n        Environment class holding information about the running system\n    \"\"\"\n    def __init__(self, environment: Environment) -> None:\n        self._env = environment\n\n        # Default TK has bad fonts under Linux. There is a better build in Conda-Forge, so set\n        # channel accordingly\n        tk_channel = \"conda-forge\" if self._env.os_version[0].lower() == \"linux\" else \"default\"\n        self._conda_required_packages: list[tuple[list[str] | str, str]] = [(\"tk\", tk_channel),\n                                                                            (\"git\", \"default\")]\n        self._update_backend_specific_conda()\n        self._installed_packages = self._get_installed_packages()\n        self._conda_installed_packages = self._get_installed_conda_packages()\n        self._required_packages: list[tuple[str, list[tuple[str, str]]]] = []\n        self._missing_packages: list[tuple[str, list[tuple[str, str]]]] = []\n        self._conda_missing_packages: list[tuple[list[str] | str, str]] = []\n\n    @property\n    def prerequisites(self) -> list[tuple[str, list[tuple[str, str]]]]:\n        \"\"\" list: Any required packages that the installer needs prior to installing the faceswap\n        environment on the specific platform that are not already installed \"\"\"\n        all_installed = self._all_installed_packages\n        candidates = self._format_requirements(\n            [pkg for pkg, plat in _INSTALLER_REQUIREMENTS\n             if self._env.os_version[0] == plat or (plat[0] == \"!\" and\n                                                    self._env.os_version[0] != plat[1:])])\n        retval = [(pkg, spec) for pkg, spec in candidates\n                  if pkg not in all_installed or (\n                    pkg in all_installed and\n                    not self._validate_spec(spec, all_installed.get(pkg, \"\"))\n                  )]\n        return retval\n\n    @property\n    def packages_need_install(self) -> bool:\n        \"\"\"bool: ``True`` if there are packages available that need to be installed \"\"\"\n        return bool(self._missing_packages or self._conda_missing_packages)\n\n    @property\n    def to_install(self) -> list[tuple[str, list[tuple[str, str]]]]:\n        \"\"\" list: The required packages that need to be installed \"\"\"\n        return self._missing_packages\n\n    @property\n    def to_install_conda(self) -> list[tuple[list[str] | str, str]]:\n        \"\"\" list: The required conda packages that need to be installed \"\"\"\n        return self._conda_missing_packages\n\n    @property\n    def _all_installed_packages(self) -> dict[str, str]:\n        \"\"\" dict[str, str]: The package names and version string for all installed packages across\n        pip and conda \"\"\"\n        return {**self._installed_packages, **self._conda_installed_packages}\n\n    def _update_backend_specific_conda(self) -> None:\n        \"\"\" Add backend specific packages to Conda required packages \"\"\"\n        assert self._env.backend is not None\n        to_add = _BACKEND_SPECIFIC_CONDA.get(self._env.backend)\n        if not to_add:\n            logger.debug(\"No backend packages to add for '%s'. All optional packages: %s\",\n                         self._env.backend, _BACKEND_SPECIFIC_CONDA)\n            return\n\n        combined_cuda = []\n        for pkg in to_add:\n            pkg, channel = _CONDA_MAPPING.get(pkg, (pkg, \"\"))\n            if pkg == \"zlib-wapi\" and self._env.os_version[0].lower() != \"windows\":\n                # TODO move this front and center\n                continue\n            if pkg in (\"cudatoolkit\", \"cudnn\"):  # TODO Handle multiple cuda/cudnn requirements\n                idx = 0 if pkg == \"cudatoolkit\" else 1\n                pkg = f\"{pkg}{list(_TENSORFLOW_REQUIREMENTS.values())[0][idx]}\"\n\n                combined_cuda.append(pkg)\n                continue\n\n            self._conda_required_packages.append((pkg, channel))\n            logger.info(\"Adding conda required package '%s' for backend '%s')\",\n                        pkg, self._env.backend)\n\n        if combined_cuda:\n            self._conda_required_packages.append((combined_cuda, channel))\n            logger.info(\"Adding conda required package '%s' for backend '%s')\",\n                        combined_cuda, self._env.backend)\n\n    @classmethod\n    def _format_requirements(cls, packages: list[str]\n                             ) -> list[tuple[str, list[tuple[str, str]]]]:\n        \"\"\" Parse a list of requirements.txt formatted package strings to a list of pkgresource\n        formatted requirements \"\"\"\n        return [(package.unsafe_name, package.specs)\n                for package in parse_requirements(packages)\n                if package.marker is None or package.marker.evaluate()]\n\n    @classmethod\n    def _validate_spec(cls,\n                       required: list[tuple[str, str]],\n                       existing: str) -> bool:\n        \"\"\" Validate whether the required specification for a package is met by the installed\n        version.\n\n        required: list[tuple[str, str]]\n            The required package version spec to check\n        existing: str\n            The version of the installed package\n\n        Returns\n        -------\n        bool\n            ``True`` if the required specification is met by the existing specification\n        \"\"\"\n        ops = {\"==\": operator.eq, \">=\": operator.ge, \"<=\": operator.le,\n               \">\": operator.gt, \"<\": operator.lt}\n        if not required:\n            return True\n\n        return all(ops[spec[0]]([int(s) for s in existing.split(\".\")],\n                                [int(s) for s in spec[1].split(\".\")])\n                   for spec in required)\n\n    def _get_installed_packages(self) -> dict[str, str]:\n        \"\"\" Get currently installed packages and add to :attr:`_installed_packages`\n\n        Returns\n        -------\n        dict[str, str]\n            The installed package name and version string\n        \"\"\"\n        installed_packages = {}\n        with Popen(f\"\\\"{sys.executable}\\\" -m pip freeze --local\", shell=True, stdout=PIPE) as chk:\n            installed = chk.communicate()[0].decode(self._env.encoding,\n                                                    errors=\"ignore\").splitlines()\n\n        for pkg in installed:\n            if \"==\" not in pkg:\n                continue\n            item = pkg.split(\"==\")\n            installed_packages[item[0]] = item[1]\n        logger.debug(installed_packages)\n        return installed_packages\n\n    def _get_installed_conda_packages(self) -> dict[str, str]:\n        \"\"\" Get currently installed conda packages\n\n        Returns\n        -------\n        dict[str, str]\n            The installed package name and version string\n        \"\"\"\n        if not self._env.is_conda:\n            return {}\n        chk = os.popen(\"conda list\").read()\n        installed = [re.sub(\" +\", \" \", line.strip())\n                     for line in chk.splitlines() if not line.startswith(\"#\")]\n        retval = {}\n        for pkg in installed:\n            item = pkg.split(\" \")\n            retval[item[0]] = item[1]\n        logger.debug(retval)\n        return retval\n\n    def get_required_packages(self) -> None:\n        \"\"\" Load the requirements from the backend specific requirements list \"\"\"\n        req_files = [\"_requirements_base.txt\", f\"requirements_{self._env.backend}.txt\"]\n        pypath = os.path.dirname(os.path.realpath(__file__))\n        requirements = []\n        for req_file in req_files:\n            requirements_file = os.path.join(pypath, \"requirements\", req_file)\n            with open(requirements_file, encoding=\"utf8\") as req:\n                for package in req.readlines():\n                    package = package.strip()\n                    if package and (not package.startswith((\"#\", \"-r\"))):\n                        requirements.append(package)\n\n        self._required_packages = self._format_requirements(requirements)\n        logger.debug(self._required_packages)\n\n    def _update_tf_dep_nvidia(self) -> None:\n        \"\"\" Update the Tensorflow dependency for global Cuda installs \"\"\"\n        if self._env.is_conda:  # Conda handles Cuda and cuDNN so nothing to do here\n            return\n        tf_ver = None\n        cuda_inst = self._env.cuda_version\n        cudnn_inst = self._env.cudnn_version\n        if len(cudnn_inst) == 1:  # Sometimes only major version is reported\n            cudnn_inst = f\"{cudnn_inst}.0\"\n        for key, val in _TENSORFLOW_REQUIREMENTS.items():\n            cuda_req = next(parse_requirements(f\"cuda{val[0]}\")).specs\n            cudnn_req = next(parse_requirements(f\"cudnn{val[1]}\")).specs\n            if (self._validate_spec(cuda_req, cuda_inst)\n                    and self._validate_spec(cudnn_req, cudnn_inst)):\n                tf_ver = key\n                break\n\n        if tf_ver:\n            # Remove the version of tensorflow in requirements file and add the correct version\n            # that corresponds to the installed Cuda/cuDNN versions\n            self._required_packages = [pkg for pkg in self._required_packages\n                                       if pkg[0] != \"tensorflow\"]\n            tf_ver = f\"tensorflow{tf_ver}\"\n            self._required_packages.append((\"tensorflow\", next(parse_requirements(tf_ver)).specs))\n            return\n\n        logger.warning(\n            \"The minimum Tensorflow requirement is 2.10 \\n\"\n            \"Tensorflow currently has no official prebuild for your CUDA, cuDNN combination.\\n\"\n            \"Either install a combination that Tensorflow supports or build and install your own \"\n            \"tensorflow.\\r\\n\"\n            \"CUDA Version: %s\\r\\n\"\n            \"cuDNN Version: %s\\r\\n\"\n            \"Help:\\n\"\n            \"Building Tensorflow: https://www.tensorflow.org/install/install_sources\\r\\n\"\n            \"Tensorflow supported versions: \"\n            \"https://www.tensorflow.org/install/source#tested_build_configurations\",\n            self._env.cuda_version, self._env.cudnn_version)\n\n        custom_tf = input(\"Location of custom tensorflow wheel (leave blank to manually \"\n                          \"install): \")\n        if not custom_tf:\n            return\n\n        custom_tf = os.path.realpath(os.path.expanduser(custom_tf))\n        global _INSTALL_FAILED  # pylint:disable=global-statement\n        if not os.path.isfile(custom_tf):\n            logger.error(\"%s not found\", custom_tf)\n            _INSTALL_FAILED = True\n        elif os.path.splitext(custom_tf)[1] != \".whl\":\n            logger.error(\"%s is not a valid pip wheel\", custom_tf)\n            _INSTALL_FAILED = True\n        elif custom_tf:\n            self._required_packages.append((custom_tf, [(custom_tf, \"\")]))\n\n    def _update_tf_dep_rocm(self) -> None:\n        \"\"\" Update the Tensorflow dependency for global ROCm installs \"\"\"\n        if not any(self._env.rocm_version):  # ROCm was not found and the install will be aborted\n            return\n\n        global _INSTALL_FAILED  # pylint:disable=global-statement\n        candidates = [key for key, val in _TENSORFLOW_ROCM_REQUIREMENTS.items()\n                      if val[0] <= self._env.rocm_version <= val[1]]\n\n        if not candidates:\n            _INSTALL_FAILED = True\n            logger.error(\"No matching Tensorflow candidates found for ROCm %s in %s\",\n                         \".\".join(str(v) for v in self._env.rocm_version),\n                         _TENSORFLOW_ROCM_REQUIREMENTS)\n            return\n\n        # set tf_ver to the minimum and maximum compatible range\n        tf_ver = f\"{candidates[0].split(',')[0]},{candidates[-1].split(',')[-1]}\"\n        # Remove the version of tensorflow-rocm in requirements file and add the correct version\n        # that corresponds to the installed ROCm version\n        self._required_packages = [pkg for pkg in self._required_packages\n                                   if not pkg[0].startswith(\"tensorflow-rocm\")]\n        tf_ver = f\"tensorflow-rocm{tf_ver}\"\n        self._required_packages.append((\"tensorflow-rocm\",\n                                        next(parse_requirements(tf_ver)).specs))\n\n    def update_tf_dep(self) -> None:\n        \"\"\" Update Tensorflow Dependency.\n\n        Selects a compatible version of Tensorflow for a globally installed GPU library\n        \"\"\"\n        if self._env.backend == \"nvidia\":\n            self._update_tf_dep_nvidia()\n        if self._env.backend == \"rocm\":\n            self._update_tf_dep_rocm()\n\n    def _check_conda_missing_dependencies(self) -> None:\n        \"\"\" Check for conda missing dependencies and add to :attr:`_conda_missing_packages` \"\"\"\n        if not self._env.is_conda:\n            return\n        for pkg in self._conda_required_packages:\n            reqs = next(parse_requirements(pkg[0]))  # TODO Handle '=' vs '==' for conda\n            key = reqs.unsafe_name\n            specs = reqs.specs\n\n            if pkg[0] == \"tk\" and self._env.os_version[0].lower() == \"linux\":\n                # Default tk has bad fonts under Linux. We pull in an explicit build from\n                # Conda-Forge that is compiled with better fonts.\n                # Ref: https://github.com/ContinuumIO/anaconda-issues/issues/6833\n                newpkg = (f\"{pkg[0]}=*=xft_*\", pkg[1])  # Swap out package for explicit XFT version\n                self._conda_missing_packages.append(newpkg)\n                # We also need to bring in xorg-libxft incase libXft does not exist on host system\n                self._conda_missing_packages.append(_CONDA_MAPPING[\"xorg-libxft\"])\n                continue\n\n            if key not in self._conda_installed_packages:\n                self._conda_missing_packages.append(pkg)\n                continue\n\n            if not self._validate_spec(specs, self._conda_installed_packages[key]):\n                self._conda_missing_packages.append(pkg)\n        logger.debug(self._conda_missing_packages)\n\n    def check_missing_dependencies(self) -> None:\n        \"\"\" Check for missing dependencies and add to :attr:`_missing_packages` \"\"\"\n        for key, specs in self._required_packages:\n\n            if self._env.is_conda:  # Get Conda alias for Key\n                key = _CONDA_MAPPING.get(key, (key, None))[0]\n\n            if key not in self._all_installed_packages:\n                # Add not installed packages to missing packages list\n                self._missing_packages.append((key, specs))\n                continue\n\n            if not self._validate_spec(specs, self._all_installed_packages.get(key, \"\")):\n                self._missing_packages.append((key, specs))\n\n        logger.debug(self._missing_packages)\n        self._check_conda_missing_dependencies()\n\n\nclass Checks():  # pylint:disable=too-few-public-methods\n    \"\"\" Pre-installation checks\n\n    Parameters\n    ----------\n    environment: :class:`Environment`\n        Environment class holding information about the running system\n    \"\"\"\n    def __init__(self, environment: Environment) -> None:\n        self._env:  Environment = environment\n        self._tips: Tips = Tips()\n    # Checks not required for installer\n        if self._env.is_installer:\n            return\n    # Checks not required for Apple Silicon\n        if self._env.backend == \"apple_silicon\":\n            return\n        self._user_input()\n        self._check_cuda()\n        self._check_rocm()\n        if self._env.os_version[0] == \"Windows\":\n            self._tips.pip()\n\n    def _rocm_ask_enable(self) -> None:\n        \"\"\" Set backend to 'rocm' if OS is Linux and ROCm support required \"\"\"\n        if self._env.os_version[0] != \"Linux\":\n            return\n        logger.info(\"ROCm support:\\r\\nIf you are using an AMD GPU, then select 'yes'.\"\n                    \"\\r\\nCPU/non-AMD GPU users should answer 'no'.\\r\\n\")\n        i = input(\"Enable ROCm Support? [y/N] \")\n        if i in (\"Y\", \"y\"):\n            logger.info(\"ROCm Support Enabled\")\n            self._env.backend = \"rocm\"\n\n    def _directml_ask_enable(self) -> None:\n        \"\"\" Set backend to 'directml' if OS is Windows and DirectML support required \"\"\"\n        if self._env.os_version[0] != \"Windows\":\n            return\n        logger.info(\"DirectML support:\\r\\nIf you are using an AMD or Intel GPU, then select 'yes'.\"\n                    \"\\r\\nNvidia users should answer 'no'.\")\n        i = input(\"Enable DirectML Support? [y/N] \")\n        if i in (\"Y\", \"y\"):\n            logger.info(\"DirectML Support Enabled\")\n            self._env.backend = \"directml\"\n\n    def _user_input(self) -> None:\n        \"\"\" Get user input for AMD/DirectML/ROCm/Cuda/Docker \"\"\"\n        self._directml_ask_enable()\n        self._rocm_ask_enable()\n        if not self._env.backend:\n            self._docker_ask_enable()\n            self._cuda_ask_enable()\n        if self._env.os_version[0] != \"Linux\" and (self._env.enable_docker\n                                                   and self._env.backend == \"nvidia\"):\n            self._docker_confirm()\n        if self._env.enable_docker:\n            self._docker_tips()\n            self._env.set_config()\n            sys.exit(0)\n\n    def _docker_ask_enable(self) -> None:\n        \"\"\" Enable or disable Docker \"\"\"\n        i = input(\"Enable  Docker? [y/N] \")\n        if i in (\"Y\", \"y\"):\n            logger.info(\"Docker Enabled\")\n            self._env.enable_docker = True\n        else:\n            logger.info(\"Docker Disabled\")\n            self._env.enable_docker = False\n\n    def _docker_confirm(self) -> None:\n        \"\"\" Warn if nvidia-docker on non-Linux system \"\"\"\n        logger.warning(\"Nvidia-Docker is only supported on Linux.\\r\\n\"\n                       \"Only CPU is supported in Docker for your system\")\n        self._docker_ask_enable()\n        if self._env.enable_docker:\n            logger.warning(\"CUDA Disabled\")\n            self._env.backend = \"cpu\"\n\n    def _docker_tips(self) -> None:\n        \"\"\" Provide tips for Docker use \"\"\"\n        if self._env.backend != \"nvidia\":\n            self._tips.docker_no_cuda()\n        else:\n            self._tips.docker_cuda()\n\n    def _cuda_ask_enable(self) -> None:\n        \"\"\" Enable or disable CUDA \"\"\"\n        i = input(\"Enable  CUDA? [Y/n] \")\n        if i in (\"\", \"Y\", \"y\"):\n            logger.info(\"CUDA Enabled\")\n            self._env.backend = \"nvidia\"\n\n    def _check_cuda(self) -> None:\n        \"\"\" Check for Cuda and cuDNN Locations. \"\"\"\n        if self._env.backend != \"nvidia\":\n            logger.debug(\"Skipping Cuda checks as not enabled\")\n            return\n\n        if self._env.is_conda:\n            logger.info(\"Skipping Cuda/cuDNN checks for Conda install\")\n            return\n\n        if self._env.os_version[0] in (\"Linux\", \"Windows\"):\n            global _INSTALL_FAILED  # pylint:disable=global-statement\n            check = CudaCheck()\n            if check.cuda_version:\n                self._env.cuda_cudnn[0] = check.cuda_version\n                logger.info(\"CUDA version: %s\", self._env.cuda_version)\n            else:\n                logger.error(\"CUDA not found. Install and try again.\\n\"\n                             \"Recommended version:      CUDA 10.1     cuDNN 7.6\\n\"\n                             \"CUDA: https://developer.nvidia.com/cuda-downloads\\n\"\n                             \"cuDNN: https://developer.nvidia.com/rdp/cudnn-download\")\n                _INSTALL_FAILED = True\n                return\n\n            if check.cudnn_version:\n                self._env.cuda_cudnn[1] = \".\".join(check.cudnn_version.split(\".\")[:2])\n                logger.info(\"cuDNN version: %s\", self._env.cudnn_version)\n            else:\n                logger.error(\"cuDNN not found. See \"\n                             \"https://github.com/deepfakes/faceswap/blob/master/INSTALL.md#\"\n                             \"cudnn for instructions\")\n                _INSTALL_FAILED = True\n            return\n\n        # If we get here we're on MacOS\n        self._tips.macos()\n        logger.warning(\"Cannot find CUDA on macOS\")\n        self._env.cuda_cudnn[0] = input(\"Manually specify CUDA version: \")\n\n    def _check_rocm(self) -> None:\n        \"\"\" Check for ROCm version \"\"\"\n        if self._env.backend != \"rocm\" or self._env.os_version[0] != \"Linux\":\n            logger.info(\"Skipping ROCm checks as not enabled\")\n            return\n\n        global _INSTALL_FAILED  # pylint:disable=global-statement\n        check = ROCmCheck()\n\n        str_min = \".\".join(str(v) for v in check.version_min)\n        str_max = \".\".join(str(v) for v in check.version_max)\n\n        if check.is_valid:\n            self._env.rocm_version = check.rocm_version\n            logger.info(\"ROCm version: %s\", \".\".join(str(v) for v in self._env.rocm_version))\n        else:\n            if check.rocm_version:\n                msg = f\"Incompatible ROCm version: {'.'.join(str(v) for v in check.rocm_version)}\"\n            else:\n                msg = \"ROCm not found\"\n            logger.error(\"%s.\\n\"\n                         \"A compatible version of ROCm must be installed to proceed.\\n\"\n                         \"ROCm versions between %s and %s are supported.\\n\"\n                         \"ROCm install guide: https://docs.amd.com/bundle/ROCm_Installation_Guide\"\n                         \"v5.0/page/Overview_of_ROCm_Installation_Methods.html\",\n                         msg,\n                         str_min,\n                         str_max)\n            _INSTALL_FAILED = True\n\n\ndef _check_ld_config(lib: str) -> str:\n    \"\"\" Locate a library in ldconfig\n\n    Parameters\n    ----------\n    lib: str The library to locate\n\n    Returns\n    -------\n    str\n        The library from ldconfig, or empty string if not found\n    \"\"\"\n    retval = \"\"\n    ldconfig = which(\"ldconfig\")\n    if not ldconfig:\n        return retval\n\n    retval = next((line.decode(\"utf-8\", errors=\"replace\").strip()\n                  for line in run([ldconfig, \"-p\"],\n                                  capture_output=True,\n                                  check=False).stdout.splitlines()\n                  if lib.encode(\"utf-8\") in line), \"\")\n\n    if retval or (not retval and not os.environ.get(\"LD_LIBRARY_PATH\")):\n        return retval\n\n    for path in os.environ[\"LD_LIBRARY_PATH\"].split(\":\"):\n        if not path or not os.path.exists(path):\n            continue\n\n        retval = next((fname.strip() for fname in reversed(os.listdir(path))\n                       if lib in fname), \"\")\n        if retval:\n            break\n\n    return retval\n\n\nclass ROCmCheck():  # pylint:disable=too-few-public-methods\n    \"\"\" Find the location of system installed ROCm on Linux \"\"\"\n    def __init__(self) -> None:\n        self.version_min = min(v[0] for v in _TENSORFLOW_ROCM_REQUIREMENTS.values())\n        self.version_max = max(v[1] for v in _TENSORFLOW_ROCM_REQUIREMENTS.values())\n        self.rocm_version: tuple[int, ...] = (0, 0, 0)\n        if platform.system() == \"Linux\":\n            self._rocm_check()\n\n    @property\n    def is_valid(self):\n        \"\"\" bool: `True` if ROCm has been detected and is between the minimum and maximum\n        compatible versions otherwise ``False`` \"\"\"\n        return self.version_min <= self.rocm_version <= self.version_max\n\n    def _rocm_check(self) -> None:\n        \"\"\" Attempt to locate the installed ROCm version from the dynamic link loader. If not found\n        with ldconfig then attempt to find it in LD_LIBRARY_PATH. If found, set the\n        :attr:`rocm_version` to the discovered version\n        \"\"\"\n        chk = _check_ld_config(\"librocm-core.so.\")\n        if not chk:\n            return\n\n        rocm_vers = chk.strip()\n        version = re.search(r\"rocm\\-(\\d+\\.\\d+\\.\\d+)\", rocm_vers)\n        if version is None:\n            return\n        try:\n            self.rocm_version = tuple(int(v) for v in version.groups()[0].split(\".\"))\n        except ValueError:\n            return\n\n\nclass CudaCheck():  # pylint:disable=too-few-public-methods\n    \"\"\" Find the location of system installed Cuda and cuDNN on Windows and Linux. \"\"\"\n\n    def __init__(self) -> None:\n        self.cuda_path: str | None = None\n        self.cuda_version: str | None = None\n        self.cudnn_version: str | None = None\n\n        self._os: str = platform.system().lower()\n        self._cuda_keys: list[str] = [key\n                                      for key in os.environ\n                                      if key.lower().startswith(\"cuda_path_v\")]\n        self._cudnn_header_files: list[str] = [\"cudnn_version.h\", \"cudnn.h\"]\n        logger.debug(\"cuda keys: %s, cudnn header files: %s\",\n                     self._cuda_keys, self._cudnn_header_files)\n        if self._os in (\"windows\", \"linux\"):\n            self._cuda_check()\n            self._cudnn_check()\n\n    def _cuda_check(self) -> None:\n        \"\"\" Obtain the location and version of Cuda and populate :attr:`cuda_version` and\n        :attr:`cuda_path`\n\n        Initially just calls `nvcc -V` to get the installed version of Cuda currently in use.\n        If this fails, drills down to more OS specific checking methods.\n        \"\"\"\n        with Popen(\"nvcc -V\", shell=True, stdout=PIPE, stderr=PIPE) as chk:\n            stdout, stderr = chk.communicate()\n        if not stderr:\n            version = re.search(r\".*release (?P<cuda>\\d+\\.\\d+)\",\n                                stdout.decode(locale.getpreferredencoding(), errors=\"ignore\"))\n            if version is not None:\n                self.cuda_version = version.groupdict().get(\"cuda\", None)\n            path = which(\"nvcc\")\n            if path:\n                path = path.split(\"\\n\")[0]  # Split multiple entries and take first found\n                while True:  # Get Cuda root folder\n                    path, split = os.path.split(path)\n                    if split == \"bin\":\n                        break\n                self.cuda_path = path\n            return\n\n        # Failed to load nvcc, manual check\n        getattr(self, f\"_cuda_check_{self._os}\")()\n        logger.debug(\"Cuda Version: %s, Cuda Path: %s\", self.cuda_version, self.cuda_path)\n\n    def _cuda_check_linux(self) -> None:\n        \"\"\" For Linux check the dynamic link loader for libcudart. If not found with ldconfig then\n        attempt to find it in LD_LIBRARY_PATH. \"\"\"\n        chk = _check_ld_config(\"libcudart.so.\")\n        if not chk:  # Cuda not found\n            return\n\n        cudavers = chk.strip().replace(\"libcudart.so.\", \"\")\n        self.cuda_version = cudavers[:cudavers.find(\" \")] if \" \" in cudavers else cudavers\n        cuda_path = chk[chk.find(\"=>\") + 3:chk.find(\"targets\") - 1]\n        if os.path.exists(cuda_path):\n            self.cuda_path = cuda_path\n\n    def _cuda_check_windows(self) -> None:\n        \"\"\" Check Windows CUDA Version and path from Environment Variables\"\"\"\n        if not self._cuda_keys:  # Cuda environment variable not found\n            return\n        self.cuda_version = self._cuda_keys[0].lower().replace(\"cuda_path_v\", \"\").replace(\"_\", \".\")\n        self.cuda_path = os.environ[self._cuda_keys[0][0]]\n\n    def _cudnn_check_files(self) -> bool:\n        \"\"\" Check header files for cuDNN version \"\"\"\n        cudnn_checkfiles = getattr(self, f\"_get_checkfiles_{self._os}\")()\n        cudnn_checkfile = next((hdr for hdr in cudnn_checkfiles if os.path.isfile(hdr)), None)\n        logger.debug(\"cudnn checkfiles: %s\", cudnn_checkfile)\n        if not cudnn_checkfile:\n            return False\n\n        found = 0\n        with open(cudnn_checkfile, \"r\", encoding=\"utf8\") as ofile:\n            for line in ofile:\n                if line.lower().startswith(\"#define cudnn_major\"):\n                    major = line[line.rfind(\" \") + 1:].strip()\n                    found += 1\n                elif line.lower().startswith(\"#define cudnn_minor\"):\n                    minor = line[line.rfind(\" \") + 1:].strip()\n                    found += 1\n                elif line.lower().startswith(\"#define cudnn_patchlevel\"):\n                    patchlevel = line[line.rfind(\" \") + 1:].strip()\n                    found += 1\n                if found == 3:\n                    break\n        if found != 3:  # Full version not determined\n            return False\n\n        self.cudnn_version = \".\".join([str(major), str(minor), str(patchlevel)])\n        logger.debug(\"cudnn version: %s\", self.cudnn_version)\n        return True\n\n    def _cudnn_check(self) -> None:\n        \"\"\" Check Linux or Windows cuDNN Version from cudnn.h and add to :attr:`cudnn_version`. \"\"\"\n        if self._cudnn_check_files():\n            return\n        if self._os == \"windows\":\n            return\n\n        chk = _check_ld_config(\"libcudnn.so.\")\n        if not chk:\n            return\n        cudnnvers = chk.strip().replace(\"libcudnn.so.\", \"\").split()[0]\n        if not cudnnvers:\n            return\n\n        self.cudnn_version = cudnnvers\n        logger.debug(\"cudnn version: %s\", self.cudnn_version)\n\n    def _get_checkfiles_linux(self) -> list[str]:\n        \"\"\" Return the the files to check for cuDNN locations for Linux by querying\n        the dynamic link loader.\n\n        Returns\n        -------\n        list\n            List of header file locations to scan for cuDNN versions\n        \"\"\"\n        chk = _check_ld_config(\"libcudnn.so.\")\n        chk = chk.strip().replace(\"libcudnn.so.\", \"\")\n        if not chk:\n            return []\n\n        cudnn_vers = chk[0]\n        header_files = [f\"cudnn_v{cudnn_vers}.h\"] + self._cudnn_header_files\n\n        cudnn_path = os.path.realpath(chk[chk.find(\"=>\") + 3:chk.find(\"libcudnn\") - 1])\n        cudnn_path = cudnn_path.replace(\"lib\", \"include\")\n        cudnn_checkfiles = [os.path.join(cudnn_path, header) for header in header_files]\n        return cudnn_checkfiles\n\n    def _get_checkfiles_windows(self) -> list[str]:\n        \"\"\" Return the check-file locations for Windows. Just looks inside the include folder of\n        the discovered :attr:`cuda_path`\n\n        Returns\n        -------\n        list\n            List of header file locations to scan for cuDNN versions\n        \"\"\"\n        # TODO A more reliable way of getting the windows location\n        if not self.cuda_path or not os.path.exists(self.cuda_path):\n            return []\n        scandir = os.path.join(self.cuda_path, \"include\")\n        cudnn_checkfiles = [os.path.join(scandir, header) for header in self._cudnn_header_files]\n        return cudnn_checkfiles\n\n\nclass Install():  # pylint:disable=too-few-public-methods\n    \"\"\" Handles installation of Faceswap requirements\n\n    Parameters\n    ----------\n    environment: :class:`Environment`\n        Environment class holding information about the running system\n    is_gui: bool, Optional\n        ``True`` if the caller is the Faceswap GUI. Used to prevent output of progress bars\n        which get scrambled in the GUI\n     \"\"\"\n    def __init__(self, environment: Environment, is_gui: bool = False) -> None:\n        self._env = environment\n        self._packages = Packages(environment)\n        self._is_gui = is_gui\n\n        if self._env.os_version[0] == \"Windows\":\n            self._installer: type[Installer] = WinPTYInstaller\n        else:\n            self._installer = PexpectInstaller\n\n        if not self._env.is_installer and not self._env.updater:\n            self._ask_continue()\n\n        self._packages.get_required_packages()\n        self._packages.update_tf_dep()\n        self._packages.check_missing_dependencies()\n\n        if self._env.updater and not self._packages.packages_need_install:\n            logger.info(\"All Dependencies are up to date\")\n            return\n\n        logger.info(\"Installing Required Python Packages. This may take some time...\")\n        self._install_setup_packages()\n        self._install_missing_dep()\n        if self._env.updater:\n            return\n        if not _INSTALL_FAILED:\n            logger.info(\"All python3 dependencies are met.\\r\\nYou are good to go.\\r\\n\\r\\n\"\n                        \"Enter:  'python faceswap.py -h' to see the options\\r\\n\"\n                        \"        'python faceswap.py gui' to launch the GUI\")\n        else:\n            logger.error(\"Some packages failed to install. This may be a temporary error which \"\n                         \"might be fixed by re-running this script. Otherwise please install \"\n                         \"these packages manually.\")\n            sys.exit(1)\n\n    def _ask_continue(self) -> None:\n        \"\"\" Ask Continue with Install \"\"\"\n        text = \"Please ensure your System Dependencies are met\"\n        if self._env.backend == \"rocm\":\n            text += (\"\\r\\nROCm users: Please ensure that your AMD GPU is supported by the \"\n                     \"installed ROCm version before proceeding.\")\n        text += \"\\r\\nContinue? [y/N] \"\n        inp = input(text)\n        if inp in (\"\", \"N\", \"n\"):\n            logger.error(\"Please install system dependencies to continue\")\n            sys.exit(1)\n\n    @classmethod\n    def _format_package(cls, package: str, version: list[tuple[str, str]]) -> str:\n        \"\"\" Format a parsed requirement package and version string to a format that can be used by\n        the installer.\n\n        Parameters\n        ----------\n        package: str\n            The package name\n        version: list\n            The parsed requirement version strings\n\n        Returns\n        -------\n        str\n            The formatted full package and version string\n        \"\"\"\n        return f\"{package}{','.join(''.join(spec) for spec in version)}\"\n\n    def _install_setup_packages(self) -> None:\n        \"\"\" Install any packages that are required for the setup.py installer to work. This\n        includes the pexpect package if it is not already installed.\n\n        Subprocess is used as we do not currently have pexpect\n        \"\"\"\n        for pkg in self._packages.prerequisites:\n            pkg_str = self._format_package(*pkg)\n            if self._env.is_conda:\n                cmd = [\"conda\", \"install\", \"-y\"]\n                if any(char in pkg_str for char in (\" \", \"<\", \">\", \"*\", \"|\")):\n                    pkg_str = f\"\\\"{pkg_str}\\\"\"\n            else:\n                cmd = [sys.executable, \"-m\", \"pip\", \"install\", \"--no-cache-dir\"]\n                if self._env.is_admin:\n                    cmd.append(\"--user\")\n            cmd.append(pkg_str)\n\n            clean_pkg = pkg_str.replace(\"\\\"\", \"\")\n            installer = SubProcInstaller(self._env, clean_pkg, cmd, self._is_gui)\n            if installer() != 0:\n                logger.error(\"Unable to install package: %s. Process aborted\", clean_pkg)\n                sys.exit(1)\n\n    def _install_conda_packages(self) -> None:\n        \"\"\" Install required conda packages \"\"\"\n        logger.info(\"Installing Required Conda Packages. This may take some time...\")\n        for pkg in self._packages.to_install_conda:\n            channel = \"\" if len(pkg) != 2 else pkg[1]\n            self._from_conda(pkg[0], channel=channel, conda_only=True)\n\n    def _install_python_packages(self) -> None:\n        \"\"\" Install required pip packages \"\"\"\n        conda_only = False\n        assert self._env.backend is not None\n        for pkg, version in self._packages.to_install:\n            if self._env.is_conda:\n                mapping = _CONDA_MAPPING.get(pkg, (pkg, \"\"))\n                channel = \"\" if mapping[1] is None else mapping[1]\n                pkg = mapping[0]\n                pip_only = pkg in _FORCE_PIP.get(self._env.backend, []) or pkg in _FORCE_PIP[\"all\"]\n            pkg = self._format_package(pkg, version) if version else pkg\n            if self._env.is_conda and not pip_only:\n                if self._from_conda(pkg, channel=channel, conda_only=conda_only):\n                    continue\n            self._from_pip(pkg)\n\n    def _install_missing_dep(self) -> None:\n        \"\"\" Install missing dependencies \"\"\"\n        self._install_conda_packages()  # Install conda packages first\n        self._install_python_packages()\n\n    def _from_conda(self,\n                    package: list[str] | str,\n                    channel: str = \"\",\n                    conda_only: bool = False) -> bool:\n        \"\"\" Install a conda package\n\n        Parameters\n        ----------\n        package: list[str] | str\n            The full formatted package(s), with version(s), to be installed\n        channel: str, optional\n            The Conda channel to install from. Select empty string for default channel.\n            Default: ``\"\"`` (empty string)\n        conda_only: bool, optional\n            ``True`` if the package is only available in Conda. Default: ``False``\n\n        Returns\n        -------\n        bool\n            ``True`` if the package was succesfully installed otherwise ``False``\n        \"\"\"\n        #  Packages with special characters need to be enclosed in double quotes\n        success = True\n        condaexe = [\"conda\", \"install\", \"-y\"]\n        if channel:\n            condaexe.extend([\"-c\", channel])\n\n        pkgs = package if isinstance(package, list) else [package]\n\n        for i, pkg in enumerate(pkgs):\n            if any(char in pkg for char in (\" \", \"<\", \">\", \"*\", \"|\")):\n                pkgs[i] = f\"\\\"{pkg}\\\"\"\n        condaexe.extend(pkgs)\n\n        clean_pkg = \" \".join([p.replace(\"\\\"\", \"\") for p in pkgs])\n        installer = self._installer(self._env, clean_pkg, condaexe, self._is_gui)\n        retcode = installer()\n\n        if retcode != 0 and not conda_only:\n            logger.info(\"%s not available in Conda. Installing with pip\", package)\n        elif retcode != 0:\n            logger.warning(\"Couldn't install %s with Conda. Please install this package \"\n                           \"manually\", package)\n        success = retcode == 0 and success\n        return success\n\n    def _from_pip(self, package: str) -> None:\n        \"\"\" Install a pip package\n\n        Parameters\n        ----------\n        package: str\n            The full formatted package, with version, to be installed\n        \"\"\"\n        pipexe = [sys.executable, \"-u\", \"-m\", \"pip\", \"install\", \"--no-cache-dir\"]\n        # install as user to solve perm restriction\n        if not self._env.is_admin and not self._env.is_virtualenv:\n            pipexe.append(\"--user\")\n        pipexe.append(package)\n\n        installer = self._installer(self._env, package, pipexe, self._is_gui)\n        if installer() != 0:\n            logger.warning(\"Couldn't install %s with pip. Please install this package manually\",\n                           package)\n            global _INSTALL_FAILED  # pylint:disable=global-statement\n            _INSTALL_FAILED = True\n\n\nclass ProgressBar():\n    \"\"\" Simple progress bar using STDLib for intercepting Conda installs and keeping the\n    terminal from getting jumbled \"\"\"\n    def __init__(self):\n        self._width_desc = 21\n        self._width_size = 9\n        self._width_bar = 35\n        self._width_pct = 4\n        self._marker = \"\u2588\"\n\n        self._cursor_visible = True\n        self._current_pos = 0\n        self._bars = []\n\n    @classmethod\n    def _display_cursor(cls, visible: bool) -> None:\n        \"\"\" Sends ANSI code to display or hide the cursor\n\n        Parameters\n        ----------\n        visible: bool\n            ``True`` to display the cursor. ``False`` to hide the cursor\n        \"\"\"\n        code = \"\\x1b[?25h\" if visible else \"\\x1b[?25l\"\n        print(code, end=\"\\r\")\n\n    def _format_bar(self, description: str, size: str, percent: int) -> str:\n        \"\"\" Format the progress bar for display\n\n        Parameters\n        ----------\n        description: str\n            The description to display for the progress bar\n        size: str\n            The size of the download, including units\n        percent: int\n            The percentage progress of the bar\n        \"\"\"\n        size = size[:self._width_size].ljust(self._width_size)\n        bar_len = int(self._width_bar * (percent / 100))\n        progress = f\"{self._marker * bar_len}\"[:self._width_bar].ljust(self._width_bar)\n        pct = f\"{percent}%\"[:self._width_pct].rjust(self._width_pct)\n        return f\"  {description}| {size} | {progress} | {pct}\"\n\n    def _move_cursor(self, position: int) -> str:\n        \"\"\" Generate ANSI code for moving the cursor to the given progress bar's position\n\n        Parameters\n        ----------\n        position: int\n            The progress bar position to move to\n\n        Returns\n        -------\n        str\n            The ansi code to move to the given position\n        \"\"\"\n        move = position - self._current_pos\n        retval = \"\\x1b[A\" if move < 0 else \"\\x1b[B\" if move > 0 else \"\"\n        retval *= abs(move)\n        return retval\n\n    def __call__(self, description: str, size: str, percent: int) -> None:\n        \"\"\" Create or update a progress bar\n\n        Parameters\n        ----------\n        description: str\n            The description to display for the progress bar\n        size: str\n            The size of the download, including units\n        percent: int\n            The percentage progress of the bar\n        \"\"\"\n        if self._cursor_visible:\n            self._display_cursor(visible=False)\n\n        desc = description[:self._width_desc].ljust(self._width_desc)\n        if desc not in self._bars:\n            self._bars.append(desc)\n\n        position = self._bars.index(desc)\n        pbar = self._format_bar(desc, size, percent)\n\n        output = f\"{self._move_cursor(position)} {pbar}\"\n\n        print(output)\n        self._current_pos = position + 1\n\n    def close(self) -> None:\n        \"\"\" Reset all progress bars and re-enable the cursor \"\"\"\n        print(self._move_cursor(len(self._bars)), end=\"\\r\")\n        self._display_cursor(True)\n        self._cursor_visible = True\n        self._current_pos = 0\n        self._bars = []\n\n\nclass Installer():\n    \"\"\" Parent class for package installers.\n\n    PyWinPty is used for Windows, Pexpect is used for Linux, as these can provide us with realtime\n    output.\n\n    Subprocess is used as a fallback if any of the above fail, but this caches output, so it can\n    look like the process has hung to the end user\n\n    Parameters\n    ----------\n    environment: :class:`Environment`\n        Environment class holding information about the running system\n    package: str\n        The package name that is being installed\n    command: list\n        The command to run\n    is_gui: bool\n        ``True`` if the process is being called from the Faceswap GUI\n    \"\"\"\n    def __init__(self,\n                 environment: Environment,\n                 package: str,\n                 command: list[str],\n                 is_gui: bool) -> None:\n        logger.info(\"Installing %s\", package)\n        logger.debug(\"argv: %s\", command)\n        self._env = environment\n        self._package = package\n        self._command = command\n        self._is_conda = \"conda\" in command\n        self._is_gui = is_gui\n\n        self._progess_bar = ProgressBar()\n        self._re_conda = re.compile(\n            rb\"(?P<lib>^\\S+)\\s+\\|\\s+(?P<tot>\\d+\\.?\\d*\\s\\w+).*\\|\\s+(?P<prg>\\d+%)\")\n        self._re_pip_pkg = re.compile(rb\"^\\s*Downloading\\s(?P<lib>\\w+-.+?)-\")\n        self._re_pip = re.compile(rb\"(?P<done>\\d+\\.?\\d*)/(?P<tot>\\d+\\.?\\d*\\s\\w+)\")\n        self._pip_pkg = \"\"\n        self._seen_lines: set[str] = set()\n\n    def __call__(self) -> int:\n        \"\"\" Call the subclassed call function\n\n        Returns\n        -------\n        int\n            The return code of the package install process\n        \"\"\"\n        try:\n            returncode = self.call()\n        except Exception as err:  # pylint:disable=broad-except\n            logger.debug(\"Failed to install with %s. Falling back to subprocess. Error: %s\",\n                         self.__class__.__name__, str(err))\n            self._progess_bar.close()\n            returncode = SubProcInstaller(self._env, self._package, self._command, self._is_gui)()\n\n        logger.debug(\"Package: %s, returncode: %s\", self._package, returncode)\n        self._progess_bar.close()\n        return returncode\n\n    def call(self) -> int:\n        \"\"\" Override for package installer specific logic.\n\n        Returns\n        -------\n        int\n            The return code of the package install process\n        \"\"\"\n        raise NotImplementedError()\n\n    def _print_conda(self, text: bytes) -> None:\n        \"\"\" Output progress for Conda installs\n\n        Parameters\n        ----------\n        text: bytes\n            The text to print\n        \"\"\"\n        data = self._re_conda.match(text)\n        if not data:\n            return\n        lib = data.groupdict()[\"lib\"].decode(\"utf-8\", errors=\"replace\")\n        size = data.groupdict()[\"tot\"].decode(\"utf-8\", errors=\"replace\")\n        progress = int(data.groupdict()[\"prg\"].decode(\"utf-8\", errors=\"replace\")[:-1])\n        self._progess_bar(lib, size, progress)\n\n    def _print_pip(self, text: bytes) -> None:\n        \"\"\" Output progress for Pip installs\n\n        Parameters\n        ----------\n        text: bytes\n            The text to print\n        \"\"\"\n        pkg = self._re_pip_pkg.match(text)\n        if pkg:\n            logger.debug(\"Collected pip package '%s'\", pkg)\n            self._pip_pkg = pkg.groupdict()[\"lib\"].decode(\"utf-8\", errors=\"replace\")\n            return\n        data = self._re_pip.search(text)\n        if not data:\n            return\n        done = float(data.groupdict()[\"done\"].decode(\"utf-8\", errors=\"replace\"))\n        size = data.groupdict()[\"tot\"].decode(\"utf-8\", errors=\"replace\")\n        progress = int(round(done / float(size.split()[0]) * 100, 0))\n        self._progess_bar(self._pip_pkg, size, progress)\n\n    def _non_gui_print(self, text: bytes) -> None:\n        \"\"\" Print output to console if not running in the GUI\n\n        Parameters\n        ----------\n        text: bytes\n            The text to print\n        \"\"\"\n        if self._is_gui:\n            return\n        if self._is_conda:\n            self._print_conda(text)\n        else:\n            self._print_pip(text)\n\n    def _seen_line_log(self, text: str) -> None:\n        \"\"\" Output gets spammed to the log file when conda is waiting/processing. Only log each\n        unique line once.\n\n        Parameters\n        ----------\n        text: str\n            The text to log\n        \"\"\"\n        if text in self._seen_lines:\n            return\n        logger.debug(text)\n        self._seen_lines.add(text)\n\n\nclass PexpectInstaller(Installer):  # pylint:disable=too-few-public-methods\n    \"\"\" Package installer for Linux/macOS using Pexpect\n\n    Uses Pexpect for installing packages allowing access to realtime feedback\n\n    Parameters\n    ----------\n    environment: :class:`Environment`\n        Environment class holding information about the running system\n    package: str\n        The package name that is being installed\n    command: list\n        The command to run\n    is_gui: bool\n        ``True`` if the process is being called from the Faceswap GUI\n    \"\"\"\n    def call(self) -> int:\n        \"\"\" Install a package using the Pexpect module\n\n        Returns\n        -------\n        int\n            The return code of the package install process\n        \"\"\"\n        import pexpect  # pylint:disable=import-outside-toplevel,import-error\n        proc = pexpect.spawn(\" \".join(self._command), timeout=None)\n        while True:\n            try:\n                proc.expect([b\"\\r\\n\", b\"\\r\"])\n                line: bytes = proc.before\n                self._seen_line_log(line.decode(\"utf-8\", errors=\"replace\").rstrip())\n                self._non_gui_print(line)\n            except pexpect.EOF:\n                break\n        proc.close()\n        return proc.exitstatus\n\n\nclass WinPTYInstaller(Installer):  # pylint:disable=too-few-public-methods\n    \"\"\" Package installer for Windows using WinPTY\n\n    Spawns a pseudo PTY for installing packages allowing access to realtime feedback\n\n    Parameters\n    ----------\n    environment: :class:`Environment`\n        Environment class holding information about the running system\n    package: str\n        The package name that is being installed\n    command: list\n        The command to run\n    is_gui: bool\n        ``True`` if the process is being called from the Faceswap GUI\n    \"\"\"\n    def __init__(self,\n                 environment: Environment,\n                 package: str,\n                 command: list[str],\n                 is_gui: bool) -> None:\n        super().__init__(environment, package, command, is_gui)\n        self._cmd = which(command[0], path=os.environ.get('PATH', os.defpath))\n        self._cmdline = list2cmdline(command)\n        logger.debug(\"cmd: '%s', cmdline: '%s'\", self._cmd, self._cmdline)\n\n        self._pbar = re.compile(r\"(?:eta\\s[\\d\\W]+)|(?:\\s+\\|\\s+\\d+%)\\Z\")\n        self._eof = False\n        self._read_bytes = 1024\n\n        self._lines: list[str] = []\n        self._out = \"\"\n\n    def _read_from_pty(self, proc: T.Any, winpty_error: T.Any) -> None:\n        \"\"\" Read :attr:`_num_bytes` from WinPTY. If there is an error reading, recursively halve\n        the number of bytes read until we get a succesful read. If we get down to 1 byte without a\n        succesful read, assume we are at EOF.\n\n        Parameters\n        ----------\n        proc: :class:`winpty.PTY`\n            The WinPTY process\n        winpty_error: :class:`winpty.WinptyError`\n            The winpty error exception. Passed in as WinPTY is not in global scope\n        \"\"\"\n        try:\n            from_pty = proc.read(self._read_bytes)\n        except winpty_error:\n            # TODO Reinsert this check\n            # The error message \"pipe has been ended\" is language specific so this check\n            # fails on non english systems. For now we just swallow all errors until no\n            # bytes are left to read and then check the return code\n            # if any(val in str(err) for val in [\"EOF\", \"pipe has been ended\"]):\n            #    # Get remaining bytes. On a comms error, the buffer remains unread so keep\n            #    # halving buffer amount until down to 1 when we know we have everything\n            #     if self._read_bytes == 1:\n            #         self._eof = True\n            #     from_pty = \"\"\n            #     self._read_bytes //= 2\n            # else:\n            #     raise\n\n            # Get remaining bytes. On a comms error, the buffer remains unread so keep\n            # halving buffer amount until down to 1 when we know we have everything\n            if self._read_bytes == 1:\n                self._eof = True\n            from_pty = \"\"\n            self._read_bytes //= 2\n\n        self._out += from_pty\n\n    def _out_to_lines(self) -> None:\n        \"\"\" Process the winpty output into separate lines. Roll over any semi-consumed lines to the\n        next proc call. \"\"\"\n        if \"\\n\" not in self._out:\n            return\n\n        self._lines.extend(self._out.split(\"\\n\"))\n\n        if self._out.endswith(\"\\n\") or self._eof:  # Ends on newline or is EOF\n            self._out = \"\"\n        else:  # roll over semi-consumed line to next read\n            self._out = self._lines[-1]\n            self._lines = self._lines[:-1]\n\n    def call(self) -> int:\n        \"\"\" Install a package using the PyWinPTY module\n\n        Returns\n        -------\n        int\n            The return code of the package install process\n        \"\"\"\n        import winpty  # pylint:disable=import-outside-toplevel,import-error\n        # For some reason with WinPTY we need to pass in the full command. Probably a bug\n        proc = winpty.PTY(\n            100,\n            24,\n            backend=winpty.enums.Backend.WinPTY,  # ConPTY hangs and has lots of Ansi Escapes\n            agent_config=winpty.enums.AgentConfig.WINPTY_FLAG_PLAIN_OUTPUT)  # Strip all Ansi\n\n        if not proc.spawn(self._cmd, cmdline=self._cmdline):\n            del proc\n            raise RuntimeError(\"Failed to spawn winpty\")\n\n        while True:\n            self._read_from_pty(proc, winpty.WinptyError)\n            self._out_to_lines()\n            for line in self._lines:\n                self._seen_line_log(line.rstrip())\n                self._non_gui_print(line.encode(\"utf-8\", errors=\"replace\"))\n            self._lines = []\n\n            if self._eof:\n                returncode = proc.get_exitstatus()\n                break\n\n        del proc\n        return returncode\n\n\nclass SubProcInstaller(Installer):\n    \"\"\" The fallback package installer if either of the OS specific installers fail.\n\n    Uses the python Subprocess module to install packages. Feedback does not return in realtime\n    so the process can look like it has hung to the end user\n\n    Parameters\n    ----------\n    environment: :class:`Environment`\n        Environment class holding information about the running system\n    package: str\n        The package name that is being installed\n    command: list\n        The command to run\n    is_gui: bool\n        ``True`` if the process is being called from the Faceswap GUI\n    \"\"\"\n    def __init__(self,\n                 environment: Environment,\n                 package: str,\n                 command: list[str],\n                 is_gui: bool) -> None:\n        super().__init__(environment, package, command, is_gui)\n        self._shell = self._env.os_version[0] == \"Windows\" and command[0] == \"conda\"\n\n    def __call__(self) -> int:\n        \"\"\" Override default call function so we don't recursively call ourselves on failure. \"\"\"\n        returncode = self.call()\n        logger.debug(\"Package: %s, returncode: %s\", self._package, returncode)\n        return returncode\n\n    def call(self) -> int:\n        \"\"\" Install a package using the Subprocess module\n\n        Returns\n        -------\n        int\n            The return code of the package install process\n        \"\"\"\n        with Popen(self._command,\n                   bufsize=0, stdout=PIPE, stderr=STDOUT, shell=self._shell) as proc:\n            while True:\n                if proc.stdout is not None:\n                    lines = proc.stdout.readline()\n                returncode = proc.poll()\n                if lines == b\"\" and returncode is not None:\n                    break\n\n                for line in lines.split(b\"\\r\"):\n                    self._seen_line_log(line.decode(\"utf-8\", errors=\"replace\").rstrip())\n                    self._non_gui_print(line)\n\n        return returncode\n\n\nclass Tips():\n    \"\"\" Display installation Tips \"\"\"\n    @classmethod\n    def docker_no_cuda(cls) -> None:\n        \"\"\" Output Tips for Docker without Cuda \"\"\"\n        logger.info(\n            \"1. Install Docker from: https://www.docker.com/get-started\\n\\n\"\n            \"2. Enter the Faceswap folder and build the Docker Image For Faceswap:\\n\"\n            \"   docker build -t faceswap-cpu -f Dockerfile.cpu .\\n\\n\"\n            \"3. Launch and enter the Faceswap container:\\n\"\n            \"  a. Headless:\\n\"\n            \"     docker run --rm -it -v ./:/srv faceswap-cpu\\n\\n\"\n            \"  b. GUI:\\n\"\n            \"     xhost +local: && \\\\ \\n\"\n            \"     docker run --rm -it \\\\ \\n\"\n            \"     -v ./:/srv \\\\ \\n\"\n            \"     -v /tmp/.X11-unix:/tmp/.X11-unix \\\\ \\n\"\n            \"     -e DISPLAY=${DISPLAY} \\\\ \\n\"\n            \"     faceswap-cpu \\n\")\n        logger.info(\"That's all you need to do with docker. Have fun.\")\n\n    @classmethod\n    def docker_cuda(cls) -> None:\n        \"\"\" Output Tips for Docker with Cuda\"\"\"\n        logger.info(\n            \"1. Install Docker from: https://www.docker.com/get-started\\n\\n\"\n            \"2. Install latest CUDA 11 and cuDNN 8 from: https://developer.nvidia.com/cuda-\"\n            \"downloads\\n\\n\"\n            \"3. Install the the Nvidia Container Toolkit from https://docs.nvidia.com/datacenter/\"\n            \"cloud-native/container-toolkit/latest/install-guide\\n\\n\"\n            \"4. Restart Docker Service\\n\\n\"\n            \"5. Enter the Faceswap folder and build the Docker Image For Faceswap:\\n\"\n            \"   docker build -t faceswap-gpu -f Dockerfile.gpu .\\n\\n\"\n            \"6. Launch and enter the Faceswap container:\\n\"\n            \"  a. Headless:\\n\"\n            \"     docker run --runtime=nvidia --rm -it -v ./:/srv faceswap-gpu\\n\\n\"\n            \"  b. GUI:\\n\"\n            \"     xhost +local: && \\\\ \\n\"\n            \"     docker run --runtime=nvidia --rm -it \\\\ \\n\"\n            \"     -v ./:/srv \\\\ \\n\"\n            \"     -v /tmp/.X11-unix:/tmp/.X11-unix \\\\ \\n\"\n            \"     -e DISPLAY=${DISPLAY} \\\\ \\n\"\n            \"     faceswap-gpu \\n\")\n        logger.info(\"That's all you need to do with docker. Have fun.\")\n\n    @classmethod\n    def macos(cls) -> None:\n        \"\"\" Output Tips for macOS\"\"\"\n        logger.info(\n            \"setup.py does not directly support macOS. The following tips should help:\\n\\n\"\n            \"1. Install system dependencies:\\n\"\n            \"XCode from the Apple Store\\n\"\n            \"XQuartz: https://www.xquartz.org/\\n\\n\"\n\n            \"2a. It is recommended to use Anaconda for your Python Virtual Environment as this\\n\"\n            \"will handle the installation of CUDA and cuDNN for you:\\n\"\n            \"https://www.anaconda.com/distribution/\\n\\n\"\n\n            \"2b. If you do not want to use Anaconda you will need to manually install CUDA and \"\n            \"cuDNN:\\n\"\n            \"CUDA: https://developer.nvidia.com/cuda-downloads\"\n            \"cuDNN: https://developer.nvidia.com/rdp/cudnn-download\\n\\n\")\n\n    @classmethod\n    def pip(cls) -> None:\n        \"\"\" Pip Tips \"\"\"\n        logger.info(\"1. Install PIP requirements\\n\"\n                    \"You may want to execute `chcp 65001` in cmd line\\n\"\n                    \"to fix Unicode issues on Windows when installing dependencies\")\n\n\nif __name__ == \"__main__\":\n    logfile = os.path.join(os.path.dirname(os.path.realpath(sys.argv[0])), \"faceswap_setup.log\")\n    log_setup(\"INFO\", logfile, \"setup\")\n    logger.debug(\"Setup called with args: %s\", sys.argv)\n    ENV = Environment()\n    Checks(ENV)\n    ENV.set_config()\n    if _INSTALL_FAILED:\n        sys.exit(1)\n    Install(ENV)\n", "update_deps.py": "#!/usr/bin/env python3\n\"\"\" Installs any required third party libs for faceswap.py\n\n    Checks for installed Conda / Pip packages and updates accordingly\n\"\"\"\nimport logging\nimport os\nimport sys\n\nfrom lib.logger import log_setup\nfrom setup import Environment, Install\n\nlogger = logging.getLogger(__name__)\n\n\ndef main(is_gui=False) -> None:\n    \"\"\" Check for and update dependencies\n\n    Parameters\n    ----------\n    is_gui: bool, optional\n        ``True`` if being called by the GUI. Prevents the updater from outputting progress bars\n        which get scrambled in the GUI\n    \"\"\"\n    logger.info(\"Updating dependencies...\")\n    update = Environment(updater=True)\n    Install(update, is_gui=is_gui)\n    logger.info(\"Dependencies updated\")\n\n\nif __name__ == \"__main__\":\n    logfile = os.path.join(os.path.dirname(os.path.realpath(sys.argv[0])), \"faceswap_update.log\")\n    log_setup(\"INFO\", logfile, \"setup\")\n    main()\n", "plugins/plugin_loader.py": "#!/usr/bin/env python3\n\"\"\" Plugin loader for Faceswap extract, training and convert tasks \"\"\"\nfrom __future__ import annotations\nimport logging\nimport os\nimport typing as T\n\nfrom importlib import import_module\n\nif T.TYPE_CHECKING:\n    from collections.abc import Callable\n    from plugins.extract.detect._base import Detector\n    from plugins.extract.align._base import Aligner\n    from plugins.extract.mask._base import Masker\n    from plugins.extract.recognition._base import Identity\n    from plugins.train.model._base import ModelBase\n    from plugins.train.trainer._base import TrainerBase\n\nlogger = logging.getLogger(__name__)\n\n\nclass PluginLoader():\n    \"\"\" Retrieve, or get information on, Faceswap plugins\n\n    Return a specific plugin, list available plugins, or get the default plugin for a\n    task.\n\n    Example\n    -------\n    >>> from plugins.plugin_loader import PluginLoader\n    >>> align_plugins = PluginLoader.get_available_extractors('align')\n    >>> aligner = PluginLoader.get_aligner('cv2-dnn')\n    \"\"\"\n    @staticmethod\n    def get_detector(name: str, disable_logging: bool = False) -> type[Detector]:\n        \"\"\" Return requested detector plugin\n\n        Parameters\n        ----------\n        name: str\n            The name of the requested detector plugin\n        disable_logging: bool, optional\n            Whether to disable the INFO log message that the plugin is being imported.\n            Default: `False`\n\n        Returns\n        -------\n        :class:`plugins.extract.detect` object:\n            An extraction detector plugin\n        \"\"\"\n        return PluginLoader._import(\"extract.detect\", name, disable_logging)\n\n    @staticmethod\n    def get_aligner(name: str, disable_logging: bool = False) -> type[Aligner]:\n        \"\"\" Return requested aligner plugin\n\n        Parameters\n        ----------\n        name: str\n            The name of the requested aligner plugin\n        disable_logging: bool, optional\n            Whether to disable the INFO log message that the plugin is being imported.\n            Default: `False`\n\n        Returns\n        -------\n        :class:`plugins.extract.align` object:\n            An extraction aligner plugin\n        \"\"\"\n        return PluginLoader._import(\"extract.align\", name, disable_logging)\n\n    @staticmethod\n    def get_masker(name: str, disable_logging: bool = False) -> type[Masker]:\n        \"\"\" Return requested masker plugin\n\n        Parameters\n        ----------\n        name: str\n            The name of the requested masker plugin\n        disable_logging: bool, optional\n            Whether to disable the INFO log message that the plugin is being imported.\n            Default: `False`\n\n        Returns\n        -------\n        :class:`plugins.extract.mask` object:\n            An extraction masker plugin\n        \"\"\"\n        return PluginLoader._import(\"extract.mask\", name, disable_logging)\n\n    @staticmethod\n    def get_recognition(name: str, disable_logging: bool = False) -> type[Identity]:\n        \"\"\" Return requested recognition plugin\n\n        Parameters\n        ----------\n        name: str\n            The name of the requested reccognition plugin\n        disable_logging: bool, optional\n            Whether to disable the INFO log message that the plugin is being imported.\n            Default: `False`\n\n        Returns\n        -------\n        :class:`plugins.extract.recognition` object:\n            An extraction recognition plugin\n        \"\"\"\n        return PluginLoader._import(\"extract.recognition\", name, disable_logging)\n\n    @staticmethod\n    def get_model(name: str, disable_logging: bool = False) -> type[ModelBase]:\n        \"\"\" Return requested training model plugin\n\n        Parameters\n        ----------\n        name: str\n            The name of the requested training model plugin\n        disable_logging: bool, optional\n            Whether to disable the INFO log message that the plugin is being imported.\n            Default: `False`\n\n        Returns\n        -------\n        :class:`plugins.train.model` object:\n            A training model plugin\n        \"\"\"\n        return PluginLoader._import(\"train.model\", name, disable_logging)\n\n    @staticmethod\n    def get_trainer(name: str, disable_logging: bool = False) -> type[TrainerBase]:\n        \"\"\" Return requested training trainer plugin\n\n        Parameters\n        ----------\n        name: str\n            The name of the requested training trainer plugin\n        disable_logging: bool, optional\n            Whether to disable the INFO log message that the plugin is being imported.\n            Default: `False`\n\n        Returns\n        -------\n        :class:`plugins.train.trainer` object:\n            A training trainer plugin\n        \"\"\"\n        return PluginLoader._import(\"train.trainer\", name, disable_logging)\n\n    @staticmethod\n    def get_converter(category: str, name: str, disable_logging: bool = False) -> Callable:\n        \"\"\" Return requested converter plugin\n\n        Converters work slightly differently to other faceswap plugins. They are created to do a\n        specific task (e.g. color adjustment, mask blending etc.), so multiple plugins will be\n        loaded in the convert phase, rather than just one plugin for the other phases.\n\n        Parameters\n        ----------\n        name: str\n            The name of the requested converter plugin\n        disable_logging: bool, optional\n            Whether to disable the INFO log message that the plugin is being imported.\n            Default: `False`\n\n        Returns\n        -------\n        :class:`plugins.convert` object:\n            A converter sub plugin\n        \"\"\"\n        return PluginLoader._import(f\"convert.{category}\", name, disable_logging)\n\n    @staticmethod\n    def _import(attr: str, name: str, disable_logging: bool):\n        \"\"\" Import the plugin's module\n\n        Parameters\n        ----------\n        name: str\n            The name of the requested converter plugin\n        disable_logging: bool\n            Whether to disable the INFO log message that the plugin is being imported.\n\n        Returns\n        -------\n        :class:`plugin` object:\n            A plugin\n        \"\"\"\n        name = name.replace(\"-\", \"_\")\n        ttl = attr.split(\".\")[-1].title()\n        if not disable_logging:\n            logger.info(\"Loading %s from %s plugin...\", ttl, name.title())\n        attr = \"model\" if attr == \"Trainer\" else attr.lower()\n        mod = \".\".join((\"plugins\", attr, name))\n        module = import_module(mod)\n        return getattr(module, ttl)\n\n    @staticmethod\n    def get_available_extractors(extractor_type: T.Literal[\"align\", \"detect\", \"mask\"],\n                                 add_none: bool = False,\n                                 extend_plugin: bool = False) -> list[str]:\n        \"\"\" Return a list of available extractors of the given type\n\n        Parameters\n        ----------\n        extractor_type: {'align', 'detect', 'mask'}\n            The type of extractor to return the plugins for\n        add_none: bool, optional\n            Append \"none\" to the list of returned plugins. Default: False\n        extend_plugin: bool, optional\n            Some plugins have configuration options that mean that multiple 'pseudo-plugins'\n            can be generated based on their settings. An example of this is the bisenet-fp mask\n            which, whilst selected as 'bisenet-fp' can be stored as 'bisenet-fp-face' and\n            'bisenet-fp-head' depending on whether hair has been included in the mask or not.\n            ``True`` will generate each pseudo-plugin, ``False`` will generate the original\n            plugin name. Default: ``False``\n\n        Returns\n        -------\n        list:\n            A list of the available extractor plugin names for the given type\n        \"\"\"\n        extractpath = os.path.join(os.path.dirname(__file__),\n                                   \"extract\",\n                                   extractor_type)\n        extractors = [item.name.replace(\".py\", \"\").replace(\"_\", \"-\")\n                      for item in os.scandir(extractpath)\n                      if not item.name.startswith(\"_\")\n                      and not item.name.endswith(\"defaults.py\")\n                      and item.name.endswith(\".py\")]\n        extendable = [\"bisenet-fp\", \"custom\"]\n        if extend_plugin and extractor_type == \"mask\" and any(ext in extendable\n                                                              for ext in extractors):\n            for msk in extendable:\n                extractors.remove(msk)\n                extractors.extend([f\"{msk}_face\", f\"{msk}_head\"])\n\n        extractors = sorted(extractors)\n        if add_none:\n            extractors.insert(0, \"none\")\n        return extractors\n\n    @staticmethod\n    def get_available_models() -> list[str]:\n        \"\"\" Return a list of available training models\n\n        Returns\n        -------\n        list:\n            A list of the available training model plugin names\n        \"\"\"\n        modelpath = os.path.join(os.path.dirname(__file__), \"train\", \"model\")\n        models = sorted(item.name.replace(\".py\", \"\").replace(\"_\", \"-\")\n                        for item in os.scandir(modelpath)\n                        if not item.name.startswith(\"_\")\n                        and not item.name.endswith(\"defaults.py\")\n                        and item.name.endswith(\".py\"))\n        return models\n\n    @staticmethod\n    def get_default_model() -> str:\n        \"\"\" Return the default training model plugin name\n\n        Returns\n        -------\n        str:\n            The default faceswap training model\n\n        \"\"\"\n        models = PluginLoader.get_available_models()\n        return 'original' if 'original' in models else models[0]\n\n    @staticmethod\n    def get_available_convert_plugins(convert_category: str, add_none: bool = True) -> list[str]:\n        \"\"\" Return a list of available converter plugins in the given category\n\n        Parameters\n        ----------\n        convert_category: {'color', 'mask', 'scaling', 'writer'}\n            The category of converter plugin to return the plugins for\n        add_none: bool, optional\n            Append \"none\" to the list of returned plugins. Default: True\n\n        Returns\n        -------\n        list\n            A list of the available converter plugin names in the given category\n        \"\"\"\n\n        convertpath = os.path.join(os.path.dirname(__file__),\n                                   \"convert\",\n                                   convert_category)\n        converters = sorted(item.name.replace(\".py\", \"\").replace(\"_\", \"-\")\n                            for item in os.scandir(convertpath)\n                            if not item.name.startswith(\"_\")\n                            and not item.name.endswith(\"defaults.py\")\n                            and item.name.endswith(\".py\"))\n        if add_none:\n            converters.insert(0, \"none\")\n        return converters\n", "plugins/__init__.py": "", "plugins/extract/pipeline.py": "#!/usr/bin/env python3\n\"\"\"\nReturn a requested detector/aligner/masker pipeline\n\nTensorflow does not like to release GPU VRAM, so parallel plugins need to be managed to work\ntogether.\n\nThis module sets up a pipeline for the extraction workflow, loading detect, align and mask\nplugins either in parallel or in series, giving easy access to input and output.\n\"\"\"\nfrom __future__ import annotations\nimport logging\nimport os\nimport typing as T\n\nfrom lib.align import LandmarkType\nfrom lib.gpu_stats import GPUStats\nfrom lib.logger import parse_class_init\nfrom lib.queue_manager import EventQueue, queue_manager, QueueEmpty\nfrom lib.serializer import get_serializer\nfrom lib.utils import get_backend, FaceswapError\nfrom plugins.plugin_loader import PluginLoader\n\nif T.TYPE_CHECKING:\n    from collections.abc import Generator\n    from ._base import Extractor as PluginExtractor\n    from .align._base import Aligner\n    from .align.external import Align as AlignImport\n    from .detect._base import Detector\n    from .detect.external import Detect as DetectImport\n    from .mask._base import Masker\n    from .recognition._base import Identity\n    from . import ExtractMedia\n\nlogger = logging.getLogger(__name__)\n_INSTANCES = -1  # Tracking for multiple instances of pipeline\n\n\ndef _get_instance():\n    \"\"\" Increment the global :attr:`_INSTANCES` and obtain the current instance value \"\"\"\n    global _INSTANCES  # pylint:disable=global-statement\n    _INSTANCES += 1\n    return _INSTANCES\n\n\nclass Extractor():\n    \"\"\" Creates a :mod:`~plugins.extract.detect`/:mod:`~plugins.extract.align``/\\\n    :mod:`~plugins.extract.mask` pipeline and yields results frame by frame from the\n    :attr:`detected_faces` generator\n\n    :attr:`input_queue` is dynamically set depending on the current :attr:`phase` of extraction\n\n    Parameters\n    ----------\n    detector: str or ``None``\n        The name of a detector plugin as exists in :mod:`plugins.extract.detect`\n    aligner: str or ``None``\n        The name of an aligner plugin as exists in :mod:`plugins.extract.align`\n    masker: str or list or ``None``\n        The name of a masker plugin(s) as exists in :mod:`plugins.extract.mask`.\n        This can be a single masker or a list of multiple maskers\n    recognition: str or ``None``\n        The name of the recognition plugin to use. ``None`` to not do face recognition.\n        Default: ``None``\n    configfile: str, optional\n        The path to a custom ``extract.ini`` configfile. If ``None`` then the system\n        :file:`config/extract.ini` file will be used.\n    multiprocess: bool, optional\n        Whether to attempt processing the plugins in parallel. This may get overridden\n        internally depending on the plugin combination. Default: ``False``\n    exclude_gpus: list, optional\n        A list of indices correlating to connected GPUs that Tensorflow should not use. Pass\n        ``None`` to not exclude any GPUs. Default: ``None``\n    rotate_images: str, optional\n        Used to set the :attr:`plugins.extract.detect.rotation` attribute. Pass in a single number\n        to use increments of that size up to 360, or pass in a ``list`` of ``ints`` to enumerate\n        exactly what angles to check. Can also pass in ``'on'`` to increment at 90 degree\n        intervals. Default: ``None``\n    min_size: int, optional\n        Used to set the :attr:`plugins.extract.detect.min_size` attribute. Filters out faces\n        detected below this size. Length, in pixels across the diagonal of the bounding box. Set\n        to ``0`` for off. Default: ``0``\n    normalize_method: {`None`, 'clahe', 'hist', 'mean'}, optional\n        Used to set the :attr:`plugins.extract.align.normalize_method` attribute. Normalize the\n        images fed to the aligner.Default: ``None``\n    re_feed: int\n        The number of times to re-feed a slightly adjusted bounding box into the aligner.\n        Default: `0`\n    re_align: bool, optional\n        ``True`` to obtain landmarks by passing the initially aligned face back through the\n        aligner. Default ``False``\n    disable_filter: bool, optional\n        Disable all aligner filters regardless of config option. Default: ``False``\n\n    Attributes\n    ----------\n    phase: str\n        The current phase that the pipeline is running. Used in conjunction with :attr:`passes` and\n        :attr:`final_pass` to indicate to the caller which phase is being processed\n    \"\"\"\n    def __init__(self,\n                 detector: str | None,\n                 aligner: str | None,\n                 masker: str | list[str] | None,\n                 recognition: str | None = None,\n                 configfile: str | None = None,\n                 multiprocess: bool = False,\n                 exclude_gpus: list[int] | None = None,\n                 rotate_images: str | None = None,\n                 min_size: int = 0,\n                 normalize_method:  T.Literal[\"none\", \"clahe\", \"hist\", \"mean\"] | None = None,\n                 re_feed: int = 0,\n                 re_align: bool = False,\n                 disable_filter: bool = False) -> None:\n        logger.debug(parse_class_init(locals()))\n        self._instance = _get_instance()\n        maskers = [T.cast(str | None,\n                   masker)] if not isinstance(masker, list) else T.cast(list[str | None],\n                                                                        masker)\n        self._flow = self._set_flow(detector, aligner, maskers, recognition)\n        self._exclude_gpus = exclude_gpus\n        # We only ever need 1 item in each queue. This is 2 items cached (1 in queue 1 waiting\n        # for queue) at each point. Adding more just stacks RAM with no speed benefit.\n        self._queue_size = 1\n        # TODO Calculate scaling for more plugins than currently exist in _parallel_scaling\n        self._scaling_fallback = 0.4\n        self._vram_stats = self._get_vram_stats()\n        self._detect = self._load_detect(detector, aligner, rotate_images, min_size, configfile)\n        self._align = self._load_align(aligner,\n                                       configfile,\n                                       normalize_method,\n                                       re_feed,\n                                       re_align,\n                                       disable_filter)\n        self._recognition = self._load_recognition(recognition, configfile)\n        self._mask = [self._load_mask(mask, configfile) for mask in maskers]\n        self._is_parallel = self._set_parallel_processing(multiprocess)\n        self._phases = self._set_phases(multiprocess)\n        self._phase_index = 0\n        self._set_extractor_batchsize()\n        self._queues = self._add_queues()\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    @property\n    def input_queue(self) -> EventQueue:\n        \"\"\" queue: Return the correct input queue depending on the current phase\n\n        The input queue is the entry point into the extraction pipeline. An :class:`ExtractMedia`\n        object should be put to the queue.\n\n        For detect/single phase operations the :attr:`ExtractMedia.filename` and\n        :attr:`~ExtractMedia.image` attributes should be populated.\n\n        For align/mask (2nd/3rd pass operations) the :attr:`ExtractMedia.detected_faces` should\n        also be populated by calling :func:`ExtractMedia.set_detected_faces`.\n        \"\"\"\n        qname = f\"extract{self._instance}_{self._current_phase[0]}_in\"\n        retval = self._queues[qname]\n        logger.trace(\"%s: %s\", qname, retval)  # type: ignore\n        return retval\n\n    @property\n    def passes(self) -> int:\n        \"\"\" int: Returns the total number of passes the extractor needs to make.\n\n        This is calculated on several factors (vram available, plugin choice,\n        :attr:`multiprocess` etc.). It is useful for iterating over the pipeline\n        and handling accordingly.\n\n        Example\n        -------\n        >>> for phase in extractor.passes:\n        >>>     if phase == 1:\n        >>>         extract_media = ExtractMedia(\"path/to/image/file\", image)\n        >>>         extractor.input_queue.put(extract_media)\n        >>>     else:\n        >>>         extract_media.set_image(image)\n        >>>         extractor.input_queue.put(extract_media)\n        \"\"\"\n        retval = len(self._phases)\n        logger.trace(retval)  # type: ignore\n        return retval\n\n    @property\n    def phase_text(self) -> str:\n        \"\"\" str: The plugins that are running in the current phase, formatted for info text\n        output. \"\"\"\n        plugin_types = set(self._get_plugin_type_and_index(phase)[0]\n                           for phase in self._current_phase)\n        retval = \", \".join(plugin_type.title() for plugin_type in list(plugin_types))\n        logger.trace(retval)  # type: ignore\n        return retval\n\n    @property\n    def final_pass(self) -> bool:\n        \"\"\" bool, Return ``True`` if this is the final extractor pass otherwise ``False``\n\n        Useful for iterating over the pipeline :attr:`passes` or :func:`detected_faces` and\n        handling accordingly.\n\n        Example\n        -------\n        >>> for face in extractor.detected_faces():\n        >>>     if extractor.final_pass:\n        >>>         <do final processing>\n        >>>     else:\n        >>>         extract_media.set_image(image)\n        >>>         <do intermediate processing>\n        >>>         extractor.input_queue.put(extract_media)\n        \"\"\"\n        retval = self._phase_index == len(self._phases) - 1\n        logger.trace(retval)  # type:ignore[attr-defined]\n        return retval\n\n    @property\n    def aligner(self) -> Aligner:\n        \"\"\" The currently selected aligner plugin \"\"\"\n        assert self._align is not None\n        return self._align\n\n    @property\n    def recognition(self) -> Identity:\n        \"\"\" The currently selected recognition plugin \"\"\"\n        assert self._recognition is not None\n        return self._recognition\n\n    def reset_phase_index(self) -> None:\n        \"\"\" Reset the current phase index back to 0. Used for when batch processing is used in\n        extract. \"\"\"\n        self._phase_index = 0\n\n    def set_batchsize(self,\n                      plugin_type: T.Literal[\"align\", \"detect\"],\n                      batchsize: int) -> None:\n        \"\"\" Set the batch size of a given :attr:`plugin_type` to the given :attr:`batchsize`.\n\n        This should be set prior to :func:`launch` if the batch size is to be manually overridden\n\n        Parameters\n        ----------\n        plugin_type: {'align', 'detect'}\n            The plugin_type to be overridden\n        batchsize: int\n            The batch size to use for this plugin type\n        \"\"\"\n        logger.debug(\"Overriding batchsize for plugin_type: %s to: %s\", plugin_type, batchsize)\n        plugin = getattr(self, f\"_{plugin_type}\")\n        plugin.batchsize = batchsize\n\n    def launch(self) -> None:\n        \"\"\" Launches the plugin(s)\n\n        This launches the plugins held in the pipeline, and should be called at the beginning\n        of each :attr:`phase`. To ensure VRAM is conserved, It will only launch the plugin(s)\n        required for the currently running phase\n\n        Example\n        -------\n        >>> for phase in extractor.passes:\n        >>>     extractor.launch():\n        >>>         <do processing>\n        \"\"\"\n        for phase in self._current_phase:\n            self._launch_plugin(phase)\n\n    def detected_faces(self) -> Generator[ExtractMedia, None, None]:\n        \"\"\" Generator that returns results, frame by frame from the extraction pipeline\n\n        This is the exit point for the extraction pipeline and is used to obtain the output\n        of any pipeline :attr:`phase`\n\n        Yields\n        ------\n        faces: :class:`~plugins.extract.extract_media.ExtractMedia`\n            The populated extracted media object.\n\n        Example\n        -------\n        >>> for extract_media in extractor.detected_faces():\n        >>>     filename = extract_media.filename\n        >>>     image = extract_media.image\n        >>>     detected_faces = extract_media.detected_faces\n        \"\"\"\n        logger.debug(\"Running Detection. Phase: '%s'\", self._current_phase)\n        # If not multiprocessing, intercept the align in queue for\n        # detection phase\n        out_queue = self._output_queue\n        while True:\n            try:\n                self._check_and_raise_error()\n                faces = out_queue.get(True, 1)\n                if faces == \"EOF\":\n                    break\n            except QueueEmpty:\n                continue\n            yield faces\n\n        self._join_threads()\n        if self.final_pass:\n            for plugin in self._all_plugins:\n                plugin.on_completion()\n            logger.debug(\"Detection Complete\")\n        else:\n            self._phase_index += 1\n            logger.debug(\"Switching to phase: %s\", self._current_phase)\n\n    def _disable_lm_maskers(self) -> None:\n        \"\"\" Disable any 68 point landmark based maskers if alignment data is not 2D 68\n        point landmarks and update the process flow/phases accordingly \"\"\"\n        logger.warning(\"Alignment data is not 68 point 2D landmarks. Some Faceswap functionality \"\n                       \"will be unavailable for these faces\")\n\n        rem_maskers = [m.name for m in self._mask\n                       if m is not None and m.landmark_type == LandmarkType.LM_2D_68]\n        self._mask = [m for m in self._mask if m is None or m.name not in rem_maskers]\n\n        self._flow = [\n            item for item in self._flow\n            if not item.startswith(\"mask\")\n            or item.startswith(\"mask\") and int(item.rsplit(\"_\", maxsplit=1)[-1]) < len(self._mask)]\n\n        self._phases = [[s for s in p if s in self._flow] for p in self._phases\n                        if any(t in p for t in self._flow)]\n\n        for queue in self._queues:\n            queue_manager.del_queue(queue)\n        del self._queues\n        self._queues = self._add_queues()\n\n        logger.warning(\"The following maskers have been disabled due to unsupported landmarks: %s\",\n                       rem_maskers)\n\n    def import_data(self, input_location: str) -> None:\n        \"\"\" Import json data to the detector and/or aligner if 'import' plugin has been selected\n\n        Parameters\n        ----------\n        input_location: str\n            Full path to the input location for the extract process\n        \"\"\"\n        assert self._detect is not None\n        import_plugins: list[DetectImport | AlignImport] = [\n            p for p in (self._detect, self.aligner)  # type:ignore[misc]\n            if T.cast(str, p.name).lower() == \"external\"]\n\n        if not import_plugins:\n            return\n\n        align_origin = None\n        assert self.aligner.name is not None\n        if self.aligner.name.lower() == \"external\":\n            align_origin = self.aligner.config[\"origin\"]\n\n        logger.info(\"Importing external data for %s from json file...\",\n                    \" and \".join([p.__class__.__name__ for p in import_plugins]))\n\n        folder = input_location\n        folder = folder if os.path.isdir(folder) else os.path.dirname(folder)\n\n        last_fname = \"\"\n        is_68_point = True\n        for plugin in import_plugins:\n            plugin_type = plugin.__class__.__name__\n            path = os.path.join(folder, plugin.config[\"file_name\"])\n            if not os.path.isfile(path):\n                raise FaceswapError(f\"{plugin_type} import file could not be found at '{path}'\")\n\n            if path != last_fname:  # Different import file for aligner data\n                last_fname = path\n                data = get_serializer(\"json\").load(path)\n\n            if plugin_type == \"Detect\":\n                plugin.import_data(data, align_origin)  # type:ignore[call-arg]\n            else:\n                plugin.import_data(data)  # type:ignore[call-arg]\n                is_68_point = plugin.landmark_type == LandmarkType.LM_2D_68  # type:ignore[union-attr]  # noqa:E501  # pylint:disable=\"line-too-long\"\n\n        if not is_68_point:\n            self._disable_lm_maskers()\n\n        logger.info(\"Imported external data\")\n\n    # <<< INTERNAL METHODS >>> #\n    @property\n    def _parallel_scaling(self) -> dict[int, float]:\n        \"\"\" dict: key is number of parallel plugins being loaded, value is the scaling factor that\n        the total base vram for those plugins should be scaled by\n\n        Notes\n        -----\n        VRAM for parallel plugins does not stack in a linear manner. Calculating the precise\n        scaling for any given plugin combination is non trivial, however the following are\n        calculations based on running 2-5 plugins in parallel using s3fd, fan, unet, vgg-clear\n        and vgg-obstructed. The worst ratio is selected for each combination, plus a little extra\n        to ensure that vram is not used up.\n\n        If OOM errors are being reported, then these ratios should be relaxed some more\n        \"\"\"\n        retval = {0: 1.0,\n                  1: 1.0,\n                  2: 0.7,\n                  3: 0.55,\n                  4: 0.5,\n                  5: 0.4}\n        logger.trace(retval)  # type: ignore\n        return retval\n\n    @property\n    def _vram_per_phase(self) -> dict[str, float]:\n        \"\"\" dict: The amount of vram required for each phase in :attr:`_flow`. \"\"\"\n        retval = {}\n        for phase in self._flow:\n            plugin_type, idx = self._get_plugin_type_and_index(phase)\n            attr = getattr(self, f\"_{plugin_type}\")\n            attr = attr[idx] if idx is not None else attr\n            retval[phase] = attr.vram\n        logger.trace(retval)  # type: ignore\n        return retval\n\n    @property\n    def _total_vram_required(self) -> float:\n        \"\"\" Return vram required for all phases plus the buffer \"\"\"\n        vrams = self._vram_per_phase\n        vram_required_count = sum(1 for p in vrams.values() if p > 0)\n        logger.debug(\"VRAM requirements: %s. Plugins requiring VRAM: %s\",\n                     vrams, vram_required_count)\n        retval = (sum(vrams.values()) *\n                  self._parallel_scaling.get(vram_required_count, self._scaling_fallback))\n        logger.debug(\"Total VRAM required: %s\", retval)\n        return retval\n\n    @property\n    def _current_phase(self) -> list[str]:\n        \"\"\" list: The current phase from :attr:`_phases` that is running through the extractor. \"\"\"\n        retval = self._phases[self._phase_index]\n        logger.trace(retval)  # type: ignore\n        return retval\n\n    @property\n    def _final_phase(self) -> str:\n        \"\"\" Return the final phase from the flow list \"\"\"\n        retval = self._flow[-1]\n        logger.trace(retval)  # type: ignore\n        return retval\n\n    @property\n    def _output_queue(self) -> EventQueue:\n        \"\"\" Return the correct output queue depending on the current phase \"\"\"\n        if self.final_pass:\n            qname = f\"extract{self._instance}_{self._final_phase}_out\"\n        else:\n            qname = f\"extract{self._instance}_{self._phases[self._phase_index + 1][0]}_in\"\n        retval = self._queues[qname]\n        logger.trace(\"%s: %s\", qname, retval)  # type: ignore\n        return retval\n\n    @property\n    def _all_plugins(self) -> list[PluginExtractor]:\n        \"\"\" Return list of all plugin objects in this pipeline \"\"\"\n        retval = []\n        for phase in self._flow:\n            plugin_type, idx = self._get_plugin_type_and_index(phase)\n            attr = getattr(self, f\"_{plugin_type}\")\n            attr = attr[idx] if idx is not None else attr\n            retval.append(attr)\n        logger.trace(\"All Plugins: %s\", retval)  # type: ignore\n        return retval\n\n    @property\n    def _active_plugins(self) -> list[PluginExtractor]:\n        \"\"\" Return the plugins that are currently active based on pass \"\"\"\n        retval = []\n        for phase in self._current_phase:\n            plugin_type, idx = self._get_plugin_type_and_index(phase)\n            attr = getattr(self, f\"_{plugin_type}\")\n            retval.append(attr[idx] if idx is not None else attr)\n        logger.trace(\"Active plugins: %s\", retval)  # type: ignore\n        return retval\n\n    @staticmethod\n    def _set_flow(detector: str | None,\n                  aligner: str | None,\n                  masker: list[str | None],\n                  recognition: str | None) -> list[str]:\n        \"\"\" Set the flow list based on the input plugins\n\n        Parameters\n        ----------\n        detector: str or ``None``\n            The name of a detector plugin as exists in :mod:`plugins.extract.detect`\n        aligner: str or ``None\n            The name of an aligner plugin as exists in :mod:`plugins.extract.align`\n        masker: str or list or ``None\n            The name of a masker plugin(s) as exists in :mod:`plugins.extract.mask`.\n            This can be a single masker or a list of multiple maskers\n        recognition: str or ``None``\n            The name of the recognition plugin to use. ``None`` to not do face recognition.\n        \"\"\"\n        logger.debug(\"detector: %s, aligner: %s, masker: %s recognition: %s\",\n                     detector, aligner, masker, recognition)\n        retval = []\n        if detector is not None and detector.lower() != \"none\":\n            retval.append(\"detect\")\n        if aligner is not None and aligner.lower() != \"none\":\n            retval.append(\"align\")\n        if recognition is not None and recognition.lower() != \"none\":\n            retval.append(\"recognition\")\n        retval.extend([f\"mask_{idx}\"\n                       for idx, mask in enumerate(masker)\n                       if mask is not None and mask.lower() != \"none\"])\n        logger.debug(\"flow: %s\", retval)\n        return retval\n\n    @staticmethod\n    def _get_plugin_type_and_index(flow_phase: str) -> tuple[str, int | None]:\n        \"\"\" Obtain the plugin type and index for the plugin for the given flow phase.\n\n        When multiple plugins for the same phase are allowed (e.g. Mask) this will return\n        the plugin type and the index of the plugin required. If only one plugin is allowed\n        then the plugin type will be returned and the index will be ``None``.\n\n        Parameters\n        ----------\n        flow_phase: str\n            The phase within :attr:`_flow` that is to have the plugin type and index returned\n\n        Returns\n        -------\n        plugin_type: str\n            The plugin type for the given flow phase\n        index: int\n            The index of this plugin type within the flow, if there are multiple plugins in use\n            otherwise ``None`` if there is only 1 plugin in use for the given phase\n        \"\"\"\n        sidx = flow_phase.split(\"_\")[-1]\n        if sidx.isdigit():\n            idx: int | None = int(sidx)\n            plugin_type = \"_\".join(flow_phase.split(\"_\")[:-1])\n        else:\n            plugin_type = flow_phase\n            idx = None\n        return plugin_type, idx\n\n    def _add_queues(self) -> dict[str, EventQueue]:\n        \"\"\" Add the required processing queues to Queue Manager \"\"\"\n        queues = {}\n        tasks = [f\"extract{self._instance}_{phase}_in\" for phase in self._flow]\n        tasks.append(f\"extract{self._instance}_{self._final_phase}_out\")\n        for task in tasks:\n            # Limit queue size to avoid stacking ram\n            queue_manager.add_queue(task, maxsize=self._queue_size)\n            queues[task] = queue_manager.get_queue(task)\n        logger.debug(\"Queues: %s\", queues)\n        return queues\n\n    @staticmethod\n    def _get_vram_stats() -> dict[str, int | str]:\n        \"\"\" Obtain statistics on available VRAM and subtract a constant buffer from available vram.\n\n        Returns\n        -------\n        dict\n            Statistics on available VRAM\n        \"\"\"\n        vram_buffer = 256  # Leave a buffer for VRAM allocation\n        gpu_stats = GPUStats()\n        stats = gpu_stats.get_card_most_free()\n        retval: dict[str, int | str] = {\"count\": gpu_stats.device_count,\n                                        \"device\": stats.device,\n                                        \"vram_free\": int(stats.free - vram_buffer),\n                                        \"vram_total\": int(stats.total)}\n        logger.debug(retval)\n        return retval\n\n    def _set_parallel_processing(self, multiprocess: bool) -> bool:\n        \"\"\" Set whether to run detect, align, and mask together or separately.\n\n        Parameters\n        ----------\n        multiprocess: bool\n            ``True`` if the single-process command line flag has not been set otherwise ``False``\n        \"\"\"\n        if not multiprocess:\n            logger.debug(\"Parallel processing disabled by cli.\")\n            return False\n\n        if self._vram_stats[\"count\"] == 0:\n            logger.debug(\"No GPU detected. Enabling parallel processing.\")\n            return True\n\n        logger.verbose(\"%s - %sMB free of %sMB\",  # type: ignore\n                       self._vram_stats[\"device\"],\n                       self._vram_stats[\"vram_free\"],\n                       self._vram_stats[\"vram_total\"])\n        if T.cast(int, self._vram_stats[\"vram_free\"]) <= self._total_vram_required:\n            logger.warning(\"Not enough free VRAM for parallel processing. \"\n                           \"Switching to serial\")\n            return False\n        return True\n\n    def _set_phases(self, multiprocess: bool) -> list[list[str]]:\n        \"\"\" If not enough VRAM is available, then chunk :attr:`_flow` up into phases that will fit\n        into VRAM, otherwise return the single flow.\n\n        Parameters\n        ----------\n        multiprocess: bool\n            ``True`` if the single-process command line flag has not been set otherwise ``False``\n\n        Returns\n        -------\n        list:\n            The jobs to be undertaken split into phases that fit into GPU RAM\n        \"\"\"\n        phases: list[list[str]] = []\n        current_phase: list[str] = []\n        available = T.cast(int, self._vram_stats[\"vram_free\"])\n        for phase in self._flow:\n            num_plugins = len([p for p in current_phase if self._vram_per_phase[p] > 0])\n            num_plugins += 1 if self._vram_per_phase[phase] > 0 else 0\n            scaling = self._parallel_scaling.get(num_plugins, self._scaling_fallback)\n            required = sum(self._vram_per_phase[p] for p in current_phase + [phase]) * scaling\n            logger.debug(\"Num plugins for phase: %s, scaling: %s, vram required: %s\",\n                         num_plugins, scaling, required)\n            if required <= available and multiprocess:\n                logger.debug(\"Required: %s, available: %s. Adding phase '%s' to current phase: %s\",\n                             required, available, phase, current_phase)\n                current_phase.append(phase)\n            elif len(current_phase) == 0 or not multiprocess:\n                # Amount of VRAM required to run a single plugin is greater than available. We add\n                # it anyway, and hope it will run with warnings, as the alternative is to not run\n                # at all.\n                # This will also run if forcing single process\n                logger.debug(\"Required: %s, available: %s. Single plugin has higher requirements \"\n                             \"than available or forcing single process: '%s'\",\n                             required, available, phase)\n                phases.append([phase])\n            else:\n                logger.debug(\"Required: %s, available: %s. Adding phase to flow: %s\",\n                             required, available, current_phase)\n                phases.append(current_phase)\n                current_phase = [phase]\n        if current_phase:\n            phases.append(current_phase)\n        logger.debug(\"Total phases: %s, Phases: %s\", len(phases), phases)\n        return phases\n\n    # << INTERNAL PLUGIN HANDLING >> #\n    def _load_align(self,\n                    aligner: str | None,\n                    configfile: str | None,\n                    normalize_method: T.Literal[\"none\", \"clahe\", \"hist\", \"mean\"] | None,\n                    re_feed: int,\n                    re_align: bool,\n                    disable_filter: bool) -> Aligner | None:\n        \"\"\" Set global arguments and load aligner plugin\n\n        Parameters\n        ----------\n        aligner: str\n            The aligner plugin to load or ``None`` for no aligner\n        configfile: str\n            Optional full path to custom config file\n        normalize_method: str\n            Optional normalization method to use\n        re_feed: int\n            The number of times to adjust the image and re-feed to get an average score\n        re_align: bool\n            ``True`` to obtain landmarks by passing the initially aligned face back through the\n            aligner.\n        disable_filter: bool\n            Disable all aligner filters regardless of config option\n\n        Returns\n        -------\n        Aligner plugin if one is specified otherwise ``None``\n        \"\"\"\n        if aligner is None or aligner.lower() == \"none\":\n            logger.debug(\"No aligner selected. Returning None\")\n            return None\n        aligner_name = aligner.replace(\"-\", \"_\").lower()\n        logger.debug(\"Loading Aligner: '%s'\", aligner_name)\n        plugin = PluginLoader.get_aligner(aligner_name)(exclude_gpus=self._exclude_gpus,\n                                                        configfile=configfile,\n                                                        normalize_method=normalize_method,\n                                                        re_feed=re_feed,\n                                                        re_align=re_align,\n                                                        disable_filter=disable_filter,\n                                                        instance=self._instance)\n        return plugin\n\n    def _load_detect(self,\n                     detector: str | None,\n                     aligner: str | None,\n                     rotation: str | None,\n                     min_size: int,\n                     configfile: str | None) -> Detector | None:\n        \"\"\" Set global arguments and load detector plugin\n\n        Parameters\n        ----------\n        detector: str | None\n            The name of the face detection plugin to use. ``None`` for no detection\n        aligner: str | None\n            The name of the face aligner plugin to use. ``None`` for no aligner\n        rotation: str | None\n            The rotation to perform on detection. ``None`` for no rotation\n        min_size: int\n            The minimum size of detected faces to accept\n        configfile: str | None\n            Full path to a custom config file to use. ``None`` for default config\n\n        Returns\n        -------\n        :class:`~plugins.extract.detect._base.Detector` | None\n            The face detection plugin to use, or ``None`` if no detection to be performed\n        \"\"\"\n        if detector is None or detector.lower() == \"none\":\n            logger.debug(\"No detector selected. Returning None\")\n            return None\n        detector_name = detector.replace(\"-\", \"_\").lower()\n\n        if aligner == \"external\" and detector_name != \"external\":\n            logger.warning(\"Unsupported '%s' detector selected for 'External' aligner. Switching \"\n                           \"detector to 'External'\", detector_name)\n            detector_name = aligner\n\n        logger.debug(\"Loading Detector: '%s'\", detector_name)\n        plugin = PluginLoader.get_detector(detector_name)(exclude_gpus=self._exclude_gpus,\n                                                          rotation=rotation,\n                                                          min_size=min_size,\n                                                          configfile=configfile,\n                                                          instance=self._instance)\n        return plugin\n\n    def _load_mask(self,\n                   masker: str | None,\n                   configfile: str | None) -> Masker | None:\n        \"\"\" Set global arguments and load masker plugin\n\n        Parameters\n        ----------\n        masker: str or ``none``\n            The name of the masker plugin to use or ``None`` if no masker\n        configfile: str\n            Full path to custom config.ini file or ``None`` to use default\n\n        Returns\n        -------\n        :class:`~plugins.extract.mask._base.Masker` or ``None``\n            The masker plugin to use or ``None`` if no masker selected\n        \"\"\"\n        if masker is None or masker.lower() == \"none\":\n            logger.debug(\"No masker selected. Returning None\")\n            return None\n        masker_name = masker.replace(\"-\", \"_\").lower()\n        logger.debug(\"Loading Masker: '%s'\", masker_name)\n        plugin = PluginLoader.get_masker(masker_name)(exclude_gpus=self._exclude_gpus,\n                                                      configfile=configfile,\n                                                      instance=self._instance)\n        return plugin\n\n    def _load_recognition(self,\n                          recognition: str | None,\n                          configfile: str | None) -> Identity | None:\n        \"\"\" Set global arguments and load recognition plugin \"\"\"\n        if recognition is None or recognition.lower() == \"none\":\n            logger.debug(\"No recognition selected. Returning None\")\n            return None\n        recognition_name = recognition.replace(\"-\", \"_\").lower()\n        logger.debug(\"Loading Recognition: '%s'\", recognition_name)\n        plugin = PluginLoader.get_recognition(recognition_name)(exclude_gpus=self._exclude_gpus,\n                                                                configfile=configfile,\n                                                                instance=self._instance)\n        return plugin\n\n    def _launch_plugin(self, phase: str) -> None:\n        \"\"\" Launch an extraction plugin \"\"\"\n        logger.debug(\"Launching %s plugin\", phase)\n        in_qname = f\"extract{self._instance}_{phase}_in\"\n        if phase == self._final_phase:\n            out_qname = f\"extract{self._instance}_{self._final_phase}_out\"\n        else:\n            next_phase = self._flow[self._flow.index(phase) + 1]\n            out_qname = f\"extract{self._instance}_{next_phase}_in\"\n        logger.debug(\"in_qname: %s, out_qname: %s\", in_qname, out_qname)\n        kwargs = {\"in_queue\": self._queues[in_qname], \"out_queue\": self._queues[out_qname]}\n\n        plugin_type, idx = self._get_plugin_type_and_index(phase)\n        plugin = getattr(self, f\"_{plugin_type}\")\n        plugin = plugin[idx] if idx is not None else plugin\n        plugin.initialize(**kwargs)\n        plugin.start()\n        logger.debug(\"Launched %s plugin\", phase)\n\n    def _set_extractor_batchsize(self) -> None:\n        \"\"\"\n        Sets the batch size of the requested plugins based on their vram, their\n        vram_per_batch_requirements and the number of plugins being loaded in the current phase.\n        Only adjusts if the the configured batch size requires more vram than is available. Nvidia\n        only.\n        \"\"\"\n        backend = get_backend()\n        if backend not in (\"nvidia\", \"directml\", \"rocm\"):\n            logger.debug(\"Not updating batchsize requirements for backend: '%s'\", backend)\n            return\n        if sum(plugin.vram for plugin in self._active_plugins) == 0:\n            logger.debug(\"No plugins use VRAM. Not updating batchsize requirements.\")\n            return\n\n        batch_required = sum(plugin.vram_per_batch * plugin.batchsize\n                             for plugin in self._active_plugins)\n        gpu_plugins = [p for p in self._current_phase if self._vram_per_phase[p] > 0]\n        scaling = self._parallel_scaling.get(len(gpu_plugins), self._scaling_fallback)\n        plugins_required = sum(self._vram_per_phase[p] for p in gpu_plugins) * scaling\n        if plugins_required + batch_required <= T.cast(int, self._vram_stats[\"vram_free\"]):\n            logger.debug(\"Plugin requirements within threshold: (plugins_required: %sMB, \"\n                         \"vram_free: %sMB)\", plugins_required, self._vram_stats[\"vram_free\"])\n            return\n        # Hacky split across plugins that use vram\n        available_vram = (T.cast(int, self._vram_stats[\"vram_free\"])\n                          - plugins_required) // len(gpu_plugins)\n        self._set_plugin_batchsize(gpu_plugins, available_vram)\n\n    def _set_plugin_batchsize(self, gpu_plugins: list[str], available_vram: float) -> None:\n        \"\"\" Set the batch size for the given plugin based on given available vram.\n        Do not update plugins which have a vram_per_batch of 0 (CPU plugins) due to\n        zero division error.\n        \"\"\"\n        plugins = [self._active_plugins[idx]\n                   for idx, plugin in enumerate(self._current_phase)\n                   if plugin in gpu_plugins]\n        vram_per_batch = [plugin.vram_per_batch for plugin in plugins]\n        ratios = [vram / sum(vram_per_batch) for vram in vram_per_batch]\n        requested_batchsizes = [plugin.batchsize for plugin in plugins]\n        batchsizes = [min(requested, max(1, int((available_vram * ratio) / plugin.vram_per_batch)))\n                      for ratio, plugin, requested in zip(ratios, plugins, requested_batchsizes)]\n        remaining = available_vram - sum(batchsize * plugin.vram_per_batch\n                                         for batchsize, plugin in zip(batchsizes, plugins))\n        sorted_indices = [i[0] for i in sorted(enumerate(plugins),\n                                               key=lambda x: x[1].vram_per_batch, reverse=True)]\n\n        logger.debug(\"requested_batchsizes: %s, batchsizes: %s, remaining vram: %s\",\n                     requested_batchsizes, batchsizes, remaining)\n\n        while remaining > min(plugin.vram_per_batch\n                              for plugin in plugins) and requested_batchsizes != batchsizes:\n            for idx in sorted_indices:\n                plugin = plugins[idx]\n                if plugin.vram_per_batch > remaining:\n                    logger.debug(\"Not enough VRAM to increase batch size of %s. Required: %sMB, \"\n                                 \"Available: %sMB\", plugin, plugin.vram_per_batch, remaining)\n                    continue\n                if plugin.batchsize == batchsizes[idx]:\n                    logger.debug(\"Threshold reached for %s. Batch size: %s\",\n                                 plugin, plugin.batchsize)\n                    continue\n                logger.debug(\"Incrementing batch size of %s to %s\", plugin, batchsizes[idx] + 1)\n                batchsizes[idx] += 1\n                remaining -= plugin.vram_per_batch\n                logger.debug(\"Remaining VRAM to allocate: %sMB\", remaining)\n\n        if batchsizes != requested_batchsizes:\n            text = \", \".join([f\"{plugin.__class__.__name__}: {batchsize}\"\n                              for plugin, batchsize in zip(plugins, batchsizes)])\n            for plugin, batchsize in zip(plugins, batchsizes):\n                plugin.batchsize = batchsize\n            logger.info(\"Reset batch sizes due to available VRAM: %s\", text)\n\n    def _join_threads(self):\n        \"\"\" Join threads for current pass \"\"\"\n        for plugin in self._active_plugins:\n            plugin.join()\n\n    def _check_and_raise_error(self) -> None:\n        \"\"\" Check all threads for errors and raise if one occurs \"\"\"\n        for plugin in self._active_plugins:\n            plugin.check_and_raise_error()\n", "plugins/extract/_config.py": "#!/usr/bin/env python3\n\"\"\" Default configurations for extract \"\"\"\n\nimport gettext\nimport logging\nimport os\n\nfrom lib.config import FaceswapConfig\n\n# LOCALES\n_LANG = gettext.translation(\"plugins.extract._config\", localedir=\"locales\", fallback=True)\n_ = _LANG.gettext\n\nlogger = logging.getLogger(__name__)\n\n\nclass Config(FaceswapConfig):\n    \"\"\" Config File for Extraction \"\"\"\n\n    def set_defaults(self) -> None:\n        \"\"\" Set the default values for config \"\"\"\n        logger.debug(\"Setting defaults\")\n        self.set_globals()\n        self._defaults_from_plugin(os.path.dirname(__file__))\n\n    def set_globals(self) -> None:\n        \"\"\"\n        Set the global options for extract\n        \"\"\"\n        logger.debug(\"Setting global config\")\n        section = \"global\"\n        self.add_section(section, _(\"Options that apply to all extraction plugins\"))\n        self.add_item(\n            section=section,\n            title=\"allow_growth\",\n            datatype=bool,\n            default=False,\n            group=_(\"settings\"),\n            info=_(\"Enable the Tensorflow GPU `allow_growth` configuration option. \"\n                   \"This option prevents Tensorflow from allocating all of the GPU VRAM at launch \"\n                   \"but can lead to higher VRAM fragmentation and slower performance. Should only \"\n                   \"be enabled if you are having problems running extraction.\"))\n        self.add_item(\n            section=section,\n            title=\"aligner_min_scale\",\n            datatype=float,\n            min_max=(0.0, 1.0),\n            rounding=2,\n            default=0.07,\n            group=_(\"filters\"),\n            info=_(\"Filters out faces below this size. This is a multiplier of the minimum \"\n                   \"dimension of the frame (i.e. 1280x720 = 720). If the original face extract \"\n                   \"box is smaller than the minimum dimension times this multiplier, it is \"\n                   \"considered a false positive and discarded. Faces which are found to be \"\n                   \"unusually smaller than the frame tend to be misaligned images, except in \"\n                   \"extreme long-shots. These can be usually be safely discarded.\"))\n        self.add_item(\n            section=section,\n            title=\"aligner_max_scale\",\n            datatype=float,\n            min_max=(0.0, 10.0),\n            rounding=2,\n            default=2.00,\n            group=_(\"filters\"),\n            info=_(\"Filters out faces above this size. This is a multiplier of the minimum \"\n                   \"dimension of the frame (i.e. 1280x720 = 720). If the original face extract \"\n                   \"box is larger than the minimum dimension times this multiplier, it is \"\n                   \"considered a false positive and discarded. Faces which are found to be \"\n                   \"unusually larger than the frame tend to be misaligned images except in \"\n                   \"extreme close-ups. These can be usually be safely discarded.\"))\n        self.add_item(\n            section=section,\n            title=\"aligner_distance\",\n            datatype=float,\n            min_max=(0.0, 45.0),\n            rounding=1,\n            default=22.5,\n            group=_(\"filters\"),\n            info=_(\"Filters out faces who's landmarks are above this distance from an 'average' \"\n                   \"face. Values above 15 tend to be fairly safe. Values above 10 will remove \"\n                   \"more false positives, but may also filter out some faces at extreme angles.\"))\n        self.add_item(\n            section=section,\n            title=\"aligner_roll\",\n            datatype=float,\n            min_max=(0.0, 90.0),\n            rounding=1,\n            default=45.0,\n            group=_(\"filters\"),\n            info=_(\"Filters out faces who's calculated roll is greater than zero +/- this value \"\n                   \"in degrees. Aligned faces should have a roll value close to zero. Values that \"\n                   \"are a significant distance from 0 degrees tend to be misaligned images. These \"\n                   \"can usually be safely disgarded.\"))\n        self.add_item(\n            section=section,\n            title=\"aligner_features\",\n            datatype=bool,\n            default=True,\n            group=_(\"filters\"),\n            info=_(\"Filters out faces where the lowest point of the aligned face's eye or eyebrow \"\n                   \"is lower than the highest point of the aligned face's mouth. Any faces where \"\n                   \"this occurs are misaligned and can be safely disgarded.\"))\n        self.add_item(\n            section=section,\n            title=\"filter_refeed\",\n            datatype=bool,\n            default=True,\n            group=_(\"filters\"),\n            info=_(\"If enabled, and 're-feed' has been selected for extraction, then interim \"\n                   \"alignments will be filtered prior to averaging the final landmarks. This can \"\n                   \"help improve the final alignments by removing any obvious misaligns from the \"\n                   \"interim results, and may also help pick up difficult alignments. If disabled, \"\n                   \"then all re-feed results will be averaged.\"))\n        self.add_item(\n            section=section,\n            title=\"save_filtered\",\n            datatype=bool,\n            default=False,\n            group=_(\"filters\"),\n            info=_(\"If enabled, saves any filtered out images into a sub-folder during the \"\n                   \"extraction process. If disabled, filtered faces are deleted. Note: The faces \"\n                   \"will always be filtered out of the alignments file, regardless of whether you \"\n                   \"keep the faces or not.\"))\n        self.add_item(\n            section=section,\n            title=\"realign_refeeds\",\n            datatype=bool,\n            default=True,\n            group=_(\"re-align\"),\n            info=_(\"If enabled, and 're-align' has been selected for extraction, then all re-feed \"\n                   \"iterations are re-aligned. If disabled, then only the final averaged output \"\n                   \"from re-feed will be re-aligned.\"))\n        self.add_item(\n            section=section,\n            title=\"filter_realign\",\n            datatype=bool,\n            default=True,\n            group=_(\"re-align\"),\n            info=_(\"If enabled, and 're-align' has been selected for extraction, then any \"\n                   \"alignments which would be filtered out will not be re-aligned.\"))\n", "plugins/extract/_base.py": "#!/usr/bin/env python3\n\"\"\" Base class for Faceswap :mod:`~plugins.extract.detect`, :mod:`~plugins.extract.align` and\n:mod:`~plugins.extract.mask` Plugins\n\"\"\"\nfrom __future__ import annotations\nimport logging\nimport typing as T\n\nfrom dataclasses import dataclass, field\n\nimport numpy as np\nfrom tensorflow.python.framework import errors_impl as tf_errors  # pylint:disable=no-name-in-module  # noqa\n\nfrom lib.multithreading import MultiThread\nfrom lib.queue_manager import queue_manager\nfrom lib.utils import GetModel, FaceswapError\nfrom ._config import Config\nfrom . import ExtractMedia\n\nif T.TYPE_CHECKING:\n    from collections.abc import Callable, Generator, Sequence\n    from queue import Queue\n    import cv2\n    from lib.align import DetectedFace\n    from lib.model.session import KSession\n    from .align._base import AlignerBatch\n    from .detect._base import DetectorBatch\n    from .mask._base import MaskerBatch\n    from .recognition._base import RecogBatch\n\nlogger = logging.getLogger(__name__)\n# TODO Run with warnings mode\n\n\ndef _get_config(plugin_name: str, configfile: str | None = None) -> dict[str, T.Any]:\n    \"\"\" Return the configuration for the requested model\n\n    Parameters\n    ----------\n    plugin_name: str\n        The module name of the child plugin.\n    configfile: str, optional\n        Path to a :file:`./config/<plugin_type>.ini` file for this plugin. Default: use system\n        configuration.\n\n    Returns\n    -------\n    config_dict, dict\n       A dictionary of configuration items from the configuration file\n    \"\"\"\n    return Config(plugin_name, configfile=configfile).config_dict\n\n\nBatchType = T.Union[\"DetectorBatch\", \"AlignerBatch\", \"MaskerBatch\", \"RecogBatch\"]\n\n\n@dataclass\nclass ExtractorBatch:\n    \"\"\" Dataclass for holding a batch flowing through post Detector plugins.\n\n    The batch size for post Detector plugins is not the same as the overall batch size.\n    An image may contain 0 or more detected faces, and these need to be split and recombined\n    to be able to utilize a plugin's internal batch size.\n\n    Plugin types will inherit from this class and add required keys.\n\n    Parameters\n    ----------\n    image: list\n        List of :class:`numpy.ndarray` containing the original frames\n    detected_faces: list\n        List of :class:`~lib.align.DetectedFace` objects\n    filename: list\n        List of original frame filenames for the batch\n    feed: :class:`numpy.ndarray`\n        Batch of feed images to feed the net with\n    prediction: :class:`numpy.nd.array`\n        Batch of predictions. Direct output from the aligner net\n    data: dict\n        Any specific data required during the processing phase for a particular plugin\n    \"\"\"\n    image: list[np.ndarray] = field(default_factory=list)\n    detected_faces: Sequence[DetectedFace | list[DetectedFace]] = field(default_factory=list)\n    filename: list[str] = field(default_factory=list)\n    feed: np.ndarray = np.array([])\n    prediction: np.ndarray = np.array([])\n    data: list[dict[str, T.Any]] = field(default_factory=list)\n\n    def __repr__(self) -> str:\n        \"\"\" Prettier repr for debug printing \"\"\"\n        data = [{k: (v.shape, v.dtype) if isinstance(v, np.ndarray) else v for k, v in dat.items()}\n                for dat in self.data]\n        return (f\"{self.__class__.__name__}(\"\n                f\"image={[(img.shape, img.dtype) for img in self.image]}, \"\n                f\"detected_faces={self.detected_faces}, \"\n                f\"filename={self.filename}, \"\n                f\"feed={[(f.shape, f.dtype) for f in self.feed]}, \"\n                f\"prediction=({self.prediction.shape}, {self.prediction.dtype}), \"\n                f\"data={data}\")\n\n\nclass Extractor():\n    \"\"\" Extractor Plugin Object\n\n    All ``_base`` classes for Aligners, Detectors and Maskers inherit from this class.\n\n    This class sets up a pipeline for working with ML plugins.\n\n    Plugins are split into 3 threads, to utilize Numpy and CV2s parallel processing, as well as\n    allow the predict function of the model to sit in a dedicated thread.\n    A plugin is expected to have 3 core functions, each in their own thread:\n    - :func:`process_input()` - Prepare the data for feeding into a model\n    - :func:`predict` - Feed the data through the model\n    - :func:`process_output()` - Perform any data post-processing\n\n    Parameters\n    ----------\n    git_model_id: int\n        The second digit in the github tag that identifies this model. See\n        https://github.com/deepfakes-models/faceswap-models for more information\n    model_filename: str\n        The name of the model file to be loaded\n    exclude_gpus: list, optional\n        A list of indices correlating to connected GPUs that Tensorflow should not use. Pass\n        ``None`` to not exclude any GPUs. Default: ``None``\n    configfile: str, optional\n        Path to a custom configuration ``ini`` file. Default: Use system configfile\n    instance: int, optional\n        If this plugin is being executed multiple times (i.e. multiple pipelines have been\n        launched), the instance of the plugin must be passed in for naming convention reasons.\n        Default: 0\n\n\n    The following attributes should be set in the plugin's :func:`__init__` method after\n    initializing the parent.\n\n    Attributes\n    ----------\n    name: str\n        Name of this plugin. Used for display purposes.\n    input_size: int\n        The input size to the model in pixels across one edge. The input size should always be\n        square.\n    color_format: str\n        Color format for model. Must be ``'BGR'``, ``'RGB'`` or ``'GRAY'``. Defaults to ``'BGR'``\n        if not explicitly set.\n    vram: int\n        Approximate VRAM used by the model at :attr:`input_size`. Used to calculate the\n        :attr:`batchsize`. Be conservative to avoid OOM.\n    vram_warnings: int\n        Approximate VRAM used by the model at :attr:`input_size` that will still run, but generates\n        warnings. Used to calculate the :attr:`batchsize`. Be conservative to avoid OOM.\n    vram_per_batch: int\n        Approximate additional VRAM used by the model for each additional batch. Used to calculate\n        the :attr:`batchsize`. Be conservative to avoid OOM.\n\n    See Also\n    --------\n    plugins.extract.detect._base : Detector parent class for extraction plugins.\n    plugins.extract.align._base : Aligner parent class for extraction plugins.\n    plugins.extract.mask._base : Masker parent class for extraction plugins.\n    plugins.extract.pipeline : The extract pipeline that configures and calls all plugins\n\n    \"\"\"\n    def __init__(self,\n                 git_model_id: int | None = None,\n                 model_filename: str | list[str] | None = None,\n                 exclude_gpus: list[int] | None = None,\n                 configfile: str | None = None,\n                 instance: int = 0) -> None:\n        logger.debug(\"Initializing %s: (git_model_id: %s, model_filename: %s, exclude_gpus: %s, \"\n                     \"configfile: %s, instance: %s, )\", self.__class__.__name__, git_model_id,\n                     model_filename, exclude_gpus, configfile, instance)\n        self._is_initialized = False\n        self._instance = instance\n        self._exclude_gpus = exclude_gpus\n        self.config = _get_config(\".\".join(self.__module__.split(\".\")[-2:]), configfile=configfile)\n        \"\"\" dict: Config for this plugin, loaded from ``extract.ini`` configfile \"\"\"\n\n        self.model_path = self._get_model(git_model_id, model_filename)\n        \"\"\" str or list: Path to the model file(s) (if required). Multiple model files should\n        be a list of strings \"\"\"\n\n        # << SET THE FOLLOWING IN PLUGINS __init__ IF DIFFERENT FROM DEFAULT >> #\n        self.name: str | None = None\n        self.input_size = 0\n        self.color_format: T.Literal[\"BGR\", \"RGB\", \"GRAY\"] = \"BGR\"\n        self.vram = 0\n        self.vram_warnings = 0  # Will run at this with warnings\n        self.vram_per_batch = 0\n\n        # << THE FOLLOWING ARE SET IN self.initialize METHOD >> #\n        self.queue_size = 1\n        \"\"\" int: Queue size for all internal queues. Set in :func:`initialize()` \"\"\"\n\n        self.model: KSession | cv2.dnn.Net | None = None\n        \"\"\"varies: The model for this plugin. Set in the plugin's :func:`init_model()` method \"\"\"\n\n        # For detectors that support batching, this should be set to  the calculated batch size\n        # that the amount of available VRAM will support.\n        self.batchsize = 1\n        \"\"\" int: Batchsize for feeding this model. The number of images the model should\n        feed through at once. \"\"\"\n\n        self._queues: dict[str, Queue] = {}\n        \"\"\" dict: in + out queues and internal queues for this plugin, \"\"\"\n\n        self._threads: list[MultiThread] = []\n        \"\"\" list: Internal threads for this plugin \"\"\"\n\n        self._extract_media: dict[str, ExtractMedia] = {}\n        \"\"\" dict: The :class:`~plugins.extract.extract_media.ExtractMedia` objects currently being\n        processed. Stored at input for pairing back up on output of extractor process \"\"\"\n\n        # << THE FOLLOWING PROTECTED ATTRIBUTES ARE SET IN PLUGIN TYPE _base.py >>> #\n        self._plugin_type: T.Literal[\"align\", \"detect\", \"recognition\", \"mask\"] | None = None\n        \"\"\" str: Plugin type. ``detect`, ``align``, ``recognise`` or ``mask`` set in\n        ``<plugin_type>._base`` \"\"\"\n\n        # << Objects for splitting frame's detected faces and rejoining them >>\n        # << for post-detector pliugins                                      >>\n        self._faces_per_filename: dict[str, int] = {}  # Tracking for recompiling batches\n        self._rollover: ExtractMedia | None = None  # batch rollover items\n        self._output_faces: list[DetectedFace] = []  # Recompiled output faces from plugin\n\n        logger.debug(\"Initialized _base %s\", self.__class__.__name__)\n\n    # <<< OVERIDABLE METHODS >>> #\n    def init_model(self) -> None:\n        \"\"\" **Override method**\n\n        Override this method to execute the specific model initialization method \"\"\"\n        raise NotImplementedError\n\n    def process_input(self, batch: BatchType) -> None:\n        \"\"\" **Override method**\n\n        Override this method for specific extractor pre-processing of image\n\n        Parameters\n        ----------\n        batch : :class:`ExtractorBatch`\n            Contains the batch that is currently being passed through the plugin process\n        \"\"\"\n        raise NotImplementedError\n\n    def predict(self, feed: np.ndarray) -> np.ndarray:\n        \"\"\" **Override method**\n\n        Override this method for specific extractor model prediction function\n\n        Parameters\n        ----------\n        feed: :class:`numpy.ndarray`\n            The feed images for the batch\n\n        Notes\n        -----\n        Input for :func:`predict` should have been set in :func:`process_input`\n\n        Output from the model should populate the key :attr:`prediction` of the :attr:`batch`.\n\n        For Detect:\n            the expected output for the :attr:`prediction` of the :attr:`batch` should be a\n            ``list`` of :attr:`batchsize` of detected face points. These points should be either\n            a ``list``, ``tuple`` or ``numpy.ndarray`` with the first 4 items being the `left`,\n            `top`, `right`, `bottom` points, in that order\n        \"\"\"\n        raise NotImplementedError\n\n    def process_output(self, batch: BatchType) -> None:\n        \"\"\" **Override method**\n\n        Override this method for specific extractor model post predict function\n\n        Parameters\n        ----------\n        batch: :class:`ExtractorBatch`\n            Contains the batch that is currently being passed through the plugin process\n\n        Notes\n        -----\n        For Align:\n            The :attr:`landmarks` must be populated in :attr:`batch` from this method.\n            This should be a ``list`` or :class:`numpy.ndarray` of :attr:`batchsize` containing a\n            ``list``, ``tuple`` or :class:`numpy.ndarray` of `(x, y)` coordinates of the 68 point\n            landmarks as calculated from the :attr:`model`.\n        \"\"\"\n        raise NotImplementedError\n\n    def on_completion(self) -> None:\n        \"\"\" Override to perform an action when the extract process has completed. By default, no\n        action is undertaken \"\"\"\n        return\n\n    def _predict(self, batch: BatchType) -> BatchType:\n        \"\"\" **Override method** (at `<plugin_type>` level)\n\n        This method should be overridden at the `<plugin_type>` level (IE.\n        ``plugins.extract.detect._base`` or ``plugins.extract.align._base``) and should not\n        be overridden within plugins themselves.\n\n        It acts as a wrapper for the plugin's ``self.predict`` method and handles any\n        predict processing that is consistent for all plugins within the `plugin_type`\n\n        Parameters\n        ----------\n        batch: :class:`ExtractorBatch`\n            Contains the batch that is currently being passed through the plugin process\n        \"\"\"\n        raise NotImplementedError\n\n    def _process_input(self, batch: BatchType) -> BatchType:\n        \"\"\" **Override method** (at `<plugin_type>` level)\n\n        This method should be overridden at the `<plugin_type>` level (IE.\n        ``plugins.extract.detect._base`` or ``plugins.extract.align._base``) and should not\n        be overridden within plugins themselves.\n\n        It acts as a wrapper for the plugin's :func:`process_input` method and handles any\n        input processing that is consistent for all plugins within the `plugin_type`.\n\n        If this method is not overridden then the plugin's :func:`process_input` is just called.\n\n        Parameters\n        ----------\n        batch: :class:`ExtractorBatch`\n            Contains the batch that is currently being passed through the plugin process\n\n        Notes\n        -----\n        When preparing an input to the model a the attribute :attr:`feed` must be added\n        to the :attr:`batch` which contains this input.\n        \"\"\"\n        self.process_input(batch)\n        return batch\n\n    def _process_output(self, batch: BatchType) -> BatchType:\n        \"\"\" **Override method** (at `<plugin_type>` level)\n\n        This method should be overridden at the `<plugin_type>` level (IE.\n        ``plugins.extract.detect._base`` or ``plugins.extract.align._base``) and should not\n        be overridden within plugins themselves.\n\n        It acts as a wrapper for the plugin's :func:`process_output` method and handles any\n        output processing that is consistent for all plugins within the `plugin_type`.\n\n        If this method is not overridden then the plugin's :func:`process_output` is just called.\n\n        Parameters\n        ----------\n        batch: :class:`ExtractorBatch`\n            Contains the batch that is currently being passed through the plugin process\n        \"\"\"\n        self.process_output(batch)\n        return batch\n\n    def finalize(self, batch: BatchType) -> Generator[ExtractMedia, None, None]:\n        \"\"\" **Override method** (at `<plugin_type>` level)\n\n        This method should be overridden at the `<plugin_type>` level (IE.\n        :mod:`plugins.extract.detect._base`, :mod:`plugins.extract.align._base` or\n        :mod:`plugins.extract.mask._base`) and should not be overridden within plugins themselves.\n\n        Handles consistent finalization for all plugins that exist within that plugin type. Its\n        input is always the output from :func:`process_output()`\n\n        Parameters\n        ----------\n        batch: :class:`ExtractorBatch`\n            Contains the batch that is currently being passed through the plugin process\n        \"\"\"\n        raise NotImplementedError\n\n    def get_batch(self, queue: Queue) -> tuple[bool, BatchType]:\n        \"\"\" **Override method** (at `<plugin_type>` level)\n\n        This method should be overridden at the `<plugin_type>` level (IE.\n        :mod:`plugins.extract.detect._base`, :mod:`plugins.extract.align._base` or\n        :mod:`plugins.extract.mask._base`) and should not be overridden within plugins themselves.\n\n        Get :class:`~plugins.extract.extract_media.ExtractMedia` items from the queue in batches of\n        :attr:`batchsize`\n\n        Parameters\n        ----------\n        queue : queue.Queue()\n            The ``queue`` that the batch will be fed from. This will be the input to the plugin.\n        \"\"\"\n        raise NotImplementedError\n\n    # <<< THREADING METHODS >>> #\n    def start(self) -> None:\n        \"\"\" Start all threads\n\n        Exposed for :mod:`~plugins.extract.pipeline` to start plugin's threads\n        \"\"\"\n        for thread in self._threads:\n            thread.start()\n\n    def join(self) -> None:\n        \"\"\" Join all threads\n\n        Exposed for :mod:`~plugins.extract.pipeline` to join plugin's threads\n        \"\"\"\n        for thread in self._threads:\n            thread.join()\n\n    def check_and_raise_error(self) -> None:\n        \"\"\" Check all threads for errors\n\n        Exposed for :mod:`~plugins.extract.pipeline` to check plugin's threads for errors\n        \"\"\"\n        for thread in self._threads:\n            thread.check_and_raise_error()\n\n    def rollover_collector(self, queue: Queue) -> T.Literal[\"EOF\"] | ExtractMedia:\n        \"\"\" For extractors after the Detectors, the number of detected faces per frame vs extractor\n        batch size mean that faces will need to be split/re-joined with frames. The rollover\n        collector can be used to rollover items that don't fit in a batch.\n\n        Collect the item from the :attr:`_rollover` dict or from the queue. Add face count per\n        frame to self._faces_per_filename for joining batches back up in finalize\n\n        Parameters\n        ----------\n        queue: :class:`queue.Queue`\n            The input queue to the aligner. Should contain\n            :class:`~plugins.extract.extract_media.ExtractMedia` objects\n\n        Returns\n        -------\n        :class:`~plugins.extract.extract_media.ExtractMedia` or EOF\n            The next extract media object, or EOF if pipe has ended\n        \"\"\"\n        if self._rollover is not None:\n            logger.trace(\"Getting from _rollover: (filename: `%s`, faces: %s)\",  # type:ignore\n                         self._rollover.filename, len(self._rollover.detected_faces))\n            item: T.Literal[\"EOF\"] | ExtractMedia = self._rollover\n            self._rollover = None\n        else:\n            next_item = self._get_item(queue)\n            # Rollover collector should only be used at entry to plugin\n            assert isinstance(next_item, (ExtractMedia, str))\n            item = next_item\n            if item != \"EOF\":\n                logger.trace(\"Getting from queue: (filename: %s, faces: %s)\",  # type:ignore\n                             item.filename, len(item.detected_faces))\n                self._faces_per_filename[item.filename] = len(item.detected_faces)\n        return item\n\n    # <<< PROTECTED ACCESS METHODS >>> #\n    # <<< INIT METHODS >>> #\n    @classmethod\n    def _get_model(cls,\n                   git_model_id: int | None,\n                   model_filename: str | list[str] | None) -> str | list[str] | None:\n        \"\"\" Check if model is available, if not, download and unzip it \"\"\"\n        if model_filename is None:\n            logger.debug(\"No model_filename specified. Returning None\")\n            return None\n        if git_model_id is None:\n            logger.debug(\"No git_model_id specified. Returning None\")\n            return None\n        model = GetModel(model_filename, git_model_id)\n        return model.model_path\n\n    # <<< PLUGIN INITIALIZATION >>> #\n    def initialize(self, *args, **kwargs) -> None:\n        \"\"\" Initialize the extractor plugin\n\n            Should be called from :mod:`~plugins.extract.pipeline`\n        \"\"\"\n        logger.debug(\"initialize %s: (args: %s, kwargs: %s)\",\n                     self.__class__.__name__, args, kwargs)\n        assert self._plugin_type is not None and self.name is not None\n        if self._is_initialized:\n            # When batch processing, plugins will be initialized on first job in batch\n            logger.debug(\"Plugin already initialized: %s (%s)\",\n                         self.name, self._plugin_type.title())\n            return\n\n        logger.info(\"Initializing %s (%s)...\", self.name, self._plugin_type.title())\n        self.queue_size = 1\n        name = self.name.replace(\" \", \"_\").lower()\n        self._add_queues(kwargs[\"in_queue\"],\n                         kwargs[\"out_queue\"],\n                         [f\"predict_{name}\", f\"post_{name}\"])\n        self._compile_threads()\n        try:\n            self.init_model()\n        except tf_errors.UnknownError as err:\n            if \"failed to get convolution algorithm\" in str(err).lower():\n                msg = (\"Tensorflow raised an unknown error. This is most likely caused by a \"\n                       \"failure to launch cuDNN which can occur for some GPU/Tensorflow \"\n                       \"combinations. You should enable `allow_growth` to attempt to resolve this \"\n                       \"issue:\"\n                       \"\\nGUI: Go to Settings > Extract Plugins > Global and enable the \"\n                       \"`allow_growth` option.\"\n                       \"\\nCLI: Go to `faceswap/config/extract.ini` and change the `allow_growth \"\n                       \"option to `True`.\")\n                raise FaceswapError(msg) from err\n            raise err\n        self._is_initialized = True\n        logger.info(\"Initialized %s (%s) with batchsize of %s\",\n                    self.name, self._plugin_type.title(), self.batchsize)\n\n    def _add_queues(self,\n                    in_queue: Queue,\n                    out_queue: Queue,\n                    queues: list[str]) -> None:\n        \"\"\" Add the queues\n            in_queue and out_queue should be previously created queue manager queues.\n            queues should be a list of queue names \"\"\"\n        self._queues[\"in\"] = in_queue\n        self._queues[\"out\"] = out_queue\n        for q_name in queues:\n            self._queues[q_name] = queue_manager.get_queue(\n                name=f\"{self._plugin_type}{self._instance}_{q_name}\",\n                maxsize=self.queue_size)\n\n    # <<< THREAD METHODS >>> #\n    def _compile_threads(self) -> None:\n        \"\"\" Compile the threads into self._threads list \"\"\"\n        assert self.name is not None\n        logger.debug(\"Compiling %s threads\", self._plugin_type)\n        name = self.name.replace(\" \", \"_\").lower()\n        base_name = f\"{self._plugin_type}_{name}\"\n        self._add_thread(f\"{base_name}_input\",\n                         self._process_input,\n                         self._queues[\"in\"],\n                         self._queues[f\"predict_{name}\"])\n        self._add_thread(f\"{base_name}_predict\",\n                         self._predict,\n                         self._queues[f\"predict_{name}\"],\n                         self._queues[f\"post_{name}\"])\n        self._add_thread(f\"{base_name}_output\",\n                         self._process_output,\n                         self._queues[f\"post_{name}\"],\n                         self._queues[\"out\"])\n        logger.debug(\"Compiled %s threads: %s\", self._plugin_type, self._threads)\n\n    def _add_thread(self,\n                    name: str,\n                    function: Callable[[BatchType], BatchType],\n                    in_queue: Queue,\n                    out_queue: Queue) -> None:\n        \"\"\" Add a MultiThread thread to self._threads \"\"\"\n        logger.debug(\"Adding thread: (name: %s, function: %s, in_queue: %s, out_queue: %s)\",\n                     name, function, in_queue, out_queue)\n        self._threads.append(MultiThread(target=self._thread_process,\n                                         name=name,\n                                         function=function,\n                                         in_queue=in_queue,\n                                         out_queue=out_queue))\n        logger.debug(\"Added thread: %s\", name)\n\n    def _obtain_batch_item(self, function: Callable[[BatchType], BatchType],\n                           in_queue: Queue,\n                           out_queue: Queue) -> BatchType | None:\n        \"\"\" Obtain the batch item from the in queue for the current process.\n\n        Parameters\n        ----------\n        function: callable\n            The current plugin function being run\n        in_queue: :class:`queue.Queue`\n            The input queue for the function\n        out_queue: :class:`queue.Queue`\n            The output queue from the function\n\n        Returns\n        -------\n        :class:`ExtractorBatch` or ``None``\n            The batch, if one exists, or ``None`` if queue is exhausted\n        \"\"\"\n        batch: T.Literal[\"EOF\"] | BatchType | ExtractMedia\n        if function.__name__ == \"_process_input\":  # Process input items to batches\n            exhausted, batch = self.get_batch(in_queue)\n            if exhausted:\n                if batch.filename:\n                    # Put the final batch\n                    batch = function(batch)\n                    out_queue.put(batch)\n                return None\n        else:\n            batch = self._get_item(in_queue)\n            if batch == \"EOF\":\n                return None\n\n        # ExtractMedia should only ever be the output of _get_item at the entry to a\n        # plugin's pipeline (ie in _process_input)\n        assert not isinstance(batch, ExtractMedia)\n        return batch\n\n    def _thread_process(self,\n                        function: Callable[[BatchType], BatchType],\n                        in_queue: Queue,\n                        out_queue: Queue) -> None:\n        \"\"\" Perform a plugin function in a thread\n\n        Parameters\n        ----------\n        function: callable\n            The current plugin function being run\n        in_queue: :class:`queue.Queue`\n            The input queue for the function\n        out_queue: :class:`queue.Queue`\n            The output queue from the function\n         \"\"\"\n        logger.debug(\"threading: (function: '%s')\", function.__name__)\n        while True:\n            batch = self._obtain_batch_item(function, in_queue, out_queue)\n            if batch is None:\n                break\n            if not batch.filename:  # Batch not populated. Possible during re-aligns\n                continue\n            try:\n                batch = function(batch)\n            except tf_errors.UnknownError as err:\n                if \"failed to get convolution algorithm\" in str(err).lower():\n                    msg = (\"Tensorflow raised an unknown error. This is most likely caused by a \"\n                           \"failure to launch cuDNN which can occur for some GPU/Tensorflow \"\n                           \"combinations. You should enable `allow_growth` to attempt to resolve \"\n                           \"this issue:\"\n                           \"\\nGUI: Go to Settings > Extract Plugins > Global and enable the \"\n                           \"`allow_growth` option.\"\n                           \"\\nCLI: Go to `faceswap/config/extract.ini` and change the \"\n                           \"`allow_growth option to `True`.\")\n                    raise FaceswapError(msg) from err\n                raise err\n            if function.__name__ == \"_process_output\":\n                # Process output items to individual items from batch\n                for item in self.finalize(batch):\n                    out_queue.put(item)\n            else:\n                out_queue.put(batch)\n        logger.debug(\"Putting EOF\")\n        out_queue.put(\"EOF\")\n\n    # <<< QUEUE METHODS >>> #\n    def _get_item(self, queue: Queue) -> T.Literal[\"EOF\"] | ExtractMedia | BatchType:\n        \"\"\" Yield one item from a queue \"\"\"\n        item = queue.get()\n        if isinstance(item, ExtractMedia):\n            logger.trace(\"filename: '%s', image shape: %s, detected_faces: %s, \"  # type:ignore\n                         \"queue: %s, item: %s\",\n                         item.filename, item.image_shape, item.detected_faces, queue, item)\n            self._extract_media[item.filename] = item\n        else:\n            logger.trace(\"item: %s, queue: %s\", item, queue)  # type:ignore\n        return item\n", "plugins/extract/extract_media.py": "#!/usr/bin/env python3\n\"\"\" Object for holding and manipulating media passing through a faceswap extraction pipeline \"\"\"\nfrom __future__ import annotations\nimport logging\nimport typing as T\n\nimport cv2\n\nfrom lib.logger import parse_class_init\n\nif T.TYPE_CHECKING:\n    import numpy as np\n    from lib.align.alignments import PNGHeaderSourceDict\n    from lib.align.detected_face import DetectedFace\n\nlogger = logging.getLogger(__name__)\n\n\nclass ExtractMedia:\n    \"\"\" An object that passes through the :class:`~plugins.extract.pipeline.Extractor` pipeline.\n\n    Parameters\n    ----------\n    filename: str\n        The base name of the original frame's filename\n    image: :class:`numpy.ndarray`\n        The original frame or a faceswap aligned face image\n    detected_faces: list, optional\n        A list of :class:`~lib.align.DetectedFace` objects. Detected faces can be added\n        later with :func:`add_detected_faces`. Setting ``None`` will default to an empty list.\n        Default: ``None``\n    is_aligned: bool, optional\n        ``True`` if the :attr:`image` is an aligned faceswap image otherwise ``False``. Used for\n        face filtering with vggface2. Aligned faceswap images will automatically skip detection,\n        alignment and masking. Default: ``False``\n    \"\"\"\n\n    def __init__(self,\n                 filename: str,\n                 image: np.ndarray,\n                 detected_faces: list[DetectedFace] | None = None,\n                 is_aligned: bool = False) -> None:\n        logger.trace(parse_class_init(locals()))  # type:ignore[attr-defined]\n        self._filename = filename\n        self._image: np.ndarray | None = image\n        self._image_shape = T.cast(tuple[int, int, int], image.shape)\n        self._detected_faces: list[DetectedFace] = ([] if detected_faces is None\n                                                    else detected_faces)\n        self._is_aligned = is_aligned\n        self._frame_metadata: PNGHeaderSourceDict | None = None\n        self._sub_folders: list[str | None] = []\n\n    @property\n    def filename(self) -> str:\n        \"\"\" str: The base name of the :attr:`image` filename. \"\"\"\n        return self._filename\n\n    @property\n    def image(self) -> np.ndarray:\n        \"\"\" :class:`numpy.ndarray`: The source frame for this object. \"\"\"\n        assert self._image is not None\n        return self._image\n\n    @property\n    def image_shape(self) -> tuple[int, int, int]:\n        \"\"\" tuple: The shape of the stored :attr:`image`. \"\"\"\n        return self._image_shape\n\n    @property\n    def image_size(self) -> tuple[int, int]:\n        \"\"\" tuple: The (`height`, `width`) of the stored :attr:`image`. \"\"\"\n        return self._image_shape[:2]\n\n    @property\n    def detected_faces(self) -> list[DetectedFace]:\n        \"\"\"list: A list of :class:`~lib.align.DetectedFace` objects in the :attr:`image`. \"\"\"\n        return self._detected_faces\n\n    @property\n    def is_aligned(self) -> bool:\n        \"\"\" bool. ``True`` if :attr:`image` is an aligned faceswap image otherwise ``False`` \"\"\"\n        return self._is_aligned\n\n    @property\n    def frame_metadata(self) -> PNGHeaderSourceDict:\n        \"\"\" dict: The frame metadata that has been added from an aligned image. This property\n        should only be called after :func:`add_frame_metadata` has been called when processing\n        an aligned face. For all other instances an assertion error will be raised.\n\n        Raises\n        ------\n        AssertionError\n            If frame metadata has not been populated from an aligned image\n        \"\"\"\n        assert self._frame_metadata is not None\n        return self._frame_metadata\n\n    @property\n    def sub_folders(self) -> list[str | None]:\n        \"\"\" list: The sub_folders that the faces should be output to. Used when binning filter\n        output is enabled. The list corresponds to the list of detected faces\n        \"\"\"\n        return self._sub_folders\n\n    def get_image_copy(self, color_format: T.Literal[\"BGR\", \"RGB\", \"GRAY\"]) -> np.ndarray:\n        \"\"\" Get a copy of the image in the requested color format.\n\n        Parameters\n        ----------\n        color_format: ['BGR', 'RGB', 'GRAY']\n            The requested color format of :attr:`image`\n\n        Returns\n        -------\n        :class:`numpy.ndarray`:\n            A copy of :attr:`image` in the requested :attr:`color_format`\n        \"\"\"\n        logger.trace(\"Requested color format '%s' for frame '%s'\",  # type:ignore[attr-defined]\n                     color_format, self._filename)\n        image = getattr(self, f\"_image_as_{color_format.lower()}\")()\n        return image\n\n    def add_detected_faces(self, faces: list[DetectedFace]) -> None:\n        \"\"\" Add detected faces to the object. Called at the end of each extraction phase.\n\n        Parameters\n        ----------\n        faces: list\n            A list of :class:`~lib.align.DetectedFace` objects\n        \"\"\"\n        logger.trace(\"Adding detected faces for filename: '%s'. \"  # type:ignore[attr-defined]\n                     \"(faces: %s, lrtb: %s)\", self._filename, faces,\n                     [(face.left, face.right, face.top, face.bottom) for face in faces])\n        self._detected_faces = faces\n\n    def add_sub_folders(self, folders: list[str | None]) -> None:\n        \"\"\" Add detected faces to the object. Called at the end of each extraction phase.\n\n        Parameters\n        ----------\n        folders: list\n            A list of str sub folder names or ``None`` if no sub folder is required. Should\n            correspond to the detected faces list\n        \"\"\"\n        logger.trace(\"Adding sub folders for filename: '%s'. \"  # type:ignore[attr-defined]\n                     \"(folders: %s)\", self._filename, folders,)\n        self._sub_folders = folders\n\n    def remove_image(self) -> None:\n        \"\"\" Delete the image and reset :attr:`image` to ``None``.\n\n        Required for multi-phase extraction to avoid the frames stacking RAM.\n        \"\"\"\n        logger.trace(\"Removing image for filename: '%s'\",  # type:ignore[attr-defined]\n                     self._filename)\n        del self._image\n        self._image = None\n\n    def set_image(self, image: np.ndarray) -> None:\n        \"\"\" Add the image back into :attr:`image`\n\n        Required for multi-phase extraction adds the image back to this object.\n\n        Parameters\n        ----------\n        image: :class:`numpy.ndarry`\n            The original frame to be re-applied to for this :attr:`filename`\n        \"\"\"\n        logger.trace(\"Reapplying image: (filename: `%s`, \"  # type:ignore[attr-defined]\n                     \"image shape: %s)\", self._filename, image.shape)\n        self._image = image\n\n    def add_frame_metadata(self, metadata: PNGHeaderSourceDict) -> None:\n        \"\"\" Add the source frame metadata from an aligned PNG's header data.\n\n        metadata: dict\n            The contents of the 'source' field in the PNG header\n        \"\"\"\n        logger.trace(\"Adding PNG Source data for '%s': %s\",  # type:ignore[attr-defined]\n                     self._filename, metadata)\n        dims = T.cast(tuple[int, int], metadata[\"source_frame_dims\"])\n        self._image_shape = (*dims, 3)\n        self._frame_metadata = metadata\n\n    def _image_as_bgr(self) -> np.ndarray:\n        \"\"\" Get a copy of the source frame in BGR format.\n\n        Returns\n        -------\n        :class:`numpy.ndarray`:\n            A copy of :attr:`image` in BGR color format \"\"\"\n        return self.image[..., :3].copy()\n\n    def _image_as_rgb(self) -> np.ndarray:\n        \"\"\" Get a copy of the source frame in RGB format.\n\n        Returns\n        -------\n        :class:`numpy.ndarray`:\n            A copy of :attr:`image` in RGB color format \"\"\"\n        return self.image[..., 2::-1].copy()\n\n    def _image_as_gray(self) -> np.ndarray:\n        \"\"\" Get a copy of the source frame in gray-scale format.\n\n        Returns\n        -------\n        :class:`numpy.ndarray`:\n            A copy of :attr:`image` in gray-scale color format \"\"\"\n        return cv2.cvtColor(self.image.copy(), cv2.COLOR_BGR2GRAY)\n", "plugins/extract/__init__.py": "#!/usr/bin/env python3\n\"\"\" Package for Faceswap's extraction pipeline \"\"\"\nfrom .extract_media import ExtractMedia\nfrom .pipeline import Extractor\n", "plugins/extract/align/fan_defaults.py": "#!/usr/bin/env python3\n\"\"\"\n    The default options for the faceswap FAN Alignments plugin.\n\n    Defaults files should be named <plugin_name>_defaults.py\n    Any items placed into this file will automatically get added to the relevant config .ini files\n    within the faceswap/config folder.\n\n    The following variables should be defined:\n        _HELPTEXT: A string describing what this plugin does\n        _DEFAULTS: A dictionary containing the options, defaults and meta information. The\n                   dictionary should be defined as:\n                       {<option_name>: {<metadata>}}\n\n                   <option_name> should always be lower text.\n                   <metadata> dictionary requirements are listed below.\n\n    The following keys are expected for the _DEFAULTS <metadata> dict:\n        datatype:  [required] A python type class. This limits the type of data that can be\n                   provided in the .ini file and ensures that the value is returned in the\n                   correct type to faceswap. Valid data types are: <class 'int'>, <class 'float'>,\n                   <class 'str'>, <class 'bool'>.\n        default:   [required] The default value for this option.\n        info:      [required] A string describing what this option does.\n        group:     [optional]. A group for grouping options together in the GUI. If not\n                   provided this will not group this option with any others.\n        choices:   [optional] If this option's datatype is of <class 'str'> then valid\n                   selections can be defined here. This validates the option and also enables\n                   a combobox / radio option in the GUI.\n        gui_radio: [optional] If <choices> are defined, this indicates that the GUI should use\n                   radio buttons rather than a combobox to display this option.\n        min_max:   [partial] For <class 'int'> and <class 'float'> data types this is required\n                   otherwise it is ignored. Should be a tuple of min and max accepted values.\n                   This is used for controlling the GUI slider range. Values are not enforced.\n        rounding:  [partial] For <class 'int'> and <class 'float'> data types this is\n                   required otherwise it is ignored. Used for the GUI slider. For floats, this\n                   is the number of decimal places to display. For ints this is the step size.\n        fixed:     [optional] [train only]. Training configurations are fixed when the model is\n                   created, and then reloaded from the state file. Marking an item as fixed=False\n                   indicates that this value can be changed for existing models, and will override\n                   the value saved in the state file with the updated value in config. If not\n                   provided this will default to True.\n\"\"\"\n\n\n_HELPTEXT = (\n    \"FAN Aligner options.\\n\"\n    \"Fast on GPU, slow on CPU. Best aligner.\"\n    )\n\n\n_DEFAULTS = {\n    \"batch-size\": {\n        \"default\": 12,\n        \"info\": \"The batch size to use. To a point, higher batch sizes equal better performance, \"\n                \"but setting it too high can harm performance.\\n\"\n                \"\\n\\tNvidia users: If the batchsize is set higher than the your GPU can \"\n                \"accomodate then this will automatically be lowered.\"\n                \"\\n\\tAMD users: A batchsize of 8 requires about 4 GB vram.\",\n        \"datatype\": int,\n        \"rounding\": 1,\n        \"min_max\": (1, 64),\n        \"choices\": [],\n        \"group\": \"settings\",\n        \"gui_radio\": False,\n        \"fixed\": True,\n    }\n}\n", "plugins/extract/align/cv2_dnn.py": "#!/usr/bin/env python3\n\"\"\" CV2 DNN landmarks extractor for faceswap.py\nAdapted from: https://github.com/yinguobing/cnn-facial-landmark\nMIT License\n\nCopyright (c) 2017 Yin Guobing\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n\"\"\"\nfrom __future__ import annotations\nimport logging\nimport typing as T\n\nimport cv2\nimport numpy as np\n\nfrom ._base import Aligner, AlignerBatch, BatchType\n\nif T.TYPE_CHECKING:\n    from lib.align.detected_face import DetectedFace\n\nlogger = logging.getLogger(__name__)\n\n\nclass Align(Aligner):\n    \"\"\" Perform transformation to align and get landmarks \"\"\"\n    def __init__(self, **kwargs) -> None:\n        git_model_id = 1\n        model_filename = \"cnn-facial-landmark_v1.pb\"\n        super().__init__(git_model_id=git_model_id, model_filename=model_filename, **kwargs)\n\n        self.model: cv2.dnn.Net\n        self.model_path: str\n        self.name = \"cv2-DNN Aligner\"\n        self.input_size = 128\n        self.color_format = \"RGB\"\n        self.vram = 0  # Doesn't use GPU\n        self.vram_per_batch = 0\n        self.batchsize = 1\n        self.realign_centering = \"legacy\"\n\n    def init_model(self) -> None:\n        \"\"\" Initialize CV2 DNN Detector Model\"\"\"\n        self.model = cv2.dnn.readNetFromTensorflow(self.model_path)\n        self.model.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)\n\n    def faces_to_feed(self, faces: np.ndarray) -> np.ndarray:\n        \"\"\" Convert a batch of face images from UINT8 (0-255) to fp32 (0.0-255.0)\n\n        Parameters\n        ----------\n        faces: :class:`numpy.ndarray`\n            The batch of faces in UINT8 format\n\n        Returns\n        -------\n        class: `numpy.ndarray`\n            The batch of faces as fp32\n        \"\"\"\n        return faces.astype(\"float32\").transpose((0, 3, 1, 2))\n\n    def process_input(self, batch: BatchType) -> None:\n        \"\"\" Compile the detected faces for prediction\n\n        Parameters\n        ----------\n        batch: :class:`AlignerBatch`\n            The current batch to process input for\n\n        Returns\n        -------\n        :class:`AlignerBatch`\n            The batch item with the :attr:`feed` populated and any required :attr:`data` added\n        \"\"\"\n        assert isinstance(batch, AlignerBatch)\n        lfaces, roi, offsets = self.align_image(batch)\n        batch.feed = np.array(lfaces)[..., :3]\n        batch.data.append({\"roi\": roi, \"offsets\": offsets})\n\n    def _get_box_and_offset(self, face: DetectedFace) -> tuple[list[int], int]:\n        \"\"\"Obtain the bounding box and offset from a detected face.\n\n\n        Parameters\n        ----------\n        face: :class:`~lib.align.DetectedFace`\n            The detected face object to obtain the bounding box and offset from\n\n        Returns\n        -------\n        box: list\n            The [left, top, right, bottom] bounding box\n        offset: int\n            The offset of the box (difference between half width vs height)\n        \"\"\"\n\n        box = T.cast(list[int], [face.left,\n                                 face.top,\n                                 face.right,\n                                 face.bottom])\n        diff_height_width = T.cast(int, face.height) - T.cast(int, face.width)\n        offset = int(abs(diff_height_width / 2))\n        return box, offset\n\n    def align_image(self, batch: AlignerBatch) -> tuple[list[np.ndarray],\n                                                        list[list[int]],\n                                                        list[tuple[int, int]]]:\n        \"\"\" Align the incoming image for prediction\n\n        Parameters\n        ----------\n        batch: :class:`AlignerBatch`\n            The current batch to align the input for\n\n        Returns\n        -------\n        faces: list\n            List of feed faces for the aligner\n        rois: list\n            List of roi's for the faces\n        offsets: list\n            List of offsets for the faces\n        \"\"\"\n        logger.trace(\"Aligning image around center\")  # type:ignore[attr-defined]\n        sizes = (self.input_size, self.input_size)\n        rois = []\n        faces = []\n        offsets = []\n        for det_face, image in zip(batch.detected_faces, batch.image):\n            box, offset_y = self._get_box_and_offset(det_face)\n            box_moved = self.move_box(box, (0, offset_y))\n            # Make box square.\n            roi = self.get_square_box(box_moved)\n\n            # Pad the image and adjust roi if face is outside of boundaries\n            image, offset = self.pad_image(roi, image)\n            face = image[roi[1] + abs(offset[1]): roi[3] + abs(offset[1]),\n                         roi[0] + abs(offset[0]): roi[2] + abs(offset[0])]\n            interpolation = cv2.INTER_CUBIC if face.shape[0] < self.input_size else cv2.INTER_AREA\n            face = cv2.resize(face, dsize=sizes, interpolation=interpolation)\n            faces.append(face)\n            rois.append(roi)\n            offsets.append(offset)\n        return faces, rois, offsets\n\n    @classmethod\n    def move_box(cls,\n                 box: list[int],\n                 offset: tuple[int, int]) -> list[int]:\n        \"\"\"Move the box to direction specified by vector offset\n\n        Parameters\n        ----------\n        box: list\n            The (`left`, `top`, `right`, `bottom`) box positions\n        offset: tuple\n            (x, y) offset to move the box\n\n        Returns\n        -------\n        list\n            The original box shifted by the offset\n        \"\"\"\n        left = box[0] + offset[0]\n        top = box[1] + offset[1]\n        right = box[2] + offset[0]\n        bottom = box[3] + offset[1]\n        return [left, top, right, bottom]\n\n    @staticmethod\n    def get_square_box(box: list[int]) -> list[int]:\n        \"\"\"Get a square box out of the given box, by expanding it.\n\n        Parameters\n        ----------\n        box: list\n            The (`left`, `top`, `right`, `bottom`) box positions\n\n        Returns\n        -------\n        list\n            The original box but made square\n        \"\"\"\n        left = box[0]\n        top = box[1]\n        right = box[2]\n        bottom = box[3]\n\n        box_width = right - left\n        box_height = bottom - top\n\n        # Check if box is already a square. If not, make it a square.\n        diff = box_height - box_width\n        delta = int(abs(diff) / 2)\n\n        if diff == 0:                   # Already a square.\n            return box\n        if diff > 0:                    # Height > width, a slim box.\n            left -= delta\n            right += delta\n            if diff % 2 == 1:\n                right += 1\n        else:                           # Width > height, a short box.\n            top -= delta\n            bottom += delta\n            if diff % 2 == 1:\n                bottom += 1\n\n        # Make sure box is always square.\n        assert ((right - left) == (bottom - top)), 'Box is not square.'\n\n        return [left, top, right, bottom]\n\n    @classmethod\n    def pad_image(cls, box: list[int], image: np.ndarray) -> tuple[np.ndarray, tuple[int, int]]:\n        \"\"\"Pad image if face-box falls outside of boundaries\n\n        Parameters\n        ----------\n        box: list\n            The (`left`, `top`, `right`, `bottom`) roi box positions\n        image: :class:`numpy.ndarray`\n            The image to be padded\n\n        Returns\n        -------\n        :class:`numpy.ndarray`\n            The padded image\n        \"\"\"\n        height, width = image.shape[:2]\n        pad_l = 1 - box[0] if box[0] < 0 else 0\n        pad_t = 1 - box[1] if box[1] < 0 else 0\n        pad_r = box[2] - width if box[2] > width else 0\n        pad_b = box[3] - height if box[3] > height else 0\n        logger.trace(\"Padding: (l: %s, t: %s, r: %s, b: %s)\",  # type:ignore[attr-defined]\n                     pad_l, pad_t, pad_r, pad_b)\n        padded_image = cv2.copyMakeBorder(image.copy(),\n                                          pad_t,\n                                          pad_b,\n                                          pad_l,\n                                          pad_r,\n                                          cv2.BORDER_CONSTANT,\n                                          value=(0, 0, 0))\n        offsets = (pad_l - pad_r, pad_t - pad_b)\n        logger.trace(\"image_shape: %s, Padded shape: %s, box: %s, \"  # type:ignore[attr-defined]\n                     \"offsets: %s\",\n                     image.shape, padded_image.shape, box, offsets)\n        return padded_image, offsets\n\n    def predict(self, feed: np.ndarray) -> np.ndarray:\n        \"\"\" Predict the 68 point landmarks\n\n        Parameters\n        ----------\n        feed: :class:`numpy.ndarray`\n            The batch to feed into the aligner\n\n        Returns\n        -------\n        :class:`numpy.ndarray`\n            The predictions from the aligner\n        \"\"\"\n        assert isinstance(self.model, cv2.dnn.Net)\n        self.model.setInput(feed)\n        retval = self.model.forward()\n        return retval\n\n    def process_output(self, batch: BatchType) -> None:\n        \"\"\" Process the output from the model\n\n        Parameters\n        ----------\n        batch: :class:`AlignerBatch`\n            The current batch from the model with :attr:`predictions` populated\n        \"\"\"\n        assert isinstance(batch, AlignerBatch)\n        self.get_pts_from_predict(batch)\n\n    def get_pts_from_predict(self, batch: AlignerBatch):\n        \"\"\" Get points from predictor and populates the :attr:`landmarks` property\n\n        Parameters\n        ----------\n        batch: :class:`AlignerBatch`\n            The current batch from the model with :attr:`predictions` populated\n        \"\"\"\n        landmarks = []\n        if batch.second_pass:\n            batch.landmarks = batch.prediction.reshape(self.batchsize, -1, 2) * self.input_size\n        else:\n            for prediction, roi, offset in zip(batch.prediction,\n                                               batch.data[0][\"roi\"],\n                                               batch.data[0][\"offsets\"]):\n                points = np.reshape(prediction, (-1, 2))\n                points *= (roi[2] - roi[0])\n                points[:, 0] += (roi[0] - offset[0])\n                points[:, 1] += (roi[1] - offset[1])\n                landmarks.append(points)\n            batch.landmarks = np.array(landmarks)\n        logger.trace(\"Predicted Landmarks: %s\", batch.landmarks)  # type:ignore[attr-defined]\n", "plugins/extract/align/external_defaults.py": "#!/usr/bin/env python3\n\"\"\"\n    The default options for the faceswap Import Alignments plugin.\n\n    Defaults files should be named <plugin_name>_defaults.py\n    Any items placed into this file will automatically get added to the relevant config .ini files\n    within the faceswap/config folder.\n\n    The following variables should be defined:\n        _HELPTEXT: A string describing what this plugin does\n        _DEFAULTS: A dictionary containing the options, defaults and meta information. The\n                   dictionary should be defined as:\n                       {<option_name>: {<metadata>}}\n\n                   <option_name> should always be lower text.\n                   <metadata> dictionary requirements are listed below.\n\n    The following keys are expected for the _DEFAULTS <metadata> dict:\n        datatype:  [required] A python type class. This limits the type of data that can be\n                   provided in the .ini file and ensures that the value is returned in the\n                   correct type to faceswap. Valid data types are: <class 'int'>, <class 'float'>,\n                   <class 'str'>, <class 'bool'>.\n        default:   [required] The default value for this option.\n        info:      [required] A string describing what this option does.\n        group:     [optional]. A group for grouping options together in the GUI. If not\n                   provided this will not group this option with any others.\n        choices:   [optional] If this option's datatype is of <class 'str'> then valid\n                   selections can be defined here. This validates the option and also enables\n                   a combobox / radio option in the GUI.\n        gui_radio: [optional] If <choices> are defined, this indicates that the GUI should use\n                   radio buttons rather than a combobox to display this option.\n        min_max:   [partial] For <class 'int'> and <class 'float'> data types this is required\n                   otherwise it is ignored. Should be a tuple of min and max accepted values.\n                   This is used for controlling the GUI slider range. Values are not enforced.\n        rounding:  [partial] For <class 'int'> and <class 'float'> data types this is\n                   required otherwise it is ignored. Used for the GUI slider. For floats, this\n                   is the number of decimal places to display. For ints this is the step size.\n        fixed:     [optional] [train only]. Training configurations are fixed when the model is\n                   created, and then reloaded from the state file. Marking an item as fixed=False\n                   indicates that this value can be changed for existing models, and will override\n                   the value saved in the state file with the updated value in config. If not\n                   provided this will default to True.\n\"\"\"\n\n\n_HELPTEXT = (\n    \"Import Aligner options.\\n\"\n    \"Imports either 68 point 2D landmarks or an aligned bounding box from an external .json file.\"\n    )\n\n\n_DEFAULTS = {\n    \"file_name\": {\n        \"default\": \"import.json\",\n        \"info\": \"The import file should be stored in the same folder as the video (if extracting \"\n        \"from a video file) or inside the folder of images (if importing from a folder of images)\",\n        \"datatype\": str,\n        \"choices\": [],\n        \"group\": \"settings\",\n        \"gui_radio\": False,\n        \"fixed\": True,\n    },\n    \"origin\": {\n        \"default\": \"top-left\",\n        \"info\": \"The origin (0, 0) location of the co-ordinates system used. \"\n                \"\\n\\t top-left: The origin (0, 0) of the canvas is at the top left \"\n                \"corner.\"\n                \"\\n\\t bottom-left: The origin (0, 0) of the canvas is at the bottom \"\n                \"left corner.\"\n                \"\\n\\t top-right: The origin (0, 0) of the canvas is at the top right \"\n                \"corner.\"\n                \"\\n\\t bottom-right: The origin (0, 0) of the canvas is at the bottom \"\n                \"right corner.\",\n        \"datatype\": str,\n        \"choices\": [\"top-left\", \"bottom-left\", \"top-right\", \"bottom-right\"],\n        \"group\": \"input\",\n        \"gui_radio\": True\n    },\n    \"4_point_centering\": {\n        \"default\": \"head\",\n        \"info\": \"4 point ROI landmarks only. The approximate centering for the location of the \"\n                \"corner points to be imported. Default faceswap extracts are generated at 'head' \"\n                \"centering, but it is possible to pass in ROI points at a tighter centering. \"\n                \"Refer to https://github.com/deepfakes/faceswap/pull/1095 for a visual guide\"\n                \"\\n\\t head: The ROI points represent a loose crop enclosing the whole head.\"\n                \"\\n\\t face: The ROI points represent a medium crop enclosing the face.\"\n                \"\\n\\t legacy: The ROI points represent a tight crop enclosing the central face \"\n                \"area.\"\n                \"\\n\\t none: Only required if importing 4 point ROI landmarks back into faceswap \"\n                \"having generated them from the 'alignments' tool 'export' job.\",\n        \"datatype\": str,\n        \"choices\": [\"head\", \"face\", \"legacy\", \"none\"],\n        \"group\": \"input\",\n        \"gui_radio\": True\n    }\n\n}\n", "plugins/extract/align/fan.py": "#!/usr/bin/env python3\n\"\"\" Facial landmarks extractor for faceswap.py\n    Code adapted and modified from:\n    https://github.com/1adrianb/face-alignment\n\"\"\"\nfrom __future__ import annotations\nimport logging\nimport typing as T\n\nimport cv2\nimport numpy as np\n\nfrom lib.model.session import KSession\nfrom ._base import Aligner, AlignerBatch, BatchType\n\nif T.TYPE_CHECKING:\n    from lib.align import DetectedFace\n\nlogger = logging.getLogger(__name__)\n\n\nclass Align(Aligner):\n    \"\"\" Perform transformation to align and get landmarks \"\"\"\n    def __init__(self, **kwargs) -> None:\n        git_model_id = 13\n        model_filename = \"face-alignment-network_2d4_keras_v2.h5\"\n        super().__init__(git_model_id=git_model_id, model_filename=model_filename, **kwargs)\n        self.model: KSession\n        self.name = \"FAN\"\n        self.input_size = 256\n        self.color_format = \"RGB\"\n        self.vram = 2240\n        self.vram_warnings = 512  # Will run at this with warnings\n        self.vram_per_batch = 64\n        self.realign_centering = \"head\"\n        self.batchsize: int = self.config[\"batch-size\"]\n        self.reference_scale = 200. / 195.\n\n    def init_model(self) -> None:\n        \"\"\" Initialize FAN model \"\"\"\n        assert isinstance(self.name, str)\n        assert isinstance(self.model_path, str)\n        self.model = KSession(self.name,\n                              self.model_path,\n                              allow_growth=self.config[\"allow_growth\"],\n                              exclude_gpus=self._exclude_gpus)\n        self.model.load_model()\n        # Feed a placeholder so Aligner is primed for Manual tool\n        placeholder_shape = (self.batchsize, self.input_size, self.input_size, 3)\n        placeholder = np.zeros(placeholder_shape, dtype=\"float32\")\n        self.model.predict(placeholder)\n\n    def faces_to_feed(self, faces: np.ndarray) -> np.ndarray:\n        \"\"\" Convert a batch of face images from UINT8 (0-255) to fp32 (0.0-1.0)\n\n        Parameters\n        ----------\n        faces: :class:`numpy.ndarray`\n            The batch of faces in UINT8 format\n\n        Returns\n        -------\n        class: `numpy.ndarray`\n            The batch of faces as fp32 in 0.0 to 1.0 range\n        \"\"\"\n        return faces.astype(\"float32\") / 255.\n\n    def process_input(self, batch: BatchType) -> None:\n        \"\"\" Compile the detected faces for prediction\n\n        Parameters\n        ----------\n        batch: :class:`AlignerBatch`\n            The current batch to process input for\n        \"\"\"\n        assert isinstance(batch, AlignerBatch)\n        logger.trace(\"Aligning faces around center\")  # type:ignore[attr-defined]\n        center_scale = self.get_center_scale(batch.detected_faces)\n        batch.feed = np.array(self.crop(batch, center_scale))[..., :3]\n        batch.data.append({\"center_scale\": center_scale})\n        logger.trace(\"Aligned image around center\")  # type:ignore[attr-defined]\n\n    def get_center_scale(self, detected_faces: list[DetectedFace]) -> np.ndarray:\n        \"\"\" Get the center and set scale of bounding box\n\n        Parameters\n        ----------\n        detected_faces: list\n            List of :class:`~lib.align.DetectedFace` objects for the batch\n\n        Returns\n        -------\n        :class:`numpy.ndarray`\n            The center and scale of the bounding box\n        \"\"\"\n        logger.trace(\"Calculating center and scale\")  # type:ignore[attr-defined]\n        center_scale = np.empty((len(detected_faces), 68, 3), dtype='float32')\n        for index, face in enumerate(detected_faces):\n            x_ctr = (T.cast(int, face.left) + face.right) / 2.0\n            y_ctr = (T.cast(int, face.top) + face.bottom) / 2.0 - T.cast(int, face.height) * 0.12\n            scale = (T.cast(int, face.width) + T.cast(int, face.height)) * self.reference_scale\n            center_scale[index, :, 0] = np.full(68, x_ctr, dtype='float32')\n            center_scale[index, :, 1] = np.full(68, y_ctr, dtype='float32')\n            center_scale[index, :, 2] = np.full(68, scale, dtype='float32')\n        logger.trace(\"Calculated center and scale: %s\", center_scale)  # type:ignore[attr-defined]\n        return center_scale\n\n    def _crop_image(self,\n                    image: np.ndarray,\n                    top_left: np.ndarray,\n                    bottom_right: np.ndarray) -> np.ndarray:\n        \"\"\" Crop a single image\n\n        Parameters\n        ----------\n        image: :class:`numpy.ndarray`\n            The image to crop\n        top_left: :class:`numpy.ndarray`\n            The top left (x, y) point to crop from\n        bottom_right: :class:`numpy.ndarray`\n            The bottom right (x, y) point to crop to\n\n        Returns\n        -------\n        :class:`numpy.ndarray`\n            The cropped image\n        \"\"\"\n        bottom_right_width, bottom_right_height = bottom_right[0].astype('int32')\n        top_left_width, top_left_height = top_left[0].astype('int32')\n        new_dim = (bottom_right_height - top_left_height,\n                   bottom_right_width - top_left_width,\n                   3 if image.ndim > 2 else 1)\n        new_img = np.zeros(new_dim, dtype=np.uint8)\n\n        new_x = slice(max(0, -top_left_width),\n                      min(bottom_right_width, image.shape[1]) - top_left_width)\n        new_y = slice(max(0, -top_left_height),\n                      min(bottom_right_height, image.shape[0]) - top_left_height)\n        old_x = slice(max(0, top_left_width), min(bottom_right_width, image.shape[1]))\n        old_y = slice(max(0, top_left_height), min(bottom_right_height, image.shape[0]))\n        new_img[new_y, new_x] = image[old_y, old_x]\n\n        interp = cv2.INTER_CUBIC if new_dim[0] < self.input_size else cv2.INTER_AREA\n        return cv2.resize(new_img,\n                          dsize=(self.input_size, self.input_size),\n                          interpolation=interp)\n\n    def crop(self, batch: AlignerBatch, center_scale: np.ndarray) -> list[np.ndarray]:\n        \"\"\" Crop image around the center point\n\n        Parameters\n        ----------\n        batch: :class:`AlignerBatch`\n            The current batch to crop the image for\n        center_scale: :class:`numpy.ndarray`\n            The center and scale for the bounding box\n\n        Returns\n        -------\n        list\n            List of cropped images for the batch\n        \"\"\"\n        logger.trace(\"Cropping images\")  # type:ignore[attr-defined]\n        batch_shape = center_scale.shape[:2]\n        resolutions = np.full(batch_shape, self.input_size, dtype='float32')\n        matrix_ones = np.ones(batch_shape + (3,), dtype='float32')\n        matrix_size = np.full(batch_shape + (3,), self.input_size, dtype='float32')\n        matrix_size[..., 2] = 1.0\n        upper_left = self.transform(matrix_ones, center_scale, resolutions)\n        bot_right = self.transform(matrix_size, center_scale, resolutions)\n\n        # TODO second pass .. convert to matrix\n        new_images = [self._crop_image(image, top_left, bottom_right)\n                      for image, top_left, bottom_right in zip(batch.image, upper_left, bot_right)]\n        logger.trace(\"Cropped images\")  # type:ignore[attr-defined]\n        return new_images\n\n    @classmethod\n    def transform(cls,\n                  points: np.ndarray,\n                  center_scales: np.ndarray,\n                  resolutions: np.ndarray) -> np.ndarray:\n        \"\"\" Transform Image\n\n        Parameters\n        ----------\n        points: :class:`numpy.ndarray`\n            The points to transform\n        center_scales: :class:`numpy.ndarray`\n            The calculated centers and scales for the batch\n        resolutions: :class:`numpy.ndarray`\n            The resolutions\n        \"\"\"\n        logger.trace(\"Transforming Points\")  # type:ignore[attr-defined]\n        num_images, num_landmarks = points.shape[:2]\n        transform_matrix = np.eye(3, dtype='float32')\n        transform_matrix = np.repeat(transform_matrix[None, :], num_landmarks, axis=0)\n        transform_matrix = np.repeat(transform_matrix[None, :, :], num_images, axis=0)\n        scales = center_scales[:, :, 2] / resolutions\n        translations = center_scales[..., 2:3] * -0.5 + center_scales[..., :2]\n        transform_matrix[:, :, 0, 0] = scales  # x scale\n        transform_matrix[:, :, 1, 1] = scales  # y scale\n        transform_matrix[:, :, 0, 2] = translations[:, :, 0]  # x translation\n        transform_matrix[:, :, 1, 2] = translations[:, :, 1]  # y translation\n        new_points = np.einsum('abij, abj -> abi', transform_matrix, points, optimize='greedy')\n        retval = new_points[:, :, :2].astype('float32')\n        logger.trace(\"Transformed Points: %s\", retval)  # type:ignore[attr-defined]\n        return retval\n\n    def predict(self, feed: np.ndarray) -> np.ndarray:\n        \"\"\" Predict the 68 point landmarks\n\n        Parameters\n        ----------\n        batch: :class:`numpy.ndarray`\n            The batch to feed into the aligner\n\n        Returns\n        -------\n        :class:`numpy.ndarray`\n            The predictions from the aligner\n        \"\"\"\n        logger.trace(\"Predicting Landmarks\")  # type:ignore[attr-defined]\n        # TODO Remove lazy transpose and change points from predict to use the correct\n        # order\n        retval = self.model.predict(feed)[-1].transpose(0, 3, 1, 2)\n        logger.trace(retval.shape)  # type:ignore[attr-defined]\n        return retval\n\n    def process_output(self, batch: BatchType) -> None:\n        \"\"\" Process the output from the model\n\n        Parameters\n        ----------\n        batch: :class:`AlignerBatch`\n            The current batch from the model with :attr:`predictions` populated\n        \"\"\"\n        assert isinstance(batch, AlignerBatch)\n        self.get_pts_from_predict(batch)\n\n    def get_pts_from_predict(self, batch: AlignerBatch) -> None:\n        \"\"\" Get points from predictor and populate the :attr:`landmarks` property of the\n        :class:`AlignerBatch`\n\n        Parameters\n        ----------\n        batch: :class:`AlignerBatch`\n            The current batch from the model with :attr:`predictions` populated\n        \"\"\"\n        logger.trace(\"Obtain points from prediction\")  # type:ignore[attr-defined]\n        num_images, num_landmarks = batch.prediction.shape[:2]\n        image_slice = np.repeat(np.arange(num_images)[:, None], num_landmarks, axis=1)\n        landmark_slice = np.repeat(np.arange(num_landmarks)[None, :], num_images, axis=0)\n        resolution = np.full((num_images, num_landmarks), 64, dtype='int32')\n        subpixel_landmarks = np.ones((num_images, num_landmarks, 3), dtype='float32')\n\n        indices = np.array(np.unravel_index(batch.prediction.reshape(num_images,\n                                                                     num_landmarks,\n                                                                     -1).argmax(-1),\n                                            (batch.prediction.shape[2],  # height\n                                             batch.prediction.shape[3])))  # width\n        min_clipped = np.minimum(indices + 1, batch.prediction.shape[2] - 1)\n        max_clipped = np.maximum(indices - 1, 0)\n        offsets = [(image_slice, landmark_slice, indices[0], min_clipped[1]),\n                   (image_slice, landmark_slice, indices[0], max_clipped[1]),\n                   (image_slice, landmark_slice, min_clipped[0], indices[1]),\n                   (image_slice, landmark_slice, max_clipped[0], indices[1])]\n        x_subpixel_shift = batch.prediction[offsets[0]] - batch.prediction[offsets[1]]\n        y_subpixel_shift = batch.prediction[offsets[2]] - batch.prediction[offsets[3]]\n        # TODO improve rudimentary sub-pixel logic to centroid of 3x3 window algorithm\n        subpixel_landmarks[:, :, 0] = indices[1] + np.sign(x_subpixel_shift) * 0.25 + 0.5\n        subpixel_landmarks[:, :, 1] = indices[0] + np.sign(y_subpixel_shift) * 0.25 + 0.5\n\n        if batch.second_pass:  # Transformation handled by plugin parent for re-aligned faces\n            batch.landmarks = subpixel_landmarks[..., :2] * 4.\n        else:\n            batch.landmarks = self.transform(subpixel_landmarks,\n                                             batch.data[0][\"center_scale\"],\n                                             resolution)\n        logger.trace(\"Obtained points from prediction: %s\",  # type:ignore[attr-defined]\n                     batch.landmarks)\n", "plugins/extract/align/external.py": "#!/usr/bin/env python3\n\"\"\" Import 68 point landmarks or ROI boxes from a json file \"\"\"\nimport logging\nimport typing as T\nimport os\nimport re\n\nimport numpy as np\n\nfrom lib.align import EXTRACT_RATIOS, LandmarkType\nfrom lib.utils import FaceswapError, IMAGE_EXTENSIONS\n\nfrom ._base import BatchType, Aligner, AlignerBatch\n\nlogger = logging.getLogger(__name__)\n\n\nclass Align(Aligner):\n    \"\"\" Import face detection bounding boxes from an external json file \"\"\"\n    def __init__(self, **kwargs) -> None:\n        kwargs[\"normalize_method\"] = None  # Disable normalization\n        kwargs[\"re_feed\"] = 0  # Disable re-feed\n        kwargs[\"re_align\"] = False  # Disablle re-align\n        kwargs[\"disable_filter\"] = True  # Disable aligner filters\n        super().__init__(git_model_id=None, model_filename=None, **kwargs)\n\n        self.name = \"External\"\n        self.batchsize = 16\n\n        self._origin: T.Literal[\"top-left\",\n                                \"bottom-left\",\n                                \"top-right\",\n                                \"bottom-right\"] = self.config[\"origin\"]\n\n        self._re_frame_no: re.Pattern = re.compile(r\"\\d+$\")\n        self._is_video: bool = False\n        self._imported: dict[str | int, tuple[int, np.ndarray]] = {}\n        \"\"\"dict[str | int, tuple[int, np.ndarray]]: filename as key, value of [number of faces\n        remaining for the frame, all landmarks in the frame] \"\"\"\n\n        self._missing: list[str] = []\n        self._roll: dict[T.Literal[\"bottom-left\", \"top-right\", \"bottom-right\"], int] = {\n            \"bottom-left\": 3, \"top-right\": 1, \"bottom-right\": 2}\n        \"\"\"dict[Literal[\"bottom-left\", \"top-right\", \"bottom-right\"], int]: Amount to roll the\n        points by for different origins when 4 Point ROI landmarks are provided \"\"\"\n\n        centering = self.config[\"4_point_centering\"]\n        self._adjustment: float = 1. if centering is None else 1. - EXTRACT_RATIOS[centering]\n        \"\"\"float: The amount to adjust 4 point ROI landmarks to standardize the points for a\n        'head' sized extracted face \"\"\"\n\n    def init_model(self) -> None:\n        \"\"\" No initialization to perform \"\"\"\n        logger.debug(\"No aligner model to initialize\")\n\n    def _check_for_video(self, filename: str) -> None:\n        \"\"\" Check a sample filename from the import file for a file extension to set\n        :attr:`_is_video`\n\n        Parameters\n        ----------\n        filename: str\n            A sample file name from the imported data\n        \"\"\"\n        logger.debug(\"Checking for video from '%s'\", filename)\n        ext = os.path.splitext(filename)[-1]\n        if ext.lower() not in IMAGE_EXTENSIONS:\n            self._is_video = True\n        logger.debug(\"Set is_video to %s from extension '%s'\", self._is_video, ext)\n\n    def _get_key(self, key: str) -> str | int:\n        \"\"\" Obtain the key for the item in the lookup table. If the input are images, the key will\n        be the image filename. If the input is a video, the key will be the frame number\n\n        Parameters\n        ----------\n        key: str\n            The initial key value from import data or an import image/frame\n\n        Returns\n        -------\n        str | int\n            The filename is the input data is images, otherwise the frame number of a video\n        \"\"\"\n        if not self._is_video:\n            return key\n        original_name = os.path.splitext(key)[0]\n        matches = self._re_frame_no.findall(original_name)\n        if not matches or len(matches) > 1:\n            raise FaceswapError(f\"Invalid import name: '{key}'. For video files, the key should \"\n                                \"end with the frame number.\")\n        retval = int(matches[0])\n        logger.trace(\"Obtained frame number %s from key '%s'\",  # type:ignore[attr-defined]\n                     retval, key)\n        return retval\n\n    def _import_face(self, face: dict[str, list[int] | list[list[float]]]) -> np.ndarray:\n        \"\"\" Import the landmarks from a single face\n\n        Parameters\n        ----------\n        face: dict[str, list[int] | list[list[float]]]\n            An import dictionary item for a face\n\n        Returns\n        -------\n        :class:`numpy.ndarray`\n            The landmark data imported from the json file\n\n        Raises\n        ------\n        FaceSwapError\n            If the landmarks_2d key does not exist or the landmarks are in an incorrect format\n        \"\"\"\n        landmarks = face.get(\"landmarks_2d\")\n        if landmarks is None:\n            raise FaceswapError(\"The provided import file is the required key 'landmarks_2d\")\n        if len(landmarks) not in (4, 68):\n            raise FaceswapError(\"Imported 'landmarks_2d' should be either 68 facial feature \"\n                                \"landmarks or 4 ROI corner locations\")\n        retval = np.array(landmarks, dtype=\"float32\")\n        if retval.shape[-1] != 2:\n            raise FaceswapError(\"Imported 'landmarks_2d' should be formatted as a list of (x, y) \"\n                                \"co-ordinates\")\n        if retval.shape[0] == 4:  # Adjust ROI landmarks based on centering selected\n            center = np.mean(retval, axis=0)\n            retval = (retval - center) * self._adjustment + center\n\n        return retval\n\n    def import_data(self, data: dict[str, list[dict[str, list[int] | list[list[float]]]]]) -> None:\n        \"\"\" Import the aligner data from the json import file and set to :attr:`_imported`\n\n        Parameters\n        ----------\n        data: dict[str, list[dict[str, list[int] | list[list[float]]]]]\n            The data to be imported\n        \"\"\"\n        logger.debug(\"Data length: %s\", len(data))\n        self._check_for_video(list(data)[0])\n        for key, faces in data.items():\n            try:\n                lms = np.array([self._import_face(face) for face in faces], dtype=\"float32\")\n                if not np.any(lms):\n                    logger.trace(\"Skipping frame '%s' with no faces\")  # type:ignore[attr-defined]\n                    continue\n\n                store_key = self._get_key(key)\n                self._imported[store_key] = (lms.shape[0], lms)\n            except FaceswapError as err:\n                logger.error(str(err))\n                msg = f\"The imported frame key that failed was '{key}'\"\n                raise FaceswapError(msg) from err\n        lm_shape = set(v[1].shape[1:] for v in self._imported.values() if v[0] > 0)\n        if len(lm_shape) > 1:\n            raise FaceswapError(\"All external data should have the same number of landmarks. \"\n                                f\"Found landmarks of shape: {lm_shape}\")\n        if (4, 2) in lm_shape:\n            self.landmark_type = LandmarkType.LM_2D_4\n\n    def process_input(self, batch: BatchType) -> None:\n        \"\"\" Put the filenames and original frame dimensions into `batch.feed` so they can be\n        collected for mapping in `.predict`\n\n        Parameters\n        ----------\n        batch: :class:`~plugins.extract.detect._base.AlignerBatch`\n            The batch to be processed by the plugin\n        \"\"\"\n        batch.feed = np.array([(self._get_key(os.path.basename(f)), i.shape[:2])\n                               for f, i in zip(batch.filename, batch.image)], dtype=\"object\")\n\n    def faces_to_feed(self, faces: np.ndarray) -> np.ndarray:\n        \"\"\" No action required for import plugin\n\n        Parameters\n        ----------\n        faces: :class:`numpy.ndarray`\n            The batch of faces in UINT8 format\n\n        Returns\n        -------\n        class: `numpy.ndarray`\n            the original batch of faces\n        \"\"\"\n        return faces\n\n    def _adjust_for_origin(self, landmarks: np.ndarray, frame_dims: tuple[int, int]) -> np.ndarray:\n        \"\"\" Adjust the landmarks to be top-left orientated based on the selected import origin\n\n        Parameters\n        ----------\n        landmarks: :class:`np.ndarray`\n            The imported facial landmarks box at original (0, 0) origin\n        frame_dims: tuple[int, int]\n            The (rows, columns) dimensions of the original frame\n\n        Returns\n        -------\n        :class:`numpy.ndarray`\n            The adjusted landmarks box for a top-left origin\n        \"\"\"\n        if not np.any(landmarks) or self._origin == \"top-left\":\n            return landmarks\n\n        if LandmarkType.from_shape(landmarks.shape) == LandmarkType.LM_2D_4:\n            landmarks = np.roll(landmarks, self._roll[self._origin], axis=0)\n\n        if self._origin.startswith(\"bottom\"):\n            landmarks[:, 1] = frame_dims[0] - landmarks[:, 1]\n        if self._origin.endswith(\"right\"):\n            landmarks[:, 0] = frame_dims[1] - landmarks[:, 0]\n\n        return landmarks\n\n    def predict(self, feed: np.ndarray) -> np.ndarray:\n        \"\"\" Pair the input filenames to the import file\n\n        Parameters\n        ----------\n        feed: :class:`numpy.ndarray`\n            The filenames in the batch to return imported alignments for\n\n        Returns\n        -------\n        :class:`numpy.ndarray`\n            The predictions for the given filenames\n        \"\"\"\n        preds = []\n        for key, frame_dims in feed:\n            if key not in self._imported:\n                self._missing.append(key)\n                continue\n\n            remaining, all_lms = self._imported[key]\n            preds.append(self._adjust_for_origin(all_lms[all_lms.shape[0] - remaining],\n                                                 frame_dims))\n\n            if remaining == 1:\n                del self._imported[key]\n            else:\n                self._imported[key] = (remaining - 1, all_lms)\n\n        return np.array(preds, dtype=\"float32\")\n\n    def process_output(self, batch: BatchType) -> None:\n        \"\"\" Process the imported data to the landmarks attribute\n\n        Parameters\n        ----------\n        batch: :class:`AlignerBatch`\n            The current batch from the model with :attr:`predictions` populated\n        \"\"\"\n        assert isinstance(batch, AlignerBatch)\n        batch.landmarks = batch.prediction\n        logger.trace(\"Imported landmarks: %s\", batch.landmarks)  # type:ignore[attr-defined]\n\n    def on_completion(self) -> None:\n        \"\"\" Output information if:\n        - Imported items were not matched in input data\n        - Input data was not matched in imported items\n        \"\"\"\n        super().on_completion()\n\n        if self._missing:\n            logger.warning(\"[ALIGN] %s input frames could not be matched in the import file \"\n                           \"'%s'. Run in verbose mode for a list of frames.\",\n                           len(self._missing), self.config[\"file_name\"])\n            logger.verbose(  # type:ignore[attr-defined]\n                \"[ALIGN] Input frames not in import file: %s\", self._missing)\n\n        if self._imported:\n            logger.warning(\"[ALIGN] %s items in the import file '%s' could not be matched to any \"\n                           \"input frames. Run in verbose mode for a list of items.\",\n                           len(self._imported), self.config[\"file_name\"])\n            logger.verbose(  # type:ignore[attr-defined]\n                \"[ALIGN] import file items not in input frames: %s\", list(self._imported))\n", "plugins/extract/align/__init__.py": "", "plugins/extract/align/_base/processing.py": "#!/usr/bin/env python3\n\"\"\" Processing methods for aligner plugins \"\"\"\nfrom __future__ import annotations\nimport logging\nimport typing as T\n\nfrom threading import Lock\n\nimport numpy as np\n\nfrom lib.align import AlignedFace\n\nif T.TYPE_CHECKING:\n    from lib.align import DetectedFace\n    from .aligner import AlignerBatch\n    from lib.align.aligned_face import CenteringType\n\nlogger = logging.getLogger(__name__)\n\n\nclass AlignedFilter():\n    \"\"\" Applies filters on the output of the aligner\n\n    Parameters\n    ----------\n    feature_filter: bool\n        ``True`` to enable filter to check relative position of eyes/eyebrows and mouth. ``False``\n        to disable.\n    min_scale: float\n        Filters out faces that have been aligned at below this value as a multiplier of the\n        minimum frame dimension. Set to ``0`` for off.\n    max_scale: float\n        Filters out faces that have been aligned at above this value as a multiplier of the\n        minimum frame dimension. Set to ``0`` for off.\n    distance: float\n        Filters out faces that are further than this distance from an \"average\" face. Set to\n        ``0`` for off.\n    roll: float\n        Filters out faces with a roll value outside of 0 +/- the value given here. Set to ``0``\n        for off.\n    save_output: bool\n        ``True`` if the filtered faces should be kept as they are being saved. ``False`` if they\n        should be deleted\n    disable: bool, Optional\n        ``True`` to disable the filter regardless of config options. Default: ``False``\n    \"\"\"\n    def __init__(self,\n                 feature_filter: bool,\n                 min_scale: float,\n                 max_scale: float,\n                 distance: float,\n                 roll: float,\n                 save_output: bool,\n                 disable: bool = False) -> None:\n        logger.debug(\"Initializing %s: (feature_filter: %s, min_scale: %s, max_scale: %s, \"\n                     \"distance: %s, roll, %s, save_output: %s, disable: %s)\",\n                     self.__class__.__name__, feature_filter, min_scale, max_scale, distance, roll,\n                     save_output, disable)\n        self._features = feature_filter\n        self._min_scale = min_scale\n        self._max_scale = max_scale\n        self._distance = distance / 100.\n        self._roll = roll\n        self._save_output = save_output\n        self._active = not disable and (feature_filter or\n                                        max_scale > 0.0 or\n                                        min_scale > 0.0 or\n                                        distance > 0.0 or\n                                        roll > 0.0)\n        self._counts: dict[str, int] = {\"features\": 0,\n                                        \"min_scale\": 0,\n                                        \"max_scale\": 0,\n                                        \"distance\": 0,\n                                        \"roll\": 0}\n        logger.debug(\"Initialized %s: \", self.__class__.__name__)\n\n    def _scale_test(self,\n                    face: AlignedFace,\n                    minimum_dimension: int) -> T.Literal[\"min\", \"max\"] | None:\n        \"\"\" Test if a face is below or above the min/max size thresholds. Returns as soon as a test\n        fails.\n\n        Parameters\n        ----------\n        face: :class:`~lib.aligned.AlignedFace`\n            The aligned face to test the original size of.\n\n        minimum_dimension: int\n            The minimum (height, width) of the original frame\n\n        Returns\n        -------\n        \"min\", \"max\" or ``None``\n            Returns min or max if the face failed the minimum or maximum test respectively.\n            ``None`` if all tests passed\n        \"\"\"\n\n        if self._min_scale <= 0.0 and self._max_scale <= 0.0:\n            return None\n\n        roi = face.original_roi.astype(\"int64\")\n        size = ((roi[1][0] - roi[0][0]) ** 2 + (roi[1][1] - roi[0][1]) ** 2) ** 0.5\n\n        if self._min_scale > 0.0 and size < minimum_dimension * self._min_scale:\n            return \"min\"\n\n        if self._max_scale > 0.0 and size > minimum_dimension * self._max_scale:\n            return \"max\"\n\n        return None\n\n    def _handle_filtered(self,\n                         key: str,\n                         face: DetectedFace,\n                         faces: list[DetectedFace],\n                         sub_folders: list[str | None],\n                         sub_folder_index: int) -> None:\n        \"\"\" Add the filtered item to the filter counts.\n\n        If config option `save_filtered` has been enabled then add the face to the output faces\n        list and update the sub_folder list with the correct name for this face.\n\n        Parameters\n        ----------\n        key: str\n            The key to use for the filter counts dictionary and the sub_folder name\n        face: :class:`~lib.align.detected_face.DetectedFace`\n            The detected face object to be filtered out\n        faces: list\n            The list of faces that will be returned from the filter\n        sub_folders: list\n            List of sub folder names corresponding to the list of detected face objects\n        sub_folder_index: int\n            The index within the sub-folder list that the filtered face belongs to\n        \"\"\"\n        self._counts[key] += 1\n        if not self._save_output:\n            return\n\n        faces.append(face)\n        sub_folders[sub_folder_index] = f\"_align_filt_{key}\"\n\n    def __call__(self, faces: list[DetectedFace], minimum_dimension: int\n                 ) -> tuple[list[DetectedFace], list[str | None]]:\n        \"\"\" Apply the filter to the incoming batch\n\n        Parameters\n        ----------\n        faces: list\n            List of detected face objects to filter out on size\n        minimum_dimension: int\n            The minimum (height, width) of the original frame\n\n        Returns\n        -------\n        detected_faces: list\n            The filtered list of detected face objects, if saving filtered faces has not been\n            selected or the full list of detected faces\n        sub_folders: list\n            List of ``Nones`` if saving filtered faces has not been selected or list of ``Nones``\n            and sub folder names corresponding the filtered face location\n        \"\"\"\n        sub_folders: list[str | None] = [None for _ in range(len(faces))]\n        if not self._active:\n            return faces, sub_folders\n\n        retval: list[DetectedFace] = []\n        for idx, face in enumerate(faces):\n            aligned = AlignedFace(landmarks=face.landmarks_xy, centering=\"face\")\n\n            if self._features and aligned.relative_eye_mouth_position < 0.0:\n                self._handle_filtered(\"features\", face, retval, sub_folders, idx)\n                continue\n\n            min_max = self._scale_test(aligned, minimum_dimension)\n            if min_max in (\"min\", \"max\"):\n                self._handle_filtered(f\"{min_max}_scale\", face, retval, sub_folders, idx)\n                continue\n\n            if 0.0 < self._distance < aligned.average_distance:\n                self._handle_filtered(\"distance\", face, retval, sub_folders, idx)\n                continue\n\n            if self._roll != 0.0 and not 0.0 < abs(aligned.pose.roll) < self._roll:\n                self._handle_filtered(\"roll\", face, retval, sub_folders, idx)\n                continue\n\n            retval.append(face)\n        return retval, sub_folders\n\n    def filtered_mask(self,\n                      batch: AlignerBatch,\n                      skip: np.ndarray | list[int] | None = None) -> np.ndarray:\n        \"\"\" Obtain a list of boolean values for the given batch indicating whether they pass the\n        filter test.\n\n        Parameters\n        ----------\n        batch: :class:`AlignerBatch`\n            The batch of face to obtain masks for\n        skip: list or :class:`numpy.ndarray`, optional\n            List or 1D numpy array of indices indicating faces that have already been filter\n            masked and so should not be filtered again. Values in these index positions will be\n            returned as ``True``\n\n        Returns\n        -------\n        :class:`numpy.ndarray`\n            Boolean mask array corresponding to any of the input DetectedFace objects that passed a\n            test. ``False`` the face passed the test. ``True`` it failed\n        \"\"\"\n        skip = [] if skip is None else skip\n        retval = np.ones((len(batch.detected_faces), ), dtype=\"bool\")\n        for idx, (landmarks, image) in enumerate(zip(batch.landmarks, batch.image)):\n            if idx in skip:\n                continue\n            face = AlignedFace(landmarks)\n            if self._features and face.relative_eye_mouth_position < 0.0:\n                continue\n            if self._scale_test(face, min(image.shape[:2])) is not None:\n                continue\n            if 0.0 < self._distance < face.average_distance:\n                continue\n            if self._roll != 0.0 and not 0.0 < abs(face.pose.roll) < self._roll:\n                continue\n            retval[idx] = False\n        return retval\n\n    def output_counts(self):\n        \"\"\" Output the counts of filtered items \"\"\"\n        if not self._active:\n            return\n        counts = [f\"{key} ({getattr(self, f'_{key}'):.2f}): {count}\"\n                  for key, count in self._counts.items()\n                  if count > 0]\n        if counts:\n            logger.info(\"Aligner filtered: (%s)\", \", \".join(counts))\n\n\nclass ReAlign():\n    \"\"\" Holds data and methods for 2nd pass re-aligns\n\n    Parameters\n    ----------\n    active: bool\n        ``True`` if re-alignment has been requested otherwise ``False``\n    do_refeeds: bool\n        ``True`` if re-feeds should be re-aligned, ``False`` if just the final output of the\n        re-feeds should be aligned\n    do_filter: bool\n        ``True`` if aligner filtered out faces should not be re-aligned. ``False`` if all faces\n        should be re-aligned\n    \"\"\"\n    def __init__(self, active: bool, do_refeeds: bool, do_filter: bool) -> None:\n        logger.debug(\"Initializing %s: (active: %s, do_refeeds: %s, do_filter: %s)\",\n                     self.__class__.__name__, active, do_refeeds, do_filter)\n        self._active = active\n        self._do_refeeds = do_refeeds\n        self._do_filter = do_filter\n        self._centering: CenteringType = \"face\"\n        self._size = 0\n        self._tracked_lock = Lock()\n        self._tracked_batchs: dict[int,\n                                   dict[T.Literal[\"filtered_landmarks\"], list[np.ndarray]]] = {}\n        # TODO. Probably does not need to be a list, just alignerbatch\n        self._queue_lock = Lock()\n        self._queued: list[AlignerBatch] = []\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    @property\n    def active(self) -> bool:\n        \"\"\"bool: ``True`` if re_aligns have been selected otherwise ``False``\"\"\"\n        return self._active\n\n    @property\n    def do_refeeds(self) -> bool:\n        \"\"\"bool: ``True`` if re-aligning is active and re-aligning re-feeds has been selected\n        otherwise ``False``\"\"\"\n        return self._active and self._do_refeeds\n\n    @property\n    def do_filter(self) -> bool:\n        \"\"\"bool: ``True`` if re-aligning is active and faces which failed the aligner filter test\n        should not be re-aligned otherwise ``False``\"\"\"\n        return self._active and self._do_filter\n\n    @property\n    def items_queued(self) -> bool:\n        \"\"\"bool: ``True`` if re-align is active and items are queued for a 2nd pass otherwise\n        ``False`` \"\"\"\n        with self._queue_lock:\n            return self._active and bool(self._queued)\n\n    @property\n    def items_tracked(self) -> bool:\n        \"\"\"bool: ``True`` if items exist in the tracker so still need to be processed \"\"\"\n        with self._tracked_lock:\n            return bool(self._tracked_batchs)\n\n    def set_input_size_and_centering(self, input_size: int, centering: CenteringType) -> None:\n        \"\"\" Set the input size of the loaded plugin once the model has been loaded\n\n        Parameters\n        ----------\n        input_size: int\n            The input size, in pixels, of the aligner plugin\n        centering: [\"face\", \"head\" or \"legacy\"]\n            The centering to align the image at for re-aligning\n        \"\"\"\n        logger.debug(\"input_size: %s, centering: %s\", input_size, centering)\n        self._size = input_size\n        self._centering = centering\n\n    def track_batch(self, batch_id: int) -> None:\n        \"\"\" Add newly seen batch id from the aligner to the batch tracker, so that we can keep\n        track of whether there are still batches to be processed when the aligner hits 'EOF'\n\n        Parameters\n        ----------\n        batch_id: int\n            The batch id to add to batch tracking\n        \"\"\"\n        if not self._active:\n            return\n        logger.trace(\"Tracking batch id: %s\", batch_id)  # type: ignore[attr-defined]\n        with self._tracked_lock:\n            self._tracked_batchs[batch_id] = {}\n\n    def untrack_batch(self, batch_id: int) -> None:\n        \"\"\" Remove the tracked batch from the tracker once the batch has been fully processed\n\n        Parameters\n        ----------\n        batch_id: int\n            The batch id to remove from batch tracking\n        \"\"\"\n        if not self._active:\n            return\n        logger.trace(\"Removing batch id from tracking: %s\", batch_id)  # type: ignore[attr-defined]\n        with self._tracked_lock:\n            del self._tracked_batchs[batch_id]\n\n    def add_batch(self, batch: AlignerBatch) -> None:\n        \"\"\" Add first pass alignments to the queue for picking up for re-alignment, update their\n        :attr:`second_pass` attribute to ``True`` and clear attributes not required.\n\n        Parameters\n        ----------\n        batch: :class:`AlignerBatch`\n            aligner batch to perform re-alignment on\n        \"\"\"\n        with self._queue_lock:\n            logger.trace(\"Queueing for second pass: %s\", batch)  # type: ignore[attr-defined]\n            batch.second_pass = True\n            batch.feed = np.array([])\n            batch.prediction = np.array([])\n            batch.refeeds = []\n            batch.data = []\n            self._queued.append(batch)\n\n    def get_batch(self) -> AlignerBatch:\n        \"\"\" Retrieve the next batch currently queued for re-alignment\n\n        Returns\n        -------\n        :class:`AlignerBatch`\n            The next :class:`AlignerBatch` for re-alignment\n        \"\"\"\n        with self._queue_lock:\n            retval = self._queued.pop(0)\n            logger.trace(\"Retrieving for second pass: %s\",  # type: ignore[attr-defined]\n                         retval.filename)\n        return retval\n\n    def process_batch(self, batch: AlignerBatch) -> list[np.ndarray]:\n        \"\"\" Pre process a batch object for re-aligning through the aligner.\n\n        Parameters\n        ----------\n        batch: :class:`AlignerBatch`\n            aligner batch to perform pre-processing on\n\n        Returns\n        -------\n        list\n            List of UINT8 aligned faces batch for each selected refeed\n        \"\"\"\n        logger.trace(\"Processing batch: %s, landmarks: %s\",  # type: ignore[attr-defined]\n                     batch.filename, [b.shape for b in batch.landmarks])\n        retval: list[np.ndarray] = []\n        filtered_landmarks: list[np.ndarray] = []\n        for landmarks, masks in zip(batch.landmarks, batch.second_pass_masks):\n            if not np.all(masks):  # At least one face has not already been filtered\n                aligned_faces = [AlignedFace(lms,\n                                             image=image,\n                                             size=self._size,\n                                             centering=self._centering)\n                                 for image, lms, msk in zip(batch.image, landmarks, masks)\n                                 if not msk]\n                faces = np.array([aligned.face for aligned in aligned_faces\n                                 if aligned.face is not None])\n                retval.append(faces)\n                batch.data.append({\"aligned_faces\": aligned_faces})\n\n            if np.any(masks):\n                # Track the original landmarks for re-insertion on the other side\n                filtered_landmarks.append(landmarks[masks])\n\n        with self._tracked_lock:\n            self._tracked_batchs[batch.batch_id] = {\"filtered_landmarks\": filtered_landmarks}\n        batch.landmarks = np.array([])  # Clear the old landmarks\n        return retval\n\n    def _transform_to_frame(self, batch: AlignerBatch) -> np.ndarray:\n        \"\"\" Transform the predicted landmarks from the aligned face image back into frame\n        co-ordinates\n\n        Parameters\n        ----------\n        batch: :class:`AlignerBatch`\n            An aligner batch containing the aligned faces in the data field and the face\n            co-ordinate landmarks in the landmarks field\n\n        Returns\n        -------\n        :class:`numpy.ndarray`\n            The landmarks transformed to frame space\n        \"\"\"\n        faces: list[AlignedFace] = batch.data[0][\"aligned_faces\"]\n        retval = np.array([aligned.transform_points(landmarks, invert=True)\n                           for landmarks, aligned in zip(batch.landmarks, faces)])\n        logger.trace(\"Transformed points: original max: %s, \"  # type: ignore[attr-defined]\n                     \"new max: %s\", batch.landmarks.max(), retval.max())\n        return retval\n\n    def _re_insert_filtered(self, batch: AlignerBatch, masks: np.ndarray) -> np.ndarray:\n        \"\"\" Re-insert landmarks that were filtered out from the re-align process back into the\n        landmark results\n\n        Parameters\n        ----------\n        batch: :class:`AlignerBatch`\n            An aligner batch containing the aligned faces in the data field and the landmarks in\n            frame space in the landmarks field\n        masks: np.ndarray\n            The original filter masks for this batch\n\n        Returns\n        -------\n        :class:`numpy.ndarray`\n            The full batch of landmarks with filtered out values re-inserted\n        \"\"\"\n        if not np.any(masks):\n            logger.trace(\"No landmarks to re-insert: %s\", masks)  # type: ignore[attr-defined]\n            return batch.landmarks\n\n        with self._tracked_lock:\n            filtered = self._tracked_batchs[batch.batch_id][\"filtered_landmarks\"].pop(0)\n\n        if np.all(masks):\n            retval = filtered\n        else:\n            retval = np.empty((masks.shape[0], *filtered.shape[1:]), dtype=filtered.dtype)\n            retval[~masks] = batch.landmarks\n            retval[masks] = filtered\n\n        logger.trace(\"Filtered re-inserted: old shape: %s, \"  # type: ignore[attr-defined]\n                     \"new shape: %s)\", batch.landmarks.shape, retval.shape)\n\n        return retval\n\n    def process_output(self, subbatches: list[AlignerBatch], batch_masks: np.ndarray) -> None:\n        \"\"\" Process the output from the re-align pass.\n\n        - Transform landmarks from aligned face space to face space\n        - Re-insert faces that were filtered out from the re-align process back into the\n          landmarks list\n\n        Parameters\n        ----------\n        subbatches: list\n            List of sub-batch results for each re-aligned re-feed performed\n        batch_masks: :class:`numpy.ndarray`\n            The original re-feed filter masks from the first pass\n        \"\"\"\n        for batch, masks in zip(subbatches, batch_masks):\n            if not np.all(masks):\n                batch.landmarks = self._transform_to_frame(batch)\n            batch.landmarks = self._re_insert_filtered(batch, masks)\n", "plugins/extract/align/_base/aligner.py": "#!/usr/bin/env python3\n\"\"\" Base class for Face Aligner plugins\n\nAll Aligner Plugins should inherit from this class.\nSee the override methods for which methods are required.\n\nThe plugin will receive a :class:`~plugins.extract.extract_media.ExtractMedia` object.\n\nFor each source item, the plugin must pass a dict to finalize containing:\n\n>>> {\"filename\": [<filename of source frame>],\n>>>  \"landmarks\": [list of 68 point face landmarks]\n>>>  \"detected_faces\": [<list of DetectedFace objects>]}\n\"\"\"\nfrom __future__ import annotations\nimport logging\nimport typing as T\n\nfrom dataclasses import dataclass, field\nfrom time import sleep\n\nimport cv2\nimport numpy as np\n\nfrom tensorflow.python.framework import errors_impl as tf_errors  # pylint:disable=no-name-in-module # noqa\n\nfrom lib.align import LandmarkType\nfrom lib.utils import FaceswapError\nfrom plugins.extract import ExtractMedia\nfrom plugins.extract._base import BatchType, ExtractorBatch, Extractor\nfrom .processing import AlignedFilter, ReAlign\n\nif T.TYPE_CHECKING:\n    from collections.abc import Generator\n    from queue import Queue\n    from lib.align import DetectedFace\n    from lib.align.aligned_face import CenteringType\n\nlogger = logging.getLogger(__name__)\n_BATCH_IDX: int = 0\n\n\ndef _get_new_batch_id() -> int:\n    \"\"\" Obtain the next available batch index\n\n    Returns\n    -------\n    int\n        The next available unique batch id\n    \"\"\"\n    global _BATCH_IDX  # pylint:disable=global-statement\n    _BATCH_IDX += 1\n    return _BATCH_IDX\n\n\n@dataclass\nclass AlignerBatch(ExtractorBatch):\n    \"\"\" Dataclass for holding items flowing through the aligner.\n\n    Inherits from :class:`~plugins.extract._base.ExtractorBatch`\n\n    Parameters\n    ----------\n    batch_id: int\n        A unique integer for tracking this batch\n    landmarks: list\n        List of 68 point :class:`numpy.ndarray` landmark points returned from the aligner\n    refeeds: list\n        List of :class:`numpy.ndarrays` for holding each of the feeds that will be put through the\n        model for each refeed\n    second_pass: bool, optional\n        ``True`` if this batch is passing through the aligner for a second time as re-align has\n        been selected otherwise ``False``. Default: ``False``\n    second_pass_masks: :class:`numpy.ndarray`, optional\n        The masks used to filter out re-feed values for passing to the re-aligner.\n    \"\"\"\n    batch_id: int = 0\n    detected_faces: list[DetectedFace] = field(default_factory=list)\n    landmarks: np.ndarray = np.array([])\n    refeeds: list[np.ndarray] = field(default_factory=list)\n    second_pass: bool = False\n    second_pass_masks: np.ndarray = np.array([])\n\n    def __repr__(self):\n        \"\"\" Prettier repr for debug printing \"\"\"\n        retval = super().__repr__()\n        retval += (f\", batch_id={self.batch_id}, \"\n                   f\"landmarks=[({self.landmarks.shape}, {self.landmarks.dtype})], \"\n                   f\"refeeds={[(f.shape, f.dtype) for f in self.refeeds]}, \"\n                   f\"second_pass={self.second_pass}, \"\n                   f\"second_pass_masks={self.second_pass_masks})\")\n        return retval\n\n    def __post_init__(self):\n        \"\"\" Make sure that we have been given a non-zero ID \"\"\"\n        assert self.batch_id != 0, (\"A batch ID must be specified for Aligner Batches\")\n\n\nclass Aligner(Extractor):  # pylint:disable=abstract-method\n    \"\"\" Aligner plugin _base Object\n\n    All Aligner plugins must inherit from this class\n\n    Parameters\n    ----------\n    git_model_id: int\n        The second digit in the github tag that identifies this model. See\n        https://github.com/deepfakes-models/faceswap-models for more information\n    model_filename: str\n        The name of the model file to be loaded\n    normalize_method: {`None`, 'clahe', 'hist', 'mean'}, optional\n        Normalize the images fed to the aligner. Default: ``None``\n    re_feed: int, optional\n        The number of times to re-feed a slightly adjusted bounding box into the aligner.\n        Default: `0`\n    re_align: bool, optional\n        ``True`` to obtain landmarks by passing the initially aligned face back through the\n        aligner. Default ``False``\n    disable_filter: bool, optional\n        Disable all aligner filters regardless of config option. Default: ``False``\n    Other Parameters\n    ----------------\n    configfile: str, optional\n        Path to a custom configuration ``ini`` file. Default: Use system configfile\n\n    See Also\n    --------\n    plugins.extract.pipeline : The extraction pipeline for calling plugins\n    plugins.extract.align : Aligner plugins\n    plugins.extract._base : Parent class for all extraction plugins\n    plugins.extract.detect._base : Detector parent class for extraction plugins.\n    plugins.extract.mask._base : Masker parent class for extraction plugins.\n    \"\"\"\n\n    def __init__(self,\n                 git_model_id: int | None = None,\n                 model_filename: str | None = None,\n                 configfile: str | None = None,\n                 instance: int = 0,\n                 normalize_method: T.Literal[\"none\", \"clahe\", \"hist\", \"mean\"] | None = None,\n                 re_feed: int = 0,\n                 re_align: bool = False,\n                 disable_filter: bool = False,\n                 **kwargs) -> None:\n        logger.debug(\"Initializing %s: (normalize_method: %s, re_feed: %s, re_align: %s, \"\n                     \"disable_filter: %s)\", self.__class__.__name__, normalize_method, re_feed,\n                     re_align, disable_filter)\n        super().__init__(git_model_id,\n                         model_filename,\n                         configfile=configfile,\n                         instance=instance,\n                         **kwargs)\n        self._plugin_type = \"align\"\n        self.realign_centering: CenteringType = \"face\"  # overide for plugin specific centering\n\n        # Override for specific landmark type:\n        self.landmark_type = LandmarkType.LM_2D_68\n\n        self._eof_seen = False\n        self._normalize_method: T.Literal[\"clahe\", \"hist\", \"mean\"] | None = None\n        self._re_feed = re_feed\n        self._filter = AlignedFilter(feature_filter=self.config[\"aligner_features\"],\n                                     min_scale=self.config[\"aligner_min_scale\"],\n                                     max_scale=self.config[\"aligner_max_scale\"],\n                                     distance=self.config[\"aligner_distance\"],\n                                     roll=self.config[\"aligner_roll\"],\n                                     save_output=self.config[\"save_filtered\"],\n                                     disable=disable_filter)\n        self._re_align = ReAlign(re_align,\n                                 self.config[\"realign_refeeds\"],\n                                 self.config[\"filter_realign\"])\n        self._needs_refeed_masks: bool = self._re_feed > 0 and (\n            self.config[\"filter_refeed\"] or (self._re_align.do_refeeds and\n                                             self._re_align.do_filter))\n        self.set_normalize_method(normalize_method)\n\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    def set_normalize_method(self, method: T.Literal[\"none\", \"clahe\", \"hist\", \"mean\"] | None\n                             ) -> None:\n        \"\"\" Set the normalization method for feeding faces into the aligner.\n\n        Parameters\n        ----------\n        method: {\"none\", \"clahe\", \"hist\", \"mean\"}\n            The normalization method to apply to faces prior to feeding into the model\n        \"\"\"\n        method = None if method is None or method.lower() == \"none\" else method\n        self._normalize_method = T.cast(T.Literal[\"clahe\", \"hist\", \"mean\"] | None, method)\n\n    def initialize(self, *args, **kwargs) -> None:\n        \"\"\" Add a call to add model input size to the re-aligner \"\"\"\n        self._re_align.set_input_size_and_centering(self.input_size, self.realign_centering)\n        super().initialize(*args, **kwargs)\n\n    def _handle_realigns(self, queue: Queue) -> tuple[bool, AlignerBatch] | None:\n        \"\"\" Handle any items waiting for a second pass through the aligner.\n\n        If EOF has been recieved and items are still being processed through the first pass\n        then wait for a short time and try again to collect them.\n\n        On EOF return exhausted flag with an empty batch\n\n         Parameters\n        ----------\n        queue : queue.Queue()\n            The ``queue`` that the plugin will be fed from.\n\n        Returns\n        -------\n        ``None`` or tuple\n            If items are processed then returns (`bool`, :class:`AlignerBatch`) containing the\n            exhausted flag and the batch to be processed. If no items are processed returns\n            ``None``\n        \"\"\"\n        if not self._re_align.active:\n            return None\n\n        exhausted = False\n        if self._re_align.items_queued:\n            batch = self._re_align.get_batch()\n            logger.trace(\"Re-align batch: %s\", batch)  # type: ignore[attr-defined]\n            return exhausted, batch\n\n        if self._eof_seen and self._re_align.items_tracked:\n            # EOF seen and items still being processed on first pass\n            logger.debug(\"Tracked re-align items waiting to be flushed, retrying...\")\n            sleep(0.25)\n            return self.get_batch(queue)\n\n        if self._eof_seen:\n            exhausted = True\n            logger.debug(\"All items processed. Returning empty batch\")\n            self._filter.output_counts()\n            self._eof_seen = False  # Reset for plugin re-use\n            return exhausted, AlignerBatch(batch_id=-1)\n\n        return None\n\n    def get_batch(self, queue: Queue) -> tuple[bool, AlignerBatch]:\n        \"\"\" Get items for inputting into the aligner from the queue in batches\n\n        Items are returned from the ``queue`` in batches of\n        :attr:`~plugins.extract._base.Extractor.batchsize`\n\n        Items are received as :class:`~plugins.extract.extract_media.ExtractMedia` objects and\n        converted to ``dict`` for internal processing.\n\n        To ensure consistent batch sizes for aligner the items are split into separate items for\n        each :class:`~lib.align.DetectedFace` object.\n\n        Remember to put ``'EOF'`` to the out queue after processing\n        the final batch\n\n        Outputs items in the following format. All lists are of length\n        :attr:`~plugins.extract._base.Extractor.batchsize`:\n\n        >>> {'filename': [<filenames of source frames>],\n        >>>  'image': [<source images>],\n        >>>  'detected_faces': [[<lib.align.DetectedFace objects]]}\n\n        Parameters\n        ----------\n        queue : queue.Queue()\n            The ``queue`` that the plugin will be fed from.\n\n        Returns\n        -------\n        exhausted, bool\n            ``True`` if queue is exhausted, ``False`` if not\n        batch, :class:`~plugins.extract._base.ExtractorBatch`\n            The batch object for the current batch\n        \"\"\"\n        exhausted = False\n\n        realign_batch = self._handle_realigns(queue)\n        if realign_batch is not None:\n            return realign_batch\n\n        batch = AlignerBatch(batch_id=_get_new_batch_id())\n        idx = 0\n        while idx < self.batchsize:\n            item = self.rollover_collector(queue)\n            if item == \"EOF\":\n                logger.debug(\"EOF received\")\n                self._eof_seen = True\n                exhausted = not self._re_align.active\n                break\n\n            # Put frames with no faces or are already aligned into the out queue\n            if not item.detected_faces or item.is_aligned:\n                self._queues[\"out\"].put(item)\n                continue\n\n            converted_image = item.get_image_copy(self.color_format)\n            for f_idx, face in enumerate(item.detected_faces):\n                batch.image.append(converted_image)\n                batch.detected_faces.append(face)\n                batch.filename.append(item.filename)\n                idx += 1\n                if idx == self.batchsize:\n                    frame_faces = len(item.detected_faces)\n                    if f_idx + 1 != frame_faces:\n                        self._rollover = ExtractMedia(\n                            item.filename,\n                            item.image,\n                            detected_faces=item.detected_faces[f_idx + 1:],\n                            is_aligned=item.is_aligned)\n                        logger.trace(\"Rolled over %s faces of %s to \"  # type: ignore[attr-defined]\n                                     \"next batch for '%s'\", len(self._rollover.detected_faces),\n                                     frame_faces, item.filename)\n                    break\n        if batch.filename:\n            logger.trace(\"Returning batch: %s\", batch)  # type: ignore[attr-defined]\n            self._re_align.track_batch(batch.batch_id)\n        else:\n            logger.debug(item)\n\n        return exhausted, batch\n\n    def faces_to_feed(self, faces: np.ndarray) -> np.ndarray:\n        \"\"\" Overide for specific plugin processing to convert a batch of face images from UINT8\n        (0-255) into the correct format for the plugin's inference\n\n        Parameters\n        ----------\n        faces: :class:`numpy.ndarray`\n            The batch of faces in UINT8 format\n\n        Returns\n        -------\n        class: `numpy.ndarray`\n            The batch of faces in the format to feed through the plugin\n        \"\"\"\n        raise NotImplementedError()\n\n    # <<< FINALIZE METHODS >>> #\n    def finalize(self, batch: BatchType) -> Generator[ExtractMedia, None, None]:\n        \"\"\" Finalize the output from Aligner\n\n        This should be called as the final task of each `plugin`.\n\n        Pairs the detected faces back up with their original frame before yielding each frame.\n\n        Parameters\n        ----------\n        batch : :class:`AlignerBatch`\n            The final batch item from the `plugin` process.\n\n        Yields\n        ------\n        :class:`~plugins.extract.extract_media.ExtractMedia`\n            The :attr:`DetectedFaces` list will be populated for this class with the bounding boxes\n            and landmarks for the detected faces found in the frame.\n        \"\"\"\n        assert isinstance(batch, AlignerBatch)\n        if not batch.second_pass and self._re_align.active:\n            # Add the batch for second pass re-alignment and return\n            self._re_align.add_batch(batch)\n            return\n        for face, landmarks in zip(batch.detected_faces, batch.landmarks):\n            if not isinstance(landmarks, np.ndarray):\n                landmarks = np.array(landmarks)\n            face.add_landmarks_xy(landmarks)\n\n        logger.trace(\"Item out: %s\", batch)  # type: ignore[attr-defined]\n\n        for frame, filename, face in zip(batch.image, batch.filename, batch.detected_faces):\n            self._output_faces.append(face)\n            if len(self._output_faces) != self._faces_per_filename[filename]:\n                continue\n\n            self._output_faces, folders = self._filter(self._output_faces, min(frame.shape[:2]))\n\n            output = self._extract_media.pop(filename)\n            output.add_detected_faces(self._output_faces)\n            output.add_sub_folders(folders)\n            self._output_faces = []\n\n            logger.trace(\"Final Output: (filename: '%s', image \"  # type: ignore[attr-defined]\n                         \"shape: %s, detected_faces: %s, item: %s)\", output.filename,\n                         output.image_shape, output.detected_faces, output)\n            yield output\n        self._re_align.untrack_batch(batch.batch_id)\n\n    def on_completion(self) -> None:\n        \"\"\" Output the filter counts when process has completed \"\"\"\n        self._filter.output_counts()\n\n    # <<< PROTECTED METHODS >>> #\n    # << PROCESS_INPUT WRAPPER >>\n    def _get_adjusted_boxes(self, original_boxes: np.ndarray) -> np.ndarray:\n        \"\"\" Obtain an array of adjusted bounding boxes based on the number of re-feed iterations\n        that have been selected and the minimum dimension of the original bounding box.\n\n        Parameters\n        ----------\n        original_boxes: :class:`numpy.ndarray`\n            The original ('x', 'y', 'w', 'h') detected face boxes corresponding to the incoming\n            detected face objects\n\n        Returns\n        -------\n        :class:`numpy.ndarray`\n            The original boxes (in position 0) and the randomly adjusted bounding boxes\n        \"\"\"\n        if self._re_feed == 0:\n            return original_boxes[None, ...]\n        beta = 0.05\n        max_shift = np.min(original_boxes[..., 2:], axis=1) * beta\n        rands = np.random.rand(self._re_feed, *original_boxes.shape) * 2 - 1\n        new_boxes = np.rint(original_boxes + (rands * max_shift[None, :, None])).astype(\"int32\")\n        retval = np.concatenate((original_boxes[None, ...], new_boxes))\n        logger.trace(retval)  # type: ignore[attr-defined]\n        return retval\n\n    def _process_input_first_pass(self, batch: AlignerBatch) -> None:\n        \"\"\" Standard pre-processing for aligners for first pass (if re-align selected) or the\n        only pass.\n\n        Process the input to the aligner model multiple times based on the user selected\n        `re-feed` command line option. This adjusts the bounding box for the face to be fed\n        into the model by a random amount within 0.05 pixels of the detected face's shortest axis.\n\n        References\n        ----------\n        https://studios.disneyresearch.com/2020/06/29/high-resolution-neural-face-swapping-for-visual-effects/\n\n        Parameters\n        ----------\n        batch: :class:`AlignerBatch`\n            Contains the batch that is currently being passed through the plugin process\n        \"\"\"\n        original_boxes = np.array([(face.left, face.top, face.width, face.height)\n                                   for face in batch.detected_faces])\n        adjusted_boxes = self._get_adjusted_boxes(original_boxes)\n\n        # Put in random re-feed data to the bounding boxes\n        for bounding_boxes in adjusted_boxes:\n            for face, box in zip(batch.detected_faces, bounding_boxes):\n                face.left, face.top, face.width, face.height = box\n\n            self.process_input(batch)\n            batch.feed = self.faces_to_feed(self._normalize_faces(batch.feed))\n            # Move the populated feed into the batch refeed list. It will be overwritten at next\n            # iteration\n            batch.refeeds.append(batch.feed)\n\n        # Place the original bounding box back to detected face objects\n        for face, box in zip(batch.detected_faces, original_boxes):\n            face.left, face.top, face.width, face.height = box\n\n    def _get_realign_masks(self, batch: AlignerBatch) -> np.ndarray:\n        \"\"\" Obtain the masks required for processing re-aligns\n\n        Parameters\n        ----------\n        batch: :class:`AlignerBatch`\n            Contains the batch that is currently being passed through the plugin process\n\n        Returns\n        -------\n        :class:`numpy.ndarray`\n            The filter masks required for masking the re-aligns\n        \"\"\"\n        if self._re_align.do_refeeds:\n            retval = batch.second_pass_masks  # Masks already calculated during re-feed\n        elif self._re_align.do_filter:\n            retval = self._filter.filtered_mask(batch)[None, ...]\n        else:\n            retval = np.zeros((batch.landmarks.shape[0], ), dtype=\"bool\")[None, ...]\n        return retval\n\n    def _process_input_second_pass(self, batch: AlignerBatch) -> None:\n        \"\"\" Process the input for 2nd-pass re-alignment\n\n        Parameters\n        ----------\n        batch: :class:`AlignerBatch`\n            Contains the batch that is currently being passed through the plugin process\n        \"\"\"\n        batch.second_pass_masks = self._get_realign_masks(batch)\n\n        if not self._re_align.do_refeeds:\n            # Expand the dimensions for re-aligns for consistent handling of code\n            batch.landmarks = batch.landmarks[None, ...]\n\n        refeeds = self._re_align.process_batch(batch)\n        batch.refeeds = [self.faces_to_feed(self._normalize_faces(faces)) for faces in refeeds]\n\n    def _process_input(self, batch: BatchType) -> AlignerBatch:\n        \"\"\" Perform pre-processing depending on whether this is the first/only pass through the\n        aligner or the 2nd pass when re-align has been selected\n\n        Parameters\n        ----------\n        batch: :class:`AlignerBatch`\n            Contains the batch that is currently being passed through the plugin process\n\n        Returns\n        -------\n        :class:`AlignerBatch`\n            The batch with input processed\n        \"\"\"\n        assert isinstance(batch, AlignerBatch)\n        if batch.second_pass:\n            self._process_input_second_pass(batch)\n        else:\n            self._process_input_first_pass(batch)\n        return batch\n\n    # <<< PREDICT WRAPPER >>> #\n    def _predict(self, batch: BatchType) -> AlignerBatch:\n        \"\"\" Just return the aligner's predict function\n\n        Parameters\n        ----------\n        batch: :class:`AlignerBatch`\n            The current batch to find alignments for\n\n        Returns\n        -------\n        :class:`AlignerBatch`\n            The batch item with the :attr:`prediction` populated\n\n        Raises\n        ------\n        FaceswapError\n            If GPU resources are exhausted\n        \"\"\"\n        assert isinstance(batch, AlignerBatch)\n        try:\n            preds = [self.predict(feed) for feed in batch.refeeds]\n            try:\n                batch.prediction = np.array(preds)\n            except ValueError as err:\n                # If refeed batches are different sizes, Numpy will error, so we need to explicitly\n                # set the dtype to 'object' rather than let it infer\n                # numpy error:\n                # ValueError: setting an array element with a sequence. The requested array has an\n                # inhomogeneous shape after 1 dimensions. The detected shape was (9,) +\n                # inhomogeneous part\n                if \"inhomogeneous\" in str(err):\n                    logger.trace(  # type:ignore[attr-defined]\n                        \"Mismatched array sizes, setting dtype to object: %s\",\n                        [p.shape for p in preds])\n                    batch.prediction = np.array(preds, dtype=\"object\")\n                else:\n                    raise\n\n            return batch\n        except tf_errors.ResourceExhaustedError as err:\n            msg = (\"You do not have enough GPU memory available to run detection at the \"\n                   \"selected batch size. You can try a number of things:\"\n                   \"\\n1) Close any other application that is using your GPU (web browsers are \"\n                   \"particularly bad for this).\"\n                   \"\\n2) Lower the batchsize (the amount of images fed into the model) by \"\n                   \"editing the plugin settings (GUI: Settings > Configure extract settings, \"\n                   \"CLI: Edit the file faceswap/config/extract.ini).\"\n                   \"\\n3) Enable 'Single Process' mode.\")\n            raise FaceswapError(msg) from err\n\n    def _process_refeeds(self, batch: AlignerBatch) -> list[AlignerBatch]:\n        \"\"\" Process the output for each selected re-feed\n\n        Parameters\n        ----------\n        batch: :class:`AlignerBatch`\n            The batch object passing through the aligner\n\n        Returns\n        -------\n        list\n            List of :class:`AlignerBatch` objects. Each object in the list contains the\n            results for each selected re-feed\n        \"\"\"\n        retval: list[AlignerBatch] = []\n        if batch.second_pass:\n            # Re-insert empty sub-patches for re-population in ReAlign for filtered out batches\n            selected_idx = 0\n            for mask in batch.second_pass_masks:\n                all_filtered = np.all(mask)\n                if not all_filtered:\n                    feed = batch.refeeds[selected_idx]\n                    pred = batch.prediction[selected_idx]\n                    data = batch.data[selected_idx] if batch.data else {}\n                    selected_idx += 1\n                else:  # All resuts have been filtered out\n                    feed = pred = np.array([])\n                    data = {}\n\n                subbatch = AlignerBatch(batch_id=batch.batch_id,\n                                        image=batch.image,\n                                        detected_faces=batch.detected_faces,\n                                        filename=batch.filename,\n                                        feed=feed,\n                                        prediction=pred,\n                                        data=[data],\n                                        second_pass=batch.second_pass)\n\n                if not all_filtered:\n                    self.process_output(subbatch)\n\n                retval.append(subbatch)\n        else:\n            b_data = batch.data if batch.data else [{}]\n            for feed, pred, dat in zip(batch.refeeds, batch.prediction, b_data):\n                subbatch = AlignerBatch(batch_id=batch.batch_id,\n                                        image=batch.image,\n                                        detected_faces=batch.detected_faces,\n                                        filename=batch.filename,\n                                        feed=feed,\n                                        prediction=pred,\n                                        data=[dat],\n                                        second_pass=batch.second_pass)\n                self.process_output(subbatch)\n                retval.append(subbatch)\n        return retval\n\n    def _get_refeed_filter_masks(self,\n                                 subbatches: list[AlignerBatch],\n                                 original_masks: np.ndarray | None = None) -> np.ndarray:\n        \"\"\" Obtain the boolean mask array for masking out failed re-feed results if filter refeed\n        has been selected\n\n        Parameters\n        ----------\n        subbatches: list\n            List of sub-batch results for each re-feed performed\n        original_masks: :class:`numpy.ndarray`, Optional\n            If passing in the second pass landmarks, these should be the original filter masks so\n            that we don't calculate the mask again for already filtered faces. Default: ``None``\n\n        Returns\n        -------\n        :class:`numpy.ndarray`\n            boolean values for every detected face indicating whether the interim landmarks have\n            passed the filter test\n        \"\"\"\n        retval = np.zeros((len(subbatches), subbatches[0].landmarks.shape[0]), dtype=\"bool\")\n\n        if not self._needs_refeed_masks:\n            return retval\n\n        retval = retval if original_masks is None else original_masks\n        for subbatch, masks in zip(subbatches, retval):\n            masks[:] = self._filter.filtered_mask(subbatch, np.flatnonzero(masks))\n        return retval\n\n    def _get_mean_landmarks(self, landmarks: np.ndarray, masks: np.ndarray) -> np.ndarray:\n        \"\"\" Obtain the averaged landmarks from the re-fed alignments. If config option\n        'filter_refeed' is enabled, then average those results which have not been filtered out\n        otherwise average all results\n\n        Parameters\n        ----------\n        landmarks: :class:`numpy.ndarray`\n            The batch of re-fed alignments\n        masks: :class:`numpy.ndarray`\n            List of boolean values indicating whether each re-fed alignments passed or failed\n            the filter test\n\n        Returns\n        -------\n        :class:`numpy.ndarray`\n            The final averaged landmarks\n        \"\"\"\n        if any(np.all(masked) for masked in masks.T):\n            # hacky fix for faces which entirely failed the filter\n            # We just unmask one value as it is junk anyway and will be discarded on output\n            for idx, masked in enumerate(masks.T):\n                if np.all(masked):\n                    masks[0, idx] = False\n\n        masks = np.broadcast_to(np.reshape(masks, (*landmarks.shape[:2], 1, 1)),\n                                landmarks.shape)\n        return np.ma.array(landmarks, mask=masks).mean(axis=0).data.astype(\"float32\")\n\n    def _process_output_first_pass(self, subbatches: list[AlignerBatch]) -> tuple[np.ndarray,\n                                                                                  np.ndarray]:\n        \"\"\" Process the output from the aligner if this is the first or only pass.\n\n        Parameters\n        ----------\n        subbatches: list\n            List of sub-batch results for each re-feed performed\n\n        Returns\n        -------\n        landmarks: :class:`numpy.ndarray`\n            If re-align is not selected or if re-align has been selected but only on the final\n            output (ie: realign_reefeeds is ``False``) then the averaged batch of landmarks for all\n            re-feeds is returned.\n            If re-align_refeeds has been selected, then this will output each batch of re-feed\n            landmarks.\n        masks: :class:`numpy.ndarray`\n            Boolean mask corresponding to the re-fed landmarks output indicating any values which\n            should be filtered out prior to further processing\n        \"\"\"\n        masks = self._get_refeed_filter_masks(subbatches)\n        all_landmarks = np.array([sub.landmarks for sub in subbatches])\n\n        # re-align not selected or not filtering the re-feeds\n        if not self._re_align.do_refeeds:\n            retval = self._get_mean_landmarks(all_landmarks, masks)\n            return retval, masks\n\n        # Re-align selected with filter re-feeds\n        return all_landmarks, masks\n\n    def _process_output_second_pass(self,\n                                    subbatches: list[AlignerBatch],\n                                    masks: np.ndarray) -> np.ndarray:\n        \"\"\" Process the output from the aligner if this is the first or only pass.\n\n        Parameters\n        ----------\n        subbatches: list\n            List of sub-batch results for each re-aligned re-feed performed\n        masks: :class:`numpy.ndarray`\n            The original re-feed filter masks from the first pass\n        \"\"\"\n        self._re_align.process_output(subbatches, masks)\n        masks = self._get_refeed_filter_masks(subbatches, original_masks=masks)\n        all_landmarks = np.array([sub.landmarks for sub in subbatches])\n        return self._get_mean_landmarks(all_landmarks, masks)\n\n    def _process_output(self, batch: BatchType) -> AlignerBatch:\n        \"\"\" Process the output from the aligner model multiple times based on the user selected\n        `re-feed amount` configuration option, then average the results for final prediction.\n\n        If the config option 'filter_refeed' is enabled, then mask out any returned alignments\n        that fail a filter test\n\n        Parameters\n        ----------\n        batch : :class:`AlignerBatch`\n            Contains the batch that is currently being passed through the plugin process\n\n        Returns\n        -------\n        :class:`AlignerBatch`\n            The batch item with :attr:`landmarks` populated\n        \"\"\"\n        assert isinstance(batch, AlignerBatch)\n        subbatches = self._process_refeeds(batch)\n        if batch.second_pass:\n            batch.landmarks = self._process_output_second_pass(subbatches, batch.second_pass_masks)\n        else:\n            landmarks, masks = self._process_output_first_pass(subbatches)\n            batch.landmarks = landmarks\n            batch.second_pass_masks = masks\n        return batch\n\n    # <<< FACE NORMALIZATION METHODS >>> #\n    def _normalize_faces(self, faces: np.ndarray) -> np.ndarray:\n        \"\"\" Normalizes the face for feeding into model\n        The normalization method is dictated by the normalization command line argument\n\n        Parameters\n        ----------\n        faces: :class:`numpy.ndarray`\n            The batch of faces to normalize\n\n        Returns\n        -------\n        :class:`numpy.ndarray`\n            The normalized faces\n        \"\"\"\n        if self._normalize_method is None:\n            return faces\n        logger.trace(\"Normalizing faces\")  # type: ignore[attr-defined]\n        meth = getattr(self, f\"_normalize_{self._normalize_method.lower()}\")\n        faces = np.array([meth(face) for face in faces])\n        logger.trace(\"Normalized faces\")  # type: ignore[attr-defined]\n        return faces\n\n    @classmethod\n    def _normalize_mean(cls, face: np.ndarray) -> np.ndarray:\n        \"\"\" Normalize Face to the Mean\n\n        Parameters\n        ----------\n        face: :class:`numpy.ndarray`\n            The face to normalize\n\n        Returns\n        -------\n        :class:`numpy.ndarray`\n            The normalized face\n        \"\"\"\n        face = face / 255.0\n        for chan in range(3):\n            layer = face[:, :, chan]\n            layer = (layer - layer.min()) / (layer.max() - layer.min())\n            face[:, :, chan] = layer\n        return face * 255.0\n\n    @classmethod\n    def _normalize_hist(cls, face: np.ndarray) -> np.ndarray:\n        \"\"\" Equalize the RGB histogram channels\n\n        Parameters\n        ----------\n        face: :class:`numpy.ndarray`\n            The face to normalize\n\n        Returns\n        -------\n        :class:`numpy.ndarray`\n            The normalized face\n        \"\"\"\n        for chan in range(3):\n            face[:, :, chan] = cv2.equalizeHist(face[:, :, chan])\n        return face\n\n    @classmethod\n    def _normalize_clahe(cls, face: np.ndarray) -> np.ndarray:\n        \"\"\" Perform Contrast Limited Adaptive Histogram Equalization\n\n        Parameters\n        ----------\n        face: :class:`numpy.ndarray`\n            The face to normalize\n\n        Returns\n        -------\n        :class:`numpy.ndarray`\n            The normalized face\n        \"\"\"\n        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(4, 4))\n        for chan in range(3):\n            face[:, :, chan] = clahe.apply(face[:, :, chan])\n        return face\n", "plugins/extract/align/_base/__init__.py": "#!/usr/bin/env python3\n\"\"\" Base class for Aligner plugins ALL aligners should at least inherit from this class. \"\"\"\n\nfrom .aligner import Aligner, AlignerBatch, BatchType\n", "plugins/extract/recognition/vgg_face2.py": "#!/usr/bin python3\n\"\"\" VGG_Face2 inference and sorting \"\"\"\n\nfrom __future__ import annotations\nimport logging\nimport typing as T\n\nimport numpy as np\nimport psutil\nfrom fastcluster import linkage, linkage_vector\n\nfrom lib.model.layers import L2_normalize\nfrom lib.model.session import KSession\nfrom lib.utils import FaceswapError\nfrom ._base import BatchType, RecogBatch, Identity\n\nif T.TYPE_CHECKING:\n    from collections.abc import Generator\n\nlogger = logging.getLogger(__name__)\n\n\nclass Recognition(Identity):\n    \"\"\" VGG Face feature extraction.\n\n    Extracts feature vectors from faces in order to compare similarity.\n\n    Notes\n    -----\n    Input images should be in BGR Order\n\n    Model exported from: https://github.com/WeidiXie/Keras-VGGFace2-ResNet50 which is based on:\n    https://www.robots.ox.ac.uk/~vgg/software/vgg_face/\n\n\n    Licensed under Creative Commons Attribution License.\n    https://creativecommons.org/licenses/by-nc/4.0/\n    \"\"\"\n\n    def __init__(self, *args, **kwargs) -> None:  # pylint:disable=unused-argument\n        logger.debug(\"Initializing %s\", self.__class__.__name__)\n        git_model_id = 10\n        model_filename = \"vggface2_resnet50_v2.h5\"\n        super().__init__(git_model_id=git_model_id, model_filename=model_filename, **kwargs)\n        self.model: KSession\n        self.name: str = \"VGGFace2\"\n        self.input_size = 224\n        self.color_format = \"BGR\"\n\n        self.vram = 2468 if not self.config[\"cpu\"] else 0\n        self.vram_warnings = 192 if not self.config[\"cpu\"] else 0\n        self.vram_per_batch = 32 if not self.config[\"cpu\"] else 0\n        self.batchsize = self.config[\"batch-size\"]\n\n        # Average image provided in https://github.com/ox-vgg/vgg_face2\n        self._average_img = np.array([91.4953, 103.8827, 131.0912])\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    # <<< GET MODEL >>> #\n    def init_model(self) -> None:\n        \"\"\" Initialize VGG Face 2 Model. \"\"\"\n        assert isinstance(self.model_path, str)\n        model_kwargs = {\"custom_objects\": {\"L2_normalize\": L2_normalize}}\n        self.model = KSession(self.name,\n                              self.model_path,\n                              model_kwargs=model_kwargs,\n                              allow_growth=self.config[\"allow_growth\"],\n                              exclude_gpus=self._exclude_gpus,\n                              cpu_mode=self.config[\"cpu\"])\n        self.model.load_model()\n\n    def process_input(self, batch: BatchType) -> None:\n        \"\"\" Compile the detected faces for prediction \"\"\"\n        assert isinstance(batch, RecogBatch)\n        batch.feed = np.array([T.cast(np.ndarray, feed.face)[..., :3]\n                               for feed in batch.feed_faces],\n                              dtype=\"float32\") - self._average_img\n        logger.trace(\"feed shape: %s\", batch.feed.shape)  # type:ignore\n\n    def predict(self, feed: np.ndarray) -> np.ndarray:\n        \"\"\" Return encodings for given image from vgg_face2.\n\n        Parameters\n        ----------\n        batch: numpy.ndarray\n            The face to be fed through the predictor. Should be in BGR channel order\n\n        Returns\n        -------\n        numpy.ndarray\n            The encodings for the face\n        \"\"\"\n        retval = self.model.predict(feed)\n        assert isinstance(retval, np.ndarray)\n        return retval\n\n    def process_output(self, batch: BatchType) -> None:\n        \"\"\" No output processing for  vgg_face2 \"\"\"\n        return\n\n\nclass Cluster():  # pylint:disable=too-few-public-methods\n    \"\"\" Cluster the outputs from a VGG-Face 2 Model\n\n    Parameters\n    ----------\n    predictions: numpy.ndarray\n        A stacked matrix of vgg_face2 predictions of the shape (`N`, `D`) where `N` is the\n        number of observations and `D` are the number of dimensions.  NB: The given\n        :attr:`predictions` will be overwritten to save memory. If you still require the\n        original values you should take a copy prior to running this method\n    method: ['single','centroid','median','ward']\n        The clustering method to use.\n    threshold: float, optional\n        The threshold to start creating bins for. Set to ``None`` to disable binning\n    \"\"\"\n\n    def __init__(self,\n                 predictions: np.ndarray,\n                 method: T.Literal[\"single\", \"centroid\", \"median\", \"ward\"],\n                 threshold: float | None = None) -> None:\n        logger.debug(\"Initializing: %s (predictions: %s, method: %s, threshold: %s)\",\n                     self.__class__.__name__, predictions.shape, method, threshold)\n        self._num_predictions = predictions.shape[0]\n\n        self._should_output_bins = threshold is not None\n        self._threshold = 0.0 if threshold is None else threshold\n        self._bins: dict[int, int] = {}\n        self._iterator = self._integer_iterator()\n\n        self._result_linkage = self._do_linkage(predictions, method)\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    @classmethod\n    def _integer_iterator(cls) -> Generator[int, None, None]:\n        \"\"\" Iterator that just yields consecutive integers \"\"\"\n        i = -1\n        while True:\n            i += 1\n            yield i\n\n    def _use_vector_linkage(self, dims: int) -> bool:\n        \"\"\" Calculate the RAM that will be required to sort these images and select the appropriate\n        clustering method.\n\n        From fastcluster documentation:\n            \"While the linkage method requires \u0398(N:sup:`2`) memory for clustering of N points, this\n            [vector] method needs \u0398(N D)for N points in RD, which is usually much smaller.\"\n            also:\n            \"half the memory can be saved by specifying :attr:`preserve_input`=``False``\"\n\n        To avoid under calculating we divide the memory calculation by 1.8 instead of 2\n\n        Parameters\n        ----------\n        dims: int\n            The number of dimensions in the vgg_face output\n\n        Returns\n        -------\n            bool:\n                ``True`` if vector_linkage should be used. ``False`` if linkage should be used\n        \"\"\"\n        np_float = 24  # bytes size of a numpy float\n        divider = 1024 * 1024  # bytes to MB\n\n        free_ram = psutil.virtual_memory().available / divider\n        linkage_required = (((self._num_predictions ** 2) * np_float) / 1.8) / divider\n        vector_required = ((self._num_predictions * dims) * np_float) / divider\n        logger.debug(\"free_ram: %sMB, linkage_required: %sMB, vector_required: %sMB\",\n                     int(free_ram), int(linkage_required), int(vector_required))\n\n        if linkage_required < free_ram:\n            logger.verbose(\"Using linkage method\")  # type:ignore\n            retval = False\n        elif vector_required < free_ram:\n            logger.warning(\"Not enough RAM to perform linkage clustering. Using vector \"\n                           \"clustering. This will be significantly slower. Free RAM: %sMB. \"\n                           \"Required for linkage method: %sMB\",\n                           int(free_ram), int(linkage_required))\n            retval = True\n        else:\n            raise FaceswapError(\"Not enough RAM available to sort faces. Try reducing \"\n                                f\"the size of  your dataset. Free RAM: {int(free_ram)}MB. \"\n                                f\"Required RAM: {int(vector_required)}MB\")\n        logger.debug(retval)\n        return retval\n\n    def _do_linkage(self,\n                    predictions: np.ndarray,\n                    method: T.Literal[\"single\", \"centroid\", \"median\", \"ward\"]) -> np.ndarray:\n        \"\"\" Use FastCluster to perform vector or standard linkage\n\n        Parameters\n        ----------\n        predictions: :class:`numpy.ndarray`\n            A stacked matrix of vgg_face2 predictions of the shape (`N`, `D`) where `N` is the\n            number of observations and `D` are the number of dimensions.\n        method: ['single','centroid','median','ward']\n            The clustering method to use.\n\n        Returns\n        -------\n        :class:`numpy.ndarray`\n            The [`num_predictions`, 4] linkage vector\n        \"\"\"\n        dims = predictions.shape[-1]\n        if self._use_vector_linkage(dims):\n            retval = linkage_vector(predictions, method=method)\n        else:\n            retval = linkage(predictions, method=method, preserve_input=False)\n        logger.debug(\"Linkage shape: %s\", retval.shape)\n        return retval\n\n    def _process_leaf_node(self,\n                           current_index: int,\n                           current_bin: int) -> list[tuple[int, int]]:\n        \"\"\" Process the output when we have hit a leaf node \"\"\"\n        if not self._should_output_bins:\n            return [(current_index, 0)]\n\n        if current_bin not in self._bins:\n            next_val = 0 if not self._bins else max(self._bins.values()) + 1\n            self._bins[current_bin] = next_val\n        return [(current_index, self._bins[current_bin])]\n\n    def _get_bin(self,\n                 tree: np.ndarray,\n                 points: int,\n                 current_index: int,\n                 current_bin: int) -> int:\n        \"\"\" Obtain the bin that we are currently in.\n\n        If we are not currently below the threshold for binning, get a new bin ID from the integer\n        iterator.\n\n        Parameters\n        ----------\n        tree: numpy.ndarray\n           A hierarchical tree (dendrogram)\n        points: int\n            The number of points given to the clustering process\n        current_index: int\n            The position in the tree for the recursive traversal\n        current_bin int, optional\n            The ID for the bin we are currently in. Only used when binning is enabled\n\n        Returns\n        -------\n        int\n            The current bin ID for the node\n        \"\"\"\n        if tree[current_index - points, 2] >= self._threshold:\n            current_bin = next(self._iterator)\n            logger.debug(\"Creating new bin ID: %s\", current_bin)\n        return current_bin\n\n    def _seriation(self,\n                   tree: np.ndarray,\n                   points: int,\n                   current_index: int,\n                   current_bin: int = 0) -> list[tuple[int, int]]:\n        \"\"\" Seriation method for sorted similarity.\n\n        Seriation computes the order implied by a hierarchical tree (dendrogram).\n\n        Parameters\n        ----------\n        tree: numpy.ndarray\n           A hierarchical tree (dendrogram)\n        points: int\n            The number of points given to the clustering process\n        current_index: int\n            The position in the tree for the recursive traversal\n        current_bin int, optional\n            The ID for the bin we are currently in. Only used when binning is enabled\n\n        Returns\n        -------\n        list:\n            The indices in the order implied by the hierarchical tree\n        \"\"\"\n        if current_index < points:  # Output the leaf node\n            return self._process_leaf_node(current_index, current_bin)\n\n        if self._should_output_bins:\n            current_bin = self._get_bin(tree, points, current_index, current_bin)\n\n        left = int(tree[current_index-points, 0])\n        right = int(tree[current_index-points, 1])\n\n        serate_left = self._seriation(tree, points, left, current_bin=current_bin)\n        serate_right = self._seriation(tree, points, right, current_bin=current_bin)\n\n        return serate_left + serate_right  # type: ignore\n\n    def __call__(self) -> list[tuple[int, int]]:\n        \"\"\" Process the linkages.\n\n        Transforms a distance matrix into a sorted distance matrix according to the order implied\n        by the hierarchical tree (dendrogram).\n\n        Returns\n        -------\n        list:\n            List of indices with the order implied by the hierarchical tree or list of tuples of\n            (`index`, `bin`) if a binning threshold was provided\n        \"\"\"\n        logger.info(\"Sorting face distances. Depending on your dataset this may take some time...\")\n        if self._threshold:\n            self._threshold = self._result_linkage[:, 2].max() * self._threshold\n        result_order = self._seriation(self._result_linkage,\n                                       self._num_predictions,\n                                       self._num_predictions + self._num_predictions - 2)\n        return result_order\n", "plugins/extract/recognition/_base.py": "#!/usr/bin/env python3\n\"\"\" Base class for Face Recognition plugins\n\nAll Recognition Plugins should inherit from this class.\nSee the override methods for which methods are required.\n\nThe plugin will receive a :class:`~plugins.extract.extract_media.ExtractMedia` object.\n\nFor each source frame, the plugin must pass a dict to finalize containing:\n\n>>> {'filename': <filename of source frame>,\n>>>  'detected_faces': <list of DetectedFace objects containing bounding box points}}\n\nTo get a :class:`~lib.align.DetectedFace` object use the function:\n\n>>> face = self.to_detected_face(<face left>, <face top>, <face right>, <face bottom>)\n\"\"\"\nfrom __future__ import annotations\nimport logging\nimport typing as T\n\nfrom dataclasses import dataclass, field\n\nimport numpy as np\nfrom tensorflow.python.framework import errors_impl as tf_errors  # pylint:disable=no-name-in-module  # noqa\n\nfrom lib.align import AlignedFace, DetectedFace, LandmarkType\nfrom lib.image import read_image_meta\nfrom lib.utils import FaceswapError\nfrom plugins.extract import ExtractMedia\nfrom plugins.extract._base import BatchType, ExtractorBatch, Extractor\n\nif T.TYPE_CHECKING:\n    from collections.abc import Generator\n    from queue import Queue\n    from lib.align.aligned_face import CenteringType\n\nlogger = logging.getLogger(__name__)\n\n\n@dataclass\nclass RecogBatch(ExtractorBatch):\n    \"\"\" Dataclass for holding items flowing through the aligner.\n\n    Inherits from :class:`~plugins.extract._base.ExtractorBatch`\n    \"\"\"\n    detected_faces: list[DetectedFace] = field(default_factory=list)\n    feed_faces: list[AlignedFace] = field(default_factory=list)\n\n\nclass Identity(Extractor):  # pylint:disable=abstract-method\n    \"\"\" Face Recognition Object\n\n    Parent class for all Recognition plugins\n\n    Parameters\n    ----------\n    git_model_id: int\n        The second digit in the github tag that identifies this model. See\n        https://github.com/deepfakes-models/faceswap-models for more information\n    model_filename: str\n        The name of the model file to be loaded\n\n    Other Parameters\n    ----------------\n    configfile: str, optional\n        Path to a custom configuration ``ini`` file. Default: Use system configfile\n\n    See Also\n    --------\n    plugins.extract.pipeline : The extraction pipeline for calling plugins\n    plugins.extract.detect : Detector plugins\n    plugins.extract._base : Parent class for all extraction plugins\n    plugins.extract.align._base : Aligner parent class for extraction plugins.\n    plugins.extract.mask._base : Masker parent class for extraction plugins.\n    \"\"\"\n\n    _logged_lm_count_once = False\n\n    def __init__(self,\n                 git_model_id: int | None = None,\n                 model_filename: str | None = None,\n                 configfile: str | None = None,\n                 instance: int = 0,\n                 **kwargs):\n        logger.debug(\"Initializing %s\", self.__class__.__name__)\n        super().__init__(git_model_id,\n                         model_filename,\n                         configfile=configfile,\n                         instance=instance,\n                         **kwargs)\n        self.input_size = 256  # Override for model specific input_size\n        self.centering: CenteringType = \"legacy\"  # Override for model specific centering\n        self.coverage_ratio = 1.0  # Override for model specific coverage_ratio\n\n        self._plugin_type = \"recognition\"\n        self._filter = IdentityFilter(self.config[\"save_filtered\"])\n        logger.debug(\"Initialized _base %s\", self.__class__.__name__)\n\n    def _get_detected_from_aligned(self, item: ExtractMedia) -> None:\n        \"\"\" Obtain detected face objects for when loading in aligned faces and a detected face\n        object does not exist\n\n        Parameters\n        ----------\n        item: :class:`~plugins.extract.extract_media.ExtractMedia`\n            The extract media to populate the detected face for\n         \"\"\"\n        detected_face = DetectedFace()\n        meta = read_image_meta(item.filename).get(\"itxt\", {}).get(\"alignments\")\n        if meta:\n            detected_face.from_png_meta(meta)\n        item.add_detected_faces([detected_face])\n        self._faces_per_filename[item.filename] += 1  # Track this added face\n        logger.debug(\"Obtained detected face: (filename: %s, detected_face: %s)\",\n                     item.filename, item.detected_faces)\n\n    def _maybe_log_warning(self, face: AlignedFace) -> None:\n        \"\"\" Log a warning, once, if we do not have full facial landmarks\n\n        Parameters\n        ----------\n        face: :class:`~lib.align.aligned_face.AlignedFace`\n            The aligned face object to test the landmark type for\n        \"\"\"\n        if face.landmark_type != LandmarkType.LM_2D_4 or self._logged_lm_count_once:\n            return\n        logger.warning(\"Extracted faces do not contain facial landmark data. '%s' \"\n                       \"identity data is likely to be sub-standard.\", self.name)\n        self._logged_lm_count_once = True\n\n    def get_batch(self, queue: Queue) -> tuple[bool, RecogBatch]:\n        \"\"\" Get items for inputting into the recognition from the queue in batches\n\n        Items are returned from the ``queue`` in batches of\n        :attr:`~plugins.extract._base.Extractor.batchsize`\n\n        Items are received as :class:`~plugins.extract.extract_media.ExtractMedia` objects and\n        converted to :class:`RecogBatch` for internal processing.\n\n        To ensure consistent batch sizes for masker the items are split into separate items for\n        each :class:`~lib.align.DetectedFace` object.\n\n        Remember to put ``'EOF'`` to the out queue after processing\n        the final batch\n\n        Outputs items in the following format. All lists are of length\n        :attr:`~plugins.extract._base.Extractor.batchsize`:\n\n        >>> {'filename': [<filenames of source frames>],\n        >>>  'detected_faces': [[<lib.align.DetectedFace objects]]}\n\n        Parameters\n        ----------\n        queue : queue.Queue()\n            The ``queue`` that the plugin will be fed from.\n\n        Returns\n        -------\n        exhausted, bool\n            ``True`` if queue is exhausted, ``False`` if not\n        batch, :class:`~plugins.extract._base.ExtractorBatch`\n            The batch object for the current batch\n        \"\"\"\n        exhausted = False\n        batch = RecogBatch()\n        idx = 0\n        while idx < self.batchsize:\n            item = self.rollover_collector(queue)\n            if item == \"EOF\":\n                logger.trace(\"EOF received\")  # type: ignore\n                exhausted = True\n                break\n            # Put frames with no faces into the out queue to keep TQDM consistent\n            if not item.is_aligned and not item.detected_faces:\n                self._queues[\"out\"].put(item)\n                continue\n            if item.is_aligned and not item.detected_faces:\n                self._get_detected_from_aligned(item)\n\n            for f_idx, face in enumerate(item.detected_faces):\n\n                image = item.get_image_copy(self.color_format)\n                feed_face = AlignedFace(face.landmarks_xy,\n                                        image=image,\n                                        centering=self.centering,\n                                        size=self.input_size,\n                                        coverage_ratio=self.coverage_ratio,\n                                        dtype=\"float32\",\n                                        is_aligned=item.is_aligned)\n\n                self._maybe_log_warning(feed_face)\n\n                batch.detected_faces.append(face)\n                batch.feed_faces.append(feed_face)\n                batch.filename.append(item.filename)\n                idx += 1\n                if idx == self.batchsize:\n                    frame_faces = len(item.detected_faces)\n                    if f_idx + 1 != frame_faces:\n                        self._rollover = ExtractMedia(\n                            item.filename,\n                            item.image,\n                            detected_faces=item.detected_faces[f_idx + 1:],\n                            is_aligned=item.is_aligned)\n                        logger.trace(\"Rolled over %s faces of %s to next batch \"  # type:ignore\n                                     \"for '%s'\", len(self._rollover.detected_faces), frame_faces,\n                                     item.filename)\n                    break\n        if batch:\n            logger.trace(\"Returning batch: %s\",  # type:ignore\n                         {k: len(v) if isinstance(v, (list, np.ndarray)) else v\n                          for k, v in batch.__dict__.items()})\n        else:\n            logger.trace(item)  # type:ignore\n\n        # TODO Move to end of process not beginning\n        if exhausted:\n            self._filter.output_counts()\n\n        return exhausted, batch\n\n    def _predict(self, batch: BatchType) -> RecogBatch:\n        \"\"\" Just return the recognition's predict function \"\"\"\n        assert isinstance(batch, RecogBatch)\n        try:\n            # slightly hacky workaround to deal with landmarks based masks:\n            batch.prediction = self.predict(batch.feed)\n            return batch\n        except tf_errors.ResourceExhaustedError as err:\n            msg = (\"You do not have enough GPU memory available to run recognition at the \"\n                   \"selected batch size. You can try a number of things:\"\n                   \"\\n1) Close any other application that is using your GPU (web browsers are \"\n                   \"particularly bad for this).\"\n                   \"\\n2) Lower the batchsize (the amount of images fed into the model) by \"\n                   \"editing the plugin settings (GUI: Settings > Configure extract settings, \"\n                   \"CLI: Edit the file faceswap/config/extract.ini).\"\n                   \"\\n3) Enable 'Single Process' mode.\")\n            raise FaceswapError(msg) from err\n\n    def finalize(self, batch: BatchType) -> Generator[ExtractMedia, None, None]:\n        \"\"\" Finalize the output from Masker\n\n        This should be called as the final task of each `plugin`.\n\n        Pairs the detected faces back up with their original frame before yielding each frame.\n\n        Parameters\n        ----------\n        batch : :class:`RecogBatch`\n            The final batch item from the `plugin` process.\n\n        Yields\n        ------\n        :class:`~plugins.extract.extract_media.ExtractMedia`\n            The :attr:`DetectedFaces` list will be populated for this class with the bounding\n            boxes, landmarks and masks for the detected faces found in the frame.\n        \"\"\"\n        assert isinstance(batch, RecogBatch)\n        assert isinstance(self.name, str)\n        for identity, face in zip(batch.prediction, batch.detected_faces):\n            face.add_identity(self.name.lower(), identity)\n        del batch.feed\n\n        logger.trace(\"Item out: %s\",  # type: ignore\n                     {key: val.shape if isinstance(val, np.ndarray) else val\n                                      for key, val in batch.__dict__.items()})\n\n        for filename, face in zip(batch.filename, batch.detected_faces):\n            self._output_faces.append(face)\n            if len(self._output_faces) != self._faces_per_filename[filename]:\n                continue\n\n            output = self._extract_media.pop(filename)\n            self._output_faces = self._filter(self._output_faces, output.sub_folders)\n\n            output.add_detected_faces(self._output_faces)\n            self._output_faces = []\n            logger.trace(\"Yielding: (filename: '%s', image: %s, \"  # type:ignore\n                         \"detected_faces: %s)\", output.filename, output.image_shape,\n                         len(output.detected_faces))\n            yield output\n\n    def add_identity_filters(self,\n                             filters: np.ndarray,\n                             nfilters: np.ndarray,\n                             threshold: float) -> None:\n        \"\"\" Add identity encodings to filter by identity in the recognition plugin\n\n        Parameters\n        ----------\n        filters: :class:`numpy.ndarray`\n            The array of filter embeddings to use\n        nfilters: :class:`numpy.ndarray`\n            The array of nfilter embeddings to use\n        threshold: float\n            The threshold for a positive filter match\n        \"\"\"\n        logger.debug(\"Adding identity filters\")\n        self._filter.add_filters(filters, nfilters, threshold)\n        logger.debug(\"Added identity filters\")\n\n\nclass IdentityFilter():\n    \"\"\" Applies filters on the output of the recognition plugin\n\n    Parameters\n    ----------\n    save_output: bool\n        ``True`` if the filtered faces should be kept as they are being saved. ``False`` if they\n        should be deleted\n    \"\"\"\n    def __init__(self, save_output: bool) -> None:\n        logger.debug(\"Initializing %s: (save_output: %s)\", self.__class__.__name__, save_output)\n        self._save_output = save_output\n        self._filter: np.ndarray | None = None\n        self._nfilter: np.ndarray | None = None\n        self._threshold = 0.0\n        self._filter_enabled: bool = False\n        self._nfilter_enabled: bool = False\n        self._active: bool = False\n        self._counts = 0\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    def add_filters(self, filters: np.ndarray, nfilters: np.ndarray, threshold) -> None:\n        \"\"\" Add identity encodings to the filter and set whether each filter is enabled\n\n        Parameters\n        ----------\n        filters: :class:`numpy.ndarray`\n            The array of filter embeddings to use\n        nfilters: :class:`numpy.ndarray`\n            The array of nfilter embeddings to use\n        threshold: float\n            The threshold for a positive filter match\n        \"\"\"\n        logger.debug(\"Adding filters: %s, nfilters: %s, threshold: %s\",\n                     filters.shape, nfilters.shape, threshold)\n        self._filter = filters\n        self._nfilter = nfilters\n        self._threshold = threshold\n        self._filter_enabled = bool(np.any(self._filter))\n        self._nfilter_enabled = bool(np.any(self._nfilter))\n        self._active = self._filter_enabled or self._nfilter_enabled\n        logger.debug(\"filter active: %s, nfilter active: %s, all active: %s\",\n                     self._filter_enabled, self._nfilter_enabled, self._active)\n\n    @classmethod\n    def _find_cosine_similiarity(cls,\n                                 source_identities: np.ndarray,\n                                 test_identity: np.ndarray) -> np.ndarray:\n        \"\"\" Find the cosine similarity between a source face identity and a test face identity\n\n        Parameters\n        ---------\n        source_identities: :class:`numpy.ndarray`\n            The identity encoding for the source face identities\n        test_identity: :class:`numpy.ndarray`\n            The identity encoding for the face identity to test against the sources\n\n        Returns\n        -------\n        :class:`numpy.ndarray`:\n            The cosine similarity between a face identity and the source identities\n        \"\"\"\n        s_norm = np.linalg.norm(source_identities, axis=1)\n        i_norm = np.linalg.norm(test_identity)\n        retval = source_identities @ test_identity / (s_norm * i_norm)\n        return retval\n\n    def _get_matches(self,\n                     filter_type: T.Literal[\"filter\", \"nfilter\"],\n                     identities: np.ndarray) -> np.ndarray:\n        \"\"\" Obtain the average and minimum distances for each face against the source identities\n        to test against\n\n        Parameters\n        ----------\n        filter_type [\"filter\", \"nfilter\"]\n            The filter type to use for calculating the distance\n        identities: :class:`numpy.ndarray`\n            The identity encodings for the current face(s) being checked\n\n        Returns\n        -------\n        :class:`numpy.ndarray`\n            Boolean array. ``True`` if identity should be filtered otherwise ``False``\n        \"\"\"\n        encodings = self._filter if filter_type == \"filter\" else self._nfilter\n        assert encodings is not None\n        distances = np.array([self._find_cosine_similiarity(encodings, identity)\n                              for identity in identities])\n        is_match = np.any(distances >= self._threshold, axis=-1)\n        # Invert for filter (set the `True` match to `False` for should filter)\n        retval = np.invert(is_match) if filter_type == \"filter\" else is_match\n        logger.trace(\"filter_type: %s, distances shape: %s, is_match: %s, \",  # type: ignore\n                     \"retval: %s\", filter_type, distances.shape, is_match, retval)\n        return retval\n\n    def _filter_faces(self,\n                      faces: list[DetectedFace],\n                      sub_folders: list[str | None],\n                      should_filter: list[bool]) -> list[DetectedFace]:\n        \"\"\" Filter the detected faces, either removing filtered faces from the list of detected\n        faces or setting the output subfolder to `\"_identity_filt\"` for any filtered faces if\n        saving output is enabled.\n\n        Parameters\n        ----------\n        faces: list\n            List of detected face objects to filter out on size\n        sub_folders: list\n            List of subfolder locations for any faces that have already been filtered when\n            config option `save_filtered` has been enabled.\n        should_filter: list\n            List of 'bool' corresponding to face that have not already been marked for filtering.\n            ``True`` indicates face should be filtered, ``False`` indicates face should be kept\n\n        Returns\n        -------\n        detected_faces: list\n            The filtered list of detected face objects, if saving filtered faces has not been\n            selected or the full list of detected faces\n        \"\"\"\n        retval: list[DetectedFace] = []\n        self._counts += sum(should_filter)\n        for idx, face in enumerate(faces):\n            fldr = sub_folders[idx]\n            if fldr is not None:\n                # Saving to sub folder is selected and face is already filtered\n                # so this face was excluded from identity check\n                retval.append(face)\n                continue\n            to_filter = should_filter.pop(0)\n            if not to_filter or self._save_output:\n                # Keep the face if not marked as filtered or we are to output to a subfolder\n                retval.append(face)\n            if to_filter and self._save_output:\n                sub_folders[idx] = \"_identity_filt\"\n\n        return retval\n\n    def __call__(self,\n                 faces: list[DetectedFace],\n                 sub_folders: list[str | None]) -> list[DetectedFace]:\n        \"\"\" Call the identity filter function\n\n        Parameters\n        ----------\n        faces: list\n            List of detected face objects to filter out on size\n        sub_folders: list\n            List of subfolder locations for any faces that have already been filtered when\n            config option `save_filtered` has been enabled.\n\n        Returns\n        -------\n        detected_faces: list\n            The filtered list of detected face objects, if saving filtered faces has not been\n            selected or the full list of detected faces\n        \"\"\"\n        if not self._active:\n            return faces\n\n        identities = np.array([face.identity[\"vggface2\"] for face, fldr in zip(faces, sub_folders)\n                               if fldr is None])\n        logger.trace(\"face_count: %s, already_filtered: %s, identity_shape: %s\",  # type: ignore\n                     len(faces), sum(x is not None for x in sub_folders), identities.shape)\n\n        if not np.any(identities):\n            logger.trace(\"All faces already filtered: %s\", sub_folders)  # type: ignore\n            return faces\n\n        should_filter: list[np.ndarray] = []\n        for f_type in T.get_args(T.Literal[\"filter\", \"nfilter\"]):\n            if not getattr(self, f\"_{f_type}_enabled\"):\n                continue\n            should_filter.append(self._get_matches(f_type, identities))\n\n        # If any of the filter or nfilter evaluate to 'should filter' then filter out face\n        final_filter: list[bool] = np.array(should_filter).max(axis=0).tolist()\n        logger.trace(\"should_filter: %s, final_filter: %s\",  # type: ignore\n                     should_filter, final_filter)\n        return self._filter_faces(faces, sub_folders, final_filter)\n\n    def output_counts(self):\n        \"\"\" Output the counts of filtered items \"\"\"\n        if not self._active or not self._counts:\n            return\n        logger.info(\"Identity filtered (%s): %s\", self._threshold, self._counts)\n", "plugins/extract/recognition/__init__.py": "", "plugins/extract/recognition/vgg_face2_defaults.py": "#!/usr/bin/env python3\n\"\"\"\n    The default options for the faceswap VGG Face2 recognition plugin.\n\n    Defaults files should be named <plugin_name>_defaults.py\n    Any items placed into this file will automatically get added to the relevant config .ini files\n    within the faceswap/config folder.\n\n    The following variables should be defined:\n        _HELPTEXT: A string describing what this plugin does\n        _DEFAULTS: A dictionary containing the options, defaults and meta information. The\n                   dictionary should be defined as:\n                       {<option_name>: {<metadata>}}\n\n                   <option_name> should always be lower text.\n                   <metadata> dictionary requirements are listed below.\n\n    The following keys are expected for the _DEFAULTS <metadata> dict:\n        datatype:  [required] A python type class. This limits the type of data that can be\n                   provided in the .ini file and ensures that the value is returned in the\n                   correct type to faceswap. Valid data types are: <class 'int'>, <class 'float'>,\n                   <class 'str'>, <class 'bool'>.\n        default:   [required] The default value for this option.\n        info:      [required] A string describing what this option does.\n        group:     [optional]. A group for grouping options together in the GUI. If not\n                   provided this will not group this option with any others.\n        choices:   [optional] If this option's datatype is of <class 'str'> then valid\n                   selections can be defined here. This validates the option and also enables\n                   a combobox / radio option in the GUI.\n        gui_radio: [optional] If <choices> are defined, this indicates that the GUI should use\n                   radio buttons rather than a combobox to display this option.\n        min_max:   [partial] For <class 'int'> and <class 'float'> data types this is required\n                   otherwise it is ignored. Should be a tuple of min and max accepted values.\n                   This is used for controlling the GUI slider range. Values are not enforced.\n        rounding:  [partial] For <class 'int'> and <class 'float'> data types this is\n                   required otherwise it is ignored. Used for the GUI slider. For floats, this\n                   is the number of decimal places to display. For ints this is the step size.\n        fixed:     [optional] [train only]. Training configurations are fixed when the model is\n                   created, and then reloaded from the state file. Marking an item as fixed=False\n                   indicates that this value can be changed for existing models, and will override\n                   the value saved in the state file with the updated value in config. If not\n                   provided this will default to True.\n\"\"\"\n\n\n_HELPTEXT = (\n    \"VGG Face 2 identity recognition.\\n\"\n    \"A Keras port of the model trained for VGGFace2: A dataset for recognising faces across pose \"\n    \"and age. (https://arxiv.org/abs/1710.08092)\"\n    )\n\n\n_DEFAULTS = {\n    \"batch-size\": {\n        \"default\": 16,\n        \"info\": \"The batch size to use. To a point, higher batch sizes equal better performance, \"\n                \"but setting it too high can harm performance.\\n\"\n                \"\\n\\tNvidia users: If the batchsize is set higher than the your GPU can \"\n                \"accomodate then this will automatically be lowered.\",\n        \"datatype\": int,\n        \"rounding\": 1,\n        \"min_max\": (1, 64),\n        \"choices\": [],\n        \"group\": \"settings\",\n        \"gui_radio\": False,\n        \"fixed\": True\n    },\n    \"cpu\": {\n        \"default\": False,\n        \"info\": \"VGG Face2 still runs fairly quickly on CPU on some setups. Enable \"\n                \"CPU mode here to use the CPU for this plugin to save some VRAM at a speed cost.\",\n        \"datatype\": bool,\n        \"group\": \"settings\"\n    },\n}\n", "plugins/extract/detect/mtcnn.py": "#!/usr/bin/env python3\n\"\"\" MTCNN Face detection plugin \"\"\"\nfrom __future__ import annotations\nimport logging\nimport typing as T\n\nimport cv2\nimport numpy as np\n\n# Ignore linting errors from Tensorflow's thoroughly broken import system\nfrom tensorflow.keras.layers import Conv2D, Dense, Flatten, Input, MaxPool2D, Permute, PReLU  # noqa:E501  # pylint:disable=import-error\n\nfrom lib.model.session import KSession\nfrom ._base import BatchType, Detector\n\nif T.TYPE_CHECKING:\n    from tensorflow import Tensor\n\nlogger = logging.getLogger(__name__)\n\n\nclass Detect(Detector):\n    \"\"\" MTCNN detector for face recognition. \"\"\"\n    def __init__(self, **kwargs) -> None:\n        git_model_id = 2\n        model_filename = [\"mtcnn_det_v2.1.h5\", \"mtcnn_det_v2.2.h5\", \"mtcnn_det_v2.3.h5\"]\n        super().__init__(git_model_id=git_model_id, model_filename=model_filename, **kwargs)\n        self.name = \"MTCNN\"\n        self.input_size = 640\n        self.vram = 320 if not self.config[\"cpu\"] else 0\n        self.vram_warnings = 64 if not self.config[\"cpu\"] else 0  # Will run at this with warnings\n        self.vram_per_batch = 32 if not self.config[\"cpu\"] else 0\n        self.batchsize = self.config[\"batch-size\"]\n        self.kwargs = self._validate_kwargs()\n        self.color_format = \"RGB\"\n\n    def _validate_kwargs(self) -> dict[str, int | float | list[float]]:\n        \"\"\" Validate that config options are correct. If not reset to default \"\"\"\n        valid = True\n        threshold = [self.config[\"threshold_1\"],\n                     self.config[\"threshold_2\"],\n                     self.config[\"threshold_3\"]]\n        kwargs = {\"minsize\": self.config[\"minsize\"],\n                  \"threshold\": threshold,\n                  \"factor\": self.config[\"scalefactor\"],\n                  \"input_size\": self.input_size}\n\n        if kwargs[\"minsize\"] < 10:\n            valid = False\n        elif not all(0.0 < threshold <= 1.0 for threshold in kwargs['threshold']):\n            valid = False\n        elif not 0.0 < kwargs['factor'] < 1.0:\n            valid = False\n\n        if not valid:\n            kwargs = {}\n            logger.warning(\"Invalid MTCNN options in config. Running with defaults\")\n\n        logger.debug(\"Using mtcnn kwargs: %s\", kwargs)\n        return kwargs\n\n    def init_model(self) -> None:\n        \"\"\" Initialize MTCNN Model. \"\"\"\n        assert isinstance(self.model_path, list)\n        self.model = MTCNN(self.model_path,\n                           self.config[\"allow_growth\"],\n                           self._exclude_gpus,\n                           self.config[\"cpu\"],\n                           **self.kwargs)  # type:ignore\n\n    def process_input(self, batch: BatchType) -> None:\n        \"\"\" Compile the detection image(s) for prediction\n\n        Parameters\n        ----------\n        batch: :class:`~plugins.extract.detect._base.DetectorBatch`\n            Contains the batch that is currently being passed through the plugin process\n        \"\"\"\n        batch.feed = (np.array(batch.image, dtype=\"float32\") - 127.5) / 127.5\n\n    def predict(self, feed: np.ndarray) -> np.ndarray:\n        \"\"\" Run model to get predictions\n\n        Parameters\n        ----------\n        batch: :class:`~plugins.extract.detect._base.DetectorBatch`\n            Contains the batch to pass through the MTCNN model\n\n        Returns\n        -------\n        dict\n            The batch with the predictions added to the dictionary\n        \"\"\"\n        assert isinstance(self.model, MTCNN)\n        prediction, points = self.model.detect_faces(feed)\n        logger.trace(\"prediction: %s, mtcnn_points: %s\",  # type:ignore\n                     prediction, points)\n        return prediction\n\n    def process_output(self, batch: BatchType) -> None:\n        \"\"\" MTCNN performs no post processing so the original batch is returned\n\n        Parameters\n        ----------\n        batch: :class:`~plugins.extract.detect._base.DetectorBatch`\n            Contains the batch to apply postprocessing to\n        \"\"\"\n        return\n\n\n# MTCNN Detector\n# Code adapted from: https://github.com/xiangrufan/keras-mtcnn\n#\n# Keras implementation of the face detection / alignment algorithm\n# found at\n# https://github.com/kpzhang93/MTCNN_face_detection_alignment\n#\n# MIT License\n#\n# Copyright (c) 2016 Kaipeng Zhang\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n\nclass PNet(KSession):\n    \"\"\" Keras P-Net model for MTCNN\n\n    Parameters\n    ----------\n    model_path: str\n        The path to the keras model file\n    allow_growth: bool, optional\n        Enable the Tensorflow GPU allow_growth configuration option. This option prevents\n        Tensorflow from allocating all of the GPU VRAM, but can lead to higher fragmentation and\n        slower performance. Default: ``False``\n    exclude_gpus: list, optional\n        A list of indices correlating to connected GPUs that Tensorflow should not use. Pass\n        ``None`` to not exclude any GPUs. Default: ``None``\n    cpu_mode: bool, optional\n        ``True`` run the model on CPU. Default: ``False``\n    input_size: int\n        The input size of the model\n    minsize: int, optional\n        The minimum size of a face to accept as a detection. Default: `20`\n    threshold: list, optional\n        Threshold for P-Net\n    \"\"\"\n    def __init__(self,\n                 model_path: str,\n                 allow_growth: bool,\n                 exclude_gpus: list[int] | None,\n                 cpu_mode: bool,\n                 input_size: int,\n                 min_size: int,\n                 factor: float,\n                 threshold: float) -> None:\n        super().__init__(\"MTCNN-PNet\",\n                         model_path,\n                         allow_growth=allow_growth,\n                         exclude_gpus=exclude_gpus,\n                         cpu_mode=cpu_mode)\n\n        self.define_model(self.model_definition)\n        self.load_model_weights()\n\n        self._input_size = input_size\n        self._threshold = threshold\n\n        self._pnet_scales = self._calculate_scales(min_size, factor)\n        self._pnet_sizes = [(int(input_size * scale), int(input_size * scale))\n                            for scale in self._pnet_scales]\n        self._pnet_input: list[np.ndarray] | None = None\n\n    @staticmethod\n    def model_definition() -> tuple[list[Tensor], list[Tensor]]:\n        \"\"\" Keras P-Network Definition for MTCNN \"\"\"\n        input_ = Input(shape=(None, None, 3))\n        var_x = Conv2D(10, (3, 3), strides=1, padding='valid', name='conv1')(input_)\n        var_x = PReLU(shared_axes=[1, 2], name='PReLU1')(var_x)\n        var_x = MaxPool2D(pool_size=2)(var_x)\n        var_x = Conv2D(16, (3, 3), strides=1, padding='valid', name='conv2')(var_x)\n        var_x = PReLU(shared_axes=[1, 2], name='PReLU2')(var_x)\n        var_x = Conv2D(32, (3, 3), strides=1, padding='valid', name='conv3')(var_x)\n        var_x = PReLU(shared_axes=[1, 2], name='PReLU3')(var_x)\n        classifier = Conv2D(2, (1, 1), activation='softmax', name='conv4-1')(var_x)\n        bbox_regress = Conv2D(4, (1, 1), name='conv4-2')(var_x)\n        return [input_], [classifier, bbox_regress]\n\n    def _calculate_scales(self,\n                          minsize: int,\n                          factor: float) -> list[float]:\n        \"\"\" Calculate multi-scale\n\n        Parameters\n        ----------\n        minsize: int\n            Minimum size for a face to be accepted\n        factor: float\n            Scaling factor\n\n        Returns\n        -------\n        list\n            List of scale floats\n        \"\"\"\n        factor_count = 0\n        var_m = 12.0 / minsize\n        minl = self._input_size * var_m\n        # create scale pyramid\n        scales = []\n        while minl >= 12:\n            scales += [var_m * np.power(factor, factor_count)]\n            minl = minl * factor\n            factor_count += 1\n        logger.trace(scales)  # type:ignore\n        return scales\n\n    def __call__(self, images: np.ndarray) -> list[np.ndarray]:\n        \"\"\" first stage - fast proposal network (p-net) to obtain face candidates\n\n        Parameters\n        ----------\n        images: :class:`numpy.ndarray`\n            The batch of images to detect faces in\n\n        Returns\n        -------\n        List\n            List of face candidates from P-Net\n        \"\"\"\n        batch_size = images.shape[0]\n        rectangles: list[list[list[int | float]]] = [[] for _ in range(batch_size)]\n        scores: list[list[np.ndarray]] = [[] for _ in range(batch_size)]\n\n        if self._pnet_input is None:\n            self._pnet_input = [np.empty((batch_size, rheight, rwidth, 3), dtype=\"float32\")\n                                for rheight, rwidth in self._pnet_sizes]\n\n        for scale, batch, (rheight, rwidth) in zip(self._pnet_scales,\n                                                   self._pnet_input,\n                                                   self._pnet_sizes):\n            _ = [cv2.resize(images[idx], (rwidth, rheight), dst=batch[idx])\n                 for idx in range(batch_size)]\n            cls_prob, roi = self.predict(batch)\n            cls_prob = cls_prob[..., 1]\n            out_side = max(cls_prob.shape[1:3])\n            cls_prob = np.swapaxes(cls_prob, 1, 2)\n            roi = np.swapaxes(roi, 1, 3)\n            for idx in range(batch_size):\n                # first index 0 = class score, 1 = one hot representation\n                rect, score = self._detect_face_12net(cls_prob[idx, ...],\n                                                      roi[idx, ...],\n                                                      out_side,\n                                                      1 / scale)\n                rectangles[idx].extend(rect)\n                scores[idx].extend(score)\n\n        return [nms(np.array(rect), np.array(score), 0.7, \"iou\")[0]  # don't output scores\n                for rect, score in zip(rectangles, scores)]\n\n    def _detect_face_12net(self,\n                           class_probabilities: np.ndarray,\n                           roi: np.ndarray,\n                           size: int,\n                           scale: float) -> tuple[np.ndarray, np.ndarray]:\n        \"\"\" Detect face position and calibrate bounding box on 12net feature map(matrix version)\n\n        Parameters\n        ----------\n        class_probabilities: :class:`numpy.ndarray`\n            softmax feature map for face classify\n        roi: :class:`numpy.ndarray`\n            feature map for regression\n        size: int\n            feature map's largest size\n        scale: float\n            current input image scale in multi-scales\n\n        Returns\n        -------\n        list\n            Calibrated face candidates\n        \"\"\"\n        in_side = 2 * size + 11\n        stride = 0. if size == 1 else float(in_side - 12) / (size - 1)\n        (var_x, var_y) = np.nonzero(class_probabilities >= self._threshold)\n        boundingbox = np.array([var_x, var_y]).T\n\n        boundingbox = np.concatenate((np.fix((stride * (boundingbox) + 0) * scale),\n                                      np.fix((stride * (boundingbox) + 11) * scale)), axis=1)\n        offset = roi[:4, var_x, var_y].T\n        boundingbox = boundingbox + offset * 12.0 * scale\n        rectangles = np.concatenate((boundingbox,\n                                     np.array([class_probabilities[var_x, var_y]]).T), axis=1)\n        rectangles = rect2square(rectangles)\n\n        np.clip(rectangles[..., :4], 0., self._input_size, out=rectangles[..., :4])\n        pick = np.where(np.logical_and(rectangles[..., 2] > rectangles[..., 0],\n                                       rectangles[..., 3] > rectangles[..., 1]))[0]\n        rects = rectangles[pick, :4].astype(\"int\")\n        scores = rectangles[pick, 4]\n\n        return nms(rects, scores, 0.3, \"iou\")\n\n\nclass RNet(KSession):\n    \"\"\" Keras R-Net model Definition for MTCNN\n\n    Parameters\n    ----------\n    model_path: str\n        The path to the keras model file\n    allow_growth: bool, optional\n        Enable the Tensorflow GPU allow_growth configuration option. This option prevents\n        Tensorflow from allocating all of the GPU VRAM, but can lead to higher fragmentation and\n        slower performance. Default: ``False``\n    exclude_gpus: list, optional\n        A list of indices correlating to connected GPUs that Tensorflow should not use. Pass\n        ``None`` to not exclude any GPUs. Default: ``None``\n    cpu_mode: bool, optional\n        ``True`` run the model on CPU. Default: ``False``\n    input_size: int\n        The input size of the model\n    threshold: list, optional\n        Threshold for R-Net\n\n    \"\"\"\n    def __init__(self,\n                 model_path: str,\n                 allow_growth: bool,\n                 exclude_gpus: list[int] | None,\n                 cpu_mode: bool,\n                 input_size: int,\n                 threshold: float) -> None:\n        super().__init__(\"MTCNN-RNet\",\n                         model_path,\n                         allow_growth=allow_growth,\n                         exclude_gpus=exclude_gpus,\n                         cpu_mode=cpu_mode)\n        self.define_model(self.model_definition)\n        self.load_model_weights()\n\n        self._input_size = input_size\n        self._threshold = threshold\n\n    @staticmethod\n    def model_definition() -> tuple[list[Tensor], list[Tensor]]:\n        \"\"\" Keras R-Network Definition for MTCNN \"\"\"\n        input_ = Input(shape=(24, 24, 3))\n        var_x = Conv2D(28, (3, 3), strides=1, padding='valid', name='conv1')(input_)\n        var_x = PReLU(shared_axes=[1, 2], name='prelu1')(var_x)\n        var_x = MaxPool2D(pool_size=3, strides=2, padding='same')(var_x)\n\n        var_x = Conv2D(48, (3, 3), strides=1, padding='valid', name='conv2')(var_x)\n        var_x = PReLU(shared_axes=[1, 2], name='prelu2')(var_x)\n        var_x = MaxPool2D(pool_size=3, strides=2)(var_x)\n\n        var_x = Conv2D(64, (2, 2), strides=1, padding='valid', name='conv3')(var_x)\n        var_x = PReLU(shared_axes=[1, 2], name='prelu3')(var_x)\n        var_x = Permute((3, 2, 1))(var_x)\n        var_x = Flatten()(var_x)\n        var_x = Dense(128, name='conv4')(var_x)\n        var_x = PReLU(name='prelu4')(var_x)\n        classifier = Dense(2, activation='softmax', name='conv5-1')(var_x)\n        bbox_regress = Dense(4, name='conv5-2')(var_x)\n        return [input_], [classifier, bbox_regress]\n\n    def __call__(self,\n                 images: np.ndarray,\n                 rectangle_batch: list[np.ndarray],\n                 ) -> list[np.ndarray]:\n        \"\"\" second stage - refinement of face candidates with r-net\n\n        Parameters\n        ----------\n        images: :class:`numpy.ndarray`\n            The batch of images to detect faces in\n        rectangle_batch:\n            List of :class:`numpy.ndarray` face candidates from P-Net\n\n        Returns\n        -------\n        List\n            List of :class:`numpy.ndarray` refined face candidates from R-Net\n        \"\"\"\n        ret: list[np.ndarray] = []\n        for idx, (rectangles, image) in enumerate(zip(rectangle_batch, images)):\n            if not np.any(rectangles):\n                ret.append(np.array([]))\n                continue\n\n            feed_batch = np.empty((rectangles.shape[0], 24, 24, 3), dtype=\"float32\")\n\n            _ = [cv2.resize(image[rect[1]: rect[3], rect[0]: rect[2]],\n                            (24, 24),\n                            dst=feed_batch[idx])\n                 for idx, rect in enumerate(rectangles)]\n\n            cls_prob, roi_prob = self.predict(feed_batch)\n            ret.append(self._filter_face_24net(cls_prob, roi_prob, rectangles))\n        return ret\n\n    def _filter_face_24net(self,\n                           class_probabilities: np.ndarray,\n                           roi: np.ndarray,\n                           rectangles: np.ndarray,\n                           ) -> np.ndarray:\n        \"\"\" Filter face position and calibrate bounding box on 12net's output\n\n        Parameters\n        ----------\n        class_probabilities: class:`np.ndarray`\n            Softmax feature map for face classify\n        roi: :class:`numpy.ndarray`\n            Feature map for regression\n        rectangles: list\n            12net's predict\n\n        Returns\n        -------\n        list\n            rectangles in the format [[x, y, x1, y1, score]]\n        \"\"\"\n        prob = class_probabilities[:, 1]\n        pick = np.nonzero(prob >= self._threshold)\n\n        bbox = rectangles.T[:4, pick]\n        scores = np.array([prob[pick]]).T.ravel()\n        deltas = roi.T[:4, pick]\n\n        dims = np.tile([bbox[2] - bbox[0], bbox[3] - bbox[1]], (2, 1, 1))\n        bbox = np.transpose(bbox + deltas * dims).reshape(-1, 4)\n        bbox = np.clip(rect2square(bbox), 0, self._input_size).astype(\"int\")\n        return nms(bbox, scores, 0.3, \"iou\")[0]\n\n\nclass ONet(KSession):\n    \"\"\" Keras O-Net model for MTCNN\n\n    Parameters\n    ----------\n    model_path: str\n        The path to the keras model file\n    allow_growth: bool, optional\n        Enable the Tensorflow GPU allow_growth configuration option. This option prevents\n        Tensorflow from allocating all of the GPU VRAM, but can lead to higher fragmentation and\n        slower performance. Default: ``False``\n    exclude_gpus: list, optional\n        A list of indices correlating to connected GPUs that Tensorflow should not use. Pass\n        ``None`` to not exclude any GPUs. Default: ``None``\n    cpu_mode: bool, optional\n        ``True`` run the model on CPU. Default: ``False``\n    input_size: int\n        The input size of the model\n    threshold: list, optional\n        Threshold for O-Net\n    \"\"\"\n    def __init__(self,\n                 model_path: str,\n                 allow_growth: bool,\n                 exclude_gpus: list[int] | None,\n                 cpu_mode: bool,\n                 input_size: int,\n                 threshold: float) -> None:\n        super().__init__(\"MTCNN-ONet\",\n                         model_path,\n                         allow_growth=allow_growth,\n                         exclude_gpus=exclude_gpus,\n                         cpu_mode=cpu_mode)\n        self.define_model(self.model_definition)\n        self.load_model_weights()\n\n        self._input_size = input_size\n        self._threshold = threshold\n\n    @staticmethod\n    def model_definition() -> tuple[list[Tensor], list[Tensor]]:\n        \"\"\" Keras O-Network for MTCNN \"\"\"\n        input_ = Input(shape=(48, 48, 3))\n        var_x = Conv2D(32, (3, 3), strides=1, padding='valid', name='conv1')(input_)\n        var_x = PReLU(shared_axes=[1, 2], name='prelu1')(var_x)\n        var_x = MaxPool2D(pool_size=3, strides=2, padding='same')(var_x)\n        var_x = Conv2D(64, (3, 3), strides=1, padding='valid', name='conv2')(var_x)\n        var_x = PReLU(shared_axes=[1, 2], name='prelu2')(var_x)\n        var_x = MaxPool2D(pool_size=3, strides=2)(var_x)\n        var_x = Conv2D(64, (3, 3), strides=1, padding='valid', name='conv3')(var_x)\n        var_x = PReLU(shared_axes=[1, 2], name='prelu3')(var_x)\n        var_x = MaxPool2D(pool_size=2)(var_x)\n        var_x = Conv2D(128, (2, 2), strides=1, padding='valid', name='conv4')(var_x)\n        var_x = PReLU(shared_axes=[1, 2], name='prelu4')(var_x)\n        var_x = Permute((3, 2, 1))(var_x)\n        var_x = Flatten()(var_x)\n        var_x = Dense(256, name='conv5')(var_x)\n        var_x = PReLU(name='prelu5')(var_x)\n\n        classifier = Dense(2, activation='softmax', name='conv6-1')(var_x)\n        bbox_regress = Dense(4, name='conv6-2')(var_x)\n        landmark_regress = Dense(10, name='conv6-3')(var_x)\n        return [input_], [classifier, bbox_regress, landmark_regress]\n\n    def __call__(self,\n                 images: np.ndarray,\n                 rectangle_batch: list[np.ndarray]\n                 ) -> list[tuple[np.ndarray, np.ndarray]]:\n        \"\"\" Third stage - further refinement and facial landmarks positions with o-net\n\n        Parameters\n        ----------\n        images: :class:`numpy.ndarray`\n            The batch of images to detect faces in\n        rectangle_batch:\n            List of :class:`numpy.ndarray` face candidates from R-Net\n\n        Returns\n        -------\n        List\n            List of refined final candidates, scores and landmark points from O-Net\n        \"\"\"\n        ret: list[tuple[np.ndarray, np.ndarray]] = []\n        for idx, rectangles in enumerate(rectangle_batch):\n            if not np.any(rectangles):\n                ret.append((np.empty((0, 5)), np.empty(0)))\n                continue\n            image = images[idx]\n            feed_batch = np.empty((rectangles.shape[0], 48, 48, 3), dtype=\"float32\")\n\n            _ = [cv2.resize(image[rect[1]: rect[3], rect[0]: rect[2]],\n                            (48, 48),\n                            dst=feed_batch[idx])\n                 for idx, rect in enumerate(rectangles)]\n\n            cls_probs, roi_probs, pts_probs = self.predict(feed_batch)\n            ret.append(self._filter_face_48net(cls_probs, roi_probs, pts_probs, rectangles))\n        return ret\n\n    def _filter_face_48net(self, class_probabilities: np.ndarray,\n                           roi: np.ndarray,\n                           points: np.ndarray,\n                           rectangles: np.ndarray) -> tuple[np.ndarray, np.ndarray]:\n        \"\"\" Filter face position and calibrate bounding box on 12net's output\n\n        Parameters\n        ----------\n        class_probabilities: :class:`numpy.ndarray`  : class_probabilities[1] is face possibility\n            Array of face probabilities\n        roi: :class:`numpy.ndarray`\n            offset\n        points: :class:`numpy.ndarray`\n            5 point face landmark\n        rectangles: :class:`numpy.ndarray`\n            12net's predict, rectangles[i][0:3] is the position, rectangles[i][4] is score\n\n        Returns\n        -------\n        boxes: :class:`numpy.ndarray`\n            The [l, t, r, b, score] bounding boxes\n        points: :class:`numpy.ndarray`\n            The 5 point landmarks\n        \"\"\"\n        prob = class_probabilities[:, 1]\n        pick = np.nonzero(prob >= self._threshold)[0]\n        scores = np.array([prob[pick]]).T.ravel()\n\n        bbox = rectangles[pick]\n        dims = np.array([bbox[..., 2] - bbox[..., 0], bbox[..., 3] - bbox[..., 1]]).T\n\n        pts = np.vstack(\n            np.hsplit(points[pick], 2)).reshape(2, -1, 5).transpose(1, 2, 0).reshape(-1, 10)\n        pts = np.tile(dims, (1, 5)) * pts + np.tile(bbox[..., :2], (1, 5))\n\n        bbox = np.clip(np.floor(bbox + roi[pick] * np.tile(dims, (1, 2))),\n                       0.,\n                       self._input_size)\n\n        indices = np.where(\n            np.logical_and(bbox[..., 2] > bbox[..., 0], bbox[..., 3] > bbox[..., 1]))[0]\n        picks = np.concatenate([bbox[indices], pts[indices]], axis=-1)\n\n        results, scores = nms(picks, scores, 0.3, \"iom\")\n        return np.concatenate([results[..., :4], scores[..., None]], axis=-1), results[..., 4:].T\n\n\nclass MTCNN():  # pylint:disable=too-few-public-methods\n    \"\"\" MTCNN Detector for face alignment\n\n    Parameters\n    ----------\n    model_path: list\n        List of paths to the 3 MTCNN subnet weights\n    allow_growth: bool, optional\n        Enable the Tensorflow GPU allow_growth configuration option. This option prevents\n        Tensorflow from allocating all of the GPU VRAM, but can lead to higher fragmentation and\n        slower performance. Default: ``False``\n    exclude_gpus: list, optional\n        A list of indices correlating to connected GPUs that Tensorflow should not use. Pass\n        ``None`` to not exclude any GPUs. Default: ``None``\n    cpu_mode: bool, optional\n        ``True`` run the model on CPU. Default: ``False``\n    input_size: int, optional\n        The height, width input size to the model. Default: 640\n    minsize: int, optional\n        The minimum size of a face to accept as a detection. Default: `20`\n    threshold: list, optional\n        List of floats for the three steps, Default: `[0.6, 0.7, 0.7]`\n    factor: float, optional\n        The factor used to create a scaling pyramid of face sizes to detect in the image.\n        Default: `0.709`\n    \"\"\"\n    def __init__(self,\n                 model_path: list[str],\n                 allow_growth: bool,\n                 exclude_gpus: list[int] | None,\n                 cpu_mode: bool,\n                 input_size: int = 640,\n                 minsize: int = 20,\n                 threshold: list[float] | None = None,\n                 factor: float = 0.709) -> None:\n        logger.debug(\"Initializing: %s: (model_path: '%s', allow_growth: %s, exclude_gpus: %s, \"\n                     \"input_size: %s, minsize: %s, threshold: %s, factor: %s)\",\n                     self.__class__.__name__, model_path, allow_growth, exclude_gpus,\n                     input_size, minsize, threshold, factor)\n\n        threshold = [0.6, 0.7, 0.7] if threshold is None else threshold\n        self._pnet = PNet(model_path[0],\n                          allow_growth,\n                          exclude_gpus,\n                          cpu_mode,\n                          input_size,\n                          minsize,\n                          factor,\n                          threshold[0])\n        self._rnet = RNet(model_path[1],\n                          allow_growth,\n                          exclude_gpus,\n                          cpu_mode,\n                          input_size,\n                          threshold[1])\n        self._onet = ONet(model_path[2],\n                          allow_growth,\n                          exclude_gpus,\n                          cpu_mode,\n                          input_size,\n                          threshold[2])\n\n        logger.debug(\"Initialized: %s\", self.__class__.__name__)\n\n    def detect_faces(self, batch: np.ndarray) -> tuple[np.ndarray, tuple[np.ndarray]]:\n        \"\"\"Detects faces in an image, and returns bounding boxes and points for them.\n\n        Parameters\n        ----------\n        batch: :class:`numpy.ndarray`\n            The input batch of images to detect face in\n\n        Returns\n        -------\n        List\n            list of numpy arrays containing the bounding box and 5 point landmarks\n            of detected faces\n        \"\"\"\n        rectangles = self._pnet(batch)\n        rectangles = self._rnet(batch, rectangles)\n\n        ret_boxes, ret_points = zip(*self._onet(batch, rectangles))\n        return np.array(ret_boxes, dtype=\"object\"), ret_points\n\n\ndef nms(rectangles: np.ndarray,\n        scores: np.ndarray,\n        threshold: float,\n        method: str = \"iom\") -> tuple[np.ndarray, np.ndarray]:\n    \"\"\" apply non-maximum suppression on ROIs in same scale(matrix version)\n\n    Parameters\n    ----------\n    rectangles: :class:`np.ndarray`\n        The [b, l, t, r, b] bounding box detection candidates\n    threshold: float\n        Threshold for succesful match\n    method: str, optional\n        \"iom\" method or default. Defalt: \"iom\"\n\n    Returns\n    -------\n    rectangles: :class:`np.ndarray`\n        The [b, l, t, r, b] bounding boxes\n    scores :class:`np.ndarray`\n        The associated scores for the rectangles\n\n    \"\"\"\n    if not np.any(rectangles):\n        return rectangles, scores\n    bboxes = rectangles[..., :4].T\n    area = np.multiply(bboxes[2] - bboxes[0] + 1, bboxes[3] - bboxes[1] + 1)\n    s_sort = scores.argsort()\n\n    pick = []\n    while len(s_sort) > 0:\n        s_bboxes = np.concatenate([  # s_sort[-1] have highest prob score, s_sort[0:-1]->others\n            np.maximum(bboxes[:2, s_sort[-1], None], bboxes[:2, s_sort[0:-1]]),\n            np.minimum(bboxes[2:, s_sort[-1], None], bboxes[2:, s_sort[0:-1]])], axis=0)\n\n        inter = (np.maximum(0.0, s_bboxes[2] - s_bboxes[0] + 1) *\n                 np.maximum(0.0, s_bboxes[3] - s_bboxes[1] + 1))\n\n        if method == \"iom\":\n            var_o = inter / np.minimum(area[s_sort[-1]], area[s_sort[0:-1]])\n        else:\n            var_o = inter / (area[s_sort[-1]] + area[s_sort[0:-1]] - inter)\n\n        pick.append(s_sort[-1])\n        s_sort = s_sort[np.where(var_o <= threshold)[0]]\n\n    result_rectangle = rectangles[pick]\n    result_scores = scores[pick]\n    return result_rectangle, result_scores\n\n\ndef rect2square(rectangles: np.ndarray) -> np.ndarray:\n    \"\"\" change rectangles into squares (matrix version)\n\n    Parameters\n    ----------\n    rectangles: :class:`numpy.ndarray`\n        [b, x, y, x1, y1] rectangles\n\n    Return\n    ------\n    list\n        Original rectangle changed to a square\n    \"\"\"\n    width = rectangles[:, 2] - rectangles[:, 0]\n    height = rectangles[:, 3] - rectangles[:, 1]\n    length = np.maximum(width, height).T\n    rectangles[:, 0] = rectangles[:, 0] + width * 0.5 - length * 0.5\n    rectangles[:, 1] = rectangles[:, 1] + height * 0.5 - length * 0.5\n    rectangles[:, 2:4] = rectangles[:, 0:2] + np.repeat([length], 2, axis=0).T\n    return rectangles\n", "plugins/extract/detect/s3fd_defaults.py": "#!/usr/bin/env python3\n\"\"\"\n    The default options for the faceswap S3Fd Detect plugin.\n\n    Defaults files should be named <plugin_name>_defaults.py\n    Any items placed into this file will automatically get added to the relevant config .ini files\n    within the faceswap/config folder.\n\n    The following variables should be defined:\n        _HELPTEXT: A string describing what this plugin does\n        _DEFAULTS: A dictionary containing the options, defaults and meta information. The\n                   dictionary should be defined as:\n                       {<option_name>: {<metadata>}}\n\n                   <option_name> should always be lower text.\n                   <metadata> dictionary requirements are listed below.\n\n    The following keys are expected for the _DEFAULTS <metadata> dict:\n    datatype:  [required] A python type class. This limits the type of data that can be\n                provided in the .ini file and ensures that the value is returned in the\n                correct type to faceswap. Valid data types are: <class 'int'>, <class 'float'>,\n                <class 'str'>, <class 'bool'>.\n    default:   [required] The default value for this option.\n    info:      [required] A string describing what this option does.\n    group:     [optional]. A group for grouping options together in the GUI. If not\n                provided this will not group this option with any others.\n    choices:   [optional] If this option's datatype is of <class 'str'> then valid\n                selections can be defined here. This validates the option and also enables\n                a combobox / radio option in the GUI.\n    gui_radio: [optional] If <choices> are defined, this indicates that the GUI should use\n                radio buttons rather than a combobox to display this option.\n    min_max:   [partial] For <class 'int'> and <class 'float'> data types this is required\n                otherwise it is ignored. Should be a tuple of min and max accepted values.\n                This is used for controlling the GUI slider range. Values are not enforced.\n    rounding:  [partial] For <class 'int'> and <class 'float'> data types this is\n                required otherwise it is ignored. Used for the GUI slider. For floats, this\n                is the number of decimal places to display. For ints this is the step size.\n    fixed:     [optional] [train only]. Training configurations are fixed when the model is\n                created, and then reloaded from the state file. Marking an item as fixed=False\n                indicates that this value can be changed for existing models, and will override\n                the value saved in the state file with the updated value in config. If not\n                provided this will default to True.\n\"\"\"\n\n\n_HELPTEXT = (\n    \"S3FD Detector options.\\n\"\n    \"Fast on GPU, slow on CPU. Can detect more faces and fewer false positives than other GPU \"\n    \"detectors, but is a lot more resource intensive.\"\n    )\n\n\n_DEFAULTS = {\n    \"confidence\": {\n        \"default\": 70,\n        \"info\": \"The confidence level at which the detector has succesfully found a face.\\n\"\n                \"Higher levels will be more discriminating, lower levels will have more false \"\n                \"positives.\",\n        \"datatype\": int,\n        \"rounding\": 5,\n        \"min_max\": (25, 100),\n        \"choices\": [],\n        \"group\": \"settings\",\n        \"gui_radio\": False,\n        \"fixed\": True,\n    },\n    \"batch-size\": {\n        \"default\": 4,\n        \"info\": \"The batch size to use. To a point, higher batch sizes equal better performance, \"\n                \"but setting it too high can harm performance.\\n\"\n                \"\\n\\tNvidia users: If the batchsize is set higher than the your GPU can \"\n                \"accomodate then this will automatically be lowered.\"\n                \"\\n\\tAMD users: A batchsize of 8 requires about 2 GB vram.\",\n        \"datatype\": int,\n        \"rounding\": 1,\n        \"min_max\": (1, 64),\n        \"choices\": [],\n        \"group\": \"settings\",\n        \"gui_radio\": False,\n        \"fixed\": True,\n    }\n}\n", "plugins/extract/detect/cv2_dnn.py": "#!/usr/bin/env python3\n\"\"\" OpenCV DNN Face detection plugin \"\"\"\nimport logging\n\nimport numpy as np\n\nfrom ._base import BatchType, cv2, Detector, DetectorBatch\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass Detect(Detector):\n    \"\"\" CV2 DNN detector for face recognition \"\"\"\n    def __init__(self, **kwargs) -> None:\n        git_model_id = 4\n        model_filename = [\"resnet_ssd_v1.caffemodel\", \"resnet_ssd_v1.prototxt\"]\n        super().__init__(git_model_id=git_model_id, model_filename=model_filename, **kwargs)\n        self.name = \"cv2-DNN Detector\"\n        self.input_size = 300\n        self.vram = 0  # CPU Only. Doesn't use VRAM\n        self.vram_per_batch = 0\n        self.batchsize = 1\n        self.confidence = self.config[\"confidence\"] / 100\n\n    def init_model(self) -> None:\n        \"\"\" Initialize CV2 DNN Detector Model\"\"\"\n        assert isinstance(self.model_path, list)\n        self.model = cv2.dnn.readNetFromCaffe(self.model_path[1],\n                                              self.model_path[0])\n        self.model.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)\n\n    def process_input(self, batch: BatchType) -> None:\n        \"\"\" Compile the detection image(s) for prediction \"\"\"\n        assert isinstance(batch, DetectorBatch)\n        batch.feed = cv2.dnn.blobFromImages(batch.image,\n                                            scalefactor=1.0,\n                                            size=(self.input_size, self.input_size),\n                                            mean=[104, 117, 123],\n                                            swapRB=False,\n                                            crop=False)\n\n    def predict(self, feed: np.ndarray) -> np.ndarray:\n        \"\"\" Run model to get predictions \"\"\"\n        assert isinstance(self.model, cv2.dnn.Net)\n        self.model.setInput(feed)\n        predictions = self.model.forward()\n        return self.finalize_predictions(predictions)\n\n    def finalize_predictions(self, predictions: np.ndarray) -> np.ndarray:\n        \"\"\" Filter faces based on confidence level \"\"\"\n        faces = []\n        for i in range(predictions.shape[2]):\n            confidence = predictions[0, 0, i, 2]\n            if confidence >= self.confidence:\n                logger.trace(\"Accepting due to confidence %s >= %s\",  # type:ignore[attr-defined]\n                             confidence, self.confidence)\n                faces.append([(predictions[0, 0, i, 3] * self.input_size),\n                              (predictions[0, 0, i, 4] * self.input_size),\n                              (predictions[0, 0, i, 5] * self.input_size),\n                              (predictions[0, 0, i, 6] * self.input_size)])\n        logger.trace(\"faces: %s\", faces)  # type:ignore[attr-defined]\n        return np.array(faces)[None, ...]\n\n    def process_output(self, batch: BatchType) -> None:\n        \"\"\" Compile found faces for output \"\"\"\n        return\n", "plugins/extract/detect/external_defaults.py": "#!/usr/bin/env python3\n\"\"\"\n    The default options for the faceswap Import Alignments plugin.\n\n    Defaults files should be named <plugin_name>_defaults.py\n    Any items placed into this file will automatically get added to the relevant config .ini files\n    within the faceswap/config folder.\n\n    The following variables should be defined:\n        _HELPTEXT: A string describing what this plugin does\n        _DEFAULTS: A dictionary containing the options, defaults and meta information. The\n                   dictionary should be defined as:\n                       {<option_name>: {<metadata>}}\n\n                   <option_name> should always be lower text.\n                   <metadata> dictionary requirements are listed below.\n\n    The following keys are expected for the _DEFAULTS <metadata> dict:\n        datatype:  [required] A python type class. This limits the type of data that can be\n                   provided in the .ini file and ensures that the value is returned in the\n                   correct type to faceswap. Valid data types are: <class 'int'>, <class 'float'>,\n                   <class 'str'>, <class 'bool'>.\n        default:   [required] The default value for this option.\n        info:      [required] A string describing what this option does.\n        group:     [optional]. A group for grouping options together in the GUI. If not\n                   provided this will not group this option with any others.\n        choices:   [optional] If this option's datatype is of <class 'str'> then valid\n                   selections can be defined here. This validates the option and also enables\n                   a combobox / radio option in the GUI.\n        gui_radio: [optional] If <choices> are defined, this indicates that the GUI should use\n                   radio buttons rather than a combobox to display this option.\n        min_max:   [partial] For <class 'int'> and <class 'float'> data types this is required\n                   otherwise it is ignored. Should be a tuple of min and max accepted values.\n                   This is used for controlling the GUI slider range. Values are not enforced.\n        rounding:  [partial] For <class 'int'> and <class 'float'> data types this is\n                   required otherwise it is ignored. Used for the GUI slider. For floats, this\n                   is the number of decimal places to display. For ints this is the step size.\n        fixed:     [optional] [train only]. Training configurations are fixed when the model is\n                   created, and then reloaded from the state file. Marking an item as fixed=False\n                   indicates that this value can be changed for existing models, and will override\n                   the value saved in the state file with the updated value in config. If not\n                   provided this will default to True.\n\"\"\"\n\n\n_HELPTEXT = (\n    \"Import Detector options.\\n\"\n    \"Imports a detected face bounding box from an external .json file.\\n\"\n    )\n\n\n_DEFAULTS = {\n    \"file_name\": {\n        \"default\": \"import.json\",\n        \"info\": \"The import file should be stored in the same folder as the video (if extracting \"\n        \"from a video file) or inside the folder of images (if importing from a folder of images)\",\n        \"datatype\": str,\n        \"choices\": [],\n        \"group\": \"settings\",\n        \"gui_radio\": False,\n        \"fixed\": True,\n    },\n    \"origin\": {\n        \"default\": \"top-left\",\n        \"info\": \"The origin (0, 0) location of the co-ordinates system used. \"\n                \"\\n\\t top-left: The origin (0, 0) of the canvas is at the top left \"\n                \"corner.\"\n                \"\\n\\t bottom-left: The origin (0, 0) of the canvas is at the bottom \"\n                \"left corner.\"\n                \"\\n\\t top-right: The origin (0, 0) of the canvas is at the top right \"\n                \"corner.\"\n                \"\\n\\t bottom-right: The origin (0, 0) of the canvas is at the bottom \"\n                \"right corner.\",\n        \"datatype\": str,\n        \"choices\": [\"top-left\", \"bottom-left\", \"top-right\", \"bottom-right\"],\n        \"group\": \"output\",\n        \"gui_radio\": True\n    }\n}\n", "plugins/extract/detect/mtcnn_defaults.py": "#!/usr/bin/env python3\n\"\"\"\n    The default options for the faceswap Mtcnn Detect plugin.\n\n    Defaults files should be named <plugin_name>_defaults.py\n    Any items placed into this file will automatically get added to the relevant config .ini files\n    within the faceswap/config folder.\n\n    The following variables should be defined:\n        _HELPTEXT: A string describing what this plugin does\n        _DEFAULTS: A dictionary containing the options, defaults and meta information. The\n                   dictionary should be defined as:\n                       {<option_name>: {<metadata>}}\n\n                   <option_name> should always be lower text.\n                   <metadata> dictionary requirements are listed below.\n\n    The following keys are expected for the _DEFAULTS <metadata> dict:\n        datatype:  [required] A python type class. This limits the type of data that can be\n                   provided in the .ini file and ensures that the value is returned in the\n                   correct type to faceswap. Valid data types are: <class 'int'>, <class 'float'>,\n                   <class 'str'>, <class 'bool'>.\n        default:   [required] The default value for this option.\n        info:      [required] A string describing what this option does.\n        group:     [optional]. A group for grouping options together in the GUI. If not\n                   provided this will not group this option with any others.\n        choices:   [optional] If this option's datatype is of <class 'str'> then valid\n                   selections can be defined here. This validates the option and also enables\n                   a combobox / radio option in the GUI.\n        gui_radio: [optional] If <choices> are defined, this indicates that the GUI should use\n                   radio buttons rather than a combobox to display this option.\n        min_max:   [partial] For <class 'int'> and <class 'float'> data types this is required\n                   otherwise it is ignored. Should be a tuple of min and max accepted values.\n                   This is used for controlling the GUI slider range. Values are not enforced.\n        rounding:  [partial] For <class 'int'> and <class 'float'> data types this is\n                   required otherwise it is ignored. Used for the GUI slider. For floats, this\n                   is the number of decimal places to display. For ints this is the step size.\n        fixed:     [optional] [train only]. Training configurations are fixed when the model is\n                   created, and then reloaded from the state file. Marking an item as fixed=False\n                   indicates that this value can be changed for existing models, and will override\n                   the value saved in the state file with the updated value in config. If not\n                   provided this will default to True.\n\"\"\"\n\n\n_HELPTEXT = (\n    \"MTCNN Detector options.\\n\"\n    \"Fast on GPU, slow on CPU. Uses fewer resources than other GPU detectors but can often return \"\n    \"more false positives.\"\n)\n\n\n_DEFAULTS = {\n    \"minsize\": {\n        \"default\": 20,\n        \"info\": \"The minimum size of a face (in pixels) to be accepted as a positive match.\"\n                \"\\nLower values use significantly more VRAM and will detect more false positives.\",\n        \"datatype\": int,\n        \"rounding\": 10,\n        \"min_max\": (20, 1000),\n        \"choices\": [],\n        \"group\": \"settings\",\n        \"gui_radio\": False,\n        \"fixed\": True,\n    },\n    \"scalefactor\": {\n        \"default\": 0.709,\n        \"info\": \"The scale factor for the image pyramid.\",\n        \"datatype\": float,\n        \"rounding\": 3,\n        \"min_max\": (0.1, 0.9),\n        \"choices\": [],\n        \"group\": \"settings\",\n        \"gui_radio\": False,\n        \"fixed\": True,\n    },\n    \"batch-size\": {\n        \"default\": 8,\n        \"info\": \"The batch size to use. To a point, higher batch sizes equal better performance, \"\n                \"but setting it too high can harm performance.\\n\"\n                \"\\n\\tNvidia users: If the batchsize is set higher than the your GPU can \"\n                \"accomodate then this will automatically be lowered.\",\n        \"datatype\": int,\n        \"rounding\": 1,\n        \"min_max\": (1, 64),\n        \"choices\": [],\n        \"group\": \"settings\",\n        \"gui_radio\": False,\n        \"fixed\": True,\n    },\n    \"cpu\": {\n        \"default\": True,\n        \"info\": \"MTCNN detector still runs fairly quickly on CPU on some setups. \"\n                \"Enable CPU mode here to use the CPU for this detector to save some VRAM at a \"\n                \"speed cost.\",\n        \"datatype\": bool,\n        \"group\": \"settings\"\n    },\n    \"threshold_1\": {\n        \"default\": 0.6,\n        \"info\": \"First stage threshold for face detection. This stage obtains face candidates.\",\n        \"datatype\": float,\n        \"rounding\": 2,\n        \"min_max\": (0.1, 0.9),\n        \"choices\": [],\n        \"group\": \"threshold\",\n        \"gui_radio\": False,\n        \"fixed\": True,\n    },\n    \"threshold_2\": {\n        \"default\": 0.7,\n        \"info\": \"Second stage threshold for face detection. This stage refines face candidates.\",\n        \"datatype\": float,\n        \"rounding\": 2,\n        \"min_max\": (0.1, 0.9),\n        \"choices\": [],\n        \"group\": \"threshold\",\n        \"gui_radio\": False,\n        \"fixed\": True,\n    },\n    \"threshold_3\": {\n        \"default\": 0.7,\n        \"info\": \"Third stage threshold for face detection. This stage further refines face \"\n                \"candidates.\",\n        \"datatype\": float,\n        \"rounding\": 2,\n        \"min_max\": (0.1, 0.9),\n        \"choices\": [],\n        \"group\": \"threshold\",\n        \"gui_radio\": False,\n        \"fixed\": True,\n    },\n}\n", "plugins/extract/detect/_base.py": "#!/usr/bin/env python3\n\"\"\" Base class for Face Detector plugins\n\nAll Detector Plugins should inherit from this class.\nSee the override methods for which methods are required.\n\nThe plugin will receive a :class:`~plugins.extract.extract_media.ExtractMedia` object.\n\nFor each source frame, the plugin must pass a dict to finalize containing:\n\n>>> {'filename': <filename of source frame>,\n>>>  'detected_faces': <list of DetectedFace objects containing bounding box points}}\n\nTo get a :class:`~lib.align.DetectedFace` object use the function:\n\n>>> face = self._to_detected_face(<face left>, <face top>, <face right>, <face bottom>)\n\"\"\"\nfrom __future__ import annotations\nimport logging\nimport typing as T\n\nfrom dataclasses import dataclass, field\n\nimport cv2\nimport numpy as np\n\nfrom tensorflow.python.framework import errors_impl as tf_errors  # pylint:disable=no-name-in-module # noqa\n\nfrom lib.align import DetectedFace\nfrom lib.utils import FaceswapError\n\nfrom plugins.extract._base import BatchType, Extractor, ExtractorBatch\nfrom plugins.extract import ExtractMedia\n\nif T.TYPE_CHECKING:\n    from collections.abc import Generator\n    from queue import Queue\n\nlogger = logging.getLogger(__name__)\n\n\n@dataclass\nclass DetectorBatch(ExtractorBatch):\n    \"\"\" Dataclass for holding items flowing through the aligner.\n\n    Inherits from :class:`~plugins.extract._base.ExtractorBatch`\n\n    Parameters\n    ----------\n    rotation_matrix: :class:`numpy.ndarray`\n        The rotation matrix for any requested rotations\n    scale: float\n        The scaling factor to take the input image back to original size\n    pad: tuple\n        The amount of padding to apply to the image to feed the network\n    initial_feed: :class:`numpy.ndarray`\n        Used to hold the initial :attr:`feed` when rotate images is enabled\n    \"\"\"\n    detected_faces: list[list[\"DetectedFace\"]] = field(default_factory=list)\n    rotation_matrix: list[np.ndarray] = field(default_factory=list)\n    scale: list[float] = field(default_factory=list)\n    pad: list[tuple[int, int]] = field(default_factory=list)\n    initial_feed: np.ndarray = np.array([])\n\n    def __repr__(self):\n        \"\"\" Prettier repr for debug printing \"\"\"\n        retval = super().__repr__()\n        retval += (f\", rotation_matrix={self.rotation_matrix}, \"\n                   f\"scale={self.scale}, \"\n                   f\"pad={self.pad}, \"\n                   f\"initial_feed=({self.initial_feed.shape}, {self.initial_feed.dtype})\")\n        return retval\n\n\nclass Detector(Extractor):  # pylint:disable=abstract-method\n    \"\"\" Detector Object\n\n    Parent class for all Detector plugins\n\n    Parameters\n    ----------\n    git_model_id: int\n        The second digit in the github tag that identifies this model. See\n        https://github.com/deepfakes-models/faceswap-models for more information\n    model_filename: str\n        The name of the model file to be loaded\n    rotation: str, optional\n        Pass in a single number to use increments of that size up to 360, or pass in a ``list`` of\n        ``ints`` to enumerate exactly what angles to check. Can also pass in ``'on'`` to increment\n        at 90 degree intervals. Default: ``None``\n    min_size: int, optional\n        Filters out faces detected below this size. Length, in pixels across the diagonal of the\n        bounding box. Set to ``0`` for off. Default: ``0``\n\n    Other Parameters\n    ----------------\n    configfile: str, optional\n        Path to a custom configuration ``ini`` file. Default: Use system configfile\n\n    See Also\n    --------\n    plugins.extract.pipeline : The extraction pipeline for calling plugins\n    plugins.extract.detect : Detector plugins\n    plugins.extract._base : Parent class for all extraction plugins\n    plugins.extract.align._base : Aligner parent class for extraction plugins.\n    plugins.extract.mask._base : Masker parent class for extraction plugins.\n    \"\"\"\n\n    def __init__(self,\n                 git_model_id: int | None = None,\n                 model_filename: str | list[str] | None = None,\n                 configfile: str | None = None,\n                 instance: int = 0,\n                 rotation: str | None = None,\n                 min_size: int = 0,\n                 **kwargs) -> None:\n        logger.debug(\"Initializing %s: (rotation: %s, min_size: %s)\", self.__class__.__name__,\n                     rotation, min_size)\n        super().__init__(git_model_id,\n                         model_filename,\n                         configfile=configfile,\n                         instance=instance,\n                         **kwargs)\n        self.rotation = self._get_rotation_angles(rotation)\n        self.min_size = min_size\n\n        self._plugin_type = \"detect\"\n\n        logger.debug(\"Initialized _base %s\", self.__class__.__name__)\n\n    # <<< QUEUE METHODS >>> #\n    def get_batch(self, queue: Queue) -> tuple[bool, DetectorBatch]:\n        \"\"\" Get items for inputting to the detector plugin in batches\n\n        Items are received as :class:`~plugins.extract.extract_media.ExtractMedia` objects and\n        converted to ``dict`` for internal processing.\n\n        Items are returned from the ``queue`` in batches of\n        :attr:`~plugins.extract._base.Extractor.batchsize`\n\n        Remember to put ``'EOF'`` to the out queue after processing\n        the final batch\n\n        Outputs items in the following format. All lists are of length\n        :attr:`~plugins.extract._base.Extractor.batchsize`:\n\n        >>> {'filename': [<filenames of source frames>],\n        >>>  'image': <numpy.ndarray of images standardized for prediction>,\n        >>>  'scale': [<scaling factors for each image>],\n        >>>  'pad': [<padding for each image>],\n        >>>  'detected_faces': [[<lib.align.DetectedFace objects]]}\n\n        Parameters\n        ----------\n        queue : queue.Queue()\n            The ``queue`` that the batch will be fed from. This will be a queue that loads\n            images.\n\n        Returns\n        -------\n        exhausted, bool\n            ``True`` if queue is exhausted, ``False`` if not.\n        batch, :class:`~plugins.extract._base.ExtractorBatch`\n            The batch object for the current batch\n        \"\"\"\n        exhausted = False\n        batch = DetectorBatch()\n        for _ in range(self.batchsize):\n            item = self._get_item(queue)\n            if item == \"EOF\":\n                exhausted = True\n                break\n            assert isinstance(item, ExtractMedia)\n            # Put items that are already aligned into the out queue\n            if item.is_aligned:\n                self._queues[\"out\"].put(item)\n                continue\n            batch.filename.append(item.filename)\n            image, scale, pad = self._compile_detection_image(item)\n            batch.image.append(image)\n            batch.scale.append(scale)\n            batch.pad.append(pad)\n\n        if batch.filename:\n            logger.trace(\"Returning batch: %s\",  # type: ignore\n                         {k: len(v) if isinstance(v, (list, np.ndarray)) else v\n                          for k, v in batch.__dict__.items()})\n        else:\n            logger.trace(item)  # type:ignore[attr-defined]\n\n        if not exhausted and not batch.filename:\n            # This occurs when face filter is fed aligned faces.\n            # Need to re-run until EOF is hit\n            return self.get_batch(queue)\n\n        return exhausted, batch\n\n    # <<< FINALIZE METHODS>>> #\n    def finalize(self, batch: BatchType) -> Generator[ExtractMedia, None, None]:\n        \"\"\" Finalize the output from Detector\n\n        This should be called as the final task of each ``plugin``.\n\n        Parameters\n        ----------\n        batch : :class:`~plugins.extract._base.ExtractorBatch`\n            The batch object for the current batch\n\n        Yields\n        ------\n        :class:`~plugins.extract.extract_media.ExtractMedia`\n            The :attr:`DetectedFaces` list will be populated for this class with the bounding boxes\n            for the detected faces found in the frame.\n        \"\"\"\n        assert isinstance(batch, DetectorBatch)\n        logger.trace(\"Item out: %s\",  # type:ignore[attr-defined]\n                     {k: len(v) if isinstance(v, (list, np.ndarray)) else v\n                      for k, v in batch.__dict__.items()})\n\n        batch_faces = [[self._to_detected_face(face[0], face[1], face[2], face[3])\n                        for face in faces]\n                       for faces in batch.prediction]\n        # Rotations\n        if any(m.any() for m in batch.rotation_matrix) and any(batch_faces):\n            batch_faces = [[self._rotate_face(face, rotmat) if rotmat.any() else face\n                            for face in faces]\n                           for faces, rotmat in zip(batch_faces, batch.rotation_matrix)]\n\n        # Remove zero sized faces\n        batch_faces = self._remove_zero_sized_faces(batch_faces)\n\n        # Scale back out to original frame\n        batch.detected_faces = [[self._to_detected_face((face.left - pad[0]) / scale,\n                                                        (face.top - pad[1]) / scale,\n                                                        (face.right - pad[0]) / scale,\n                                                        (face.bottom - pad[1]) / scale)\n                                 for face in faces\n                                 if face.left is not None and face.top is not None]\n                                for scale, pad, faces in zip(batch.scale,\n                                                             batch.pad,\n                                                             batch_faces)]\n\n        if self.min_size > 0 and batch.detected_faces:\n            batch.detected_faces = self._filter_small_faces(batch.detected_faces)\n\n        for idx, filename in enumerate(batch.filename):\n            output = self._extract_media.pop(filename)\n            output.add_detected_faces(batch.detected_faces[idx])\n\n            logger.trace(\"final output: (filename: '%s', \"  # type:ignore[attr-defined]\n                         \"image shape: %s, detected_faces: %s, item: %s\",\n                         output.filename, output.image_shape, output.detected_faces, output)\n            yield output\n\n    @staticmethod\n    def _to_detected_face(left: float, top: float, right: float, bottom: float) -> DetectedFace:\n        \"\"\" Convert a bounding box to a detected face object\n\n        Parameters\n        ----------\n        left: float\n            The left point of the detection bounding box\n        top: float\n            The top point of the detection bounding box\n        right: float\n            The right point of the detection bounding box\n        bottom: float\n            The bottom point of the detection bounding box\n\n        Returns\n        -------\n        class:`~lib.align.DetectedFace`\n            The detected face object for the given bounding box\n        \"\"\"\n        return DetectedFace(left=int(round(left)),\n                            width=int(round(right - left)),\n                            top=int(round(top)),\n                            height=int(round(bottom - top)))\n\n    # <<< PROTECTED ACCESS METHODS >>> #\n    # <<< PREDICT WRAPPER >>> #\n    def _predict(self, batch: BatchType) -> DetectorBatch:\n        \"\"\" Wrap models predict function in rotations \"\"\"\n        assert isinstance(batch, DetectorBatch)\n        batch.rotation_matrix = [np.array([]) for _ in range(len(batch.feed))]\n        found_faces: list[np.ndarray] = [np.array([]) for _ in range(len(batch.feed))]\n        for angle in self.rotation:\n            # Rotate the batch and insert placeholders for already found faces\n            self._rotate_batch(batch, angle)\n            try:\n                pred = self.predict(batch.feed)\n                if angle == 0:\n                    batch.prediction = pred\n                else:\n                    try:\n                        batch.prediction = np.array([b if b.any() else p\n                                                    for b, p in zip(batch.prediction, pred)])\n                    except ValueError as err:\n                        # If batches are different sizes after rotation Numpy will error, so we\n                        # need to explicitly set the dtype to 'object' rather than let it infer\n                        # numpy error:\n                        # ValueError: setting an array element with a sequence. The requested array\n                        # has an inhomogeneous shape after 1 dimensions. The detected shape was\n                        # (8,) + inhomogeneous part\n                        if \"inhomogeneous\" in str(err):\n                            batch.prediction = np.array([b if b.any() else p\n                                                         for b, p in zip(batch.prediction, pred)],\n                                                        dtype=\"object\")\n                            logger.trace(  # type:ignore[attr-defined]\n                                \"Mismatched array sizes, setting dtype to object: %s\",\n                                [p.shape for p in batch.prediction])\n                        else:\n                            raise\n\n                logger.trace(\"angle: %s, filenames: %s, \"  # type:ignore[attr-defined]\n                             \"prediction: %s\",\n                             angle, batch.filename, pred)\n            except tf_errors.ResourceExhaustedError as err:\n                msg = (\"You do not have enough GPU memory available to run detection at the \"\n                       \"selected batch size. You can try a number of things:\"\n                       \"\\n1) Close any other application that is using your GPU (web browsers are \"\n                       \"particularly bad for this).\"\n                       \"\\n2) Lower the batchsize (the amount of images fed into the model) by \"\n                       \"editing the plugin settings (GUI: Settings > Configure extract settings, \"\n                       \"CLI: Edit the file faceswap/config/extract.ini).\"\n                       \"\\n3) Enable 'Single Process' mode.\")\n                raise FaceswapError(msg) from err\n\n            if angle != 0 and any(face.any() for face in batch.prediction):\n                logger.verbose(\"found face(s) by rotating image %s \"  # type:ignore[attr-defined]\n                               \"degrees\",\n                               angle)\n\n            found_faces = T.cast(list[np.ndarray], ([face if not found.any() else found\n                                                     for face, found in zip(batch.prediction,\n                                                                            found_faces)]))\n            if all(face.any() for face in found_faces):\n                logger.trace(\"Faces found for all images\")  # type:ignore[attr-defined]\n                break\n\n        batch.prediction = np.array(found_faces, dtype=\"object\")\n        logger.trace(\"detect_prediction output: (filenames: %s, \"  # type:ignore[attr-defined]\n                     \"prediction: %s, rotmat: %s)\",\n                     batch.filename, batch.prediction, batch.rotation_matrix)\n        return batch\n\n    # <<< DETECTION IMAGE COMPILATION METHODS >>> #\n    def _compile_detection_image(self, item: ExtractMedia\n                                 ) -> tuple[np.ndarray, float, tuple[int, int]]:\n        \"\"\" Compile the detection image for feeding into the model\n\n        Parameters\n        ----------\n        item: :class:`~plugins.extract.extract_media.ExtractMedia`\n            The input item from the pipeline\n\n        Returns\n        -------\n        image: :class:`numpy.ndarray`\n            The original image formatted for detection\n        scale: float\n            The scaling factor for the image\n        pad: int\n            The amount of padding applied to the image\n        \"\"\"\n        image = item.get_image_copy(self.color_format)\n        scale = self._set_scale(item.image_size)\n        pad = self._set_padding(item.image_size, scale)\n\n        image = self._scale_image(image, item.image_size, scale)\n        image = self._pad_image(image)\n        logger.trace(\"compiled: (images shape: %s, \"  # type:ignore[attr-defined]\n                     \"scale: %s, pad: %s)\",\n                     image.shape, scale, pad)\n        return image, scale, pad\n\n    def _set_scale(self, image_size: tuple[int, int]) -> float:\n        \"\"\" Set the scale factor for incoming image\n\n        Parameters\n        ----------\n        image_size: tuple\n            The (height, width) of the original image\n\n        Returns\n        -------\n        float\n            The scaling factor from original image size to model input size\n        \"\"\"\n        scale = self.input_size / max(image_size)\n        logger.trace(\"Detector scale: %s\", scale)  # type:ignore[attr-defined]\n        return scale\n\n    def _set_padding(self, image_size: tuple[int, int], scale: float) -> tuple[int, int]:\n        \"\"\" Set the image padding for non-square images\n\n        Parameters\n        ----------\n        image_size: tuple\n            The (height, width) of the original image\n        scale: float\n            The scaling factor from original image size to model input size\n\n        Returns\n        -------\n        tuple\n            The amount of padding to apply to the x and y axes\n        \"\"\"\n        pad_left = int(self.input_size - int(image_size[1] * scale)) // 2\n        pad_top = int(self.input_size - int(image_size[0] * scale)) // 2\n        return pad_left, pad_top\n\n    @staticmethod\n    def _scale_image(image: np.ndarray, image_size: tuple[int, int], scale: float) -> np.ndarray:\n        \"\"\" Scale the image and optional pad to given size\n\n        Parameters\n        ----------\n        image: :class:`numpy.ndarray`\n            The image to be scalued\n        image_size: tuple\n            The image (height, width)\n        scale: float\n            The scaling factor to apply to the image\n\n        Returns\n        -------\n        :class:`numpy.ndarray`\n            The scaled image\n        \"\"\"\n        interpln = cv2.INTER_CUBIC if scale > 1.0 else cv2.INTER_AREA\n        if scale != 1.0:\n            dims = (int(image_size[1] * scale), int(image_size[0] * scale))\n            logger.trace(\"Resizing detection image from %s to %s. \"  # type:ignore[attr-defined]\n                         \"Scale=%s\",\n                         \"x\".join(str(i) for i in reversed(image_size)),\n                         \"x\".join(str(i) for i in dims), scale)\n            image = cv2.resize(image, dims, interpolation=interpln)\n        logger.trace(\"Resized image shape: %s\", image.shape)  # type:ignore[attr-defined]\n        return image\n\n    def _pad_image(self, image: np.ndarray) -> np.ndarray:\n        \"\"\" Pad a resized image to input size\n\n        Parameters\n        ----------\n        image: :class:`numpy.ndarray`\n            The image to have padding applied\n\n        Returns\n        -------\n        :class:`numpy.ndarray`\n            The image with padding applied\n        \"\"\"\n        height, width = image.shape[:2]\n        if width < self.input_size or height < self.input_size:\n            pad_l = (self.input_size - width) // 2\n            pad_r = (self.input_size - width) - pad_l\n            pad_t = (self.input_size - height) // 2\n            pad_b = (self.input_size - height) - pad_t\n            image = cv2.copyMakeBorder(image,\n                                       pad_t,\n                                       pad_b,\n                                       pad_l,\n                                       pad_r,\n                                       cv2.BORDER_CONSTANT)\n        logger.trace(\"Padded image shape: %s\", image.shape)  # type:ignore[attr-defined]\n        return image\n\n    # <<< FINALIZE METHODS >>> #\n    def _remove_zero_sized_faces(self, batch_faces: list[list[DetectedFace]]\n                                 ) -> list[list[DetectedFace]]:\n        \"\"\" Remove items from batch_faces where detected face is of zero size or face falls\n        entirely outside of image\n\n        Parameters\n        ----------\n        batch_faces: list\n            List of detected face objects\n\n        Returns\n        -------\n        list\n            List of detected face objects with filtered out faces removed\n        \"\"\"\n        logger.trace(\"Input sizes: %s\", [len(face) for face in batch_faces])  # type: ignore\n        retval = [[face\n                   for face in faces\n                   if face.right > 0 and face.left is not None and face.left < self.input_size\n                   and face.bottom > 0 and face.top is not None and face.top < self.input_size]\n                  for faces in batch_faces]\n        logger.trace(\"Output sizes: %s\", [len(face) for face in retval])  # type: ignore\n        return retval\n\n    def _filter_small_faces(self, detected_faces: list[list[DetectedFace]]\n                            ) -> list[list[DetectedFace]]:\n        \"\"\" Filter out any faces smaller than the min size threshold\n\n        Parameters\n        ----------\n        detected_faces: list\n            List of detected face objects\n\n        Returns\n        -------\n        list\n            List of detected face objects with filtered out faces removed\n        \"\"\"\n        retval = []\n        for faces in detected_faces:\n            this_image = []\n            for face in faces:\n                assert face.width is not None and face.height is not None\n                face_size = (face.width ** 2 + face.height ** 2) ** 0.5\n                if face_size < self.min_size:\n                    logger.debug(\"Removing detected face: (face_size: %s, min_size: %s\",\n                                 face_size, self.min_size)\n                    continue\n                this_image.append(face)\n            retval.append(this_image)\n        return retval\n\n    # <<< IMAGE ROTATION METHODS >>> #\n    @staticmethod\n    def _get_rotation_angles(rotation: str | None) -> list[int]:\n        \"\"\" Set the rotation angles.\n\n        Parameters\n        ----------\n        str\n            List of requested rotation angles\n\n        Returns\n        -------\n        list\n            The complete list of rotation angles to apply\n        \"\"\"\n        rotation_angles = [0]\n\n        if not rotation:\n            logger.debug(\"Not setting rotation angles\")\n            return rotation_angles\n\n        passed_angles = [int(angle)\n                         for angle in rotation.split(\",\")\n                         if int(angle) != 0]\n        if len(passed_angles) == 1:\n            rotation_step_size = passed_angles[0]\n            rotation_angles.extend(range(rotation_step_size,\n                                         360,\n                                         rotation_step_size))\n        elif len(passed_angles) > 1:\n            rotation_angles.extend(passed_angles)\n\n        logger.debug(\"Rotation Angles: %s\", rotation_angles)\n        return rotation_angles\n\n    def _rotate_batch(self, batch: DetectorBatch, angle: int) -> None:\n        \"\"\" Rotate images in a batch by given angle\n\n            if any faces have already been detected for a batch, store the existing rotation\n            matrix and replace the feed image with a placeholder\n\n            Parameters\n            ----------\n            batch: :class:`DetectorBatch`\n                The batch to apply rotation to\n            angle: int\n                The amount of degrees to rotate the image by\n            \"\"\"\n        if angle == 0:\n            # Set the initial batch so we always rotate from zero\n            batch.initial_feed = batch.feed.copy()\n            return\n\n        feeds: list[np.ndarray] = []\n        rotmats: list[np.ndarray] = []\n        for img, faces, rotmat in zip(batch.initial_feed,\n                                      batch.prediction,\n                                      batch.rotation_matrix):\n            if faces.any():\n                image = np.zeros_like(img)\n                matrix = rotmat\n            else:\n                image, matrix = self._rotate_image_by_angle(img, angle)\n            feeds.append(image)\n            rotmats.append(matrix)\n        batch.feed = np.array(feeds, dtype=\"float32\")\n        batch.rotation_matrix = rotmats\n\n    @staticmethod\n    def _rotate_face(face: DetectedFace, rotation_matrix: np.ndarray) -> DetectedFace:\n        \"\"\" Rotates the detection bounding box around the given rotation matrix.\n\n        Parameters\n        ----------\n        face: :class:`DetectedFace`\n            A :class:`DetectedFace` containing the `x`, `w`, `y`, `h` detection bounding box\n            points.\n        rotation_matrix: numpy.ndarray\n            The rotation matrix to rotate the given object by.\n\n        Returns\n        -------\n        :class:`DetectedFace`\n            The same class with the detection bounding box points rotated by the given matrix.\n        \"\"\"\n        logger.trace(\"Rotating face: (face: %s, rotation_matrix: %s)\",  # type: ignore\n                     face, rotation_matrix)\n        bounding_box = [[face.left, face.top],\n                        [face.right, face.top],\n                        [face.right, face.bottom],\n                        [face.left, face.bottom]]\n        rotation_matrix = cv2.invertAffineTransform(rotation_matrix)\n\n        points = np.array(bounding_box, \"int32\")\n        points = np.expand_dims(points, axis=0)\n        transformed = cv2.transform(points, rotation_matrix).astype(\"int32\")\n        rotated = transformed.squeeze()\n\n        # Bounding box should follow x, y planes, so get min/max for non-90 degree rotations\n        pt_x = min(pnt[0] for pnt in rotated)\n        pt_y = min(pnt[1] for pnt in rotated)\n        pt_x1 = max(pnt[0] for pnt in rotated)\n        pt_y1 = max(pnt[1] for pnt in rotated)\n        width = pt_x1 - pt_x\n        height = pt_y1 - pt_y\n\n        face.left = int(pt_x)\n        face.top = int(pt_y)\n        face.width = int(width)\n        face.height = int(height)\n        return face\n\n    def _rotate_image_by_angle(self,\n                               image: np.ndarray,\n                               angle: int) -> tuple[np.ndarray, np.ndarray]:\n        \"\"\" Rotate an image by a given angle.\n\n        Parameters\n        ----------\n        image: :class:`numpy.ndarray`\n            The image to be rotated\n        angle: int\n            The angle, in degrees, to rotate the image by\n\n        Returns\n        -------\n        image: :class:`numpy.ndarray`\n            The rotated image\n        rotation_matrix: :class:`numpy.ndarray`\n            The rotation matrix used to rotate the image\n\n        Reference\n        ---------\n        https://stackoverflow.com/questions/22041699\n        \"\"\"\n\n        logger.trace(\"Rotating image: (image: %s, angle: %s)\",  # type:ignore[attr-defined]\n                     image.shape, angle)\n        channels_first = image.shape[0] <= 4\n        if channels_first:\n            image = np.moveaxis(image, 0, 2)\n\n        height, width = image.shape[:2]\n        image_center = (width/2, height/2)\n        rotation_matrix = cv2.getRotationMatrix2D(image_center, -1.*angle, 1.)\n        rotation_matrix[0, 2] += self.input_size / 2 - image_center[0]\n        rotation_matrix[1, 2] += self.input_size / 2 - image_center[1]\n        logger.trace(\"Rotated image: (rotation_matrix: %s\",  # type:ignore[attr-defined]\n                     rotation_matrix)\n        image = cv2.warpAffine(image, rotation_matrix, (self.input_size, self.input_size))\n        if channels_first:\n            image = np.moveaxis(image, 2, 0)\n\n        return image, rotation_matrix\n", "plugins/extract/detect/external.py": "#!/usr/bin/env python3\n\"\"\" Import face detection ROI boxes from a json file \"\"\"\nfrom __future__ import annotations\n\nimport logging\nimport os\nimport re\nimport typing as T\n\nimport numpy as np\n\nfrom lib.align import AlignedFace\nfrom lib.utils import FaceswapError, IMAGE_EXTENSIONS\n\nfrom ._base import Detector\n\nif T.TYPE_CHECKING:\n    from lib.align import DetectedFace\n    from plugins.extract import ExtractMedia\n    from ._base import BatchType\n\nlogger = logging.getLogger(__name__)\n\n\nclass Detect(Detector):\n    \"\"\" Import face detection bounding boxes from an external json file \"\"\"\n    def __init__(self, **kwargs) -> None:\n        kwargs[\"rotation\"] = None  # Disable rotation\n        kwargs[\"min_size\"] = 0  # Disable min_size\n        super().__init__(git_model_id=None, model_filename=None, **kwargs)\n\n        self.name = \"External\"\n        self.batchsize = 16\n\n        self._origin: T.Literal[\"top-left\",\n                                \"bottom-left\",\n                                \"top-right\",\n                                \"bottom-right\"] = self.config[\"origin\"]\n\n        self._re_frame_no: re.Pattern = re.compile(r\"\\d+$\")\n        self._missing: list[str] = []\n        self._log_once = True\n        self._is_video = False\n        self._imported: dict[str | int, np.ndarray] = {}\n        \"\"\"dict[str | int, np.ndarray]: The imported data from external .json file\"\"\"\n\n    def init_model(self) -> None:\n        \"\"\" No initialization to perform \"\"\"\n        logger.debug(\"No detector model to initialize\")\n\n    def _compile_detection_image(self, item: ExtractMedia\n                                 ) -> tuple[np.ndarray, float, tuple[int, int]]:\n        \"\"\" Override _compile_detection_image method, to obtain the source frame dimensions\n\n        Parameters\n        ----------\n        item: :class:`~plugins.extract.extract_media.ExtractMedia`\n            The input item from the pipeline\n\n        Returns\n        -------\n        image: :class:`numpy.ndarray`\n            dummy empty array\n        scale: float\n            The scaling factor for the image (1.0)\n        pad: int\n            The amount of padding applied to the image (0, 0)\n        \"\"\"\n        return np.array(item.image_shape[:2], dtype=\"int64\"), 1.0, (0, 0)\n\n    def _check_for_video(self, filename: str) -> None:\n        \"\"\" Check a sample filename from the import file for a file extension to set\n        :attr:`_is_video`\n\n        Parameters\n        ----------\n        filename: str\n            A sample file name from the imported data\n        \"\"\"\n        logger.debug(\"Checking for video from '%s'\", filename)\n        ext = os.path.splitext(filename)[-1]\n        if ext.lower() not in IMAGE_EXTENSIONS:\n            self._is_video = True\n        logger.debug(\"Set is_video to %s from extension '%s'\", self._is_video, ext)\n\n    def _get_key(self, key: str) -> str | int:\n        \"\"\" Obtain the key for the item in the lookup table. If the input are images, the key will\n        be the image filename. If the input is a video, the key will be the frame number\n\n        Parameters\n        ----------\n        key: str\n            The initial key value from import data or an import image/frame\n\n        Returns\n        -------\n        str | int\n            The filename is the input data is images, otherwise the frame number of a video\n        \"\"\"\n        if not self._is_video:\n            return key\n        original_name = os.path.splitext(key)[0]\n        matches = self._re_frame_no.findall(original_name)\n        if not matches or len(matches) > 1:\n            raise FaceswapError(f\"Invalid import name: '{key}'. For video files, the key should \"\n                                \"end with the frame number.\")\n        retval = int(matches[0])\n        logger.trace(\"Obtained frame number %s from key '%s'\",  # type:ignore[attr-defined]\n                     retval, key)\n        return retval\n\n    @classmethod\n    def _bbox_from_detected(cls, bounding_box: list[int]) -> np.ndarray:\n        \"\"\" Import the detected face roi from a `detected` item in the import file\n\n        Parameters\n        ----------\n        bounding_box: list[int]\n            a bounding box contained within the import file\n\n        Returns\n        -------\n        :class:`numpy.ndarray`\n            The \"left\", \"top\", \"right\", \"bottom\" bounding box for the face\n\n        Raises\n        ------\n        FaceSwapError\n            If the number of bounding box co-ordinates is incorrect\n        \"\"\"\n        if len(bounding_box) != 4:\n            raise FaceswapError(\"Imported 'detected' bounding boxes should be a list of 4 numbers \"\n                                \"representing the 'left', 'top', 'right', `bottom` of a face.\")\n        return np.rint(bounding_box)\n\n    def _validate_landmarks(self, landmarks: list[list[float]]) -> np.ndarray:\n        \"\"\" Validate that the there are 4 or 68 landmarks and are a complete list of (x, y)\n        co-ordinates\n\n        Parameters\n        ----------\n        landmarks: list[float]\n            The 4 point ROI or 68 point 2D landmarks that are being imported\n\n        Returns\n        -------\n        :class:`numpy.ndarray`\n            The original landmarks as a numpy array\n\n        Raises\n        ------\n        FaceSwapError\n            If the landmarks being imported are not correct\n        \"\"\"\n        if len(landmarks) not in (4, 68):\n            raise FaceswapError(\"Imported 'landmarks_2d' should be either 68 facial feature \"\n                                \"landmarks or 4 ROI corner locations\")\n        retval = np.array(landmarks, dtype=\"float32\")\n        if retval.shape[-1] != 2:\n            raise FaceswapError(\"Imported 'landmarks_2d' should be formatted as a list of (x, y) \"\n                                \"co-ordinates\")\n        return retval\n\n    def _bbox_from_landmarks2d(self, landmarks: list[list[float]]) -> np.ndarray:\n        \"\"\" Import the detected face roi by estimating from imported landmarks\n\n        Parameters\n        ----------\n        landmarks: list[float]\n            The 4 point ROI or 68 point 2D landmarks that are being imported\n\n        Returns\n        -------\n        :class:`numpy.ndarray`\n            The \"left\", \"top\", \"right\", \"bottom\" bounding box for the face\n        \"\"\"\n        n_landmarks = self._validate_landmarks(landmarks)\n        face = AlignedFace(n_landmarks, centering=\"legacy\", coverage_ratio=0.75)\n        return np.concatenate([np.min(face.original_roi, axis=0),\n                               np.max(face.original_roi, axis=0)])\n\n    def _import_frame_face(self,\n                           face: dict[str, list[int] | list[list[float]]],\n                           align_origin: T.Literal[\"top-left\",\n                                                   \"bottom-left\",\n                                                   \"top-right\",\n                                                   \"bottom-right\"] | None) -> np.ndarray:\n        \"\"\" Import a detected face ROI from the import file\n\n        Parameters\n        ----------\n        face: dict[str, list[int] | list[list[float]]]\n            The data that exists within the import file for the frame\n        align_origin: Literal[\"top-left\", \"bottom-left\", \"top-right\", \"bottom-right\"] | None\n            The origin of the imported aligner data. Used if the detected ROI is being estimated\n            from imported aligner data\n\n        Returns\n        -------\n        :class:`numpy.ndarray`\n            The \"left\", \"top\", \"right\", \"bottom\" bounding box for the face\n\n        Raises\n        ------\n        FaceSwapError\n            If the required keys for the bounding boxes are not present for the face\n        \"\"\"\n        if \"detected\" in face:\n            return self._bbox_from_detected(T.cast(list[int], face[\"detected\"]))\n        if \"landmarks_2d\" in face:\n            if self._log_once and align_origin is None:\n                logger.warning(\"You are importing Detection data, but have only provided \"\n                               \"Alignment data. This is most likely incorrect and will lead \"\n                               \"to poor results\")\n                self._log_once = False\n\n            if self._log_once and align_origin is not None and align_origin != self._origin:\n                logger.info(\"Updating Detect origin from Aligner config to '%s'\", align_origin)\n                self._origin = align_origin\n                self._log_once = False\n\n            return self._bbox_from_landmarks2d(T.cast(list[list[float]], face[\"landmarks_2d\"]))\n\n        raise FaceswapError(\"The provided import file is missing both of the required keys \"\n                            \"'detected' and 'landmarks_2d\")\n\n    def import_data(self,\n                    data: dict[str, list[dict[str, list[int] | list[list[float]]]]],\n                    align_origin: T.Literal[\"top-left\",\n                                            \"bottom-left\",\n                                            \"top-right\",\n                                            \"bottom-right\"] | None) -> None:\n        \"\"\" Import the detection data from the json import file and set to :attr:`_imported`\n\n        Parameters\n        ----------\n        data: dict[str, list[dict[str, list[int] | list[list[float]]]]]\n            The data to be imported\n        align_origin: Literal[\"top-left\", \"bottom-left\", \"top-right\", \"bottom-right\"] | None\n            The origin of the imported aligner data. Used if the detected ROI is being estimated\n            from imported aligner data\n        \"\"\"\n        logger.debug(\"Data length: %s, align_origin: %s\", len(data), align_origin)\n        self._check_for_video(list(data)[0])\n        for key, faces in data.items():\n            try:\n                store_key = self._get_key(key)\n                self._imported[store_key] = np.array([self._import_frame_face(face, align_origin)\n                                                      for face in faces], dtype=\"int32\")\n            except FaceswapError as err:\n                logger.error(str(err))\n                msg = f\"The imported frame key that failed was '{key}'\"\n                raise FaceswapError(msg) from err\n\n    def process_input(self, batch: BatchType) -> None:\n        \"\"\" Put the lookup key into `batch.feed` so they can be collected for mapping in `.predict`\n\n        Parameters\n        ----------\n        batch: :class:`~plugins.extract.detect._base.DetectorBatch`\n            The batch to be processed by the plugin\n        \"\"\"\n        batch.feed = np.array([(self._get_key(os.path.basename(f)), i)\n                               for f, i in zip(batch.filename, batch.image)], dtype=\"object\")\n\n    def _adjust_for_origin(self, box: np.ndarray, frame_dims: tuple[int, int]) -> np.ndarray:\n        \"\"\" Adjust the bounding box to be top-left orientated based on the selected import origin\n\n        Parameters\n        ----------\n        box: :class:`np.ndarray`\n            The imported bounding box at original (0, 0) origin\n        frame_dims: tuple[int, int]\n            The (rows, columns) dimensions of the original frame\n\n        Returns\n        -------\n        :class:`numpy.ndarray`\n            The adjusted bounding box for a top-left origin\n        \"\"\"\n        if not np.any(box) or self._origin == \"top-left\":\n            return box\n        if self._origin.startswith(\"bottom\"):\n            box[:, [1, 3]] = frame_dims[0] - box[:, [1, 3]]\n        if self._origin.endswith(\"right\"):\n            box[:, [0, 2]] = frame_dims[1] - box[:, [0, 2]]\n\n        return box\n\n    def predict(self, feed: np.ndarray) -> list[np.ndarray]:  # type:ignore[override]\n        \"\"\" Pair the input filenames to the import file\n\n        Parameters\n        ----------\n        feed: :class:`numpy.ndarray`\n            The filenames with original frame dimensions to obtain the imported bounding boxes for\n\n        Returns\n        -------\n        list[]:class:`numpy.ndarray`]\n            The bounding boxes for the given filenames\n        \"\"\"\n        self._missing.extend(f[0] for f in feed if f[0] not in self._imported)\n        return [self._adjust_for_origin(self._imported.pop(f[0], np.array([], dtype=\"int32\")),\n                                        f[1])\n                for f in feed]\n\n    def process_output(self, batch: BatchType) -> None:\n        \"\"\" No output processing required for import plugin\n\n        Parameters\n        ----------\n        batch: :class:`~plugins.extract.detect._base.DetectorBatch`\n            The batch to be processed by the plugin\n        \"\"\"\n        logger.trace(\"No output processing for import plugin\")  # type:ignore[attr-defined]\n\n    def _remove_zero_sized_faces(self, batch_faces: list[list[DetectedFace]]\n                                 ) -> list[list[DetectedFace]]:\n        \"\"\" Override _remove_zero_sized_faces to just return the faces that have been imported\n\n        Parameters\n        ----------\n        batch_faces: list[list[DetectedFace]\n            List of detected face objects\n\n        Returns\n        -------\n        list[list[DetectedFace]\n            Original list of detected face objects\n        \"\"\"\n        return batch_faces\n\n    def on_completion(self) -> None:\n        \"\"\" Output information if:\n        - Imported items were not matched in input data\n        - Input data was not matched in imported items\n        \"\"\"\n        super().on_completion()\n\n        if self._missing:\n            logger.warning(\"[DETECT] %s input frames could not be matched in the import file \"\n                           \"'%s'. Run in verbose mode for a list of frames.\",\n                           len(self._missing), self.config[\"file_name\"])\n            logger.verbose(  # type:ignore[attr-defined]\n                \"[DETECT] Input frames not in import file: %s\", self._missing)\n\n        if self._imported:\n            logger.warning(\"[DETECT] %s items in the import file '%s' could not be matched to any \"\n                           \"input frames. Run in verbose mode for a list of items.\",\n                           len(self._imported), self.config[\"file_name\"])\n            logger.verbose(  # type:ignore[attr-defined]\n                \"[DETECT] import file items not in input frames: %s\", list(self._imported))\n", "plugins/extract/detect/cv2_dnn_defaults.py": "#!/usr/bin/env python3\n\"\"\"\n    The default options for the faceswap Cv2_Dnn Detect plugin.\n\n    Defaults files should be named <plugin_name>_defaults.py\n    Any items placed into this file will automatically get added to the relevant config .ini files\n    within the faceswap/config folder.\n\n    The following variables should be defined:\n        _HELPTEXT: A string describing what this plugin does\n        _DEFAULTS: A dictionary containing the options, defaults and meta information. The\n                   dictionary should be defined as:\n                       {<option_name>: {<metadata>}}\n\n                   <option_name> should always be lower text.\n                   <metadata> dictionary requirements are listed below.\n\n    The following keys are expected for the _DEFAULTS <metadata> dict:\n        datatype:  [required] A python type class. This limits the type of data that can be\n                   provided in the .ini file and ensures that the value is returned in the\n                   correct type to faceswap. Valid data types are: <class 'int'>, <class 'float'>,\n                   <class 'str'>, <class 'bool'>.\n        default:   [required] The default value for this option.\n        info:      [required] A string describing what this option does.\n        group:     [optional]. A group for grouping options together in the GUI. If not\n                   provided this will not group this option with any others.\n        choices:   [optional] If this option's datatype is of <class 'str'> then valid\n                   selections can be defined here. This validates the option and also enables\n                   a combobox / radio option in the GUI.\n        gui_radio: [optional] If <choices> are defined, this indicates that the GUI should use\n                   radio buttons rather than a combobox to display this option.\n        min_max:   [partial] For <class 'int'> and <class 'float'> data types this is required\n                   otherwise it is ignored. Should be a tuple of min and max accepted values.\n                   This is used for controlling the GUI slider range. Values are not enforced.\n        rounding:  [partial] For <class 'int'> and <class 'float'> data types this is\n                   required otherwise it is ignored. Used for the GUI slider. For floats, this\n                   is the number of decimal places to display. For ints this is the step size.\n        fixed:     [optional] [train only]. Training configurations are fixed when the model is\n                   created, and then reloaded from the state file. Marking an item as fixed=False\n                   indicates that this value can be changed for existing models, and will override\n                   the value saved in the state file with the updated value in config. If not\n                   provided this will default to True.\n\"\"\"\n\n\n_HELPTEXT = (\n    \"CV2 DNN Detector options.\\n\"\n    \"A CPU only extractor, is the least reliable, but uses least resources and runs fast on CPU. \"\n    \"Use this if not using a GPU and time is important\"\n)\n\n\n_DEFAULTS = {\n    \"confidence\": {\n        \"default\": 50,\n        \"info\": \"The confidence level at which the detector has succesfully found a face.\\nHigher \"\n                \"levels will be more discriminating, lower levels will have more false positives.\",\n        \"datatype\": int,\n        \"rounding\": 5,\n        \"min_max\": (25, 100),\n        \"choices\": [],\n        \"group\": \"settings\",\n        \"gui_radio\": False,\n        \"fixed\": True,\n    },\n}\n", "plugins/extract/detect/s3fd.py": "#!/usr/bin/env python3\n\"\"\" S3FD Face detection plugin\nhttps://arxiv.org/abs/1708.05237\n\nAdapted from S3FD Port in FAN:\nhttps://github.com/1adrianb/face-alignment\n\"\"\"\nfrom __future__ import annotations\nimport logging\nimport typing as T\n\nfrom scipy.special import logsumexp\nimport numpy as np\n\n# Ignore linting errors from Tensorflow's thoroughly broken import system\nfrom tensorflow import keras\nfrom tensorflow.keras import backend as K  # pylint:disable=import-error\nfrom tensorflow.keras.layers import (  # pylint:disable=import-error\n    Concatenate, Conv2D, Input, Maximum, MaxPooling2D, ZeroPadding2D)\n\nfrom lib.model.session import KSession\nfrom ._base import BatchType, Detector\n\nif T.TYPE_CHECKING:\n    from tensorflow import Tensor\n\nlogger = logging.getLogger(__name__)\n\n\nclass Detect(Detector):\n    \"\"\" S3FD detector for face recognition \"\"\"\n    def __init__(self, **kwargs) -> None:\n        git_model_id = 11\n        model_filename = \"s3fd_keras_v2.h5\"\n        super().__init__(git_model_id=git_model_id, model_filename=model_filename, **kwargs)\n        self.name = \"S3FD\"\n        self.input_size = 640\n        self.vram = 4112\n        self.vram_warnings = 1024  # Will run at this with warnings\n        self.vram_per_batch = 208\n        self.batchsize = self.config[\"batch-size\"]\n\n    def init_model(self) -> None:\n        \"\"\" Initialize S3FD Model\"\"\"\n        assert isinstance(self.model_path, str)\n        confidence = self.config[\"confidence\"] / 100\n        model_kwargs = {\"custom_objects\": {\"L2Norm\": L2Norm, \"SliceO2K\": SliceO2K}}\n        self.model = S3fd(self.model_path,\n                          model_kwargs,\n                          self.config[\"allow_growth\"],\n                          self._exclude_gpus,\n                          confidence)\n\n    def process_input(self, batch: BatchType) -> None:\n        \"\"\" Compile the detection image(s) for prediction \"\"\"\n        assert isinstance(self.model, S3fd)\n        batch.feed = self.model.prepare_batch(np.array(batch.image))\n\n    def predict(self, feed: np.ndarray) -> np.ndarray:\n        \"\"\" Run model to get predictions \"\"\"\n        assert isinstance(self.model, S3fd)\n        predictions = self.model.predict(feed)\n        assert isinstance(predictions, list)\n        return self.model.finalize_predictions(predictions)\n\n    def process_output(self, batch) -> None:\n        \"\"\" Compile found faces for output \"\"\"\n        return\n\n\n################################################################################\n# CUSTOM KERAS LAYERS\n################################################################################\nclass L2Norm(keras.layers.Layer):\n    \"\"\" L2 Normalization layer for S3FD.\n\n    Parameters\n    ----------\n    n_channels: int\n        The number of channels to normalize\n    scale: float, optional\n        The scaling for initial weights. Default: `1.0`\n    \"\"\"\n    def __init__(self, n_channels: int, scale: float = 1.0, **kwargs) -> None:\n        super().__init__(**kwargs)\n        self._n_channels = n_channels\n        self._scale = scale\n        self.w = self.add_weight(\"l2norm\",  # pylint:disable=invalid-name\n                                 (self._n_channels, ),\n                                 trainable=True,\n                                 initializer=keras.initializers.Constant(value=self._scale),\n                                 dtype=\"float32\")\n\n    def call(self, inputs: Tensor) -> Tensor:  # pylint:disable=arguments-differ\n        \"\"\" Call the L2 Normalization Layer.\n\n        Parameters\n        ----------\n        inputs: tensor\n            The input to the L2 Normalization Layer\n\n        Returns\n        -------\n        tensor:\n            The output from the L2 Normalization Layer\n        \"\"\"\n        norm = K.sqrt(K.sum(K.pow(inputs, 2), axis=-1, keepdims=True)) + 1e-10\n        var_x = inputs / norm * self.w\n        return var_x\n\n    def get_config(self) -> dict:\n        \"\"\" Returns the config of the layer.\n\n        Returns\n        -------\n        dict\n            The configuration for the layer\n        \"\"\"\n        config = super().get_config()\n        config.update({\"n_channels\": self._n_channels,\n                       \"scale\": self._scale})\n        return config\n\n\nclass SliceO2K(keras.layers.Layer):\n    \"\"\" Custom Keras Slice layer generated by onnx2keras. \"\"\"\n    def __init__(self,\n                 starts: list[int],\n                 ends: list[int],\n                 axes: list[int] | None = None,\n                 steps: list[int] | None = None,\n                 **kwargs) -> None:\n        self._starts = starts\n        self._ends = ends\n        self._axes = axes\n        self._steps = steps\n        super().__init__(**kwargs)\n\n    def _get_slices(self, dimensions: int) -> list[tuple[int, ...]]:\n        \"\"\" Obtain slices for the given number of dimensions.\n\n        Parameters\n        ----------\n        dimensions: int\n            The number of dimensions to obtain slices for\n\n        Returns\n        -------\n        list\n            The slices for the given number of dimensions\n        \"\"\"\n        axes = tuple(range(dimensions)) if self._axes is None else self._axes\n        steps = (1,) * len(axes) if self._steps is None else self._steps\n        assert len(axes) == len(steps) == len(self._starts) == len(self._ends)\n        return list(zip(axes, self._starts, self._ends, steps))\n\n    def compute_output_shape(self, input_shape: tuple[int, ...]) -> tuple[int, ...]:\n        \"\"\"Computes the output shape of the layer.\n\n        Assumes that the layer will be built to match that input shape provided.\n\n        Parameters\n        ----------\n        input_shape: tuple or list of tuples\n            Shape tuple (tuple of integers) or list of shape tuples (one per output tensor of the\n            layer). Shape tuples can include ``None`` for free dimensions, instead of an integer.\n\n        Returns\n        -------\n        tuple\n            An output shape tuple.\n        \"\"\"\n        in_shape = list(input_shape)\n        for a_x, start, end, steps in self._get_slices(len(in_shape)):\n            size = in_shape[a_x]\n            if a_x == 0:\n                raise AttributeError(\"Can not slice batch axis.\")\n            if size is None:\n                if start < 0 or end < 0:\n                    raise AttributeError(\"Negative slices not supported on symbolic axes\")\n                logger.warning(\"Slicing symbolic axis might lead to problems.\")\n                in_shape[a_x] = (end - start) // steps\n                continue\n            if start < 0:\n                start = size - start\n            if end < 0:\n                end = size - end\n            in_shape[a_x] = (min(size, end) - start) // steps\n        return tuple(in_shape)\n\n    def call(self, inputs, **kwargs):  # pylint:disable=unused-argument,arguments-differ\n        \"\"\"This is where the layer's logic lives.\n\n        Parameters\n        ----------\n        inputs: Input tensor, or list/tuple of input tensors.\n            The input to the layer\n        **kwargs: Additional keyword arguments.\n            Required for parent class but unused\n        Returns\n        -------\n        A tensor or list/tuple of tensors.\n            The layer output\n        \"\"\"\n        ax_map = dict((x[0], slice(*x[1:])) for x in self._get_slices(K.ndim(inputs)))\n        shape = K.int_shape(inputs)\n        slices = [(ax_map[a] if a in ax_map else slice(None)) for a in range(len(shape))]\n        retval = inputs[tuple(slices)]\n        return retval\n\n    def get_config(self) -> dict:\n        \"\"\" Returns the config of the layer.\n\n        Returns\n        -------\n        dict\n            The configuration for the layer\n        \"\"\"\n        config = super().get_config()\n        config.update({\"starts\": self._starts,\n                       \"ends\": self._ends,\n                       \"axes\": self._axes,\n                       \"steps\": self._steps})\n        return config\n\n\nclass S3fd(KSession):\n    \"\"\" Keras Network \"\"\"\n    def __init__(self,\n                 model_path: str,\n                 model_kwargs: dict,\n                 allow_growth: bool,\n                 exclude_gpus: list[int] | None,\n                 confidence: float) -> None:\n        logger.debug(\"Initializing: %s: (model_path: '%s', model_kwargs: %s, allow_growth: %s, \"\n                     \"exclude_gpus: %s, confidence: %s)\", self.__class__.__name__, model_path,\n                     model_kwargs, allow_growth, exclude_gpus, confidence)\n        super().__init__(\"S3FD\",\n                         model_path,\n                         model_kwargs=model_kwargs,\n                         allow_growth=allow_growth,\n                         exclude_gpus=exclude_gpus)\n        self.define_model(self.model_definition)\n        self.load_model_weights()\n        self.confidence = confidence\n        self.average_img = np.array([104.0, 117.0, 123.0])\n        logger.debug(\"Initialized: %s\", self.__class__.__name__)\n\n    def model_definition(self) -> tuple[list[Tensor], list[Tensor]]:\n        \"\"\" Keras S3FD Model Definition, adapted from FAN pytorch implementation. \"\"\"\n        input_ = Input(shape=(640, 640, 3))\n        var_x = self.conv_block(input_, 64, 1, 2)\n        var_x = MaxPooling2D(pool_size=2, strides=2)(var_x)\n\n        var_x = self.conv_block(var_x, 128, 2, 2)\n        var_x = MaxPooling2D(pool_size=2, strides=2)(var_x)\n\n        var_x = self.conv_block(var_x, 256, 3, 3)\n        f3_3 = var_x\n        var_x = MaxPooling2D(pool_size=2, strides=2)(var_x)\n\n        var_x = self.conv_block(var_x, 512, 4, 3)\n        f4_3 = var_x\n        var_x = MaxPooling2D(pool_size=2, strides=2)(var_x)\n\n        var_x = self.conv_block(var_x, 512, 5, 3)\n        f5_3 = var_x\n        var_x = MaxPooling2D(pool_size=2, strides=2)(var_x)\n\n        var_x = ZeroPadding2D(3)(var_x)\n        var_x = Conv2D(1024, kernel_size=3, strides=1, activation=\"relu\", name=\"fc6\")(var_x)\n        var_x = Conv2D(1024, kernel_size=1, strides=1, activation=\"relu\", name=\"fc7\")(var_x)\n        ffc7 = var_x\n\n        f6_2 = self.conv_up(var_x, 256, 6)\n        f7_2 = self.conv_up(f6_2, 128, 7)\n\n        f3_3 = L2Norm(256, scale=10, name=\"conv3_3_norm\")(f3_3)\n        f4_3 = L2Norm(512, scale=8, name=\"conv4_3_norm\")(f4_3)\n        f5_3 = L2Norm(512, scale=5, name=\"conv5_3_norm\")(f5_3)\n\n        f3_3 = ZeroPadding2D(1)(f3_3)\n        cls1 = Conv2D(4, kernel_size=3, strides=1, name=\"conv3_3_norm_mbox_conf\")(f3_3)\n        reg1 = Conv2D(4, kernel_size=3, strides=1, name=\"conv3_3_norm_mbox_loc\")(f3_3)\n\n        f4_3 = ZeroPadding2D(1)(f4_3)\n        cls2 = Conv2D(2, kernel_size=3, strides=1, name=\"conv4_3_norm_mbox_conf\")(f4_3)\n        reg2 = Conv2D(4, kernel_size=3, strides=1, name=\"conv4_3_norm_mbox_loc\")(f4_3)\n\n        f5_3 = ZeroPadding2D(1)(f5_3)\n        cls3 = Conv2D(2, kernel_size=3, strides=1, name=\"conv5_3_norm_mbox_conf\")(f5_3)\n        reg3 = Conv2D(4, kernel_size=3, strides=1, name=\"conv5_3_norm_mbox_loc\")(f5_3)\n\n        ffc7 = ZeroPadding2D(1)(ffc7)\n        cls4 = Conv2D(2, kernel_size=3, strides=1, name=\"fc7_mbox_conf\")(ffc7)\n        reg4 = Conv2D(4, kernel_size=3, strides=1, name=\"fc7_mbox_loc\")(ffc7)\n\n        f6_2 = ZeroPadding2D(1)(f6_2)\n        cls5 = Conv2D(2, kernel_size=3, strides=1, name=\"conv6_2_mbox_conf\")(f6_2)\n        reg5 = Conv2D(4, kernel_size=3, strides=1, name=\"conv6_2_mbox_loc\")(f6_2)\n\n        f7_2 = ZeroPadding2D(1)(f7_2)\n        cls6 = Conv2D(2, kernel_size=3, strides=1, name=\"conv7_2_mbox_conf\")(f7_2)\n        reg6 = Conv2D(4, kernel_size=3, strides=1, name=\"conv7_2_mbox_loc\")(f7_2)\n\n        # max-out background label\n        chunks = [SliceO2K(starts=[0], ends=[1], axes=[3], steps=None)(cls1),\n                  SliceO2K(starts=[1], ends=[2], axes=[3], steps=None)(cls1),\n                  SliceO2K(starts=[2], ends=[3], axes=[3], steps=None)(cls1),\n                  SliceO2K(starts=[3], ends=[4], axes=[3], steps=None)(cls1)]\n\n        bmax = Maximum()([chunks[0], chunks[1], chunks[2]])\n        cls1 = Concatenate()([bmax, chunks[3]])\n\n        return [input_], [cls1, reg1, cls2, reg2, cls3, reg3, cls4, reg4, cls5, reg5, cls6, reg6]\n\n    @classmethod\n    def conv_block(cls, inputs: Tensor, filters: int, idx: int, recursions: int) -> Tensor:\n        \"\"\" First round convolutions with zero padding added.\n\n        Parameters\n        ----------\n        inputs: tensor\n            The input tensor to the convolution block\n        filters: int\n            The number of filters\n        idx: int\n            The layer index for naming\n        recursions: int\n            The number of recursions of the block to perform\n\n        Returns\n        -------\n        tensor\n            The output tensor from the convolution block\n        \"\"\"\n        name = f\"conv{idx}\"\n        var_x = inputs\n        for i in range(1, recursions + 1):\n            rec_name = f\"{name}_{i}\"\n            var_x = ZeroPadding2D(1, name=f\"{rec_name}.zeropad\")(var_x)\n            var_x = Conv2D(filters,\n                           kernel_size=3,\n                           strides=1,\n                           activation=\"relu\",\n                           name=rec_name)(var_x)\n        return var_x\n\n    @classmethod\n    def conv_up(cls, inputs: Tensor, filters: int, idx: int) -> Tensor:\n        \"\"\" Convolution up filter blocks with zero padding added.\n\n        Parameters\n        ----------\n        inputs: tensor\n            The input tensor to the convolution block\n        filters: int\n            The initial number of filters\n        idx: int\n            The layer index for naming\n\n        Returns\n        -------\n        tensor\n            The output tensor from the convolution block\n        \"\"\"\n        name = f\"conv{idx}\"\n        var_x = inputs\n        for i in range(1, 3):\n            rec_name = f\"{name}_{i}\"\n            size = 1 if i == 1 else 3\n            if i == 2:\n                var_x = ZeroPadding2D(1, name=f\"{rec_name}.zeropad\")(var_x)\n            var_x = Conv2D(filters * i,\n                           kernel_size=size,\n                           strides=i,\n                           activation=\"relu\",\n                           name=rec_name)(var_x)\n        return var_x\n\n    def prepare_batch(self, batch: np.ndarray) -> np.ndarray:\n        \"\"\" Prepare a batch for prediction.\n\n        Normalizes the feed images.\n\n        Parameters\n        ----------\n        batch: class:`numpy.ndarray`\n            The batch to be fed to the model\n\n        Returns\n        -------\n        class:`numpy.ndarray`\n            The normalized images for feeding to the model\n        \"\"\"\n        batch = batch - self.average_img\n        return batch\n\n    def finalize_predictions(self, bounding_boxes_scales: list[np.ndarray]) -> np.ndarray:\n        \"\"\" Process the output from the model to obtain faces\n\n        Parameters\n        ----------\n        bounding_boxes_scales: list\n            The output predictions from the S3FD model\n        \"\"\"\n        ret = []\n        batch_size = range(bounding_boxes_scales[0].shape[0])\n        for img in batch_size:\n            bboxlist = [scale[img:img+1] for scale in bounding_boxes_scales]\n            boxes = self._post_process(bboxlist)\n            finallist = self._nms(boxes, 0.5)\n            ret.append(finallist)\n        return np.array(ret, dtype=\"object\")\n\n    def _post_process(self, bboxlist: list[np.ndarray]) -> np.ndarray:\n        \"\"\" Perform post processing on output\n            TODO: do this on the batch.\n        \"\"\"\n        retval = []\n        for i in range(len(bboxlist) // 2):\n            bboxlist[i * 2] = self.softmax(bboxlist[i * 2], axis=3)\n        for i in range(len(bboxlist) // 2):\n            ocls, oreg = bboxlist[i * 2], bboxlist[i * 2 + 1]\n            stride = 2 ** (i + 2)    # 4,8,16,32,64,128\n            poss = zip(*np.where(ocls[:, :, :, 1] > 0.05))\n            for _, hindex, windex in poss:\n                axc, ayc = stride / 2 + windex * stride, stride / 2 + hindex * stride\n                score = ocls[0, hindex, windex, 1]\n                if score >= self.confidence:\n                    loc = np.ascontiguousarray(oreg[0, hindex, windex, :]).reshape((1, 4))\n                    priors = np.array([[axc / 1.0, ayc / 1.0, stride * 4 / 1.0, stride * 4 / 1.0]])\n                    box = self.decode(loc, priors)\n                    x_1, y_1, x_2, y_2 = box[0] * 1.0\n                    retval.append([x_1, y_1, x_2, y_2, score])\n        return_numpy = np.array(retval) if len(retval) != 0 else np.zeros((1, 5))\n        return return_numpy\n\n    @staticmethod\n    def softmax(inp, axis: int) -> np.ndarray:\n        \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n        return np.exp(inp - logsumexp(inp, axis=axis, keepdims=True))\n\n    @staticmethod\n    def decode(location: np.ndarray, priors: np.ndarray) -> np.ndarray:\n        \"\"\"Decode locations from predictions using priors to undo the encoding we did for offset\n        regression at train time.\n\n        Parameters\n        ----------\n        location: tensor\n            location predictions for location layers,\n        priors: tensor\n            Prior boxes in center-offset form.\n\n        Returns\n        -------\n        :class:`numpy.ndarray`\n            decoded bounding box predictions\n        \"\"\"\n        variances = [0.1, 0.2]\n        boxes = np.concatenate((priors[:, :2] + location[:, :2] * variances[0] * priors[:, 2:],\n                                priors[:, 2:] * np.exp(location[:, 2:] * variances[1])), axis=1)\n        boxes[:, :2] -= boxes[:, 2:] / 2\n        boxes[:, 2:] += boxes[:, :2]\n        return boxes\n\n    @staticmethod\n    def _nms(boxes: np.ndarray, threshold: float) -> np.ndarray:\n        \"\"\" Perform Non-Maximum Suppression \"\"\"\n        retained_box_indices = []\n\n        areas = (boxes[:, 2] - boxes[:, 0] + 1) * (boxes[:, 3] - boxes[:, 1] + 1)\n        ranked_indices = boxes[:, 4].argsort()[::-1]\n        while ranked_indices.size > 0:\n            best_rest = ranked_indices[0], ranked_indices[1:]\n\n            max_of_xy = np.maximum(boxes[best_rest[0], :2], boxes[best_rest[1], :2])\n            min_of_xy = np.minimum(boxes[best_rest[0], 2:4], boxes[best_rest[1], 2:4])\n            width_height = np.maximum(0, min_of_xy - max_of_xy + 1)\n            intersection_areas = width_height[:, 0] * width_height[:, 1]\n            iou = intersection_areas / (areas[best_rest[0]] +\n                                        areas[best_rest[1]] - intersection_areas)\n\n            overlapping_boxes = (iou > threshold).nonzero()[0]\n            if len(overlapping_boxes) != 0:\n                overlap_set = ranked_indices[overlapping_boxes + 1]\n                vote = np.average(boxes[overlap_set, :4], axis=0, weights=boxes[overlap_set, 4])\n                boxes[best_rest[0], :4] = vote\n            retained_box_indices.append(best_rest[0])\n\n            non_overlapping_boxes = (iou <= threshold).nonzero()[0]\n            ranked_indices = ranked_indices[non_overlapping_boxes + 1]\n        return boxes[retained_box_indices]\n", "plugins/extract/detect/__init__.py": "", "plugins/extract/mask/vgg_obstructed.py": "#!/usr/bin/env python3\n\"\"\" VGG Obstructed face mask plugin \"\"\"\nfrom __future__ import annotations\nimport logging\nimport typing as T\n\nimport numpy as np\n\n# Ignore linting errors from Tensorflow's thoroughly broken import system\nfrom tensorflow.keras.layers import (  # pylint:disable=import-error\n    Add, Conv2D, Conv2DTranspose, Cropping2D, Dropout, Input, Lambda, MaxPooling2D,\n    ZeroPadding2D)\n\nfrom lib.model.session import KSession\nfrom ._base import BatchType, Masker, MaskerBatch\n\nif T.TYPE_CHECKING:\n    from tensorflow import Tensor\n\nlogger = logging.getLogger(__name__)\n\n\nclass Mask(Masker):\n    \"\"\" Neural network to process face image into a segmentation mask of the face \"\"\"\n    def __init__(self, **kwargs) -> None:\n        git_model_id = 5\n        model_filename = \"Nirkin_500_softmax_v1.h5\"\n        super().__init__(git_model_id=git_model_id, model_filename=model_filename, **kwargs)\n        self.model: KSession\n        self.name = \"VGG Obstructed\"\n        self.input_size = 500\n        self.vram = 3936\n        self.vram_warnings = 1088  # at BS 1. OOMs at higher batch sizes\n        self.vram_per_batch = 304\n        self.batchsize = self.config[\"batch-size\"]\n\n    def init_model(self) -> None:\n        assert isinstance(self.model_path, str)\n        self.model = VGGObstructed(self.model_path,\n                                   allow_growth=self.config[\"allow_growth\"],\n                                   exclude_gpus=self._exclude_gpus)\n        self.model.append_softmax_activation(layer_index=-1)\n        placeholder = np.zeros((self.batchsize, self.input_size, self.input_size, 3),\n                               dtype=\"float32\")\n        self.model.predict(placeholder)\n\n    def process_input(self, batch: BatchType) -> None:\n        \"\"\" Compile the detected faces for prediction \"\"\"\n        assert isinstance(batch, MaskerBatch)\n        input_ = [T.cast(np.ndarray, feed.face)[..., :3] for feed in batch.feed_faces]\n        batch.feed = input_ - np.mean(input_, axis=(1, 2))[:, None, None, :]\n        logger.trace(\"feed shape: %s\", batch.feed.shape)  # type:ignore\n\n    def predict(self, feed: np.ndarray) -> np.ndarray:\n        \"\"\" Run model to get predictions \"\"\"\n        predictions = self.model.predict(feed)\n        assert isinstance(predictions, np.ndarray)\n        return predictions[..., 0] * -1.0 + 1.0\n\n    def process_output(self, batch: BatchType) -> None:\n        \"\"\" Compile found faces for output \"\"\"\n        return\n\n\nclass VGGObstructed(KSession):\n    \"\"\" VGG Obstructed mask for Faceswap.\n\n    Caffe model re-implemented in Keras by Kyle Vrooman.\n    Re-implemented for Tensorflow 2 by TorzDF\n\n    Parameters\n    ----------\n    model_path: str\n        The path to the keras model file\n    allow_growth: bool\n        Enable the Tensorflow GPU allow_growth configuration option. This option prevents\n        Tensorflow from allocating all of the GPU VRAM, but can lead to higher fragmentation and\n        slower performance\n    exclude_gpus: list\n        A list of indices correlating to connected GPUs that Tensorflow should not use. Pass\n        ``None`` to not exclude any GPUs\n\n    References\n    ----------\n    On Face Segmentation, Face Swapping, and Face Perception (https://arxiv.org/abs/1704.06729)\n    Source Implementation: https://github.com/YuvalNirkin/face_segmentation\n    Model file sourced from:\n    https://github.com/YuvalNirkin/face_segmentation/releases/download/1.0/face_seg_fcn8s.zip\n    \"\"\"\n    def __init__(self,\n                 model_path: str,\n                 allow_growth: bool,\n                 exclude_gpus: list[int] | None) -> None:\n        super().__init__(\"VGG Obstructed\",\n                         model_path,\n                         allow_growth=allow_growth,\n                         exclude_gpus=exclude_gpus)\n        self.define_model(self._model_definition)\n        self.load_model_weights()\n\n    @classmethod\n    def _model_definition(cls) -> tuple[Tensor, Tensor]:\n        \"\"\" Definition of the VGG Obstructed Model.\n\n        Returns\n        -------\n        tuple\n            The tensor input to the model and tensor output to the model for compilation by\n            :func`define_model`\n        \"\"\"\n        input_ = Input(shape=(500, 500, 3))\n        var_x = ZeroPadding2D(padding=((100, 100), (100, 100)))(input_)\n\n        var_x = _ConvBlock(1, 64, 2)(var_x)\n        var_x = _ConvBlock(2, 128, 2)(var_x)\n        var_x = _ConvBlock(3, 256, 3)(var_x)\n\n        score_pool3 = _ScorePool(3, 0.0001, 9)(var_x)\n        var_x = _ConvBlock(4, 512, 3)(var_x)\n        score_pool4 = _ScorePool(4, 0.01, 5)(var_x)\n        var_x = _ConvBlock(5, 512, 3)(var_x)\n\n        var_x = Conv2D(4096, 7, padding=\"valid\", activation=\"relu\", name=\"fc6\")(var_x)\n        var_x = Dropout(rate=0.5)(var_x)\n        var_x = Conv2D(4096, 1, padding=\"valid\", activation=\"relu\", name=\"fc7\")(var_x)\n        var_x = Dropout(rate=0.5)(var_x)\n\n        var_x = Conv2D(21, 1, padding=\"valid\", activation=\"linear\", name=\"score_fr\")(var_x)\n        var_x = Conv2DTranspose(21,\n                                4,\n                                strides=2,\n                                activation=\"linear\",\n                                use_bias=False,\n                                name=\"upscore2\")(var_x)\n\n        var_x = Add()([var_x, score_pool4])\n        var_x = Conv2DTranspose(21,\n                                4,\n                                strides=2,\n                                activation=\"linear\",\n                                use_bias=False,\n                                name=\"upscore_pool4\")(var_x)\n\n        var_x = Add()([var_x, score_pool3])\n        var_x = Conv2DTranspose(21,\n                                16,\n                                strides=8,\n                                activation=\"linear\",\n                                use_bias=False,\n                                name=\"upscore8\")(var_x)\n        var_x = Cropping2D(cropping=((31, 37), (31, 37)), name=\"score\")(var_x)\n        return input_, var_x\n\n\nclass _ConvBlock():  # pylint:disable=too-few-public-methods\n    \"\"\" Convolutional loop with max pooling layer for VGG Obstructed.\n\n    Parameters\n    ----------\n    level: int\n        For naming. The current level for this convolutional loop\n    filters: int\n        The number of filters that should appear in each Conv2D layer\n    iterations: int\n        The number of consecutive Conv2D layers to create\n    \"\"\"\n    def __init__(self, level: int, filters: int, iterations: int) -> None:\n        self._name = f\"conv{level}_\"\n        self._level = level\n        self._filters = filters\n        self._iterator = range(1, iterations + 1)\n\n    def __call__(self, inputs: Tensor) -> Tensor:\n        \"\"\" Call the convolutional loop.\n\n        Parameters\n        ----------\n        inputs: tensor\n            The input tensor to the block\n\n        Returns\n        -------\n        tensor\n            The output tensor from the convolutional block\n        \"\"\"\n        var_x = inputs\n        for i in self._iterator:\n            padding = \"valid\" if self._level == i == 1 else \"same\"\n            var_x = Conv2D(self._filters,\n                           3,\n                           padding=padding,\n                           activation=\"relu\",\n                           name=f\"{self._name}{i}\")(var_x)\n        var_x = MaxPooling2D(padding=\"same\",\n                             strides=(2, 2),\n                             name=f\"pool{self._level}\")(var_x)\n        return var_x\n\n\nclass _ScorePool():  # pylint:disable=too-few-public-methods\n    \"\"\" Cropped scaling of the pooling layer.\n\n    Parameters\n    ----------\n    level: int\n        For naming. The current level for this score pool\n    scale: float\n        The scaling to apply to the pool\n    crop: int\n        The amount of 2D cropping to apply\n    \"\"\"\n    def __init__(self, level: int, scale: float, crop: int) -> None:\n        self._name = f\"_pool{level}\"\n        self._cropping = ((crop, crop), (crop, crop))\n        self._scale = scale\n\n    def __call__(self, inputs: Tensor) -> Tensor:\n        \"\"\" Score pool block.\n\n        Parameters\n        ----------\n        inputs: tensor\n            The input tensor to the block\n\n        Returns\n        -------\n        tensor\n            The output tensor from the score pool block\n        \"\"\"\n        var_x = Lambda(lambda x: x * self._scale, name=\"scale\" + self._name)(inputs)\n        var_x = Conv2D(21,\n                       1,\n                       padding=\"valid\",\n                       activation=\"linear\",\n                       name=\"score\" + self._name)(var_x)\n        var_x = Cropping2D(cropping=self._cropping, name=\"score\" + self._name + \"c\")(var_x)\n        return var_x\n", "plugins/extract/mask/components.py": "#!/usr/bin/env python3\n\"\"\" Components Mask for faceswap.py \"\"\"\nfrom __future__ import annotations\nimport logging\nimport typing as T\n\nimport cv2\nimport numpy as np\n\nfrom lib.align import LandmarkType\n\nfrom ._base import BatchType, Masker\n\nif T.TYPE_CHECKING:\n    from lib.align.aligned_face import AlignedFace\n\nlogger = logging.getLogger(__name__)\n\n\nclass Mask(Masker):\n    \"\"\" Perform transformation to align and get landmarks \"\"\"\n    def __init__(self, **kwargs) -> None:\n        git_model_id = None\n        model_filename = None\n        super().__init__(git_model_id=git_model_id, model_filename=model_filename, **kwargs)\n        self.input_size = 256\n        self.name = \"Components\"\n        self.vram = 0  # Doesn't use GPU\n        self.vram_per_batch = 0\n        self.batchsize = 1\n        self.landmark_type = LandmarkType.LM_2D_68\n\n    def init_model(self) -> None:\n        logger.debug(\"No mask model to initialize\")\n\n    def process_input(self, batch: BatchType) -> None:\n        \"\"\" Compile the detected faces for prediction \"\"\"\n        batch.feed = np.zeros((self.batchsize, self.input_size, self.input_size, 1),\n                              dtype=\"float32\")\n\n    def predict(self, feed: np.ndarray) -> np.ndarray:\n        \"\"\" Run model to get predictions \"\"\"\n        faces: list[AlignedFace] = feed[1]\n        feed = feed[0]\n        for mask, face in zip(feed, faces):\n            if LandmarkType.from_shape(face.landmarks.shape) != self.landmark_type:\n                # Called from the manual tool. # TODO This will only work with BS1\n                feed = np.zeros_like(feed)\n                continue\n            parts = self.parse_parts(np.array(face.landmarks))\n            for item in parts:\n                a_item = np.rint(np.concatenate(item)).astype(\"int32\")\n                hull = cv2.convexHull(a_item)\n                cv2.fillConvexPoly(mask, hull, [1.0], lineType=cv2.LINE_AA)\n        return feed\n\n    def process_output(self, batch: BatchType) -> None:\n        \"\"\" Compile found faces for output \"\"\"\n        return\n\n    @staticmethod\n    def parse_parts(landmarks: np.ndarray) -> list[tuple[np.ndarray, ...]]:\n        \"\"\" Component face hull mask \"\"\"\n        r_jaw = (landmarks[0:9], landmarks[17:18])\n        l_jaw = (landmarks[8:17], landmarks[26:27])\n        r_cheek = (landmarks[17:20], landmarks[8:9])\n        l_cheek = (landmarks[24:27], landmarks[8:9])\n        nose_ridge = (landmarks[19:25], landmarks[8:9],)\n        r_eye = (landmarks[17:22],\n                 landmarks[27:28],\n                 landmarks[31:36],\n                 landmarks[8:9])\n        l_eye = (landmarks[22:27],\n                 landmarks[27:28],\n                 landmarks[31:36],\n                 landmarks[8:9])\n        nose = (landmarks[27:31], landmarks[31:36])\n        parts = [r_jaw, l_jaw, r_cheek, l_cheek, nose_ridge, r_eye, l_eye, nose]\n        return parts\n", "plugins/extract/mask/extended.py": "#!/usr/bin/env python3\n\"\"\" Extended Mask for faceswap.py \"\"\"\nfrom __future__ import annotations\nimport logging\nimport typing as T\n\nimport cv2\nimport numpy as np\n\nfrom lib.align import LandmarkType\n\nfrom ._base import BatchType, Masker\n\nlogger = logging.getLogger(__name__)\n\nif T.TYPE_CHECKING:\n    from lib.align.aligned_face import AlignedFace\n\n\nclass Mask(Masker):\n    \"\"\" Perform transformation to align and get landmarks \"\"\"\n    def __init__(self, **kwargs):\n        git_model_id = None\n        model_filename = None\n        super().__init__(git_model_id=git_model_id, model_filename=model_filename, **kwargs)\n        self.input_size = 256\n        self.name = \"Extended\"\n        self.vram = 0  # Doesn't use GPU\n        self.vram_per_batch = 0\n        self.batchsize = 1\n        self.landmark_type = LandmarkType.LM_2D_68\n\n    def init_model(self) -> None:\n        logger.debug(\"No mask model to initialize\")\n\n    def process_input(self, batch: BatchType) -> None:\n        \"\"\" Compile the detected faces for prediction \"\"\"\n        batch.feed = np.zeros((self.batchsize, self.input_size, self.input_size, 1),\n                              dtype=\"float32\")\n\n    def predict(self, feed: np.ndarray) -> np.ndarray:\n        \"\"\" Run model to get predictions \"\"\"\n        faces: list[AlignedFace] = feed[1]\n        feed = feed[0]\n        for mask, face in zip(feed, faces):\n            if LandmarkType.from_shape(face.landmarks.shape) != self.landmark_type:\n                # Called from the manual tool. # TODO This will only work with BS1\n                feed = np.zeros_like(feed)\n                continue\n            parts = self.parse_parts(np.array(face.landmarks))\n            for item in parts:\n                a_item = np.rint(np.concatenate(item)).astype(\"int32\")\n                hull = cv2.convexHull(a_item)\n                cv2.fillConvexPoly(mask, hull, [1.0], lineType=cv2.LINE_AA)\n        return feed\n\n    def process_output(self, batch: BatchType) -> None:\n        \"\"\" Compile found faces for output \"\"\"\n        return\n\n    @classmethod\n    def _adjust_mask_top(cls, landmarks: np.ndarray) -> None:\n        \"\"\" Adjust the top of the mask to extend above eyebrows\n\n        Parameters\n        ----------\n        landmarks: :class:`numpy.ndarray`\n            The 68 point landmarks to be adjusted\n        \"\"\"\n        # mid points between the side of face and eye point\n        ml_pnt = (landmarks[36] + landmarks[0]) // 2\n        mr_pnt = (landmarks[16] + landmarks[45]) // 2\n\n        # mid points between the mid points and eye\n        ql_pnt = (landmarks[36] + ml_pnt) // 2\n        qr_pnt = (landmarks[45] + mr_pnt) // 2\n\n        # Top of the eye arrays\n        bot_l = np.array((ql_pnt, landmarks[36], landmarks[37], landmarks[38], landmarks[39]))\n        bot_r = np.array((landmarks[42], landmarks[43], landmarks[44], landmarks[45], qr_pnt))\n\n        # Eyebrow arrays\n        top_l = landmarks[17:22]\n        top_r = landmarks[22:27]\n\n        # Adjust eyebrow arrays\n        landmarks[17:22] = top_l + ((top_l - bot_l) // 2)\n        landmarks[22:27] = top_r + ((top_r - bot_r) // 2)\n\n    def parse_parts(self, landmarks: np.ndarray) -> list[tuple[np.ndarray, ...]]:\n        \"\"\" Extended face hull mask \"\"\"\n        self._adjust_mask_top(landmarks)\n\n        r_jaw = (landmarks[0:9], landmarks[17:18])\n        l_jaw = (landmarks[8:17], landmarks[26:27])\n        r_cheek = (landmarks[17:20], landmarks[8:9])\n        l_cheek = (landmarks[24:27], landmarks[8:9])\n        nose_ridge = (landmarks[19:25], landmarks[8:9],)\n        r_eye = (landmarks[17:22],\n                 landmarks[27:28],\n                 landmarks[31:36],\n                 landmarks[8:9])\n        l_eye = (landmarks[22:27],\n                 landmarks[27:28],\n                 landmarks[31:36],\n                 landmarks[8:9])\n        nose = (landmarks[27:31], landmarks[31:36])\n        parts = [r_jaw, l_jaw, r_cheek, l_cheek, nose_ridge, r_eye, l_eye, nose]\n        return parts\n", "plugins/extract/mask/unet_dfl.py": "#!/usr/bin/env python3\n\"\"\" UNET DFL face mask plugin\n\nArchitecture and Pre-Trained Model based on...\nTernausNet: U-Net with VGG11 Encoder Pre-Trained on ImageNet for Image Segmentation\nhttps://arxiv.org/abs/1801.05746\nhttps://github.com/ternaus/TernausNet\n\nSource Implementation and fine-tune training....\nhttps://github.com/iperov/DeepFaceLab/blob/master/nnlib/TernausNet.py\n\nModel file sourced from...\nhttps://github.com/iperov/DeepFaceLab/blob/master/nnlib/FANSeg_256_full_face.h5\n\"\"\"\nimport logging\nimport typing as T\n\nimport numpy as np\nfrom lib.model.session import KSession\nfrom ._base import BatchType, Masker, MaskerBatch\n\nlogger = logging.getLogger(__name__)\n\n\nclass Mask(Masker):\n    \"\"\" Neural network to process face image into a segmentation mask of the face \"\"\"\n    def __init__(self, **kwargs) -> None:\n        git_model_id = 6\n        model_filename = \"DFL_256_sigmoid_v1.h5\"\n        super().__init__(git_model_id=git_model_id, model_filename=model_filename, **kwargs)\n        self.model: KSession\n        self.name = \"U-Net\"\n        self.input_size = 256\n        self.vram = 3424\n        self.vram_warnings = 256\n        self.vram_per_batch = 80\n        self.batchsize = self.config[\"batch-size\"]\n        self._storage_centering = \"legacy\"\n\n    def init_model(self) -> None:\n        assert self.name is not None and isinstance(self.model_path, str)\n        self.model = KSession(self.name,\n                              self.model_path,\n                              model_kwargs={},\n                              allow_growth=self.config[\"allow_growth\"],\n                              exclude_gpus=self._exclude_gpus)\n        self.model.load_model()\n        placeholder = np.zeros((self.batchsize, self.input_size, self.input_size, 3),\n                               dtype=\"float32\")\n        self.model.predict(placeholder)\n\n    def process_input(self, batch: BatchType) -> None:\n        \"\"\" Compile the detected faces for prediction \"\"\"\n        assert isinstance(batch, MaskerBatch)\n        batch.feed = np.array([T.cast(np.ndarray, feed.face)[..., :3]\n                               for feed in batch.feed_faces], dtype=\"float32\") / 255.0\n        logger.trace(\"feed shape: %s\", batch.feed.shape)  # type: ignore\n\n    def predict(self, feed: np.ndarray) -> np.ndarray:\n        \"\"\" Run model to get predictions \"\"\"\n        retval = self.model.predict(feed)\n        assert isinstance(retval, np.ndarray)\n        return retval\n\n    def process_output(self, batch: BatchType) -> None:\n        \"\"\" Compile found faces for output \"\"\"\n        return\n", "plugins/extract/mask/_base.py": "#!/usr/bin/env python3\n\"\"\" Base class for Face Masker plugins\n\nPlugins should inherit from this class\n\nSee the override methods for which methods are required.\n\nThe plugin will receive a :class:`~plugins.extract.extract_media.ExtractMedia` object.\n\nFor each source item, the plugin must pass a dict to finalize containing:\n\n>>> {\"filename\": <filename of source frame>,\n>>>  \"detected_faces\": <list of bounding box dicts from lib/plugins/extract/detect/_base>}\n\"\"\"\nfrom __future__ import annotations\nimport logging\nimport typing as T\n\nfrom dataclasses import dataclass, field\n\nimport cv2\nimport numpy as np\n\nfrom tensorflow.python.framework import errors_impl as tf_errors  # pylint:disable=no-name-in-module  # noqa\n\nfrom lib.align import AlignedFace, LandmarkType, transform_image\nfrom lib.utils import FaceswapError\nfrom plugins.extract import ExtractMedia\nfrom plugins.extract._base import BatchType, ExtractorBatch, Extractor\n\nif T.TYPE_CHECKING:\n    from collections.abc import Generator\n    from queue import Queue\n    from lib.align import DetectedFace\n    from lib.align.aligned_face import CenteringType\n\nlogger = logging.getLogger(__name__)\n\n\n@dataclass\nclass MaskerBatch(ExtractorBatch):\n    \"\"\" Dataclass for holding items flowing through the aligner.\n\n    Inherits from :class:`~plugins.extract._base.ExtractorBatch`\n\n    Parameters\n    ----------\n    roi_masks: list\n        The region of interest masks for the batch\n    \"\"\"\n    detected_faces: list[DetectedFace] = field(default_factory=list)\n    roi_masks: list[np.ndarray] = field(default_factory=list)\n    feed_faces: list[AlignedFace] = field(default_factory=list)\n\n\nclass Masker(Extractor):  # pylint:disable=abstract-method\n    \"\"\" Masker plugin _base Object\n\n    All Masker plugins must inherit from this class\n\n    Parameters\n    ----------\n    git_model_id: int\n        The second digit in the github tag that identifies this model. See\n        https://github.com/deepfakes-models/faceswap-models for more information\n    model_filename: str\n        The name of the model file to be loaded\n\n    Other Parameters\n    ----------------\n    configfile: str, optional\n        Path to a custom configuration ``ini`` file. Default: Use system configfile\n\n    See Also\n    --------\n    plugins.extract.pipeline : The extraction pipeline for calling plugins\n    plugins.extract.align : Aligner plugins\n    plugins.extract._base : Parent class for all extraction plugins\n    plugins.extract.detect._base : Detector parent class for extraction plugins.\n    plugins.extract.align._base : Aligner parent class for extraction plugins.\n    \"\"\"\n\n    _logged_lm_count_once = False\n\n    def __init__(self,\n                 git_model_id: int | None = None,\n                 model_filename: str | None = None,\n                 configfile: str | None = None,\n                 instance: int = 0,\n                 **kwargs) -> None:\n        logger.debug(\"Initializing %s: (configfile: %s)\", self.__class__.__name__, configfile)\n        super().__init__(git_model_id,\n                         model_filename,\n                         configfile=configfile,\n                         instance=instance,\n                         **kwargs)\n        self.input_size = 256  # Override for model specific input_size\n        self.coverage_ratio = 1.0  # Override for model specific coverage_ratio\n\n        # Override if a specific type of landmark data is required:\n        self.landmark_type: LandmarkType | None = None\n\n        self._plugin_type = \"mask\"\n        self._storage_name = self.__module__.rsplit(\".\", maxsplit=1)[-1].replace(\"_\", \"-\")\n        self._storage_centering: CenteringType = \"face\"  # Centering to store the mask at\n        self._storage_size = 128  # Size to store masks at. Leave this at default\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    def _maybe_log_warning(self, face: AlignedFace) -> None:\n        \"\"\" Log a warning, once, if we do not have full facial landmarks\n\n        Parameters\n        ----------\n        face: :class:`~lib.align.aligned_face.AlignedFace`\n            The aligned face object to test the landmark type for\n        \"\"\"\n        if face.landmark_type != LandmarkType.LM_2D_4 or self._logged_lm_count_once:\n            return\n\n        msg = \"are likely to be sub-standard\"\n        msg = \"can not be be generated\" if self.name in (\"Components\", \"Extended\") else msg\n\n        logger.warning(\"Extracted faces do not contain facial landmark data. '%s' masks %s.\",\n                       self.name, msg)\n        self._logged_lm_count_once = True\n\n    def get_batch(self, queue: Queue) -> tuple[bool, MaskerBatch]:\n        \"\"\" Get items for inputting into the masker from the queue in batches\n\n        Items are returned from the ``queue`` in batches of\n        :attr:`~plugins.extract._base.Extractor.batchsize`\n\n        Items are received as :class:`~plugins.extract.extract_media.ExtractMedia` objects and\n        converted to ``dict`` for internal processing.\n\n        To ensure consistent batch sizes for masker the items are split into separate items for\n        each :class:`~lib.align.DetectedFace` object.\n\n        Remember to put ``'EOF'`` to the out queue after processing\n        the final batch\n\n        Outputs items in the following format. All lists are of length\n        :attr:`~plugins.extract._base.Extractor.batchsize`:\n\n        >>> {'filename': [<filenames of source frames>],\n        >>>  'detected_faces': [[<lib.align.DetectedFace objects]]}\n\n        Parameters\n        ----------\n        queue : queue.Queue()\n            The ``queue`` that the plugin will be fed from.\n\n        Returns\n        -------\n        exhausted, bool\n            ``True`` if queue is exhausted, ``False`` if not\n        batch, :class:`~plugins.extract._base.ExtractorBatch`\n            The batch object for the current batch\n        \"\"\"\n        exhausted = False\n        batch = MaskerBatch()\n        idx = 0\n        while idx < self.batchsize:\n            item = self.rollover_collector(queue)\n            if item == \"EOF\":\n                logger.trace(\"EOF received\")  # type: ignore\n                exhausted = True\n                break\n            # Put frames with no faces into the out queue to keep TQDM consistent\n            if not item.detected_faces:\n                self._queues[\"out\"].put(item)\n                continue\n            for f_idx, face in enumerate(item.detected_faces):\n\n                image = item.get_image_copy(self.color_format)\n                roi = np.ones((*item.image_size[:2], 1), dtype=\"float32\")\n\n                if not item.is_aligned:\n                    # Add the ROI mask to image so we can get the ROI mask with a single warp\n                    image = np.concatenate([image, roi], axis=-1)\n\n                feed_face = AlignedFace(face.landmarks_xy,\n                                        image=image,\n                                        centering=self._storage_centering,\n                                        size=self.input_size,\n                                        coverage_ratio=self.coverage_ratio,\n                                        dtype=\"float32\",\n                                        is_aligned=item.is_aligned)\n\n                self._maybe_log_warning(feed_face)\n\n                assert feed_face.face is not None\n                if not item.is_aligned:\n                    # Split roi mask from feed face alpha channel\n                    roi_mask = feed_face.split_mask()\n                else:\n                    # We have to do the warp here as AlignedFace did not perform it\n                    roi_mask = transform_image(roi,\n                                               feed_face.matrix,\n                                               feed_face.size,\n                                               padding=feed_face.padding)\n\n                batch.roi_masks.append(roi_mask)\n                batch.detected_faces.append(face)\n                batch.feed_faces.append(feed_face)\n                batch.filename.append(item.filename)\n                idx += 1\n                if idx == self.batchsize:\n                    frame_faces = len(item.detected_faces)\n                    if f_idx + 1 != frame_faces:\n                        self._rollover = ExtractMedia(\n                            item.filename,\n                            item.image,\n                            detected_faces=item.detected_faces[f_idx + 1:],\n                            is_aligned=item.is_aligned)\n                        logger.trace(\"Rolled over %s faces of %s to next batch \"  # type:ignore\n                                     \"for '%s'\", len(self._rollover.detected_faces), frame_faces,\n                                     item.filename)\n                    break\n        if batch:\n            logger.trace(\"Returning batch: %s\",  # type:ignore\n                         {k: len(v) if isinstance(v, (list, np.ndarray)) else v\n                          for k, v in batch.__dict__.items()})\n        else:\n            logger.trace(item)  # type:ignore\n        return exhausted, batch\n\n    def _predict(self, batch: BatchType) -> MaskerBatch:\n        \"\"\" Just return the masker's predict function \"\"\"\n        assert isinstance(batch, MaskerBatch)\n        assert self.name is not None\n        try:\n            # slightly hacky workaround to deal with landmarks based masks:\n            if self.name.lower() in (\"components\", \"extended\"):\n                feed = np.empty(2, dtype=\"object\")\n                feed[0] = batch.feed\n                feed[1] = batch.feed_faces\n            else:\n                feed = batch.feed\n\n            batch.prediction = self.predict(feed)\n            return batch\n        except tf_errors.ResourceExhaustedError as err:\n            msg = (\"You do not have enough GPU memory available to run detection at the \"\n                   \"selected batch size. You can try a number of things:\"\n                   \"\\n1) Close any other application that is using your GPU (web browsers are \"\n                   \"particularly bad for this).\"\n                   \"\\n2) Lower the batchsize (the amount of images fed into the model) by \"\n                   \"editing the plugin settings (GUI: Settings > Configure extract settings, \"\n                   \"CLI: Edit the file faceswap/config/extract.ini).\"\n                   \"\\n3) Enable 'Single Process' mode.\")\n            raise FaceswapError(msg) from err\n\n    def finalize(self, batch: BatchType) -> Generator[ExtractMedia, None, None]:\n        \"\"\" Finalize the output from Masker\n\n        This should be called as the final task of each `plugin`.\n\n        Pairs the detected faces back up with their original frame before yielding each frame.\n\n        Parameters\n        ----------\n        batch : dict\n            The final ``dict`` from the `plugin` process. It must contain the `keys`:\n            ``detected_faces``, ``filename``, ``feed_faces``, ``roi_masks``\n\n        Yields\n        ------\n        :class:`~plugins.extract.extract_media.ExtractMedia`\n            The :attr:`DetectedFaces` list will be populated for this class with the bounding\n            boxes, landmarks and masks for the detected faces found in the frame.\n        \"\"\"\n        assert isinstance(batch, MaskerBatch)\n        for mask, face, feed_face, roi_mask in zip(batch.prediction,\n                                                   batch.detected_faces,\n                                                   batch.feed_faces,\n                                                   batch.roi_masks):\n            if self.name in (\"Components\", \"Extended\") and not np.any(mask):\n                # Components/Extended masks can return empty when called from the manual tool with\n                # 4 Point ROI landmarks\n                continue\n            self._crop_out_of_bounds(mask, roi_mask)\n            face.add_mask(self._storage_name,\n                          mask,\n                          feed_face.adjusted_matrix,\n                          feed_face.interpolators[1],\n                          storage_size=self._storage_size,\n                          storage_centering=self._storage_centering)\n        del batch.feed\n\n        logger.trace(\"Item out: %s\",  # type: ignore\n                     {key: val.shape if isinstance(val, np.ndarray) else val\n                                      for key, val in batch.__dict__.items()})\n        for filename, face in zip(batch.filename, batch.detected_faces):\n            self._output_faces.append(face)\n            if len(self._output_faces) != self._faces_per_filename[filename]:\n                continue\n\n            output = self._extract_media.pop(filename)\n            output.add_detected_faces(self._output_faces)\n            self._output_faces = []\n            logger.trace(\"Yielding: (filename: '%s', image: %s, \"  # type:ignore\n                         \"detected_faces: %s)\", output.filename, output.image_shape,\n                         len(output.detected_faces))\n            yield output\n\n    # <<< PROTECTED ACCESS METHODS >>> #\n    @classmethod\n    def _resize(cls, image: np.ndarray, target_size: int) -> np.ndarray:\n        \"\"\" resize input and output of mask models appropriately \"\"\"\n        height, width, channels = image.shape\n        image_size = max(height, width)\n        scale = target_size / image_size\n        if scale == 1.:\n            return image\n        method = cv2.INTER_CUBIC if scale > 1. else cv2.INTER_AREA  # pylint:disable=no-member\n        resized = cv2.resize(image, (0, 0), fx=scale, fy=scale, interpolation=method)\n        resized = resized if channels > 1 else resized[..., None]\n        return resized\n\n    @classmethod\n    def _crop_out_of_bounds(cls, mask: np.ndarray, roi_mask: np.ndarray) -> None:\n        \"\"\" Un-mask any area of the predicted mask that falls outside of the original frame.\n\n        Parameters\n        ----------\n        masks: :class:`numpy.ndarray`\n            The predicted masks from the plugin\n        roi_mask: :class:`numpy.ndarray`\n            The roi mask. In frame is white, out of frame is black\n        \"\"\"\n        if np.all(roi_mask):\n            return  # The whole of the face is within the frame\n        roi_mask = roi_mask[..., None] if mask.ndim == 3 else roi_mask\n        mask *= roi_mask\n", "plugins/extract/mask/bisenet_fp_defaults.py": "#!/usr/bin/env python3\n\"\"\"\n    The default options for the faceswap BiSeNet Face Parsing plugin.\n\n    Defaults files should be named <plugin_name>_defaults.py\n    Any items placed into this file will automatically get added to the relevant config .ini files\n    within the faceswap/config folder.\n\n    The following variables should be defined:\n        _HELPTEXT: A string describing what this plugin does\n        _DEFAULTS: A dictionary containing the options, defaults and meta information. The\n                   dictionary should be defined as:\n                       {<option_name>: {<metadata>}}\n\n                   <option_name> should always be lower text.\n                   <metadata> dictionary requirements are listed below.\n\n    The following keys are expected for the _DEFAULTS <metadata> dict:\n        datatype:  [required] A python type class. This limits the type of data that can be\n                   provided in the .ini file and ensures that the value is returned in the\n                   correct type to faceswap. Valid data types are: <class 'int'>, <class 'float'>,\n                   <class 'str'>, <class 'bool'>.\n        default:   [required] The default value for this option.\n        info:      [required] A string describing what this option does.\n        group:     [optional]. A group for grouping options together in the GUI. If not\n                   provided this will not group this option with any others.\n        choices:   [optional] If this option's datatype is of <class 'str'> then valid\n                   selections can be defined here. This validates the option and also enables\n                   a combobox / radio option in the GUI.\n        gui_radio: [optional] If <choices> are defined, this indicates that the GUI should use\n                   radio buttons rather than a combobox to display this option.\n        min_max:   [partial] For <class 'int'> and <class 'float'> data types this is required\n                   otherwise it is ignored. Should be a tuple of min and max accepted values.\n                   This is used for controlling the GUI slider range. Values are not enforced.\n        rounding:  [partial] For <class 'int'> and <class 'float'> data types this is\n                   required otherwise it is ignored. Used for the GUI slider. For floats, this\n                   is the number of decimal places to display. For ints this is the step size.\n        fixed:     [optional] [train only]. Training configurations are fixed when the model is\n                   created, and then reloaded from the state file. Marking an item as fixed=False\n                   indicates that this value can be changed for existing models, and will override\n                   the value saved in the state file with the updated value in config. If not\n                   provided this will default to True.\n\"\"\"\n\n\n_HELPTEXT = (\n    \"BiSeNet Face Parsing options.\\n\"\n    \"Mask ported from https://github.com/zllrunning/face-parsing.PyTorch.\"\n    )\n\n\n_DEFAULTS = {\n    \"batch-size\": {\n        \"default\": 8,\n        \"info\": \"The batch size to use. To a point, higher batch sizes equal better performance, \"\n                \"but setting it too high can harm performance.\\n\"\n                \"\\n\\tNvidia users: If the batchsize is set higher than the your GPU can \"\n                \"accomodate then this will automatically be lowered.\",\n        \"datatype\": int,\n        \"rounding\": 1,\n        \"min_max\": (1, 64),\n        \"choices\": [],\n        \"group\": \"settings\",\n        \"gui_radio\": False,\n        \"fixed\": True\n    },\n    \"cpu\": {\n        \"default\": False,\n        \"info\": \"BiseNet mask still runs fairly quickly on CPU on some setups. Enable \"\n                \"CPU mode here to use the CPU for this masker to save some VRAM at a speed cost.\",\n        \"datatype\": bool,\n        \"group\": \"settings\"\n    },\n    \"weights\": {\n        \"default\": \"faceswap\",\n        \"info\": \"The trained weights to use.\\n\"\n                \"\\n\\tfaceswap - Weights trained on wildly varied Faceswap extracted data to \"\n                \"better handle varying conditions, obstructions, glasses and multiple targets \"\n                \"within a single extracted image.\"\n                \"\\n\\toriginal - The original weights trained on the CelebAMask-HQ dataset.\",\n        \"choices\": [\"faceswap\", \"original\"],\n        \"datatype\": str,\n        \"group\": \"settings\",\n        \"gui_radio\": True,\n    },\n    \"include_ears\": {\n        \"default\": False,\n        \"info\": \"Whether to include ears within the face mask.\",\n        \"datatype\": bool,\n        \"group\": \"settings\"\n    },\n    \"include_hair\": {\n        \"default\": False,\n        \"info\": \"Whether to include hair within the face mask.\",\n        \"datatype\": bool,\n        \"group\": \"settings\"\n    },\n    \"include_glasses\": {\n        \"default\": True,\n        \"info\": \"Whether to include glasses within the face mask.\\n\\tFor 'original' weights \"\n                \"excluding glasses will mask out the lenses as well as the frames.\\n\\tFor \"\n                \"'faceswap' weights, the model has been trained to mask out lenses if eyes cannot \"\n                \"be seen (i.e. dark sunglasses) or just the frames if the eyes can be seen.\",\n        \"datatype\": bool,\n        \"group\": \"settings\"\n    },\n}\n", "plugins/extract/mask/vgg_clear_defaults.py": "#!/usr/bin/env python3\n\"\"\"\n    The default options for the faceswap VGG clear plugin.\n\n    Defaults files should be named <plugin_name>_defaults.py\n    Any items placed into this file will automatically get added to the relevant config .ini files\n    within the faceswap/config folder.\n\n    The following variables should be defined:\n        _HELPTEXT: A string describing what this plugin does\n        _DEFAULTS: A dictionary containing the options, defaults and meta information. The\n                   dictionary should be defined as:\n                       {<option_name>: {<metadata>}}\n\n                   <option_name> should always be lower text.\n                   <metadata> dictionary requirements are listed below.\n\n    The following keys are expected for the _DEFAULTS <metadata> dict:\n        datatype:  [required] A python type class. This limits the type of data that can be\n                   provided in the .ini file and ensures that the value is returned in the\n                   correct type to faceswap. Valid data types are: <class 'int'>, <class 'float'>,\n                   <class 'str'>, <class 'bool'>.\n        default:   [required] The default value for this option.\n        info:      [required] A string describing what this option does.\n        group:     [optional]. A group for grouping options together in the GUI. If not\n                   provided this will not group this option with any others.\n        choices:   [optional] If this option's datatype is of <class 'str'> then valid\n                   selections can be defined here. This validates the option and also enables\n                   a combobox / radio option in the GUI.\n        gui_radio: [optional] If <choices> are defined, this indicates that the GUI should use\n                   radio buttons rather than a combobox to display this option.\n        min_max:   [partial] For <class 'int'> and <class 'float'> data types this is required\n                   otherwise it is ignored. Should be a tuple of min and max accepted values.\n                   This is used for controlling the GUI slider range. Values are not enforced.\n        rounding:  [partial] For <class 'int'> and <class 'float'> data types this is\n                   required otherwise it is ignored. Used for the GUI slider. For floats, this\n                   is the number of decimal places to display. For ints this is the step size.\n        fixed:     [optional] [train only]. Training configurations are fixed when the model is\n                   created, and then reloaded from the state file. Marking an item as fixed=False\n                   indicates that this value can be changed for existing models, and will override\n                   the value saved in the state file with the updated value in config. If not\n                   provided this will default to True.\n\"\"\"\n\n\n_HELPTEXT = (\n    \"VGG_Clear options. Mask designed to provide smart segmentation of mostly frontal faces clear \"\n    \"of obstructions.\\nProfile faces and obstructions may result in sub-par performance.\"\n    )\n\n\n_DEFAULTS = {\n    \"batch-size\": {\n        \"default\": 6,\n        \"info\": \"The batch size to use. To a point, higher batch sizes equal better performance, \"\n                \"but setting it too high can harm performance.\\n\"\n                \"\\n\\tNvidia users: If the batchsize is set higher than the your GPU can \"\n                \"accomodate then this will automatically be lowered.\",\n        \"datatype\": int,\n        \"rounding\": 1,\n        \"min_max\": (1, 64),\n        \"choices\": [],\n        \"group\": \"settings\",\n        \"gui_radio\": False,\n        \"fixed\": True,\n    }\n}\n", "plugins/extract/mask/__init__.py": "", "plugins/extract/mask/vgg_obstructed_defaults.py": "#!/usr/bin/env python3\n\"\"\"\n    The default options for the faceswap VGG obstructed plugin.\n\n    Defaults files should be named <plugin_name>_defaults.py\n    Any items placed into this file will automatically get added to the relevant config .ini files\n    within the faceswap/config folder.\n\n    The following variables should be defined:\n        _HELPTEXT: A string describing what this plugin does\n        _DEFAULTS: A dictionary containing the options, defaults and meta information. The\n                   dictionary should be defined as:\n                       {<option_name>: {<metadata>}}\n\n                   <option_name> should always be lower text.\n                   <metadata> dictionary requirements are listed below.\n\n    The following keys are expected for the _DEFAULTS <metadata> dict:\n        datatype:  [required] A python type class. This limits the type of data that can be\n                   provided in the .ini file and ensures that the value is returned in the\n                   correct type to faceswap. Valid data types are: <class 'int'>, <class 'float'>,\n                   <class 'str'>, <class 'bool'>.\n        default:   [required] The default value for this option.\n        info:      [required] A string describing what this option does.\n        group:     [optional]. A group for grouping options together in the GUI. If not\n                   provided this will not group this option with any others.\n        choices:   [optional] If this option's datatype is of <class 'str'> then valid\n                   selections can be defined here. This validates the option and also enables\n                   a combobox / radio option in the GUI.\n        gui_radio: [optional] If <choices> are defined, this indicates that the GUI should use\n                   radio buttons rather than a combobox to display this option.\n        min_max:   [partial] For <class 'int'> and <class 'float'> data types this is required\n                   otherwise it is ignored. Should be a tuple of min and max accepted values.\n                   This is used for controlling the GUI slider range. Values are not enforced.\n        rounding:  [partial] For <class 'int'> and <class 'float'> data types this is\n                   required otherwise it is ignored. Used for the GUI slider. For floats, this\n                   is the number of decimal places to display. For ints this is the step size.\n        fixed:     [optional] [train only]. Training configurations are fixed when the model is\n                   created, and then reloaded from the state file. Marking an item as fixed=False\n                   indicates that this value can be changed for existing models, and will override\n                   the value saved in the state file with the updated value in config. If not\n                   provided this will default to True.\n\"\"\"\n\n\n_HELPTEXT = (\n    \"VGG_Obstructed options. Mask designed to provide smart segmentation of mostly frontal \"\n    \"faces.\\nThe mask model has been specifically trained to recognize some facial obstructions \"\n    \"(hands and eyeglasses). Profile faces may result in sub-par performance.\"\n    )\n\n\n_DEFAULTS = {\n    \"batch-size\": {\n        \"default\": 2,\n        \"info\": \"The batch size to use. To a point, higher batch sizes equal better performance, \"\n                \"but setting it too high can harm performance.\\n\"\n                \"\\n\\tNvidia users: If the batchsize is set higher than the your GPU can \"\n                \"accomodate then this will automatically be lowered.\",\n        \"datatype\": int,\n        \"rounding\": 1,\n        \"min_max\": (1, 64),\n        \"choices\": [],\n        \"group\": \"settings\",\n        \"gui_radio\": False,\n        \"fixed\": True,\n    }\n}\n", "plugins/extract/mask/vgg_clear.py": "#!/usr/bin/env python3\n\"\"\" VGG Clear face mask plugin. \"\"\"\nfrom __future__ import annotations\nimport logging\nimport typing as T\n\nimport numpy as np\n\n# Ignore linting errors from Tensorflow's thoroughly broken import system\nfrom tensorflow.keras.layers import (  # pylint:disable=import-error\n    Add, Conv2D, Conv2DTranspose, Cropping2D, Dropout, Input, Lambda, MaxPooling2D,\n    ZeroPadding2D)\n\nfrom lib.model.session import KSession\nfrom ._base import BatchType, Masker, MaskerBatch\n\nif T.TYPE_CHECKING:\n    from tensorflow import Tensor\n\nlogger = logging.getLogger(__name__)\n\n\nclass Mask(Masker):\n    \"\"\" Neural network to process face image into a segmentation mask of the face \"\"\"\n    def __init__(self, **kwargs) -> None:\n        git_model_id = 8\n        model_filename = \"Nirkin_300_softmax_v1.h5\"\n        super().__init__(git_model_id=git_model_id, model_filename=model_filename, **kwargs)\n        self.model: KSession\n        self.name = \"VGG Clear\"\n        self.input_size = 300\n        self.vram = 2944\n        self.vram_warnings = 1088  # at BS 1. OOMs at higher batch sizes\n        self.vram_per_batch = 400\n        self.batchsize = self.config[\"batch-size\"]\n\n    def init_model(self) -> None:\n        assert isinstance(self.model_path, str)\n        self.model = VGGClear(self.model_path,\n                              allow_growth=self.config[\"allow_growth\"],\n                              exclude_gpus=self._exclude_gpus)\n        self.model.append_softmax_activation(layer_index=-1)\n        placeholder = np.zeros((self.batchsize, self.input_size, self.input_size, 3),\n                               dtype=\"float32\")\n        self.model.predict(placeholder)\n\n    def process_input(self, batch: BatchType) -> None:\n        \"\"\" Compile the detected faces for prediction \"\"\"\n        assert isinstance(batch, MaskerBatch)\n        input_ = np.array([T.cast(np.ndarray, feed.face)[..., :3]\n                           for feed in batch.feed_faces], dtype=\"float32\")\n        batch.feed = input_ - np.mean(input_, axis=(1, 2))[:, None, None, :]\n        logger.trace(\"feed shape: %s\", batch.feed.shape)  # type: ignore\n\n    def predict(self, feed: np.ndarray) -> np.ndarray:\n        \"\"\" Run model to get predictions \"\"\"\n        predictions = self.model.predict(feed)\n        assert isinstance(predictions, np.ndarray)\n        return predictions[..., -1]\n\n    def process_output(self, batch: BatchType) -> None:\n        \"\"\" Compile found faces for output \"\"\"\n        return\n\n\nclass VGGClear(KSession):\n    \"\"\" VGG Clear mask for Faceswap.\n\n    Caffe model re-implemented in Keras by Kyle Vrooman.\n    Re-implemented for Tensorflow 2 by TorzDF\n\n    Parameters\n    ----------\n    model_path: str\n        The path to the keras model file\n    allow_growth: bool\n        Enable the Tensorflow GPU allow_growth configuration option. This option prevents\n        Tensorflow from allocating all of the GPU VRAM, but can lead to higher fragmentation and\n        slower performance\n    exclude_gpus: list\n        A list of indices correlating to connected GPUs that Tensorflow should not use. Pass\n        ``None`` to not exclude any GPUs\n\n    References\n    ----------\n    On Face Segmentation, Face Swapping, and Face Perception (https://arxiv.org/abs/1704.06729)\n\n    Source Implementation: https://github.com/YuvalNirkin/face_segmentation\n\n    Model file sourced from:\n    https://github.com/YuvalNirkin/face_segmentation/releases/download/1.1/face_seg_fcn8s_300_no_aug.zip\n\n    \"\"\"\n    def __init__(self,\n                 model_path: str,\n                 allow_growth: bool,\n                 exclude_gpus: list[int] | None):\n        super().__init__(\"VGG Obstructed\",\n                         model_path,\n                         allow_growth=allow_growth,\n                         exclude_gpus=exclude_gpus)\n        self.define_model(self._model_definition)\n        self.load_model_weights()\n\n    @classmethod\n    def _model_definition(cls) -> tuple[Tensor, Tensor]:\n        \"\"\" Definition of the VGG Obstructed Model.\n\n        Returns\n        -------\n        tuple\n            The tensor input to the model and tensor output to the model for compilation by\n            :func`define_model`\n        \"\"\"\n        input_ = Input(shape=(300, 300, 3))\n        var_x = ZeroPadding2D(padding=((100, 100), (100, 100)), name=\"zero_padding2d_1\")(input_)\n\n        var_x = _ConvBlock(1, 64, 2)(var_x)\n        var_x = _ConvBlock(2, 128, 2)(var_x)\n        pool3 = _ConvBlock(3, 256, 3)(var_x)\n        pool4 = _ConvBlock(4, 512, 3)(pool3)\n        var_x = _ConvBlock(5, 512, 3)(pool4)\n\n        score_pool3 = _ScorePool(3, 0.0001, (9, 8))(pool3)\n        score_pool4 = _ScorePool(4, 0.01, (5, 5))(pool4)\n\n        var_x = Conv2D(4096, 7, activation=\"relu\", name=\"fc6\")(var_x)\n        var_x = Dropout(rate=0.5, name=\"drop6\")(var_x)\n        var_x = Conv2D(4096, 1, activation=\"relu\", name=\"fc7\")(var_x)\n        var_x = Dropout(rate=0.5, name=\"drop7\")(var_x)\n        var_x = Conv2D(2, 1, activation=\"linear\", name=\"score_fr_r\")(var_x)\n        var_x = Conv2DTranspose(2,\n                                4,\n                                strides=2,\n                                activation=\"linear\",\n                                use_bias=False, name=\"upscore2_r\")(var_x)\n\n        var_x = Add(name=\"fuse_pool4\")([var_x, score_pool4])\n        var_x = Conv2DTranspose(2,\n                                4,\n                                strides=2,\n                                activation=\"linear\",\n                                use_bias=False,\n                                name=\"upscore_pool4_r\")(var_x)\n        var_x = Add(name=\"fuse_pool3\")([var_x, score_pool3])\n        var_x = Conv2DTranspose(2,\n                                16,\n                                strides=8,\n                                activation=\"linear\",\n                                use_bias=False,\n                                name=\"upscore8_r\")(var_x)\n        var_x = Cropping2D(cropping=((31, 45), (31, 45)), name=\"score\")(var_x)\n        return input_, var_x\n\n\nclass _ConvBlock():  # pylint:disable=too-few-public-methods\n    \"\"\" Convolutional loop with max pooling layer for VGG Clear.\n\n    Parameters\n    ----------\n    level: int\n        For naming. The current level for this convolutional loop\n    filters: int\n        The number of filters that should appear in each Conv2D layer\n    iterations: int\n        The number of consecutive Conv2D layers to create\n    \"\"\"\n    def __init__(self, level: int, filters: int, iterations: int) -> None:\n        self._name = f\"conv{level}_\"\n        self._level = level\n        self._filters = filters\n        self._iterator = range(1, iterations + 1)\n\n    def __call__(self, inputs: Tensor) -> Tensor:\n        \"\"\" Call the convolutional loop.\n\n        Parameters\n        ----------\n        inputs: tensor\n            The input tensor to the block\n\n        Returns\n        -------\n        tensor\n            The output tensor from the convolutional block\n        \"\"\"\n        var_x = inputs\n        for i in self._iterator:\n            padding = \"valid\" if self._level == i == 1 else \"same\"\n            var_x = Conv2D(self._filters,\n                           3,\n                           padding=padding,\n                           activation=\"relu\",\n                           name=f\"{self._name}{i}\")(var_x)\n        var_x = MaxPooling2D(padding=\"same\",\n                             strides=(2, 2),\n                             name=f\"pool{self._level}\")(var_x)\n        return var_x\n\n\nclass _ScorePool():  # pylint:disable=too-few-public-methods\n    \"\"\" Cropped scaling of the pooling layer.\n\n    Parameters\n    ----------\n    level: int\n        For naming. The current level for this score pool\n    scale: float\n        The scaling to apply to the pool\n    crop: tuple\n        The amount of 2D cropping to apply. Tuple of `ints`\n    \"\"\"\n    def __init__(self, level: int, scale: float, crop: tuple[int, int]):\n        self._name = f\"_pool{level}\"\n        self._cropping = (crop, crop)\n        self._scale = scale\n\n    def __call__(self, inputs: Tensor) -> Tensor:\n        \"\"\" Score pool block.\n\n        Parameters\n        ----------\n        inputs: tensor\n            The input tensor to the block\n\n        Returns\n        -------\n        tensor\n            The output tensor from the score pool block\n        \"\"\"\n        var_x = Lambda(lambda x: x * self._scale, name=\"scale\" + self._name)(inputs)\n        var_x = Conv2D(2, 1, activation=\"linear\", name=\"score\" + self._name + \"_r\")(var_x)\n        var_x = Cropping2D(cropping=self._cropping, name=\"score\" + self._name + \"c\")(var_x)\n        return var_x\n", "plugins/extract/mask/unet_dfl_defaults.py": "#!/usr/bin/env python3\n\"\"\"\n    The default options for the faceswap UNET dfl plugin.\n\n    Defaults files should be named <plugin_name>_defaults.py\n    Any items placed into this file will automatically get added to the relevant config .ini files\n    within the faceswap/config folder.\n\n    The following variables should be defined:\n        _HELPTEXT: A string describing what this plugin does\n        _DEFAULTS: A dictionary containing the options, defaults and meta information. The\n                   dictionary should be defined as:\n                       {<option_name>: {<metadata>}}\n\n                   <option_name> should always be lower text.\n                   <metadata> dictionary requirements are listed below.\n\n    The following keys are expected for the _DEFAULTS <metadata> dict:\n        datatype:  [required] A python type class. This limits the type of data that can be\n                   provided in the .ini file and ensures that the value is returned in the\n                   correct type to faceswap. Valid data types are: <class 'int'>, <class 'float'>,\n                   <class 'str'>, <class 'bool'>.\n        default:   [required] The default value for this option.\n        info:      [required] A string describing what this option does.\n        group:     [optional]. A group for grouping options together in the GUI. If not\n                   provided this will not group this option with any others.\n        choices:   [optional] If this option's datatype is of <class 'str'> then valid\n                   selections can be defined here. This validates the option and also enables\n                   a combobox / radio option in the GUI.\n        gui_radio: [optional] If <choices> are defined, this indicates that the GUI should use\n                   radio buttons rather than a combobox to display this option.\n        min_max:   [partial] For <class 'int'> and <class 'float'> data types this is required\n                   otherwise it is ignored. Should be a tuple of min and max accepted values.\n                   This is used for controlling the GUI slider range. Values are not enforced.\n        rounding:  [partial] For <class 'int'> and <class 'float'> data types this is\n                   required otherwise it is ignored. Used for the GUI slider. For floats, this\n                   is the number of decimal places to display. For ints this is the step size.\n        fixed:     [optional] [train only]. Training configurations are fixed when the model is\n                   created, and then reloaded from the state file. Marking an item as fixed=False\n                   indicates that this value can be changed for existing models, and will override\n                   the value saved in the state file with the updated value in config. If not\n                   provided this will default to True.\n\"\"\"\n\n\n_HELPTEXT = (\n    \"UNET_DFL options. Mask designed to provide smart segmentation of mostly frontal faces.\\n\"\n    \"The mask model has been trained by community members. Insert more commentary on testing \"\n    \"here. Profile faces may result in sub-par performance.\"\n    )\n\n\n_DEFAULTS = {\n    \"batch-size\": {\n        \"default\": 8,\n        \"info\": \"The batch size to use. To a point, higher batch sizes equal better performance, \"\n                \"but setting it too high can harm performance.\\n\"\n                \"\\n\\tNvidia users: If the batchsize is set higher than the your GPU can \"\n                \"accomodate then this will automatically be lowered.\",\n        \"datatype\": int,\n        \"rounding\": 1,\n        \"min_max\": (1, 64),\n        \"choices\": [],\n        \"group\": \"settings\",\n        \"gui_radio\": False,\n        \"fixed\": True,\n    }\n}\n", "plugins/extract/mask/custom_defaults.py": "#!/usr/bin/env python3\n\"\"\"\n    The default options for the faceswap BiSeNet Face Parsing plugin.\n\n    Defaults files should be named <plugin_name>_defaults.py\n    Any items placed into this file will automatically get added to the relevant config .ini files\n    within the faceswap/config folder.\n\n    The following variables should be defined:\n        _HELPTEXT: A string describing what this plugin does\n        _DEFAULTS: A dictionary containing the options, defaults and meta information. The\n                   dictionary should be defined as:\n                       {<option_name>: {<metadata>}}\n\n                   <option_name> should always be lower text.\n                   <metadata> dictionary requirements are listed below.\n\n    The following keys are expected for the _DEFAULTS <metadata> dict:\n        datatype:  [required] A python type class. This limits the type of data that can be\n                   provided in the .ini file and ensures that the value is returned in the\n                   correct type to faceswap. Valid data types are: <class 'int'>, <class 'float'>,\n                   <class 'str'>, <class 'bool'>.\n        default:   [required] The default value for this option.\n        info:      [required] A string describing what this option does.\n        group:     [optional]. A group for grouping options together in the GUI. If not\n                   provided this will not group this option with any others.\n        choices:   [optional] If this option's datatype is of <class 'str'> then valid\n                   selections can be defined here. This validates the option and also enables\n                   a combobox / radio option in the GUI.\n        gui_radio: [optional] If <choices> are defined, this indicates that the GUI should use\n                   radio buttons rather than a combobox to display this option.\n        min_max:   [partial] For <class 'int'> and <class 'float'> data types this is required\n                   otherwise it is ignored. Should be a tuple of min and max accepted values.\n                   This is used for controlling the GUI slider range. Values are not enforced.\n        rounding:  [partial] For <class 'int'> and <class 'float'> data types this is\n                   required otherwise it is ignored. Used for the GUI slider. For floats, this\n                   is the number of decimal places to display. For ints this is the step size.\n        fixed:     [optional] [train only]. Training configurations are fixed when the model is\n                   created, and then reloaded from the state file. Marking an item as fixed=False\n                   indicates that this value can be changed for existing models, and will override\n                   the value saved in the state file with the updated value in config. If not\n                   provided this will default to True.\n\"\"\"\n\n\n_HELPTEXT = (\n    \"Custom (dummy) Mask options..\\n\"\n    \"The custom mask just fills a face patch with all 0's (masked out) or all 1's (masked in) for \"\n    \"later manual editing. It does not use the GPU for creation.\"\n    )\n\n\n_DEFAULTS = {\n    \"batch-size\": {\n        \"default\": 8,\n        \"info\": \"The batch size to use. To a point, higher batch sizes equal better performance, \"\n                \"but setting it too high can harm performance.\",\n        \"datatype\": int,\n        \"rounding\": 1,\n        \"min_max\": (1, 64),\n        \"group\": \"settings\"\n    },\n    \"centering\": {\n        \"default\": \"face\",\n        \"info\": \"Whether to create a dummy mask with face or head centering.\",\n        \"choices\": [\"face\", \"head\"],\n        \"datatype\": str,\n        \"group\": \"settings\",\n        \"gui_radio\": True\n    },\n    \"fill\": {\n        \"default\": False,\n        \"info\": \"Whether the mask should be filled (True) in which case the custom mask will be \"\n                \"created with the whole area masked in (i.e. you would need to manually edit out \"\n                \"the background) or unfilled (False) in which case you would need to manually \"\n                \"edit in the face.\",\n        \"datatype\": bool,\n        \"group\": \"settings\",\n        \"gui_radio\": True,\n    },\n}\n", "plugins/extract/mask/custom.py": "#!/usr/bin/env python3\n\"\"\" Components Mask for faceswap.py \"\"\"\nimport logging\nimport numpy as np\nfrom ._base import BatchType, Masker\n\nlogger = logging.getLogger(__name__)\n\n\nclass Mask(Masker):\n    \"\"\" A mask that fills the whole face area with 1s or 0s (depending on user selected settings)\n    for custom editing. \"\"\"\n    def __init__(self, **kwargs):\n        git_model_id = None\n        model_filename = None\n        super().__init__(git_model_id=git_model_id, model_filename=model_filename, **kwargs)\n        self.input_size = 256\n        self.name = \"Custom\"\n        self.vram = 0  # Doesn't use GPU\n        self.vram_per_batch = 0\n        self.batchsize = self.config[\"batch-size\"]\n        self._storage_centering = self.config[\"centering\"]\n        # Separate storage for face and head masks\n        self._storage_name = f\"{self._storage_name}_{self._storage_centering}\"\n\n    def init_model(self) -> None:\n        logger.debug(\"No mask model to initialize\")\n\n    def process_input(self, batch: BatchType) -> None:\n        \"\"\" Compile the detected faces for prediction \"\"\"\n        batch.feed = np.zeros((self.batchsize, self.input_size, self.input_size, 1),\n                              dtype=\"float32\")\n\n    def predict(self, feed: np.ndarray) -> np.ndarray:\n        \"\"\" Run model to get predictions \"\"\"\n        if self.config[\"fill\"]:\n            feed[:] = 1.0\n        return feed\n\n    def process_output(self, batch: BatchType) -> None:\n        \"\"\" Compile found faces for output \"\"\"\n        return\n", "plugins/extract/mask/bisenet_fp.py": "#!/usr/bin/env python3\n\"\"\" BiSeNet Face-Parsing mask plugin\n\nArchitecture and Pre-Trained Model ported from PyTorch to Keras by TorzDF from\nhttps://github.com/zllrunning/face-parsing.PyTorch\n\"\"\"\nfrom __future__ import annotations\nimport logging\nimport typing as T\n\nimport numpy as np\n\n# Ignore linting errors from Tensorflow's thoroughly broken import system\nfrom tensorflow.keras import backend as K  # pylint:disable=import-error\nfrom tensorflow.keras.layers import (  # pylint:disable=import-error\n    Activation, Add, BatchNormalization, Concatenate, Conv2D, GlobalAveragePooling2D, Input,\n    MaxPooling2D, Multiply, Reshape, UpSampling2D, ZeroPadding2D)\n\nfrom lib.model.session import KSession\nfrom plugins.extract._base import _get_config\nfrom ._base import BatchType, Masker, MaskerBatch\n\nif T.TYPE_CHECKING:\n    from tensorflow import Tensor\n\nlogger = logging.getLogger(__name__)\n\n\nclass Mask(Masker):\n    \"\"\" Neural network to process face image into a segmentation mask of the face \"\"\"\n    def __init__(self, **kwargs) -> None:\n        self._is_faceswap, version = self._check_weights_selection(kwargs.get(\"configfile\"))\n\n        git_model_id = 14\n        model_filename = f\"bisnet_face_parsing_v{version}.h5\"\n        super().__init__(git_model_id=git_model_id, model_filename=model_filename, **kwargs)\n\n        self.model: KSession\n        self.name = \"BiSeNet - Face Parsing\"\n        self.input_size = 512\n        self.color_format = \"RGB\"\n        self.vram = 2304 if not self.config[\"cpu\"] else 0\n        self.vram_warnings = 256 if not self.config[\"cpu\"] else 0\n        self.vram_per_batch = 64 if not self.config[\"cpu\"] else 0\n        self.batchsize = self.config[\"batch-size\"]\n\n        self._segment_indices = self._get_segment_indices()\n        self._storage_centering = \"head\" if self.config[\"include_hair\"] else \"face\"\n        # Separate storage for face and head masks\n        self._storage_name = f\"{self._storage_name}_{self._storage_centering}\"\n\n    def _check_weights_selection(self, configfile: str | None) -> tuple[bool, int]:\n        \"\"\" Check which weights have been selected.\n\n        This is required for passing along the correct file name for the corresponding weights\n        selection, so config needs to be loaded and scanned prior to parent loading it.\n\n        Parameters\n        ----------\n        configfile: str\n            Path to a custom configuration ``ini`` file. ``None`` to use system configfile\n\n        Returns\n        -------\n        tuple (bool, int)\n            First position is ``True`` if `faceswap` trained weights have been selected.\n            ``False`` if `original` weights have been selected.\n            Second position is the version of the model to use (``1`` for non-faceswap, ``1`` if\n            faceswap and full-head model is required. ``3`` if faceswap and full-face is required)\n        \"\"\"\n        config = _get_config(\".\".join(self.__module__.split(\".\")[-2:]), configfile=configfile)\n        is_faceswap = config.get(\"weights\", \"faceswap\").lower() == \"faceswap\"\n        version = 1 if not is_faceswap else 2 if config.get(\"include_hair\") else 3\n        return is_faceswap, version\n\n    def _get_segment_indices(self) -> list[int]:\n        \"\"\" Obtain the segment indices to include within the face mask area based on user\n        configuration settings.\n\n        Returns\n        -------\n        list\n            The segment indices to include within the face mask area\n\n        Notes\n        -----\n        'original' Model segment indices:\n        0: background, 1: skin, 2: left brow, 3: right brow, 4: left eye, 5: right eye, 6: glasses\n        7: left ear, 8: right ear, 9: earing, 10: nose, 11: mouth, 12: upper lip, 13: lower_lip,\n        14: neck, 15: neck ?, 16: cloth, 17: hair, 18: hat\n\n        'faceswap' Model segment indices:\n        0: background, 1: skin, 2: ears, 3: hair, 4: glasses\n        \"\"\"\n        retval = [1] if self._is_faceswap else [1, 2, 3, 4, 5, 10, 11, 12, 13]\n\n        if self.config[\"include_glasses\"]:\n            retval.append(4 if self._is_faceswap else 6)\n        if self.config[\"include_ears\"]:\n            retval.extend([2] if self._is_faceswap else [7, 8, 9])\n        if self.config[\"include_hair\"]:\n            retval.append(3 if self._is_faceswap else 17)\n        logger.debug(\"Selected segment indices: %s\", retval)\n        return retval\n\n    def init_model(self) -> None:\n        \"\"\" Initialize the BiSeNet Face Parsing model. \"\"\"\n        assert isinstance(self.model_path, str)\n        lbls = 5 if self._is_faceswap else 19\n        self.model = BiSeNet(self.model_path,\n                             self.config[\"allow_growth\"],\n                             self._exclude_gpus,\n                             self.input_size,\n                             lbls,\n                             self.config[\"cpu\"])\n\n        placeholder = np.zeros((self.batchsize, self.input_size, self.input_size, 3),\n                               dtype=\"float32\")\n        self.model.predict(placeholder)\n\n    def process_input(self, batch: BatchType) -> None:\n        \"\"\" Compile the detected faces for prediction \"\"\"\n        assert isinstance(batch, MaskerBatch)\n        mean = (0.384, 0.314, 0.279) if self._is_faceswap else (0.485, 0.456, 0.406)\n        std = (0.324, 0.286, 0.275) if self._is_faceswap else (0.229, 0.224, 0.225)\n\n        batch.feed = ((np.array([T.cast(np.ndarray, feed.face)[..., :3]\n                                 for feed in batch.feed_faces],\n                                dtype=\"float32\") / 255.0) - mean) / std\n        logger.trace(\"feed shape: %s\", batch.feed.shape)  # type:ignore\n\n    def predict(self, feed: np.ndarray) -> np.ndarray:\n        \"\"\" Run model to get predictions \"\"\"\n        return self.model.predict(feed)[0]\n\n    def process_output(self, batch: BatchType) -> None:\n        \"\"\" Compile found faces for output \"\"\"\n        pred = batch.prediction.argmax(-1).astype(\"uint8\")\n        batch.prediction = np.isin(pred, self._segment_indices).astype(\"float32\")\n\n# BiSeNet Face-Parsing Model\n\n# MIT License\n\n# Copyright (c) 2019 zll\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n\n_NAME_TRACKER: set[str] = set()\n\n\ndef _get_name(name: str, start_idx: int = 1) -> str:\n    \"\"\" Auto numbering to keep track of layer names.\n\n    Names are kept the same as the PyTorch original model, to enable easier porting of weights.\n\n    Names are tracked and auto-appended with an integer to ensure they are unique.\n\n    Parameters\n    ----------\n    name: str\n        The name of the layer to get auto named.\n    start_idx\n        The first index number to start auto naming layers with the same name. Usually 0 or 1.\n        Pass -1 if the name should not be auto-named (i.e. should not have an integer appended\n        to the end)\n\n    Returns\n    -------\n    str\n        A unique version of the original name\n    \"\"\"\n    i = start_idx\n    while True:\n        retval = f\"{name}{i}\" if i != -1 else name\n        if retval not in _NAME_TRACKER:\n            break\n        i += 1\n    _NAME_TRACKER.add(retval)\n    return retval\n\n\nclass ConvBn():  # pylint:disable=too-few-public-methods\n    \"\"\" Convolutional 3D with Batch Normalization block.\n\n    Parameters\n    ----------\n    filters: int\n        The dimensionality of the output space (i.e. the number of output filters in the\n        convolution).\n    kernel_size: int, optional\n        The height and width of the 2D convolution window. Default: `3`\n    strides: int, optional\n        The strides of the convolution along the height and width. Default: `1`\n    padding: int, optional\n        The amount of padding to apply prior to the first Convolutional Layer. Default: `1`\n    activation: bool\n        Whether to include ReLu Activation at the end of the block. Default: ``True``\n    prefix: str, optional\n        The prefix to name the layers within the block. Default: ``\"\"`` (empty string, i.e. no\n        prefix)\n    start_idx: int, optional\n        The starting index for naming the layers within the block. See :func:`_get_name` for\n        more information. Default: `1`\n    \"\"\"\n    def __init__(self, filters: int,\n                 kernel_size: int = 3,\n                 strides: int = 1,\n                 padding: int = 1,\n                 activation: int = True,\n                 prefix: str = \"\",\n                 start_idx: int = 1) -> None:\n        self._filters = filters\n        self._kernel_size = kernel_size\n        self._strides = strides\n        self._padding = padding\n        self._activation = activation\n        self._prefix = f\"{prefix}.\" if prefix else prefix\n        self._start_idx = start_idx\n\n    def __call__(self, inputs: Tensor) -> Tensor:\n        \"\"\" Call the Convolutional Batch Normalization block.\n\n        Parameters\n        ----------\n        inputs: tensor\n            The input to the block\n\n        Returns\n        -------\n        tensor\n            The output from the block\n        \"\"\"\n        var_x = inputs\n        if self._padding > 0 and self._kernel_size != 1:\n            var_x = ZeroPadding2D(self._padding,\n                                  name=_get_name(f\"{self._prefix}zeropad\",\n                                                 start_idx=self._start_idx))(var_x)\n        padding = \"valid\" if self._padding != -1 else \"same\"\n        var_x = Conv2D(self._filters,\n                       self._kernel_size,\n                       strides=self._strides,\n                       padding=padding,\n                       use_bias=False,\n                       name=_get_name(f\"{self._prefix}conv\", start_idx=self._start_idx))(var_x)\n        var_x = BatchNormalization(epsilon=1e-5,\n                                   name=_get_name(f\"{self._prefix}bn\",\n                                                  start_idx=self._start_idx))(var_x)\n        if self._activation:\n            var_x = Activation(\"relu\",\n                               name=_get_name(f\"{self._prefix}relu\",\n                                              start_idx=self._start_idx))(var_x)\n        return var_x\n\n\nclass ResNet18():  # pylint:disable=too-few-public-methods\n    \"\"\" ResNet 18 block. Used at the start of BiSeNet Face Parsing. \"\"\"\n    def __init__(self):\n        self._feature_index = 1 if K.image_data_format() == \"channels_first\" else -1\n\n    def _basic_block(self, inputs: Tensor, prefix: str, filters: int, strides: int = 1) -> Tensor:\n        \"\"\" The basic building block for ResNet 18.\n\n        Parameters\n        ----------\n        inputs: tensor\n            The input to the block\n        prefix: str\n            The prefix to name the layers within the block\n        filters: int\n            The dimensionality of the output space (i.e. the number of output filters in the\n            convolution).\n        strides: int, optional\n            The strides of the convolution along the height and width. Default: `1`\n\n        Returns\n        -------\n        tensor\n            The output from the block\n        \"\"\"\n        res = ConvBn(filters, strides=strides, padding=1, prefix=prefix)(inputs)\n        res = ConvBn(filters, strides=1, padding=1, activation=False, prefix=prefix)(res)\n\n        shortcut = inputs\n        filts = (K.int_shape(shortcut)[self._feature_index], K.int_shape(res)[self._feature_index])\n        if strides != 1 or filts[0] != filts[1]:  # Downsample\n            name = f\"{prefix}.downsample.\"\n            shortcut = Conv2D(filters, 1,\n                              strides=strides,\n                              use_bias=False,\n                              name=_get_name(f\"{name}\", start_idx=0))(shortcut)\n            shortcut = BatchNormalization(epsilon=1e-5,\n                                          name=_get_name(f\"{name}\", start_idx=0))(shortcut)\n\n        var_x = Add(name=f\"{prefix}.add\")([res, shortcut])\n        var_x = Activation(\"relu\", name=f\"{prefix}.relu\")(var_x)\n        return var_x\n\n    def _basic_layer(self,\n                     inputs: Tensor,\n                     prefix: str,\n                     filters: int,\n                     num_blocks: int,\n                     strides: int = 1) -> Tensor:\n        \"\"\" The basic layer for ResNet 18. Recursively builds from :func:`_basic_block`.\n\n        Parameters\n        ----------\n        inputs: tensor\n            The input to the block\n        prefix: str\n            The prefix to name the layers within the block\n        filters: int\n            The dimensionality of the output space (i.e. the number of output filters in the\n            convolution).\n        num_blocks: int\n            The number of basic blocks to recursively build\n        strides: int, optional\n            The strides of the convolution along the height and width. Default: `1`\n\n        Returns\n        -------\n        tensor\n            The output from the block\n        \"\"\"\n        var_x = self._basic_block(inputs, f\"{prefix}.0\", filters, strides=strides)\n        for i in range(num_blocks - 1):\n            var_x = self._basic_block(var_x, f\"{prefix}.{i + 1}\", filters, strides=1)\n        return var_x\n\n    def __call__(self, inputs: Tensor) -> Tensor:\n        \"\"\" Call the ResNet 18 block.\n\n        Parameters\n        ----------\n        inputs: tensor\n            The input to the block\n\n        Returns\n        -------\n        tensor\n            The output from the block\n        \"\"\"\n        var_x = ConvBn(64, kernel_size=7, strides=2, padding=3, prefix=\"cp.resnet\")(inputs)\n        var_x = ZeroPadding2D(1, name=\"cp.resnet.zeropad\")(var_x)\n        var_x = MaxPooling2D(pool_size=3, strides=2, name=\"cp.resnet.maxpool\")(var_x)\n\n        var_x = self._basic_layer(var_x, \"cp.resnet.layer1\", 64, 2)\n        feat8 = self._basic_layer(var_x, \"cp.resnet.layer2\", 128, 2, strides=2)\n        feat16 = self._basic_layer(feat8, \"cp.resnet.layer3\", 256, 2, strides=2)\n        feat32 = self._basic_layer(feat16, \"cp.resnet.layer4\", 512, 2, strides=2)\n\n        return feat8, feat16, feat32\n\n\nclass AttentionRefinementModule():  # pylint:disable=too-few-public-methods\n    \"\"\" The Attention Refinement block for BiSeNet Face Parsing\n\n    Parameters\n    ----------\n    filters: int\n        The dimensionality of the output space (i.e. the number of output filters in the\n        convolution).\n    \"\"\"\n    def __init__(self, filters: int) -> None:\n        self._filters = filters\n\n    def __call__(self, inputs: Tensor, feats: int) -> Tensor:\n        \"\"\" Call the Attention Refinement block.\n\n        Parameters\n        ----------\n        inputs: tensor\n            The input to the block\n        feats: int\n            The number of features. Used for naming.\n\n        Returns\n        -------\n        tensor\n            The output from the block\n        \"\"\"\n        prefix = f\"cp.arm{feats}\"\n        feat = ConvBn(self._filters, prefix=f\"{prefix}.conv\", start_idx=-1, padding=-1)(inputs)\n        atten = GlobalAveragePooling2D(name=f\"{prefix}.avgpool\")(feat)\n        atten = Reshape((1, 1, K.int_shape(atten)[-1]))(atten)\n        atten = Conv2D(self._filters, 1, use_bias=False, name=f\"{prefix}.conv_atten\")(atten)\n        atten = BatchNormalization(epsilon=1e-5, name=f\"{prefix}.bn_atten\")(atten)\n        atten = Activation(\"sigmoid\", name=f\"{prefix}.sigmoid\")(atten)\n        var_x = Multiply(name=f\"{prefix}.mul\")([feat, atten])\n        return var_x\n\n\nclass ContextPath():  # pylint:disable=too-few-public-methods\n    \"\"\" The Context Path block for BiSeNet Face Parsing. \"\"\"\n    def __init__(self):\n        self._resnet = ResNet18()\n\n    def __call__(self, inputs: Tensor) -> Tensor:\n        \"\"\" Call the Context Path block.\n\n        Parameters\n        ----------\n        inputs: tensor\n            The input to the block\n\n        Returns\n        -------\n        tensor\n            The output from the block\n        \"\"\"\n        feat8, feat16, feat32 = self._resnet(inputs)\n\n        avg = GlobalAveragePooling2D(name=\"cp.avgpool\")(feat32)\n        avg = Reshape((1, 1, K.int_shape(avg)[-1]))(avg)\n        avg = ConvBn(128, kernel_size=1, padding=0, prefix=\"cp.conv_avg\", start_idx=-1)(avg)\n\n        avg_up = UpSampling2D(size=K.int_shape(feat32)[1:3], name=\"cp.upsample\")(avg)\n\n        feat32 = AttentionRefinementModule(128)(feat32, 32)\n        feat32 = Add(name=\"cp.add\")([feat32, avg_up])\n        feat32 = UpSampling2D(name=\"cp.upsample1\")(feat32)\n        feat32 = ConvBn(128, kernel_size=3, prefix=\"cp.conv_head32\", start_idx=-1)(feat32)\n\n        feat16 = AttentionRefinementModule(128)(feat16, 16)\n        feat16 = Add(name=\"cp.add2\")([feat16, feat32])\n        feat16 = UpSampling2D(name=\"cp.upsample2\")(feat16)\n        feat16 = ConvBn(128, kernel_size=3, prefix=\"cp.conv_head16\", start_idx=-1)(feat16)\n\n        return feat8, feat16, feat32\n\n\nclass FeatureFusionModule():  # pylint:disable=too-few-public-methods\n    \"\"\" The Feature Fusion block for BiSeNet Face Parsing\n\n    Parameters\n    ----------\n    filters: int\n        The dimensionality of the output space (i.e. the number of output filters in the\n        convolution).\n    \"\"\"\n    def __init__(self, filters: int) -> None:\n        self._filters = filters\n\n    def __call__(self, inputs: Tensor) -> Tensor:\n        \"\"\" Call the Feature Fusion block.\n\n        Parameters\n        ----------\n        inputs: tensor\n            The input to the block\n\n        Returns\n        -------\n        tensor\n            The output from the block\n        \"\"\"\n        feat = Concatenate(name=\"ffm.concat\")(inputs)\n        feat = ConvBn(self._filters,\n                      kernel_size=1,\n                      padding=0,\n                      prefix=\"ffm.convblk\",\n                      start_idx=-1)(feat)\n\n        atten = GlobalAveragePooling2D(name=\"ffm.avgpool\")(feat)\n        atten = Reshape((1, 1, K.int_shape(atten)[-1]))(atten)\n        atten = Conv2D(self._filters // 4, 1, use_bias=False, name=\"ffm.conv1\")(atten)\n        atten = Activation(\"relu\", name=\"ffm.relu\")(atten)\n        atten = Conv2D(self._filters, 1, use_bias=False, name=\"ffm.conv2\")(atten)\n        atten = Activation(\"sigmoid\", name=\"ffm.sigmoid\")(atten)\n\n        var_x = Multiply(name=\"ffm.mul\")([feat, atten])\n        var_x = Add(name=\"ffm.add\")([var_x, feat])\n        return var_x\n\n\nclass BiSeNetOutput():  # pylint:disable=too-few-public-methods\n    \"\"\" The BiSeNet Output block for Face Parsing\n\n    Parameters\n    ----------\n    filters: int\n        The dimensionality of the output space (i.e. the number of output filters in the\n        convolution).\n    num_class: int\n        The number of classes to generate\n    label, str, optional\n        The label for this output (for naming). Default: `\"\"` (i.e. empty string, or no label)\n    \"\"\"\n    def __init__(self, filters: int, num_classes: int, label: str = \"\") -> None:\n        self._filters = filters\n        self._num_classes = num_classes\n        self._label = label\n\n    def __call__(self, inputs: Tensor) -> Tensor:\n        \"\"\" Call the BiSeNet Output block.\n\n        Parameters\n        ----------\n        inputs: tensor\n            The input to the block\n\n        Returns\n        -------\n        tensor\n            The output from the block\n        \"\"\"\n        var_x = ConvBn(self._filters, prefix=f\"conv_out{self._label}.conv\", start_idx=-1)(inputs)\n        var_x = Conv2D(self._num_classes, 1,\n                       use_bias=False, name=f\"conv_out{self._label}.conv_out\")(var_x)\n        return var_x\n\n\nclass BiSeNet(KSession):\n    \"\"\" BiSeNet Face-Parsing Mask from https://github.com/zllrunning/face-parsing.PyTorch\n\n    PyTorch model implemented in Keras by TorzDF\n\n    Parameters\n    ----------\n    model_path: str\n        The path to the keras model file\n    allow_growth: bool\n        Enable the Tensorflow GPU allow_growth configuration option. This option prevents\n        Tensorflow from allocating all of the GPU VRAM, but can lead to higher fragmentation and\n        slower performance\n    exclude_gpus: list\n        A list of indices correlating to connected GPUs that Tensorflow should not use. Pass\n        ``None`` to not exclude any GPUs\n    input_size: int\n        The input size to the model\n    num_classes: int\n        The number of segmentation classes to create\n    cpu_mode: bool, optional\n        ``True`` run the model on CPU. Default: ``False``\n    \"\"\"\n    def __init__(self,\n                 model_path: str,\n                 allow_growth: bool,\n                 exclude_gpus: list[int] | None,\n                 input_size: int,\n                 num_classes: int,\n                 cpu_mode: bool) -> None:\n        super().__init__(\"BiSeNet Face Parsing\",\n                         model_path,\n                         allow_growth=allow_growth,\n                         exclude_gpus=exclude_gpus,\n                         cpu_mode=cpu_mode)\n        self._input_size = input_size\n        self._num_classes = num_classes\n        self._cp = ContextPath()\n        self.define_model(self._model_definition)\n        self.load_model_weights()\n\n    def _model_definition(self) -> tuple[Tensor, list[Tensor]]:\n        \"\"\" Definition of the VGG Obstructed Model.\n\n        Returns\n        -------\n        tuple\n            The tensor input to the model and tensor output to the model for compilation by\n            :func`define_model`\n        \"\"\"\n        input_ = Input((self._input_size, self._input_size, 3))\n\n        features = self._cp(input_)  # res8, cp8, cp16\n        feat_fuse = FeatureFusionModule(256)([features[0], features[1]])\n\n        feat_out = BiSeNetOutput(256, self._num_classes)(feat_fuse)\n        feat_out16 = BiSeNetOutput(64, self._num_classes, label=\"16\")(features[1])\n        feat_out32 = BiSeNetOutput(64, self._num_classes, label=\"32\")(features[2])\n\n        height, width = K.int_shape(input_)[1:3]\n        f_h, f_w = K.int_shape(feat_out)[1:3]\n        f_h16, f_w16 = K.int_shape(feat_out16)[1:3]\n        f_h32, f_w32 = K.int_shape(feat_out32)[1:3]\n\n        feat_out = UpSampling2D(size=(height // f_h, width // f_w),\n                                interpolation=\"bilinear\")(feat_out)\n        feat_out16 = UpSampling2D(size=(height // f_h16, width // f_w16),\n                                  interpolation=\"bilinear\")(feat_out16)\n        feat_out32 = UpSampling2D(size=(height // f_h32, width // f_w32),\n                                  interpolation=\"bilinear\")(feat_out32)\n\n        return input_, [feat_out, feat_out16, feat_out32]\n", "plugins/train/_config.py": "#!/usr/bin/env python3\n\"\"\" Default configurations for models \"\"\"\n\nimport gettext\nimport logging\nimport os\n\nfrom lib.config import FaceswapConfig\nfrom plugins.plugin_loader import PluginLoader\n\n# LOCALES\n_LANG = gettext.translation(\"plugins.train._config\", localedir=\"locales\", fallback=True)\n_ = _LANG.gettext\n\nlogger = logging.getLogger(__name__)\n\nADDITIONAL_INFO = _(\"\\nNB: Unless specifically stated, values changed here will only take effect \"\n                    \"when creating a new model.\")\n\n_LOSS_HELP = {\n    \"ffl\": _(\n        \"Focal Frequency Loss. Analyzes the frequency spectrum of the images rather than the \"\n        \"images themselves. This loss function can be used on its own, but the original paper \"\n        \"found increased benefits when using it as a complementary loss to another spacial loss \"\n        \"function (e.g. MSE). Ref: Focal Frequency Loss for Image Reconstruction and Synthesis \"\n        \"https://arxiv.org/pdf/2012.12821.pdf NB: This loss does not currently work on AMD \"\n        \"cards.\"),\n    \"flip\": _(\n        \"Nvidia FLIP. A perceptual loss measure that approximates the difference perceived by \"\n        \"humans as they alternate quickly (or flip) between two images. Used on its own and this \"\n        \"loss function creates a distinct grid on the output. However it can be helpful when \"\n        \"used as a complimentary loss function. Ref: FLIP: A Difference Evaluator for \"\n        \"Alternating Images: \"\n        \"https://research.nvidia.com/sites/default/files/node/3260/FLIP_Paper.pdf\"),\n    \"gmsd\": _(\n        \"Gradient Magnitude Similarity Deviation seeks to match the global standard deviation of \"\n        \"the pixel to pixel differences between two images. Similar in approach to SSIM. Ref: \"\n        \"Gradient Magnitude Similarity Deviation: An Highly Efficient Perceptual Image Quality \"\n        \"Index https://arxiv.org/ftp/arxiv/papers/1308/1308.3052.pdf\"),\n    \"l_inf_norm\": _(\n        \"The L_inf norm will reduce the largest individual pixel error in an image. As \"\n        \"each largest error is minimized sequentially, the overall error is improved. This loss \"\n        \"will be extremely focused on outliers.\"),\n    \"laploss\": _(\n        \"Laplacian Pyramid Loss. Attempts to improve results by focussing on edges using \"\n        \"Laplacian Pyramids. As this loss function gives priority to edges over other low-\"\n        \"frequency information, like color, it should not be used on its own. The original \"\n        \"implementation uses this loss as a complimentary function to MSE. \"\n        \"Ref: Optimizing the Latent Space of Generative Networks \"\n        \"https://arxiv.org/abs/1707.05776\"),\n    \"lpips_alex\": _(\n        \"LPIPS is a perceptual loss that uses the feature outputs of other pretrained models as a \"\n        \"loss metric. Be aware that this loss function will use more VRAM. Used on its own and \"\n        \"this loss will create a distinct moire pattern on the output, however it can be helpful \"\n        \"as a complimentary loss function. The output of this function is strong, so depending \"\n        \"on your chosen primary loss function, you are unlikely going to want to set the weight \"\n        \"above about 25%. Ref: The Unreasonable Effectiveness of Deep Features as a Perceptual \"\n        \"Metric http://arxiv.org/abs/1801.03924\\nThis variant uses the AlexNet backbone. A fairly \"\n        \"light and old model which performed best in the paper's original implementation.\\nNB: \"\n        \"For AMD Users the final linear layer is not implemented.\"),\n    \"lpips_squeeze\": _(\n        \"Same as lpips_alex, but using the SqueezeNet backbone. A more lightweight \"\n        \"version of AlexNet.\\nNB: For AMD Users the final linear layer is not implemented.\"),\n    \"lpips_vgg16\": _(\n        \"Same as lpips_alex, but using the VGG16 backbone. A more heavyweight model.\\n\"\n        \"NB: For AMD Users the final linear layer is not implemented.\"),\n    \"logcosh\": _(\n        \"log(cosh(x)) acts similar to MSE for small errors and to MAE for large errors. Like \"\n        \"MSE, it is very stable and prevents overshoots when errors are near zero. Like MAE, it \"\n        \"is robust to outliers.\"),\n    \"mae\": _(\n        \"Mean absolute error will guide reconstructions of each pixel towards its median value in \"\n        \"the training dataset. Robust to outliers but as a median, it can potentially ignore some \"\n        \"infrequent image types in the dataset.\"),\n    \"mse\": _(\n        \"Mean squared error will guide reconstructions of each pixel towards its average value in \"\n        \"the training dataset. As an avg, it will be susceptible to outliers and typically \"\n        \"produces slightly blurrier results. Ref: Multi-Scale Structural Similarity for Image \"\n        \"Quality Assessment https://www.cns.nyu.edu/pub/eero/wang03b.pdf\"),\n    \"ms_ssim\": _(\n        \"Multiscale Structural Similarity Index Metric is similar to SSIM except that it \"\n        \"performs the calculations along multiple scales of the input image.\"),\n    \"smooth_loss\": _(\n        \"Smooth_L1 is a modification of the MAE loss to correct two of its disadvantages. \"\n        \"This loss has improved stability and guidance for small errors. Ref: A General and \"\n        \"Adaptive Robust Loss Function https://arxiv.org/pdf/1701.03077.pdf\"),\n    \"ssim\": _(\n        \"Structural Similarity Index Metric is a perception-based loss that considers changes in \"\n        \"texture, luminance, contrast, and local spatial statistics of an image. Potentially \"\n        \"delivers more realistic looking images. Ref: Image Quality Assessment: From Error \"\n        \"Visibility to Structural Similarity http://www.cns.nyu.edu/pub/eero/wang03-reprint.pdf\"),\n    \"pixel_gradient_diff\": _(\n        \"Instead of minimizing the difference between the absolute value of each \"\n        \"pixel in two reference images, compute the pixel to pixel spatial difference in each \"\n        \"image and then minimize that difference between two images. Allows for large color \"\n        \"shifts, but maintains the structure of the image.\"),\n    \"none\": _(\"Do not use an additional loss function.\")}\n\n_NON_PRIMARY_LOSS = [\"flip\", \"lpips_alex\", \"lpips_squeeze\", \"lpips_vgg16\", \"none\"]\n\n\nclass Config(FaceswapConfig):\n    \"\"\" Config File for Models \"\"\"\n    # pylint:disable=too-many-statements\n    def set_defaults(self) -> None:\n        \"\"\" Set the default values for config \"\"\"\n        logger.debug(\"Setting defaults\")\n        self._set_globals()\n        self._set_loss()\n        self._defaults_from_plugin(os.path.dirname(__file__))\n\n    def _set_globals(self) -> None:\n        \"\"\" Set the global options for training \"\"\"\n        logger.debug(\"Setting global config\")\n        section = \"global\"\n        self.add_section(section,\n                         _(\"Options that apply to all models\") + ADDITIONAL_INFO)\n        self.add_item(\n            section=section,\n            title=\"centering\",\n            datatype=str,\n            gui_radio=True,\n            default=\"face\",\n            choices=[\"face\", \"head\", \"legacy\"],\n            fixed=True,\n            group=_(\"face\"),\n            info=_(\n                \"How to center the training image. The extracted images are centered on the \"\n                \"middle of the skull based on the face's estimated pose. A subsection of these \"\n                \"images are used for training. The centering used dictates how this subsection \"\n                \"will be cropped from the aligned images.\"\n                \"\\n\\tface: Centers the training image on the center of the face, adjusting for \"\n                \"pitch and yaw.\"\n                \"\\n\\thead: Centers the training image on the center of the head, adjusting for \"\n                \"pitch and yaw. NB: You should only select head centering if you intend to \"\n                \"include the full head (including hair) in the final swap. This may give mixed \"\n                \"results. Additionally, it is only worth choosing head centering if you are \"\n                \"training with a mask that includes the hair (e.g. BiSeNet-FP-Head).\"\n                \"\\n\\tlegacy: The 'original' extraction technique. Centers the training image \"\n                \"near the tip of the nose with no adjustment. Can result in the edges of the \"\n                \"face appearing outside of the training area.\"))\n        self.add_item(\n            section=section,\n            title=\"coverage\",\n            datatype=float,\n            default=87.5,\n            min_max=(62.5, 100.0),\n            rounding=2,\n            fixed=True,\n            group=_(\"face\"),\n            info=_(\n                \"How much of the extracted image to train on. A lower coverage will limit the \"\n                \"model's scope to a zoomed-in central area while higher amounts can include the \"\n                \"entire face. A trade-off exists between lower amounts given more detail \"\n                \"versus higher amounts avoiding noticeable swap transitions. For 'Face' \"\n                \"centering you will want to leave this above 75%. For Head centering you will \"\n                \"most likely want to set this to 100%. Sensible values for 'Legacy' \"\n                \"centering are:\"\n                \"\\n\\t62.5% spans from eyebrow to eyebrow.\"\n                \"\\n\\t75.0% spans from temple to temple.\"\n                \"\\n\\t87.5% spans from ear to ear.\"\n                \"\\n\\t100.0% is a mugshot.\"))\n        self.add_item(\n            section=section,\n            title=\"icnr_init\",\n            datatype=bool,\n            default=False,\n            group=_(\"initialization\"),\n            info=_(\n                \"Use ICNR to tile the default initializer in a repeating pattern. \"\n                \"This strategy is designed for pairing with sub-pixel / pixel shuffler \"\n                \"to reduce the 'checkerboard effect' in image reconstruction. \"\n                \"\\n\\t https://arxiv.org/ftp/arxiv/papers/1707/1707.02937.pdf\"))\n        self.add_item(\n            section=section,\n            title=\"conv_aware_init\",\n            datatype=bool,\n            default=False,\n            group=_(\"initialization\"),\n            info=_(\n                \"Use Convolution Aware Initialization for convolutional layers. \"\n                \"This can help eradicate the vanishing and exploding gradient problem \"\n                \"as well as lead to higher accuracy, lower loss and faster convergence.\\nNB:\"\n                \"\\n\\t This can use more VRAM when creating a new model so you may want to \"\n                \"lower the batch size for the first run. The batch size can be raised \"\n                \"again when reloading the model. \"\n                \"\\n\\t Multi-GPU is not supported for this option, so you should start the model \"\n                \"on a single GPU. Once training has started, you can stop training, enable \"\n                \"multi-GPU and resume.\"\n                \"\\n\\t Building the model will likely take several minutes as the calculations \"\n                \"for this initialization technique are expensive. This will only impact starting \"\n                \"a new model.\"))\n        self.add_item(\n            section=section,\n            title=\"optimizer\",\n            datatype=str,\n            gui_radio=True,\n            group=_(\"optimizer\"),\n            default=\"adam\",\n            choices=[\"adabelief\", \"adam\", \"nadam\", \"rms-prop\"],\n            info=_(\n                \"The optimizer to use.\"\n                \"\\n\\t adabelief - Adapting Stepsizes by the Belief in Observed Gradients. An \"\n                \"optimizer with the aim to converge faster, generalize better and remain more \"\n                \"stable. (https://arxiv.org/abs/2010.07468). NB: Epsilon for AdaBelief needs to \"\n                \"be set to a smaller value than other Optimizers. Generally setting the 'Epsilon \"\n                \"Exponent' to around '-16' should work.\"\n                \"\\n\\t adam - Adaptive Moment Optimization. A stochastic gradient descent method \"\n                \"that is based on adaptive estimation of first-order and second-order moments.\"\n                \"\\n\\t nadam - Adaptive Moment Optimization with Nesterov Momentum. Much like \"\n                \"Adam but uses a different formula for calculating momentum.\"\n                \"\\n\\t rms-prop - Root Mean Square Propagation. Maintains a moving (discounted) \"\n                \"average of the square of the gradients. Divides the gradient by the root of \"\n                \"this average.\"))\n        self.add_item(\n            section=section,\n            title=\"learning_rate\",\n            datatype=float,\n            default=5e-5,\n            min_max=(1e-6, 1e-4),\n            rounding=6,\n            fixed=False,\n            group=_(\"optimizer\"),\n            info=_(\n                \"Learning rate - how fast your network will learn (how large are the \"\n                \"modifications to the model weights after one batch of training). Values that \"\n                \"are too large might result in model crashes and the inability of the model to \"\n                \"find the best solution. Values that are too small might be unable to escape \"\n                \"from dead-ends and find the best global minimum.\"))\n        self.add_item(\n            section=section,\n            title=\"epsilon_exponent\",\n            datatype=int,\n            default=-7,\n            min_max=(-20, 0),\n            rounding=1,\n            fixed=False,\n            group=_(\"optimizer\"),\n            info=_(\n                \"The epsilon adds a small constant to weight updates to attempt to avoid 'divide \"\n                \"by zero' errors. Unless you are using the AdaBelief Optimizer, then Generally \"\n                \"this option should be left at default value, For AdaBelief, setting this to \"\n                \"around '-16' should work.\\n\"\n                \"In all instances if you are getting 'NaN' loss values, and have been unable to \"\n                \"resolve the issue any other way (for example, increasing batch size, or \"\n                \"lowering learning rate), then raising the epsilon can lead to a more stable \"\n                \"model. It may, however, come at the cost of slower training and a less accurate \"\n                \"final result.\\n\"\n                \"NB: The value given here is the 'exponent' to the epsilon. For example, \"\n                \"choosing '-7' will set the epsilon to 1e-7. Choosing '-3' will set the epsilon \"\n                \"to 0.001 (1e-3).\"))\n        self.add_item(\n            section=section,\n            title=\"save_optimizer\",\n            datatype=str,\n            group=_(\"optimizer\"),\n            default=\"exit\",\n            fixed=False,\n            gui_radio=True,\n            choices=[\"never\", \"always\", \"exit\"],\n            info=_(\n                \"When to save the Optimizer Weights. Saving the optimizer weights is not \"\n                \"necessary and will increase the model file size 3x (and by extension the amount \"\n                \"of time it takes to save the model). However, it can be useful to save these \"\n                \"weights if you want to guarantee that a resumed model carries off exactly from \"\n                \"where it left off, rather than spending a few hundred iterations catching up.\"\n                \"\\n\\t never - Don't save optimizer weights.\"\n                \"\\n\\t always - Save the optimizer weights at every save iteration. Model saving \"\n                \"will take longer, due to the increased file size, but you will always have the \"\n                \"last saved optimizer state in your model file.\"\n                \"\\n\\t exit - Only save the optimizer weights when explicitly terminating a \"\n                \"model. This can be when the model is actively stopped or when the target \"\n                \"iterations are met. Note: If the training session ends because of another \"\n                \"reason (e.g. power outage, Out of Memory Error, NaN detected) then the \"\n                \"optimizer weights will NOT be saved.\"))\n\n        self.add_item(\n            section=section,\n            title=\"lr_finder_iterations\",\n            datatype=int,\n            default=1000,\n            min_max=(100, 10000),\n            rounding=100,\n            fixed=True,\n            group=_(\"Learning Rate Finder\"),\n            info=_(\n                \"The number of iterations to process to find the optimal learning rate. Higher \"\n                \"values will take longer, but will be more accurate.\"))\n        self.add_item(\n            section=section,\n            title=\"lr_finder_mode\",\n            datatype=str,\n            default=\"set\",\n            fixed=True,\n            gui_radio=True,\n            choices=[\"set\", \"graph_and_set\", \"graph_and_exit\"],\n            group=_(\"Learning Rate Finder\"),\n            info=_(\n                \"The operation mode for the learning rate finder. Only applicable to new models. \"\n                \"For existing models this will always default to 'set'.\"\n                \"\\n\\tset - Train with the discovered optimal learning rate.\"\n                \"\\n\\tgraph_and_set - Output a graph in the training folder showing the discovered \"\n                \"learning rates and train with the optimal learning rate.\"\n                \"\\n\\tgraph_and_exit - Output a graph in the training folder with the discovered \"\n                \"learning rates and exit.\"))\n        self.add_item(\n            section=section,\n            title=\"lr_finder_strength\",\n            datatype=str,\n            default=\"default\",\n            fixed=True,\n            gui_radio=True,\n            choices=[\"default\", \"aggressive\", \"extreme\"],\n            group=_(\"Learning Rate Finder\"),\n            info=_(\n                \"How aggressively to set the Learning Rate. More aggressive can learn faster, but \"\n                \"is more likely to lead to exploding gradients.\"\n                \"\\n\\tdefault - The default optimal learning rate. A safe choice for nearly all \"\n                \"use cases.\"\n                \"\\n\\taggressive - Set's a higher learning rate than the default. May learn faster \"\n                \"but with a higher chance of exploding gradients.\"\n                \"\\n\\textreme - The highest optimal learning rate. A much higher risk of exploding \"\n                \"gradients.\"))\n        self.add_item(\n            section=section,\n            title=\"autoclip\",\n            datatype=bool,\n            default=False,\n            info=_(\n                \"Apply AutoClipping to the gradients. AutoClip analyzes the \"\n                \"gradient weights and adjusts the normalization value dynamically to fit the \"\n                \"data. Can help prevent NaNs and improve model optimization at the expense of \"\n                \"VRAM. Ref: AutoClip: Adaptive Gradient Clipping for Source Separation Networks \"\n                \"https://arxiv.org/abs/2007.14469\"),\n            fixed=False,\n            gui_radio=True,\n            group=_(\"optimizer\"))\n        self.add_item(\n            section=section,\n            title=\"reflect_padding\",\n            datatype=bool,\n            default=False,\n            group=_(\"network\"),\n            info=_(\n                \"Use reflection padding rather than zero padding with convolutions. \"\n                \"Each convolution must pad the image boundaries to maintain the proper \"\n                \"sizing. More complex padding schemes can reduce artifacts at the \"\n                \"border of the image.\"\n                \"\\n\\t http://www-cs.engr.ccny.cuny.edu/~wolberg/cs470/hw/hw2_pad.txt\"))\n        self.add_item(\n            section=section,\n            title=\"allow_growth\",\n            datatype=bool,\n            default=False,\n            group=_(\"network\"),\n            fixed=False,\n            info=_(\n                \"Enable the Tensorflow GPU 'allow_growth' configuration option. \"\n                \"This option prevents Tensorflow from allocating all of the GPU VRAM at launch \"\n                \"but can lead to higher VRAM fragmentation and slower performance. Should only \"\n                \"be enabled if you are receiving errors regarding 'cuDNN fails to initialize' \"\n                \"when commencing training.\"))\n        self.add_item(\n            section=section,\n            title=\"mixed_precision\",\n            datatype=bool,\n            default=False,\n            fixed=False,\n            group=_(\"network\"),\n            info=_(\n                \"NVIDIA GPUs can run operations in float16 faster than in \"\n                \"float32. Mixed precision allows you to use a mix of float16 with float32, to \"\n                \"get the performance benefits from float16 and the numeric stability benefits \"\n                \"from float32.\\n\\nThis is untested on DirectML backend, but will run on most \"\n                \"Nvidia models. it will only speed up training on more recent GPUs. Those with \"\n                \"compute capability 7.0 or higher will see the greatest performance benefit from \"\n                \"mixed precision because they have Tensor Cores. Older GPUs offer no math \"\n                \"performance benefit for using mixed precision, however memory and bandwidth \"\n                \"savings can enable some speedups. Generally RTX GPUs and later will offer the \"\n                \"most benefit.\"))\n        self.add_item(\n            section=section,\n            title=\"nan_protection\",\n            datatype=bool,\n            default=True,\n            group=_(\"network\"),\n            info=_(\n                \"If a 'NaN' is generated in the model, this means that the model has corrupted \"\n                \"and the model is likely to start deteriorating from this point on. Enabling NaN \"\n                \"protection will stop training immediately in the event of a NaN. The last save \"\n                \"will not contain the NaN, so you may still be able to rescue your model.\"),\n            fixed=False)\n        self.add_item(\n            section=section,\n            title=\"convert_batchsize\",\n            datatype=int,\n            default=16,\n            min_max=(1, 32),\n            rounding=1,\n            fixed=False,\n            group=_(\"convert\"),\n            info=_(\n                \"[GPU Only]. The number of faces to feed through the model at once when running \"\n                \"the Convert process.\\n\\nNB: Increasing this figure is unlikely to improve \"\n                \"convert speed, however, if you are getting Out of Memory errors, then you may \"\n                \"want to reduce the batch size.\"))\n\n    def _set_loss(self) -> None:\n        # pylint:disable=line-too-long\n        \"\"\" Set the default loss options.\n\n        Loss Documentation\n        MAE https://heartbeat.fritz.ai/5-regression-loss-functions-all-machine-learners-should-know-4fb140e9d4b0\n        MSE https://heartbeat.fritz.ai/5-regression-loss-functions-all-machine-learners-should-know-4fb140e9d4b0\n        LogCosh https://heartbeat.fritz.ai/5-regression-loss-functions-all-machine-learners-should-know-4fb140e9d4b0\n        L_inf_norm https://medium.com/@montjoile/l0-norm-l1-norm-l2-norm-l-infinity-norm-7a7d18a4f40c\n        \"\"\"  # noqa\n        # pylint:enable=line-too-long\n        logger.debug(\"Setting Loss config\")\n        section = \"global.loss\"\n        self.add_section(section,\n                         _(\"Loss configuration options\\n\"\n                           \"Loss is the mechanism by which a Neural Network judges how well it \"\n                           \"thinks that it is recreating a face.\") + ADDITIONAL_INFO)\n        self.add_item(\n            section=section,\n            title=\"loss_function\",\n            datatype=str,\n            group=_(\"loss\"),\n            default=\"ssim\",\n            fixed=False,\n            choices=[x for x in sorted(_LOSS_HELP) if x not in _NON_PRIMARY_LOSS],\n            info=(_(\"The loss function to use.\") +\n                  \"\\n\\n\\t\" + \"\\n\\n\\t\".join(f\"{k}: {v}\"\n                                           for k, v in sorted(_LOSS_HELP.items())\n                                           if k not in _NON_PRIMARY_LOSS)))\n        self.add_item(\n            section=section,\n            title=\"loss_function_2\",\n            datatype=str,\n            group=_(\"loss\"),\n            default=\"mse\",\n            fixed=False,\n            choices=list(sorted(_LOSS_HELP)),\n            info=(_(\"The second loss function to use. If using a structural based loss (such as \"\n                    \"SSIM, MS-SSIM or GMSD) it is common to add an L1 regularization(MAE) or L2 \"\n                    \"regularization (MSE) function. You can adjust the weighting of this loss \"\n                    \"function with the loss_weight_2 option.\") +\n                  \"\\n\\n\\t\" + \"\\n\\n\\t\".join(f\"{k}: {v}\" for k, v in sorted(_LOSS_HELP.items()))))\n        self.add_item(\n            section=section,\n            title=\"loss_weight_2\",\n            datatype=int,\n            group=_(\"loss\"),\n            min_max=(0, 400),\n            rounding=1,\n            default=100,\n            fixed=False,\n            info=_(\n                \"The amount of weight to apply to the second loss function.\\n\\n\"\n                \"\\n\\nThe value given here is as a percentage denoting how much the selected \"\n                \"function should contribute to the overall loss cost of the model. For example:\"\n                \"\\n\\t 100 - The loss calculated for the second loss function will be applied at \"\n                \"its full amount towards the overall loss score. \"\n                \"\\n\\t 25 - The loss calculated for the second loss function will be reduced by a \"\n                \"quarter prior to adding to the overall loss score. \"\n                \"\\n\\t 400 - The loss calculated for the second loss function will be mulitplied \"\n                \"4 times prior to adding to the overall loss score. \"\n                \"\\n\\t 0 - Disables the second loss function altogether.\"))\n        self.add_item(\n            section=section,\n            title=\"loss_function_3\",\n            datatype=str,\n            group=_(\"loss\"),\n            default=\"none\",\n            fixed=False,\n            choices=list(sorted(_LOSS_HELP)),\n            info=(_(\"The third loss function to use. You can adjust the weighting of this loss \"\n                    \"function with the loss_weight_3 option.\") +\n                  \"\\n\\n\\t\" +\n                  \"\\n\\n\\t\".join(f\"{k}: {v}\" for k, v in sorted(_LOSS_HELP.items()))))\n        self.add_item(\n            section=section,\n            title=\"loss_weight_3\",\n            datatype=int,\n            group=_(\"loss\"),\n            min_max=(0, 400),\n            rounding=1,\n            default=0,\n            fixed=False,\n            info=_(\n                \"The amount of weight to apply to the third loss function.\\n\\n\"\n                \"\\n\\nThe value given here is as a percentage denoting how much the selected \"\n                \"function should contribute to the overall loss cost of the model. For example:\"\n                \"\\n\\t 100 - The loss calculated for the third loss function will be applied at \"\n                \"its full amount towards the overall loss score. \"\n                \"\\n\\t 25 - The loss calculated for the third loss function will be reduced by a \"\n                \"quarter prior to adding to the overall loss score. \"\n                \"\\n\\t 400 - The loss calculated for the third loss function will be mulitplied 4 \"\n                \"times prior to adding to the overall loss score. \"\n                \"\\n\\t 0 - Disables the third loss function altogether.\"))\n        self.add_item(\n            section=section,\n            title=\"loss_function_4\",\n            datatype=str,\n            group=_(\"loss\"),\n            default=\"none\",\n            fixed=False,\n            choices=list(sorted(_LOSS_HELP)),\n            info=(_(\"The fourth loss function to use. You can adjust the weighting of this loss \"\n                    \"function with the loss_weight_3 option.\") +\n                  \"\\n\\n\\t\" +\n                  \"\\n\\n\\t\".join(f\"{k}: {v}\" for k, v in sorted(_LOSS_HELP.items()))))\n        self.add_item(\n            section=section,\n            title=\"loss_weight_4\",\n            datatype=int,\n            group=_(\"loss\"),\n            min_max=(0, 400),\n            rounding=1,\n            default=0,\n            fixed=False,\n            info=_(\n                \"The amount of weight to apply to the fourth loss function.\\n\\n\"\n                \"\\n\\nThe value given here is as a percentage denoting how much the selected \"\n                \"function should contribute to the overall loss cost of the model. For example:\"\n                \"\\n\\t 100 - The loss calculated for the fourth loss function will be applied at \"\n                \"its full amount towards the overall loss score. \"\n                \"\\n\\t 25 - The loss calculated for the fourth loss function will be reduced by a \"\n                \"quarter prior to adding to the overall loss score. \"\n                \"\\n\\t 400 - The loss calculated for the fourth loss function will be mulitplied \"\n                \"4 times prior to adding to the overall loss score. \"\n                \"\\n\\t 0 - Disables the fourth loss function altogether.\"))\n        self.add_item(\n            section=section,\n            title=\"mask_loss_function\",\n            datatype=str,\n            group=_(\"loss\"),\n            default=\"mse\",\n            fixed=False,\n            choices=[\"mae\", \"mse\"],\n            info=_(\n                \"The loss function to use when learning a mask.\"\n                \"\\n\\t MAE - Mean absolute error will guide reconstructions of each pixel \"\n                \"towards its median value in the training dataset. Robust to outliers but as \"\n                \"a median, it can potentially ignore some infrequent image types in the dataset.\"\n                \"\\n\\t MSE - Mean squared error will guide reconstructions of each pixel \"\n                \"towards its average value in the training dataset. As an average, it will be \"\n                \"susceptible to outliers and typically produces slightly blurrier results.\"))\n        self.add_item(\n            section=section,\n            title=\"eye_multiplier\",\n            datatype=int,\n            group=_(\"loss\"),\n            min_max=(1, 40),\n            rounding=1,\n            default=3,\n            fixed=False,\n            info=_(\n                \"The amount of priority to give to the eyes.\\n\\nThe value given here is as a \"\n                \"multiplier of the main loss score. For example:\"\n                \"\\n\\t 1 - The eyes will receive the same priority as the rest of the face. \"\n                \"\\n\\t 10 - The eyes will be given a score 10 times higher than the rest of the \"\n                \"face.\"\n                \"\\n\\nNB: Penalized Mask Loss must be enable to use this option.\"))\n        self.add_item(\n            section=section,\n            title=\"mouth_multiplier\",\n            datatype=int,\n            group=_(\"loss\"),\n            min_max=(1, 40),\n            rounding=1,\n            default=2,\n            fixed=False,\n            info=_(\n                \"The amount of priority to give to the mouth.\\n\\nThe value given here is as a \"\n                \"multiplier of the main loss score. For Example:\"\n                \"\\n\\t 1 - The mouth will receive the same priority as the rest of the face. \"\n                \"\\n\\t 10 - The mouth will be given a score 10 times higher than the rest of the \"\n                \"face.\"\n                \"\\n\\nNB: Penalized Mask Loss must be enable to use this option.\"))\n        self.add_item(\n            section=section,\n            title=\"penalized_mask_loss\",\n            datatype=bool,\n            default=True,\n            group=_(\"loss\"),\n            info=_(\n                \"Image loss function is weighted by mask presence. For areas of \"\n                \"the image without the facial mask, reconstruction errors will be \"\n                \"ignored while the masked face area is prioritized. May increase \"\n                \"overall quality by focusing attention on the core face area.\"))\n        self.add_item(\n            section=section,\n            title=\"mask_type\",\n            datatype=str,\n            default=\"extended\",\n            choices=PluginLoader.get_available_extractors(\"mask\",\n                                                          add_none=True, extend_plugin=True),\n            group=_(\"mask\"),\n            gui_radio=True,\n            info=_(\n                \"The mask to be used for training. If you have selected 'Learn Mask' or \"\n                \"'Penalized Mask Loss' you must select a value other than 'none'. The required \"\n                \"mask should have been selected as part of the Extract process. If it does not \"\n                \"exist in the alignments file then it will be generated prior to training \"\n                \"commencing.\"\n                \"\\n\\tnone: Don't use a mask.\"\n                \"\\n\\tbisenet-fp_face: Relatively lightweight NN based mask that provides more \"\n                \"refined control over the area to be masked (configurable in mask settings). \"\n                \"Use this version of bisenet-fp if your model is trained with 'face' or \"\n                \"'legacy' centering.\"\n                \"\\n\\tbisenet-fp_head: Relatively lightweight NN based mask that provides more \"\n                \"refined control over the area to be masked (configurable in mask settings). \"\n                \"Use this version of bisenet-fp if your model is trained with 'head' centering.\"\n                \"\\n\\tcomponents: Mask designed to provide facial segmentation based on the \"\n                \"positioning of landmark locations. A convex hull is constructed around the \"\n                \"exterior of the landmarks to create a mask.\"\n                \"\\n\\tcustom_face: Custom user created, face centered mask.\"\n                \"\\n\\tcustom_head: Custom user created, head centered mask.\"\n                \"\\n\\textended: Mask designed to provide facial segmentation based on the \"\n                \"positioning of landmark locations. A convex hull is constructed around the \"\n                \"exterior of the landmarks and the mask is extended upwards onto the forehead.\"\n                \"\\n\\tvgg-clear: Mask designed to provide smart segmentation of mostly frontal \"\n                \"faces clear of obstructions. Profile faces and obstructions may result in \"\n                \"sub-par performance.\"\n                \"\\n\\tvgg-obstructed: Mask designed to provide smart segmentation of mostly \"\n                \"frontal faces. The mask model has been specifically trained to recognize \"\n                \"some facial obstructions (hands and eyeglasses). Profile faces may result in \"\n                \"sub-par performance.\"\n                \"\\n\\tunet-dfl: Mask designed to provide smart segmentation of mostly frontal \"\n                \"faces. The mask model has been trained by community members and will need \"\n                \"testing for further description. Profile faces may result in sub-par \"\n                \"performance.\"))\n        self.add_item(\n            section=section,\n            title=\"mask_dilation\",\n            datatype=float,\n            min_max=(-5.0, 5.0),\n            rounding=1,\n            default=0,\n            fixed=False,\n            group=_(\"mask\"),\n            info=_(\n                \"Dilate or erode the mask. Negative values erode the mask (make it smaller). \"\n                \"Positive values dilate the mask (make it larger). The value given is a \"\n                \"percentage of the total mask size.\"))\n        self.add_item(\n            section=section,\n            title=\"mask_blur_kernel\",\n            datatype=int,\n            min_max=(0, 9),\n            rounding=1,\n            default=3,\n            fixed=False,\n            group=_(\"mask\"),\n            info=_(\n                \"Apply gaussian blur to the mask input. This has the effect of smoothing the \"\n                \"edges of the mask, which can help with poorly calculated masks and give less \"\n                \"of a hard edge to the predicted mask. The size is in pixels (calculated from \"\n                \"a 128px mask). Set to 0 to not apply gaussian blur. This value should be odd, \"\n                \"if an even number is passed in then it will be rounded to the next odd number.\"))\n        self.add_item(\n            section=section,\n            title=\"mask_threshold\",\n            datatype=int,\n            default=4,\n            min_max=(0, 50),\n            rounding=1,\n            fixed=False,\n            group=_(\"mask\"),\n            info=_(\n                \"Sets pixels that are near white to white and near black to black. Set to 0 for \"\n                \"off.\"))\n        self.add_item(\n            section=section,\n            title=\"learn_mask\",\n            datatype=bool,\n            default=False,\n            group=_(\"mask\"),\n            info=_(\n                \"Dedicate a portion of the model to learning how to duplicate the input \"\n                \"mask. Increases VRAM usage in exchange for learning a quick ability to try \"\n                \"to replicate more complex mask models.\"))\n", "plugins/train/__init__.py": "", "plugins/train/trainer/original_defaults.py": "#!/usr/bin/env python3\n\"\"\"\n    The default options for the faceswap Original Model plugin.\n\n    Defaults files should be named <plugin_name>_defaults.py\n    Any items placed into this file will automatically get added to the relevant config .ini files\n    within the faceswap/config folder.\n\n    The following variables should be defined:\n        _HELPTEXT: A string describing what this plugin does\n        _DEFAULTS: A dictionary containing the options, defaults and meta information. The\n                   dictionary should be defined as:\n                       {<option_name>: {<metadata>}}\n\n                   <option_name> should always be lower text.\n                   <metadata> dictionary requirements are listed below.\n\n    The following keys are expected for the _DEFAULTS <metadata> dict:\n        datatype:  [required] A python type class. This limits the type of data that can be\n                   provided in the .ini file and ensures that the value is returned in the\n                   correct type to faceswap. Valid datatypes are: <class 'int'>, <class 'float'>,\n                   <class 'str'>, <class 'bool'>.\n        default:   [required] The default value for this option.\n        info:      [required] A string describing what this option does.\n        choices:   [optional] If this option's datatype is of <class 'str'> then valid\n                   selections can be defined here. This validates the option and also enables\n                   a combobox / radio option in the GUI.\n        gui_radio: [optional] If <choices> are defined, this indicates that the GUI should use\n                   radio buttons rather than a combobox to display this option.\n        min_max:   [partial] For <class 'int'> and <class 'float'> datatypes this is required\n                   otherwise it is ignored. Should be a tuple of min and max accepted values.\n                   This is used for controlling the GUI slider range. Values are not enforced.\n        rounding:  [partial] For <class 'int'> and <class 'float'> datatypes this is\n                   required otherwise it is ignored. Used for the GUI slider. For floats, this\n                   is the number of decimal places to display. For ints this is the step size.\n        fixed:     [optional] [train only]. Training configurations are fixed when the model is\n                   created, and then reloaded from the state file. Marking an item as fixed=False\n                   indicates that this value can be changed for existing models, and will override\n                   the value saved in the state file with the updated value in config. If not\n                   provided this will default to True.\n\"\"\"\n\n\n_HELPTEXT = (\"Original Trainer Options.\\n\"\n             \"WARNING: The defaults for augmentation will be fine for 99.9% of use cases. \"\n             \"Only change them if you absolutely know what you are doing!\")\n\n\n_DEFAULTS = dict(\n    preview_images=dict(\n        default=14,\n        info=\"Number of sample faces to display for each side in the preview when training.\",\n        datatype=int,\n        rounding=2,\n        min_max=(2, 16),\n        group=\"evaluation\"),\n    mask_opacity=dict(\n        default=30,\n        info=\"The opacity of the mask overlay in the training preview. Lower values are more \"\n             \"transparent.\",\n        datatype=int,\n        rounding=2,\n        min_max=(0, 100),\n        group=\"evaluation\"),\n    mask_color=dict(\n        default=\"#ff0000\",\n        choices=\"colorchooser\",\n        info=\"The RGB hex color to use for the mask overlay in the training preview.\",\n        datatype=str,\n        group=\"evaluation\"),\n    zoom_amount=dict(\n        default=5,\n        info=\"Percentage amount to randomly zoom each training image in and out.\",\n        datatype=int,\n        rounding=1,\n        min_max=(0, 25),\n        group=\"image augmentation\"),\n    rotation_range=dict(\n        default=10,\n        info=\"Percentage amount to randomly rotate each training image.\",\n        datatype=int,\n        rounding=1,\n        min_max=(0, 25),\n        group=\"image augmentation\"),\n    shift_range=dict(\n        default=5,\n        info=\"Percentage amount to randomly shift each training image horizontally and \"\n             \"vertically.\",\n        datatype=int,\n        rounding=1,\n        min_max=(0, 25),\n        group=\"image augmentation\"),\n    flip_chance=dict(\n        default=50,\n        info=\"Percentage chance to randomly flip each training image horizontally.\\n\"\n             \"NB: This is ignored if the 'no-flip' option is enabled\",\n        datatype=int,\n        rounding=1,\n        min_max=(0, 75),\n        group=\"image augmentation\"),\n\n    color_lightness=dict(\n        default=30,\n        info=\"Percentage amount to randomly alter the lightness of each training image.\\n\"\n             \"NB: This is ignored if the 'no-augment-color' option is enabled\",\n        datatype=int,\n        rounding=1,\n        min_max=(0, 75),\n        group=\"color augmentation\"),\n    color_ab=dict(\n        default=8,\n        info=\"Percentage amount to randomly alter the 'a' and 'b' colors of the L*a*b* color \"\n             \"space of each training image.\\nNB: This is ignored if the 'no-augment-color' option\"\n             \"is enabled\",\n        datatype=int,\n        rounding=1,\n        min_max=(0, 50),\n        group=\"color augmentation\"),\n    color_clahe_chance=dict(\n        default=50,\n        info=\"Percentage chance to perform Contrast Limited Adaptive Histogram Equalization on \"\n             \"each training image.\\nNB: This is ignored if the 'no-augment-color' option is \"\n             \"enabled\",\n        datatype=int,\n        rounding=1,\n        min_max=(0, 75),\n        fixed=False,\n        group=\"color augmentation\"),\n    color_clahe_max_size=dict(\n        default=4,\n        info=\"The grid size dictates how much Contrast Limited Adaptive Histogram Equalization is \"\n             \"performed on any training image selected for clahe. Contrast will be applied \"\n             \"randomly with a gridsize of 0 up to the maximum. This value is a multiplier \"\n             \"calculated from the training image size.\\nNB: This is ignored if the \"\n             \"'no-augment-color' option is enabled\",\n        datatype=int,\n        rounding=1,\n        min_max=(1, 8),\n        group=\"color augmentation\"),\n)\n", "plugins/train/trainer/original.py": "#!/usr/bin/env python3\n\"\"\" Original Trainer \"\"\"\n\nfrom ._base import TrainerBase\n\n\nclass Trainer(TrainerBase):\n    \"\"\" Original is currently identical to Base \"\"\"\n    def __init__(self, *args, **kwargs):  # pylint:disable=useless-super-delegation\n        super().__init__(*args, **kwargs)\n", "plugins/train/trainer/_base.py": "#!/usr/bin/env python3\n\"\"\" Base Class for Faceswap Trainer plugins. All Trainer plugins should be inherited from\nthis class.\n\nAt present there is only the :class:`~plugins.train.trainer.original` plugin, so that entirely\ninherits from this class. If further plugins are developed, then common code should be kept here,\nwith \"original\" unique code split out to the original plugin.\n\"\"\"\nfrom __future__ import annotations\nimport logging\nimport os\nimport time\nimport typing as T\n\nimport cv2\nimport numpy as np\n\nimport tensorflow as tf\nfrom tensorflow.python.framework import (  # pylint:disable=no-name-in-module\n    errors_impl as tf_errors)\n\nfrom lib.image import hex_to_rgb\nfrom lib.training import Feeder, LearningRateFinder\nfrom lib.utils import FaceswapError, get_folder, get_image_paths\nfrom plugins.train._config import Config\n\nif T.TYPE_CHECKING:\n    from collections.abc import Callable\n    from plugins.train.model._base import ModelBase\n    from lib.config import ConfigValueType\n\nlogger = logging.getLogger(__name__)\n\n\ndef _get_config(plugin_name: str,\n                configfile: str | None = None) -> dict[str, ConfigValueType]:\n    \"\"\" Return the configuration for the requested trainer.\n\n    Parameters\n    ----------\n    plugin_name: str\n        The name of the plugin to load the configuration for\n    configfile: str, optional\n        A custom configuration file. If ``None`` then configuration is loaded from the default\n        :file:`.config.train.ini` file. Default: ``None``\n\n    Returns\n    -------\n    dict\n        The configuration dictionary for the requested plugin\n    \"\"\"\n    return Config(plugin_name, configfile=configfile).config_dict\n\n\nclass TrainerBase():\n    \"\"\" Handles the feeding of training images to Faceswap models, the generation of Tensorboard\n    logs and the creation of sample/time-lapse preview images.\n\n    All Trainer plugins must inherit from this class.\n\n    Parameters\n    ----------\n    model: plugin from :mod:`plugins.train.model`\n        The model that will be running this trainer\n    images: dict\n        The file paths for the images to be trained on for each side. The dictionary should contain\n        2 keys (\"a\" and \"b\") with the values being a list of full paths corresponding to each side.\n    batch_size: int\n        The requested batch size for iteration to be trained through the model.\n    configfile: str\n        The path to a custom configuration file. If ``None`` is passed then configuration is loaded\n        from the default :file:`.config.train.ini` file.\n    \"\"\"\n\n    def __init__(self,\n                 model: ModelBase,\n                 images: dict[T.Literal[\"a\", \"b\"], list[str]],\n                 batch_size: int,\n                 configfile: str | None) -> None:\n        logger.debug(\"Initializing %s: (model: '%s', batch_size: %s)\",\n                     self.__class__.__name__, model, batch_size)\n        self._model = model\n        self._config = self._get_config(configfile)\n\n        self._feeder = Feeder(images, model, batch_size, self._config)\n\n        self._exit_early = self._handle_lr_finder()\n        if self._exit_early:\n            return\n\n        self._model.state.add_session_batchsize(batch_size)\n        self._images = images\n        self._sides = sorted(key for key in self._images.keys())\n\n        self._tensorboard = self._set_tensorboard()\n        self._samples = _Samples(self._model,\n                                 self._model.coverage_ratio,\n                                 T.cast(int, self._config[\"mask_opacity\"]),\n                                 T.cast(str, self._config[\"mask_color\"]))\n\n        num_images = self._config.get(\"preview_images\", 14)\n        assert isinstance(num_images, int)\n        self._timelapse = _Timelapse(self._model,\n                                     self._model.coverage_ratio,\n                                     num_images,\n                                     T.cast(int, self._config[\"mask_opacity\"]),\n                                     T.cast(str, self._config[\"mask_color\"]),\n                                     self._feeder,\n                                     self._images)\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    @property\n    def exit_early(self) -> bool:\n        \"\"\" True if the trainer should exit early, without perfoming any training steps \"\"\"\n        return self._exit_early\n\n    def _get_config(self, configfile: str | None) -> dict[str, ConfigValueType]:\n        \"\"\" Get the saved training config options. Override any global settings with the setting\n        provided from the model's saved config.\n\n        Parameters\n        -----------\n        configfile: str\n            The path to a custom configuration file. If ``None`` is passed then configuration is\n            loaded from the default :file:`.config.train.ini` file.\n\n        Returns\n        -------\n        dict\n            The trainer configuration options\n        \"\"\"\n        config = _get_config(\".\".join(self.__module__.split(\".\")[-2:]),\n                             configfile=configfile)\n        for key, val in config.items():\n            if key in self._model.config and val != self._model.config[key]:\n                new_val = self._model.config[key]\n                logger.debug(\"Updating global training config item for '%s' form '%s' to '%s'\",\n                             key, val, new_val)\n                config[key] = new_val\n        return config\n\n    def _handle_lr_finder(self) -> bool:\n        \"\"\" Handle the learning rate finder.\n\n        If this is a new model, then find the optimal learning rate and return ``True`` if user has\n        just requested the graph, otherwise return ``False`` to continue training\n\n        If it as existing model, set the learning rate to the value found by the learing rate\n        finder and return ``False`` to continue training\n\n        Returns\n        -------\n        bool\n            ``True`` if the learning rate finder options dictate that training should not continue\n            after finding the optimal leaning rate\n        \"\"\"\n        if not self._model.command_line_arguments.use_lr_finder:\n            return False\n\n        if self._model.state.iterations == 0 and self._model.state.session_id == 1:\n            lrf = LearningRateFinder(self._model, self._config, self._feeder)\n            success = lrf.find()\n            return self._config[\"lr_finder_mode\"] == \"graph_and_exit\" or not success\n\n        learning_rate = self._model.state.sessions[1][\"config\"][\"learning_rate\"]\n        logger.info(\"Setting learning rate from Learning Rate Finder to %s\",\n                    f\"{learning_rate:.1e}\")\n        return False\n\n    def _set_tensorboard(self) -> tf.keras.callbacks.TensorBoard:\n        \"\"\" Set up Tensorboard callback for logging loss.\n\n        Bypassed if command line option \"no-logs\" has been selected.\n\n        Returns\n        -------\n        :class:`tf.keras.callbacks.TensorBoard`\n            Tensorboard object for the the current training session.\n        \"\"\"\n        if self._model.state.current_session[\"no_logs\"]:\n            logger.verbose(\"TensorBoard logging disabled\")  # type: ignore\n            return None\n        logger.debug(\"Enabling TensorBoard Logging\")\n\n        logger.debug(\"Setting up TensorBoard Logging\")\n        log_dir = os.path.join(str(self._model.io.model_dir),\n                               f\"{self._model.name}_logs\",\n                               f\"session_{self._model.state.session_id}\")\n        tensorboard = tf.keras.callbacks.TensorBoard(log_dir=log_dir,\n                                                     histogram_freq=0,  # Must be 0 or hangs\n                                                     write_graph=True,\n                                                     write_images=False,\n                                                     update_freq=\"batch\",\n                                                     profile_batch=0,\n                                                     embeddings_freq=0,\n                                                     embeddings_metadata=None)\n        tensorboard.set_model(self._model.model)\n        tensorboard.on_train_begin(0)\n        logger.verbose(\"Enabled TensorBoard Logging\")  # type: ignore\n        return tensorboard\n\n    def toggle_mask(self) -> None:\n        \"\"\" Toggle the mask overlay on or off based on user input. \"\"\"\n        self._samples.toggle_mask_display()\n\n    def train_one_step(self,\n                       viewer: Callable[[np.ndarray, str], None] | None,\n                       timelapse_kwargs: dict[T.Literal[\"input_a\", \"input_b\", \"output\"],\n                                              str] | None) -> None:\n        \"\"\" Running training on a batch of images for each side.\n\n        Triggered from the training cycle in :class:`scripts.train.Train`.\n\n        * Runs a training batch through the model.\n\n        * Outputs the iteration's loss values to the console\n\n        * Logs loss to Tensorboard, if logging is requested.\n\n        * If a preview or time-lapse has been requested, then pushes sample images through the \\\n        model to generate the previews\n\n        * Creates a snapshot if the total iterations trained so far meet the requested snapshot \\\n        criteria\n\n        Notes\n        -----\n        As every iteration is called explicitly, the Parameters defined should always be ``None``\n        except on save iterations.\n\n        Parameters\n        ----------\n        viewer: :func:`scripts.train.Train._show` or ``None``\n            The function that will display the preview image\n        timelapse_kwargs: dict\n            The keyword arguments for generating time-lapse previews. If a time-lapse preview is\n            not required then this should be ``None``. Otherwise all values should be full paths\n            the keys being `input_a`, `input_b`, `output`.\n        \"\"\"\n        self._model.state.increment_iterations()\n        logger.trace(\"Training one step: (iteration: %s)\", self._model.iterations)  # type: ignore\n        snapshot_interval = self._model.command_line_arguments.snapshot_interval\n        do_snapshot = (snapshot_interval != 0 and\n                       self._model.iterations - 1 >= snapshot_interval and\n                       (self._model.iterations - 1) % snapshot_interval == 0)\n\n        model_inputs, model_targets = self._feeder.get_batch()\n\n        try:\n            loss: list[float] = self._model.model.train_on_batch(model_inputs, y=model_targets)\n        except tf_errors.ResourceExhaustedError as err:\n            msg = (\"You do not have enough GPU memory available to train the selected model at \"\n                   \"the selected settings. You can try a number of things:\"\n                   \"\\n1) Close any other application that is using your GPU (web browsers are \"\n                   \"particularly bad for this).\"\n                   \"\\n2) Lower the batchsize (the amount of images fed into the model each \"\n                   \"iteration).\"\n                   \"\\n3) Try enabling 'Mixed Precision' training.\"\n                   \"\\n4) Use a more lightweight model, or select the model's 'LowMem' option \"\n                   \"(in config) if it has one.\")\n            raise FaceswapError(msg) from err\n        self._log_tensorboard(loss)\n        loss = self._collate_and_store_loss(loss[1:])\n        self._print_loss(loss)\n        if do_snapshot:\n            self._model.io.snapshot()\n        self._update_viewers(viewer, timelapse_kwargs)\n\n    def _log_tensorboard(self, loss: list[float]) -> None:\n        \"\"\" Log current loss to Tensorboard log files\n\n        Parameters\n        ----------\n        loss: list\n            The list of loss ``floats`` output from the model\n        \"\"\"\n        if not self._tensorboard:\n            return\n        logger.trace(\"Updating TensorBoard log\")  # type: ignore\n        logs = {log[0]: log[1]\n                for log in zip(self._model.state.loss_names, loss)}\n\n        # Bug in TF 2.8/2.9/2.10 where batch recording got deleted.\n        # ref: https://github.com/keras-team/keras/issues/16173\n        with tf.summary.record_if(True), self._tensorboard._train_writer.as_default():  # noqa:E501  pylint:disable=protected-access,not-context-manager\n            for name, value in logs.items():\n                tf.summary.scalar(\n                    \"batch_\" + name,\n                    value,\n                    step=self._tensorboard._train_step)  # pylint:disable=protected-access\n        # TODO revert this code if fixed in tensorflow\n        # self._tensorboard.on_train_batch_end(self._model.iterations, logs=logs)\n\n    def _collate_and_store_loss(self, loss: list[float]) -> list[float]:\n        \"\"\" Collate the loss into totals for each side.\n\n        The losses are summed into a total for each side. Loss totals are added to\n        :attr:`model.state._history` to track the loss drop per save iteration for backup purposes.\n\n        If NaN protection is enabled, Checks for NaNs and raises an error if detected.\n\n        Parameters\n        ----------\n        loss: list\n            The list of loss ``floats`` for each side this iteration (excluding total combined\n            loss)\n\n        Returns\n        -------\n        list\n            List of 2 ``floats`` which is the total loss for each side (eg sum of face + mask loss)\n\n        Raises\n        ------\n        FaceswapError\n            If a NaN is detected, a :class:`FaceswapError` will be raised\n        \"\"\"\n        # NaN protection\n        if self._config[\"nan_protection\"] and not all(np.isfinite(val) for val in loss):\n            logger.critical(\"NaN Detected. Loss: %s\", loss)\n            raise FaceswapError(\"A NaN was detected and you have NaN protection enabled. Training \"\n                                \"has been terminated.\")\n\n        split = len(loss) // 2\n        combined_loss = [sum(loss[:split]), sum(loss[split:])]\n        self._model.add_history(combined_loss)\n        logger.trace(\"original loss: %s, combined_loss: %s\", loss, combined_loss)  # type: ignore\n        return combined_loss\n\n    def _print_loss(self, loss: list[float]) -> None:\n        \"\"\" Outputs the loss for the current iteration to the console.\n\n        Parameters\n        ----------\n        loss: list\n            The loss for each side. List should contain 2 ``floats`` side \"a\" in position 0 and\n            side \"b\" in position `.\n         \"\"\"\n        output = \", \".join([f\"Loss {side}: {side_loss:.5f}\"\n                            for side, side_loss in zip((\"A\", \"B\"), loss)])\n        timestamp = time.strftime(\"%H:%M:%S\")\n        output = f\"[{timestamp}] [#{self._model.iterations:05d}] {output}\"\n        try:\n            print(f\"\\r{output}\", end=\"\")\n        except OSError as err:\n            logger.warning(\"Swallowed OS Error caused by Tensorflow distributed training. output \"\n                           \"line: %s, error: %s\", output, str(err))\n\n    def _update_viewers(self,\n                        viewer: Callable[[np.ndarray, str], None] | None,\n                        timelapse_kwargs: dict[T.Literal[\"input_a\", \"input_b\", \"output\"],\n                                               str] | None) -> None:\n        \"\"\" Update the preview viewer and timelapse output\n\n        Parameters\n        ----------\n        viewer: :func:`scripts.train.Train._show` or ``None``\n            The function that will display the preview image\n        timelapse_kwargs: dict\n            The keyword arguments for generating time-lapse previews. If a time-lapse preview is\n            not required then this should be ``None``. Otherwise all values should be full paths\n            the keys being `input_a`, `input_b`, `output`.\n        \"\"\"\n        if viewer is not None:\n            self._samples.images = self._feeder.generate_preview()\n            samples = self._samples.show_sample()\n            if samples is not None:\n                viewer(samples,\n                       \"Training - 'S': Save Now. 'R': Refresh Preview. 'M': Toggle Mask. 'F': \"\n                       \"Toggle Screen Fit-Actual Size. 'ENTER': Save and Quit\")\n\n        if timelapse_kwargs:\n            self._timelapse.output_timelapse(timelapse_kwargs)\n\n    def clear_tensorboard(self) -> None:\n        \"\"\" Stop Tensorboard logging.\n\n        Tensorboard logging needs to be explicitly shutdown on training termination. Called from\n        :class:`scripts.train.Train` when training is stopped.\n         \"\"\"\n        if not self._tensorboard:\n            return\n        logger.debug(\"Ending Tensorboard Session: %s\", self._tensorboard)\n        self._tensorboard.on_train_end(None)\n\n\nclass _Samples():  # pylint:disable=too-few-public-methods\n    \"\"\" Compile samples for display for preview and time-lapse\n\n    Parameters\n    ----------\n    model: plugin from :mod:`plugins.train.model`\n        The selected model that will be running this trainer\n    coverage_ratio: float\n        Ratio of face to be cropped out of the training image.\n    mask_opacity: int\n        The opacity (as a percentage) to use for the mask overlay\n    mask_color: str\n        The hex RGB value to use the mask overlay\n\n    Attributes\n    ----------\n    images: dict\n        The :class:`numpy.ndarray` training images for generating previews on each side. The\n        dictionary should contain 2 keys (\"a\" and \"b\") with the values being the training images\n        for generating samples corresponding to each side.\n    \"\"\"\n    def __init__(self,\n                 model: ModelBase,\n                 coverage_ratio: float,\n                 mask_opacity: int,\n                 mask_color: str) -> None:\n        logger.debug(\"Initializing %s: model: '%s', coverage_ratio: %s, mask_opacity: %s, \"\n                     \"mask_color: %s)\",\n                     self.__class__.__name__, model, coverage_ratio, mask_opacity, mask_color)\n        self._model = model\n        self._display_mask = model.config[\"learn_mask\"] or model.config[\"penalized_mask_loss\"]\n        self.images: dict[T.Literal[\"a\", \"b\"], list[np.ndarray]] = {}\n        self._coverage_ratio = coverage_ratio\n        self._mask_opacity = mask_opacity / 100.0\n        self._mask_color = np.array(hex_to_rgb(mask_color))[..., 2::-1] / 255.\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    def toggle_mask_display(self) -> None:\n        \"\"\" Toggle the mask overlay on or off depending on user input. \"\"\"\n        if not (self._model.config[\"learn_mask\"] or self._model.config[\"penalized_mask_loss\"]):\n            return\n        display_mask = not self._display_mask\n        print(\"\")  # Break to not garble loss output\n        logger.info(\"Toggling mask display %s...\", \"on\" if display_mask else \"off\")\n        self._display_mask = display_mask\n\n    def show_sample(self) -> np.ndarray:\n        \"\"\" Compile a preview image.\n\n        Returns\n        -------\n        :class:`numpy.ndarry`\n            A compiled preview image ready for display or saving\n        \"\"\"\n        logger.debug(\"Showing sample\")\n        feeds: dict[T.Literal[\"a\", \"b\"], np.ndarray] = {}\n        for idx, side in enumerate(T.get_args(T.Literal[\"a\", \"b\"])):\n            feed = self.images[side][0]\n            input_shape = self._model.model.input_shape[idx][1:]\n            if input_shape[0] / feed.shape[1] != 1.0:\n                feeds[side] = self._resize_sample(side, feed, input_shape[0])\n            else:\n                feeds[side] = feed\n\n        preds = self._get_predictions(feeds[\"a\"], feeds[\"b\"])\n        return self._compile_preview(preds)\n\n    @classmethod\n    def _resize_sample(cls,\n                       side: T.Literal[\"a\", \"b\"],\n                       sample: np.ndarray,\n                       target_size: int) -> np.ndarray:\n        \"\"\" Resize a given image to the target size.\n\n        Parameters\n        ----------\n        side: str\n            The side (\"a\" or \"b\") that the samples are being generated for\n        sample: :class:`numpy.ndarray`\n            The sample to be resized\n        target_size: int\n            The size that the sample should be resized to\n\n        Returns\n        -------\n        :class:`numpy.ndarray`\n            The sample resized to the target size\n        \"\"\"\n        scale = target_size / sample.shape[1]\n        if scale == 1.0:\n            # cv2 complains if we don't do this :/\n            return np.ascontiguousarray(sample)\n        logger.debug(\"Resizing sample: (side: '%s', sample.shape: %s, target_size: %s, scale: %s)\",\n                     side, sample.shape, target_size, scale)\n        interpn = cv2.INTER_CUBIC if scale > 1.0 else cv2.INTER_AREA\n        retval = np.array([cv2.resize(img, (target_size, target_size), interpolation=interpn)\n                           for img in sample])\n        logger.debug(\"Resized sample: (side: '%s' shape: %s)\", side, retval.shape)\n        return retval\n\n    def _get_predictions(self, feed_a: np.ndarray, feed_b: np.ndarray) -> dict[str, np.ndarray]:\n        \"\"\" Feed the samples to the model and return predictions\n\n        Parameters\n        ----------\n        feed_a: :class:`numpy.ndarray`\n            Feed images for the \"a\" side\n        feed_a: :class:`numpy.ndarray`\n            Feed images for the \"b\" side\n\n        Returns\n        -------\n        list:\n            List of :class:`numpy.ndarray` of predictions received from the model\n        \"\"\"\n        logger.debug(\"Getting Predictions\")\n        preds: dict[str, np.ndarray] = {}\n\n        # Calling model.predict() can lead to both VRAM and system memory leaks, so call model\n        # directly\n        standard = self._model.model([feed_a, feed_b])\n        swapped = self._model.model([feed_b, feed_a])\n\n        if self._model.config[\"learn_mask\"]:  # Add mask to 4th channel of final output\n            standard = [np.concatenate(side[-2:], axis=-1)\n                        for side in [[s.numpy() for s in t] for t in standard]]\n            swapped = [np.concatenate(side[-2:], axis=-1)\n                       for side in [[s.numpy() for s in t] for t in swapped]]\n        else:  # Retrieve final output\n            standard = [side[-1] if isinstance(side, list) else side\n                        for side in [t.numpy() for t in standard]]\n            swapped = [side[-1] if isinstance(side, list) else side\n                       for side in [t.numpy() for t in swapped]]\n\n        preds[\"a_a\"] = standard[0]\n        preds[\"b_b\"] = standard[1]\n        preds[\"a_b\"] = swapped[0]\n        preds[\"b_a\"] = swapped[1]\n\n        logger.debug(\"Returning predictions: %s\", {key: val.shape for key, val in preds.items()})\n        return preds\n\n    def _compile_preview(self, predictions: dict[str, np.ndarray]) -> np.ndarray:\n        \"\"\" Compile predictions and images into the final preview image.\n\n        Parameters\n        ----------\n        predictions: dict\n            The predictions from the model\n\n        Returns\n        -------\n        :class:`numpy.ndarry`\n            A compiled preview image ready for display or saving\n        \"\"\"\n        figures: dict[T.Literal[\"a\", \"b\"], np.ndarray] = {}\n        headers: dict[T.Literal[\"a\", \"b\"], np.ndarray] = {}\n\n        for side, samples in self.images.items():\n            other_side = \"a\" if side == \"b\" else \"b\"\n            preds = [predictions[f\"{side}_{side}\"],\n                     predictions[f\"{other_side}_{side}\"]]\n            display = self._to_full_frame(side, samples, preds)\n            headers[side] = self._get_headers(side, display[0].shape[1])\n            figures[side] = np.stack([display[0], display[1], display[2], ], axis=1)\n            if self.images[side][1].shape[0] % 2 == 1:\n                figures[side] = np.concatenate([figures[side],\n                                                np.expand_dims(figures[side][0], 0)])\n\n        width = 4\n        if width // 2 != 1:\n            headers = self._duplicate_headers(headers, width // 2)\n\n        header = np.concatenate([headers[\"a\"], headers[\"b\"]], axis=1)\n        figure = np.concatenate([figures[\"a\"], figures[\"b\"]], axis=0)\n        height = int(figure.shape[0] / width)\n        figure = figure.reshape((width, height) + figure.shape[1:])\n        figure = _stack_images(figure)\n        figure = np.concatenate((header, figure), axis=0)\n\n        logger.debug(\"Compiled sample\")\n        return np.clip(figure * 255, 0, 255).astype('uint8')\n\n    def _to_full_frame(self,\n                       side: T.Literal[\"a\", \"b\"],\n                       samples: list[np.ndarray],\n                       predictions: list[np.ndarray]) -> list[np.ndarray]:\n        \"\"\" Patch targets and prediction images into images of model output size.\n\n        Parameters\n        ----------\n        side: {\"a\" or \"b\"}\n            The side that these samples are for\n        samples: list\n            List of :class:`numpy.ndarray` of feed images and sample images\n        predictions: list\n            List of :class: `numpy.ndarray` of predictions from the model\n\n        Returns\n        -------\n        list\n            The images resized and collated for display in the preview frame\n        \"\"\"\n        logger.debug(\"side: '%s', number of sample arrays: %s, prediction.shapes: %s)\",\n                     side, len(samples), [pred.shape for pred in predictions])\n        faces, full = samples[:2]\n\n        if self._model.color_order.lower() == \"rgb\":  # Switch color order for RGB model display\n            full = full[..., ::-1]\n            faces = faces[..., ::-1]\n            predictions = [pred[..., 2::-1] for pred in predictions]\n\n        full = self._process_full(side, full, predictions[0].shape[1], (0., 0., 1.0))\n        images = [faces] + predictions\n\n        if self._display_mask:\n            images = self._compile_masked(images, samples[-1])\n        elif self._model.config[\"learn_mask\"]:\n            # Remove masks when learn mask is selected but mask toggle is off\n            images = [batch[..., :3] for batch in images]\n\n        images = [self._overlay_foreground(full.copy(), image) for image in images]\n\n        return images\n\n    def _process_full(self,\n                      side: T.Literal[\"a\", \"b\"],\n                      images: np.ndarray,\n                      prediction_size: int,\n                      color: tuple[float, float, float]) -> np.ndarray:\n        \"\"\" Add a frame overlay to preview images indicating the region of interest.\n\n        This applies the red border that appears in the preview images.\n\n        Parameters\n        ----------\n        side: {\"a\" or \"b\"}\n            The side that these samples are for\n        images: :class:`numpy.ndarray`\n            The input training images to to process\n        prediction_size: int\n            The size of the predicted output from the model\n        color: tuple\n            The (Blue, Green, Red) color to use for the frame\n\n        Returns\n        -------\n        :class:`numpy,ndarray`\n            The input training images, sized for output and annotated for coverage\n        \"\"\"\n        logger.debug(\"full_size: %s, prediction_size: %s, color: %s\",\n                     images.shape[1], prediction_size, color)\n\n        display_size = int((prediction_size / self._coverage_ratio // 2) * 2)\n        images = self._resize_sample(side, images, display_size)  # Resize targets to display size\n        padding = (display_size - prediction_size) // 2\n        if padding == 0:\n            logger.debug(\"Resized background. Shape: %s\", images.shape)\n            return images\n\n        length = display_size // 4\n        t_l, b_r = (padding - 1, display_size - padding)\n        for img in images:\n            cv2.rectangle(img, (t_l, t_l), (t_l + length, t_l + length), color, 1)\n            cv2.rectangle(img, (b_r, t_l), (b_r - length, t_l + length), color, 1)\n            cv2.rectangle(img, (b_r, b_r), (b_r - length, b_r - length), color, 1)\n            cv2.rectangle(img, (t_l, b_r), (t_l + length, b_r - length), color, 1)\n        logger.debug(\"Overlayed background. Shape: %s\", images.shape)\n        return images\n\n    def _compile_masked(self, faces: list[np.ndarray], masks: np.ndarray) -> list[np.ndarray]:\n        \"\"\" Add the mask to the faces for masked preview.\n\n        Places an opaque red layer over areas of the face that are masked out.\n\n        Parameters\n        ----------\n        faces: list\n            The :class:`numpy.ndarray` sample faces and predictions that are to have the mask\n            applied\n        masks: :class:`numpy.ndarray`\n            The masks that are to be applied to the faces\n\n        Returns\n        -------\n        list\n            List of :class:`numpy.ndarray` faces with the opaque mask layer applied\n        \"\"\"\n        orig_masks = 1. - masks\n        masks3: list[np.ndarray] | np.ndarray = []\n\n        if faces[-1].shape[-1] == 4:  # Mask contained in alpha channel of predictions\n            pred_masks = [1. - face[..., -1][..., None] for face in faces[-2:]]\n            faces[-2:] = [face[..., :-1] for face in faces[-2:]]\n            masks3 = [orig_masks, *pred_masks]\n        else:\n            masks3 = np.repeat(np.expand_dims(orig_masks, axis=0), 3, axis=0)\n\n        retval: list[np.ndarray] = []\n        overlays3 = np.ones_like(faces) * self._mask_color\n        for previews, overlays, compiled_masks in zip(faces, overlays3, masks3):\n            compiled_masks *= self._mask_opacity\n            overlays *= compiled_masks\n            previews *= (1. - compiled_masks)\n            retval.append(previews + overlays)\n        logger.debug(\"masked shapes: %s\", [faces.shape for faces in retval])\n        return retval\n\n    @classmethod\n    def _overlay_foreground(cls, backgrounds: np.ndarray, foregrounds: np.ndarray) -> np.ndarray:\n        \"\"\" Overlay the preview images into the center of the background images\n\n        Parameters\n        ----------\n        backgrounds: :class:`numpy.ndarray`\n            Background images for placing the preview images onto\n        backgrounds: :class:`numpy.ndarray`\n            Preview images for placing onto the background images\n\n        Returns\n        -------\n        :class:`numpy.ndarray`\n            The preview images compiled into the full frame size for each preview\n        \"\"\"\n        offset = (backgrounds.shape[1] - foregrounds.shape[1]) // 2\n        for foreground, background in zip(foregrounds, backgrounds):\n            background[offset:offset + foreground.shape[0],\n                       offset:offset + foreground.shape[1], :3] = foreground\n        logger.debug(\"Overlayed foreground. Shape: %s\", backgrounds.shape)\n        return backgrounds\n\n    @classmethod\n    def _get_headers(cls, side: T.Literal[\"a\", \"b\"], width: int) -> np.ndarray:\n        \"\"\" Set header row for the final preview frame\n\n        Parameters\n        ----------\n        side: {\"a\" or \"b\"}\n            The side that the headers should be generated for\n        width: int\n            The width of each column in the preview frame\n\n        Returns\n        -------\n        :class:`numpy.ndarray`\n            The column headings for the given side\n        \"\"\"\n        logger.debug(\"side: '%s', width: %s\",\n                     side, width)\n        titles = (\"Original\", \"Swap\") if side == \"a\" else (\"Swap\", \"Original\")\n        height = int(width / 4.5)\n        total_width = width * 3\n        logger.debug(\"height: %s, total_width: %s\", height, total_width)\n        font = cv2.FONT_HERSHEY_SIMPLEX\n        texts = [f\"{titles[0]} ({side.upper()})\",\n                 f\"{titles[0]} > {titles[0]}\",\n                 f\"{titles[0]} > {titles[1]}\"]\n        scaling = (width / 144) * 0.45\n        text_sizes = [cv2.getTextSize(texts[idx], font, scaling, 1)[0]\n                      for idx in range(len(texts))]\n        text_y = int((height + text_sizes[0][1]) / 2)\n        text_x = [int((width - text_sizes[idx][0]) / 2) + width * idx\n                  for idx in range(len(texts))]\n        logger.debug(\"texts: %s, text_sizes: %s, text_x: %s, text_y: %s\",\n                     texts, text_sizes, text_x, text_y)\n        header_box = np.ones((height, total_width, 3), np.float32)\n        for idx, text in enumerate(texts):\n            cv2.putText(header_box,\n                        text,\n                        (text_x[idx], text_y),\n                        font,\n                        scaling,\n                        (0, 0, 0),\n                        1,\n                        lineType=cv2.LINE_AA)\n        logger.debug(\"header_box.shape: %s\", header_box.shape)\n        return header_box\n\n    @classmethod\n    def _duplicate_headers(cls,\n                           headers: dict[T.Literal[\"a\", \"b\"], np.ndarray],\n                           columns: int) -> dict[T.Literal[\"a\", \"b\"], np.ndarray]:\n        \"\"\" Duplicate headers for the number of columns displayed for each side.\n\n        Parameters\n        ----------\n        headers: dict\n            The headers to be duplicated for each side\n        columns: int\n            The number of columns that the header needs to be duplicated for\n\n        Returns\n        -------\n        :class:dict\n            The original headers duplicated by the number of columns for each side\n        \"\"\"\n        for side, header in headers.items():\n            duped = tuple(header for _ in range(columns))\n            headers[side] = np.concatenate(duped, axis=1)\n            logger.debug(\"side: %s header.shape: %s\", side, header.shape)\n        return headers\n\n\nclass _Timelapse():  # pylint:disable=too-few-public-methods\n    \"\"\" Create a time-lapse preview image.\n\n    Parameters\n    ----------\n    model: plugin from :mod:`plugins.train.model`\n        The selected model that will be running this trainer\n    coverage_ratio: float\n        Ratio of face to be cropped out of the training image.\n    image_count: int\n        The number of preview images to be displayed in the time-lapse\n    mask_opacity: int\n        The opacity (as a percentage) to use for the mask overlay\n    mask_color: str\n        The hex RGB value to use the mask overlay\n    feeder: :class:`~lib.training.generator.Feeder`\n        The feeder for generating the time-lapse images.\n    image_paths: dict\n        The full paths to the training images for each side of the model\n    \"\"\"\n    def __init__(self,\n                 model: ModelBase,\n                 coverage_ratio: float,\n                 image_count: int,\n                 mask_opacity: int,\n                 mask_color: str,\n                 feeder: Feeder,\n                 image_paths: dict[T.Literal[\"a\", \"b\"], list[str]]) -> None:\n        logger.debug(\"Initializing %s: model: %s, coverage_ratio: %s, image_count: %s, \"\n                     \"mask_opacity: %s, mask_color: %s, feeder: %s, image_paths: %s)\",\n                     self.__class__.__name__, model, coverage_ratio, image_count, mask_opacity,\n                     mask_color, feeder, len(image_paths))\n        self._num_images = image_count\n        self._samples = _Samples(model, coverage_ratio, mask_opacity, mask_color)\n        self._model = model\n        self._feeder = feeder\n        self._image_paths = image_paths\n        self._output_file = \"\"\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    def _setup(self, input_a: str, input_b: str, output: str) -> None:\n        \"\"\" Setup the time-lapse folder locations and the time-lapse feed.\n\n        Parameters\n        ----------\n        input_a: str\n            The full path to the time-lapse input folder containing faces for the \"a\" side\n        input_b: str\n            The full path to the time-lapse input folder containing faces for the \"b\" side\n        output: str, optional\n            The full path to the time-lapse output folder. If ``None`` is provided this will\n            default to the model folder\n        \"\"\"\n        logger.debug(\"Setting up time-lapse\")\n        if not output:\n            output = get_folder(os.path.join(str(self._model.io.model_dir),\n                                             f\"{self._model.name}_timelapse\"))\n        self._output_file = output\n        logger.debug(\"Time-lapse output set to '%s'\", self._output_file)\n\n        # Rewrite paths to pull from the training images so mask and face data can be accessed\n        images: dict[T.Literal[\"a\", \"b\"], list[str]] = {}\n        for side, input_ in zip(T.get_args(T.Literal[\"a\", \"b\"]), (input_a, input_b)):\n            training_path = os.path.dirname(self._image_paths[side][0])\n            images[side] = [os.path.join(training_path, os.path.basename(pth))\n                            for pth in get_image_paths(input_)]\n\n        batchsize = min(len(images[\"a\"]),\n                        len(images[\"b\"]),\n                        self._num_images)\n        self._feeder.set_timelapse_feed(images, batchsize)\n        logger.debug(\"Set up time-lapse\")\n\n    def output_timelapse(self, timelapse_kwargs: dict[T.Literal[\"input_a\",\n                                                                \"input_b\",\n                                                                \"output\"], str]) -> None:\n        \"\"\" Generate the time-lapse samples and output the created time-lapse to the specified\n        output folder.\n\n        Parameters\n        ----------\n        timelapse_kwargs: dict:\n            The keyword arguments for setting up the time-lapse. All values should be full paths\n            the keys being `input_a`, `input_b`, `output`\n        \"\"\"\n        logger.debug(\"Ouputting time-lapse\")\n        if not self._output_file:\n            self._setup(**T.cast(dict[str, str], timelapse_kwargs))\n\n        logger.debug(\"Getting time-lapse samples\")\n        self._samples.images = self._feeder.generate_preview(is_timelapse=True)\n        logger.debug(\"Got time-lapse samples: %s\",\n                     {side: len(images) for side, images in self._samples.images.items()})\n\n        image = self._samples.show_sample()\n        if image is None:\n            return\n        filename = os.path.join(self._output_file, str(int(time.time())) + \".jpg\")\n\n        cv2.imwrite(filename, image)\n        logger.debug(\"Created time-lapse: '%s'\", filename)\n\n\ndef _stack_images(images: np.ndarray) -> np.ndarray:\n    \"\"\" Stack images evenly for preview.\n\n    Parameters\n    ----------\n    images: :class:`numpy.ndarray`\n        The preview images to be stacked\n\n    Returns\n    -------\n    :class:`numpy.ndarray`\n        The stacked preview images\n    \"\"\"\n    logger.debug(\"Stack images\")\n\n    def get_transpose_axes(num):\n        if num % 2 == 0:\n            logger.debug(\"Even number of images to stack\")\n            y_axes = list(range(1, num - 1, 2))\n            x_axes = list(range(0, num - 1, 2))\n        else:\n            logger.debug(\"Odd number of images to stack\")\n            y_axes = list(range(0, num - 1, 2))\n            x_axes = list(range(1, num - 1, 2))\n        return y_axes, x_axes, [num - 1]\n\n    images_shape = np.array(images.shape)\n    new_axes = get_transpose_axes(len(images_shape))\n    new_shape = [np.prod(images_shape[x]) for x in new_axes]\n    logger.debug(\"Stacked images\")\n    return np.transpose(images, axes=np.concatenate(new_axes)).reshape(new_shape)\n", "plugins/train/trainer/__init__.py": "", "plugins/train/model/original_defaults.py": "#!/usr/bin/env python3\n\"\"\"\n    The default options for the faceswap Original Model plugin.\n\n    Defaults files should be named <plugin_name>_defaults.py\n    Any items placed into this file will automatically get added to the relevant config .ini files\n    within the faceswap/config folder.\n\n    The following variables should be defined:\n        _HELPTEXT: A string describing what this plugin does\n        _DEFAULTS: A dictionary containing the options, defaults and meta information. The\n                   dictionary should be defined as:\n                       {<option_name>: {<metadata>}}\n\n                   <option_name> should always be lower text.\n                   <metadata> dictionary requirements are listed below.\n\n    The following keys are expected for the _DEFAULTS <metadata> dict:\n        datatype:  [required] A python type class. This limits the type of data that can be\n                   provided in the .ini file and ensures that the value is returned in the\n                   correct type to faceswap. Valid data types are: <class 'int'>, <class 'float'>,\n                   <class 'str'>, <class 'bool'>.\n        default:   [required] The default value for this option.\n        info:      [required] A string describing what this option does.\n        choices:   [optional] If this option's datatype is of <class 'str'> then valid\n                   selections can be defined here. This validates the option and also enables\n                   a combobox / radio option in the GUI.\n        gui_radio: [optional] If <choices> are defined, this indicates that the GUI should use\n                   radio buttons rather than a combobox to display this option.\n        min_max:   [partial] For <class 'int'> and <class 'float'> data types this is required\n                   otherwise it is ignored. Should be a tuple of min and max accepted values.\n                   This is used for controlling the GUI slider range. Values are not enforced.\n        rounding:  [partial] For <class 'int'> and <class 'float'> data types this is\n                   required otherwise it is ignored. Used for the GUI slider. For floats, this\n                   is the number of decimal places to display. For ints this is the step size.\n                   You can also pass in a list of discreet values for this item, which should be\n                   of the same data type as the given 'datatype'. This will lock the scale to\n                   only those values displayed in the list.\n        fixed:     [optional] [train only]. Training configurations are fixed when the model is\n                   created, and then reloaded from the state file. Marking an item as fixed=False\n                   indicates that this value can be changed for existing models, and will override\n                   the value saved in the state file with the updated value in config. If not\n                   provided this will default to True.\n\"\"\"\n\n\n_HELPTEXT = \"Original Faceswap Model.\"\n\n\n_DEFAULTS = dict(\n    lowmem=dict(\n        default=False,\n        info=\"Lower memory mode. Set to 'True' if having issues with VRAM useage.\\n\"\n             \"NB: Models with a changed lowmem mode are not compatible with each other.\",\n        datatype=bool,\n        rounding=None,\n        min_max=None,\n        choices=[],\n        gui_radio=False,\n        fixed=True,\n        group=\"settings\",\n    ),\n)\n", "plugins/train/model/unbalanced.py": "#!/usr/bin/env python3\n\"\"\" Unbalanced Model\n    Based on the original https://www.reddit.com/r/deepfakes/\n        code sample + contributions \"\"\"\n\n# Ignore linting errors from Tensorflow's thoroughly broken import system\nfrom tensorflow.keras.initializers import RandomNormal  # pylint:disable=import-error\nfrom tensorflow.keras.layers import (  # pylint:disable=import-error\n    Dense, Flatten, Input, LeakyReLU, Reshape, SpatialDropout2D)\nfrom tensorflow.keras.models import Model as KModel  # pylint:disable=import-error\n\nfrom lib.model.nn_blocks import Conv2DOutput, Conv2DBlock, ResidualBlock, UpscaleBlock\nfrom ._base import ModelBase\n\n\nclass Model(ModelBase):\n    \"\"\" Unbalanced Faceswap Model \"\"\"\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.input_shape = (self.config[\"input_size\"], self.config[\"input_size\"], 3)\n        self.low_mem = self.config.get(\"lowmem\", False)\n        self.encoder_dim = 512 if self.low_mem else self.config[\"nodes\"]\n        self.kernel_initializer = RandomNormal(0, 0.02)\n\n    def build_model(self, inputs):\n        \"\"\" build the Unbalanced Model. \"\"\"\n        encoder = self.encoder()\n        encoder_a = encoder(inputs[0])\n        encoder_b = encoder(inputs[1])\n\n        outputs = [self.decoder_a()(encoder_a), self.decoder_b()(encoder_b)]\n\n        autoencoder = KModel(inputs, outputs, name=self.model_name)\n        return autoencoder\n\n    def encoder(self):\n        \"\"\" Unbalanced Encoder \"\"\"\n        kwargs = {\"kernel_initializer\": self.kernel_initializer}\n        encoder_complexity = 128 if self.low_mem else self.config[\"complexity_encoder\"]\n        dense_dim = 384 if self.low_mem else 512\n        dense_shape = self.input_shape[0] // 16\n        input_ = Input(shape=self.input_shape)\n\n        var_x = input_\n        var_x = Conv2DBlock(encoder_complexity,\n                            normalization=\"instance\",\n                            activation=\"leakyrelu\",\n                            **kwargs)(var_x)\n        var_x = Conv2DBlock(encoder_complexity * 2,\n                            normalization=\"instance\",\n                            activation=\"leakyrelu\",\n                            **kwargs)(var_x)\n        var_x = Conv2DBlock(encoder_complexity * 4, **kwargs, activation=\"leakyrelu\")(var_x)\n        var_x = Conv2DBlock(encoder_complexity * 6, **kwargs, activation=\"leakyrelu\")(var_x)\n        var_x = Conv2DBlock(encoder_complexity * 8, **kwargs, activation=\"leakyrelu\")(var_x)\n        var_x = Dense(self.encoder_dim,\n                      kernel_initializer=self.kernel_initializer)(Flatten()(var_x))\n        var_x = Dense(dense_shape * dense_shape * dense_dim,\n                      kernel_initializer=self.kernel_initializer)(var_x)\n        var_x = Reshape((dense_shape, dense_shape, dense_dim))(var_x)\n        return KModel(input_, var_x, name=\"encoder\")\n\n    def decoder_a(self):\n        \"\"\" Decoder for side A \"\"\"\n        kwargs = {\"kernel_size\": 5, \"kernel_initializer\": self.kernel_initializer}\n        decoder_complexity = 320 if self.low_mem else self.config[\"complexity_decoder_a\"]\n        dense_dim = 384 if self.low_mem else 512\n        decoder_shape = self.input_shape[0] // 16\n        input_ = Input(shape=(decoder_shape, decoder_shape, dense_dim))\n\n        var_x = input_\n\n        var_x = UpscaleBlock(decoder_complexity, activation=\"leakyrelu\", **kwargs)(var_x)\n        var_x = SpatialDropout2D(0.25)(var_x)\n        var_x = UpscaleBlock(decoder_complexity, activation=\"leakyrelu\", **kwargs)(var_x)\n        if self.low_mem:\n            var_x = SpatialDropout2D(0.15)(var_x)\n        else:\n            var_x = SpatialDropout2D(0.25)(var_x)\n        var_x = UpscaleBlock(decoder_complexity // 2, activation=\"leakyrelu\", **kwargs)(var_x)\n        var_x = UpscaleBlock(decoder_complexity // 4, activation=\"leakyrelu\", **kwargs)(var_x)\n        var_x = Conv2DOutput(3, 5, name=\"face_out_a\")(var_x)\n        outputs = [var_x]\n\n        if self.config.get(\"learn_mask\", False):\n            var_y = input_\n            var_y = UpscaleBlock(decoder_complexity, activation=\"leakyrelu\")(var_y)\n            var_y = UpscaleBlock(decoder_complexity, activation=\"leakyrelu\")(var_y)\n            var_y = UpscaleBlock(decoder_complexity // 2, activation=\"leakyrelu\")(var_y)\n            var_y = UpscaleBlock(decoder_complexity // 4, activation=\"leakyrelu\")(var_y)\n            var_y = Conv2DOutput(1, 5, name=\"mask_out_a\")(var_y)\n            outputs.append(var_y)\n        return KModel(input_, outputs=outputs, name=\"decoder_a\")\n\n    def decoder_b(self):\n        \"\"\" Decoder for side B \"\"\"\n        kwargs = {\"kernel_size\": 5, \"kernel_initializer\": self.kernel_initializer}\n        decoder_complexity = 384 if self.low_mem else self.config[\"complexity_decoder_b\"]\n        dense_dim = 384 if self.low_mem else 512\n        decoder_shape = self.input_shape[0] // 16\n        input_ = Input(shape=(decoder_shape, decoder_shape, dense_dim))\n\n        var_x = input_\n        if self.low_mem:\n            var_x = UpscaleBlock(decoder_complexity, activation=\"leakyrelu\", **kwargs)(var_x)\n            var_x = UpscaleBlock(decoder_complexity // 2, activation=\"leakyrelu\", **kwargs)(var_x)\n            var_x = UpscaleBlock(decoder_complexity // 4, activation=\"leakyrelu\", **kwargs)(var_x)\n            var_x = UpscaleBlock(decoder_complexity // 8, activation=\"leakyrelu\", **kwargs)(var_x)\n        else:\n            var_x = UpscaleBlock(decoder_complexity, activation=None, **kwargs)(var_x)\n            var_x = LeakyReLU(alpha=0.2)(var_x)\n            var_x = ResidualBlock(decoder_complexity,\n                                  kernel_initializer=self.kernel_initializer)(var_x)\n            var_x = UpscaleBlock(decoder_complexity, activation=None, **kwargs)(var_x)\n            var_x = LeakyReLU(alpha=0.2)(var_x)\n            var_x = ResidualBlock(decoder_complexity,\n                                  kernel_initializer=self.kernel_initializer)(var_x)\n            var_x = UpscaleBlock(decoder_complexity // 2, activation=None, **kwargs)(var_x)\n            var_x = LeakyReLU(alpha=0.2)(var_x)\n            var_x = ResidualBlock(decoder_complexity // 2,\n                                  kernel_initializer=self.kernel_initializer)(var_x)\n            var_x = UpscaleBlock(decoder_complexity // 4, activation=\"leakyrelu\", **kwargs)(var_x)\n        var_x = Conv2DOutput(3, 5, name=\"face_out_b\")(var_x)\n        outputs = [var_x]\n\n        if self.config.get(\"learn_mask\", False):\n            var_y = input_\n            var_y = UpscaleBlock(decoder_complexity, activation=\"leakyrelu\")(var_y)\n            if not self.low_mem:\n                var_y = UpscaleBlock(decoder_complexity, activation=\"leakyrelu\")(var_y)\n            var_y = UpscaleBlock(decoder_complexity // 2, activation=\"leakyrelu\")(var_y)\n            var_y = UpscaleBlock(decoder_complexity // 4, activation=\"leakyrelu\")(var_y)\n            if self.low_mem:\n                var_y = UpscaleBlock(decoder_complexity // 8, activation=\"leakyrelu\")(var_y)\n            var_y = Conv2DOutput(1, 5, name=\"mask_out_b\")(var_y)\n            outputs.append(var_y)\n        return KModel(input_, outputs=outputs, name=\"decoder_b\")\n\n    def _legacy_mapping(self):\n        \"\"\" The mapping of legacy separate model names to single model names \"\"\"\n        return {f\"{self.name}_encoder.h5\": \"encoder\",\n                f\"{self.name}_decoder_A.h5\": \"decoder_a\",\n                f\"{self.name}_decoder_B.h5\": \"decoder_b\"}\n", "plugins/train/model/phaze_a.py": "#!/usr/bin/env python3\n\"\"\" Phaze-A Model by TorzDF with thanks to BirbFakes and the myriad of testers. \"\"\"\n\n# pylint:disable=too-many-lines\nfrom __future__ import annotations\nimport logging\nimport typing as T\nfrom dataclasses import dataclass\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom lib.model.nn_blocks import (\n    Conv2D, Conv2DBlock, Conv2DOutput, ResidualBlock, UpscaleBlock, Upscale2xBlock,\n    UpscaleResizeImagesBlock, UpscaleDNYBlock)\nfrom lib.model.normalization import (\n    AdaInstanceNormalization, GroupNormalization, InstanceNormalization, RMSNormalization)\nfrom lib.model.networks import ViT, TypeModelsViT\nfrom lib.utils import get_tf_version, FaceswapError\n\nfrom ._base import ModelBase, get_all_sub_models\n\nlogger = logging.getLogger(__name__)\n\nK = tf.keras.backend\nkapp = tf.keras.applications\nkl = tf.keras.layers\nkeras = tf.keras\n\n\n@dataclass\nclass _EncoderInfo:\n    \"\"\" Contains model configuration options for various Phaze-A Encoders.\n\n    Parameters\n    ----------\n    keras_name: str\n        The name of the encoder in Keras Applications. Empty string `\"\"` if the encoder does not\n        exist in Keras Applications\n    default_size: int\n        The default input size of the encoder\n    tf_min: float, optional\n        The lowest version of Tensorflow that the encoder can be used for. Default: `2.0`\n    scaling: tuple, optional\n        The float scaling that the encoder expects. Default: `(0, 1)`\n    min_size: int, optional\n        The minimum input size that the encoder will allow. Default: 32\n    enforce_for_weights: bool, optional\n        ``True`` if the input size for the model must be forced to the default size when loading\n        imagenet weights, otherwise ``False``. Default: ``False``\n    color_order: str, optional\n        The color order that the model expects (`\"bgr\"` or `\"rgb\"`). Default: `\"rgb\"`\n    \"\"\"\n    keras_name: str\n    default_size: int\n    tf_min: tuple[int, int] = (2, 0)\n    scaling: tuple[int, int] = (0, 1)\n    min_size: int = 32\n    enforce_for_weights: bool = False\n    color_order: T.Literal[\"bgr\", \"rgb\"] = \"rgb\"\n\n\n_MODEL_MAPPING: dict[str, _EncoderInfo] = {\n    \"clipv_farl-b-16-16\": _EncoderInfo(\n        keras_name=\"FaRL-B-16-16\", default_size=224),\n    \"clipv_farl-b-16-64\": _EncoderInfo(\n        keras_name=\"FaRL-B-16-64\", default_size=224),\n    \"clipv_vit-b-16\": _EncoderInfo(\n        keras_name=\"ViT-B-16\", default_size=224),\n    \"clipv_vit-b-32\": _EncoderInfo(\n        keras_name=\"ViT-B-32\", default_size=224),\n    \"clipv_vit-l-14\": _EncoderInfo(\n        keras_name=\"ViT-L-14\", default_size=224),\n    \"clipv_vit-l-14-336px\": _EncoderInfo(\n        keras_name=\"ViT-L-14-336px\", default_size=336),\n    \"densenet121\": _EncoderInfo(\n        keras_name=\"DenseNet121\", default_size=224),\n    \"densenet169\": _EncoderInfo(\n        keras_name=\"DenseNet169\", default_size=224),\n    \"densenet201\": _EncoderInfo(\n        keras_name=\"DenseNet201\", default_size=224),\n    \"efficientnet_b0\": _EncoderInfo(\n        keras_name=\"EfficientNetB0\", tf_min=(2, 3), scaling=(0, 255), default_size=224),\n    \"efficientnet_b1\": _EncoderInfo(\n        keras_name=\"EfficientNetB1\", tf_min=(2, 3), scaling=(0, 255), default_size=240),\n    \"efficientnet_b2\": _EncoderInfo(\n        keras_name=\"EfficientNetB2\", tf_min=(2, 3), scaling=(0, 255), default_size=260),\n    \"efficientnet_b3\": _EncoderInfo(\n        keras_name=\"EfficientNetB3\", tf_min=(2, 3), scaling=(0, 255), default_size=300),\n    \"efficientnet_b4\": _EncoderInfo(\n        keras_name=\"EfficientNetB4\", tf_min=(2, 3), scaling=(0, 255), default_size=380),\n    \"efficientnet_b5\": _EncoderInfo(\n        keras_name=\"EfficientNetB5\", tf_min=(2, 3), scaling=(0, 255), default_size=456),\n    \"efficientnet_b6\": _EncoderInfo(\n        keras_name=\"EfficientNetB6\", tf_min=(2, 3), scaling=(0, 255), default_size=528),\n    \"efficientnet_b7\": _EncoderInfo(\n        keras_name=\"EfficientNetB7\", tf_min=(2, 3), scaling=(0, 255), default_size=600),\n    \"efficientnet_v2_b0\": _EncoderInfo(\n        keras_name=\"EfficientNetV2B0\", tf_min=(2, 8), scaling=(-1, 1), default_size=224),\n    \"efficientnet_v2_b1\": _EncoderInfo(\n        keras_name=\"EfficientNetV2B1\", tf_min=(2, 8), scaling=(-1, 1), default_size=240),\n    \"efficientnet_v2_b2\": _EncoderInfo(\n        keras_name=\"EfficientNetV2B2\", tf_min=(2, 8), scaling=(-1, 1), default_size=260),\n    \"efficientnet_v2_b3\": _EncoderInfo(\n        keras_name=\"EfficientNetV2B3\", tf_min=(2, 8), scaling=(-1, 1), default_size=300),\n    \"efficientnet_v2_s\": _EncoderInfo(\n        keras_name=\"EfficientNetV2S\", tf_min=(2, 8), scaling=(-1, 1), default_size=384),\n    \"efficientnet_v2_m\": _EncoderInfo(\n        keras_name=\"EfficientNetV2M\", tf_min=(2, 8), scaling=(-1, 1), default_size=480),\n    \"efficientnet_v2_l\": _EncoderInfo(\n        keras_name=\"EfficientNetV2L\", tf_min=(2, 8), scaling=(-1, 1), default_size=480),\n    \"inception_resnet_v2\": _EncoderInfo(\n        keras_name=\"InceptionResNetV2\", scaling=(-1, 1), min_size=75, default_size=299),\n    \"inception_v3\": _EncoderInfo(\n        keras_name=\"InceptionV3\", scaling=(-1, 1), min_size=75, default_size=299),\n    \"mobilenet\": _EncoderInfo(\n        keras_name=\"MobileNet\", scaling=(-1, 1), default_size=224),\n    \"mobilenet_v2\": _EncoderInfo(\n        keras_name=\"MobileNetV2\", scaling=(-1, 1), default_size=224),\n    \"mobilenet_v3_large\": _EncoderInfo(\n        keras_name=\"MobileNetV3Large\", tf_min=(2, 4), scaling=(-1, 1), default_size=224),\n    \"mobilenet_v3_small\": _EncoderInfo(\n        keras_name=\"MobileNetV3Small\", tf_min=(2, 4), scaling=(-1, 1), default_size=224),\n    \"nasnet_large\": _EncoderInfo(\n        keras_name=\"NASNetLarge\", scaling=(-1, 1), default_size=331, enforce_for_weights=True),\n    \"nasnet_mobile\": _EncoderInfo(\n        keras_name=\"NASNetMobile\", scaling=(-1, 1), default_size=224, enforce_for_weights=True),\n    \"resnet50\": _EncoderInfo(\n        keras_name=\"ResNet50\", scaling=(-1, 1), min_size=32, default_size=224),\n    \"resnet50_v2\": _EncoderInfo(\n        keras_name=\"ResNet50V2\", scaling=(-1, 1), default_size=224),\n    \"resnet101\": _EncoderInfo(\n        keras_name=\"ResNet101\", scaling=(-1, 1), default_size=224),\n    \"resnet101_v2\": _EncoderInfo(\n        keras_name=\"ResNet101V2\", scaling=(-1, 1), default_size=224),\n    \"resnet152\": _EncoderInfo(\n        keras_name=\"ResNet152\", scaling=(-1, 1), default_size=224),\n    \"resnet152_v2\": _EncoderInfo(\n        keras_name=\"ResNet152V2\", scaling=(-1, 1), default_size=224),\n    \"vgg16\": _EncoderInfo(\n        keras_name=\"VGG16\", color_order=\"bgr\", scaling=(0, 255), default_size=224),\n    \"vgg19\": _EncoderInfo(\n        keras_name=\"VGG19\", color_order=\"bgr\", scaling=(0, 255), default_size=224),\n    \"xception\": _EncoderInfo(\n        keras_name=\"Xception\", scaling=(-1, 1), min_size=71, default_size=299),\n    \"fs_original\": _EncoderInfo(\n        keras_name=\"\", color_order=\"bgr\", min_size=32, default_size=1024)}\n\n\nclass Model(ModelBase):\n    \"\"\" Phaze-A Faceswap Model.\n\n    An highly adaptable and configurable model by torzDF\n\n    Parameters\n    ----------513\n    args: varies\n        The default command line arguments passed in from :class:`~scripts.train.Train` or\n        :class:`~scripts.train.Convert`\n    kwargs: varies\n        The default keyword arguments passed in from :class:`~scripts.train.Train` or\n        :class:`~scripts.train.Convert`\n    \"\"\"\n    def __init__(self, *args, **kwargs) -> None:\n        super().__init__(*args, **kwargs)\n        if self.config[\"output_size\"] % 16 != 0:\n            raise FaceswapError(\"Phaze-A output shape must be a multiple of 16\")\n\n        self._validate_encoder_architecture()\n        self.config[\"freeze_layers\"] = self._select_freeze_layers()\n\n        self.input_shape: tuple[int, int, int] = self._get_input_shape()\n        self.color_order = _MODEL_MAPPING[self.config[\"enc_architecture\"]].color_order\n\n    def build(self) -> None:\n        \"\"\" Build the model and assign to :attr:`model`.\n\n        Override's the default build function for allowing the setting of dropout rate for pre-\n        existing models.\n        \"\"\"\n        is_summary = hasattr(self._args, \"summary\") and self._args.summary\n        if not self._io.model_exists or self._is_predict or is_summary:\n            logger.debug(\"New model, inference or summary. Falling back to default build: \"\n                         \"(exists: %s, inference: %s, is_summary: %s)\",\n                         self._io.model_exists, self._is_predict, is_summary)\n            super().build()\n            return\n        with self._settings.strategy_scope():\n            model = self.io.load()\n            model = self._update_dropouts(model)\n            self._model = model\n            self._compile_model()\n            self._output_summary()\n\n    def _update_dropouts(self, model: tf.keras.models.Model) -> tf.keras.models.Model:\n        \"\"\" Update the saved model with new dropout rates.\n\n        Keras, annoyingly, does not actually change the dropout of the underlying layer, so we need\n        to update the rate, then clone the model into a new model and reload weights.\n\n        Parameters\n        ----------\n        model: :class:`keras.models.Model`\n            The loaded saved Keras Model to update the dropout rates for\n\n        Returns\n        -------\n        :class:`keras.models.Model`\n            The loaded Keras Model with the dropout rates updated\n        \"\"\"\n        dropouts = {\"fc\": self.config[\"fc_dropout\"],\n                    \"gblock\": self.config[\"fc_gblock_dropout\"]}\n        logger.debug(\"Config dropouts: %s\", dropouts)\n        updated = False\n        for mod in get_all_sub_models(model):\n            if not mod.name.startswith(\"fc_\"):\n                continue\n            key = \"gblock\" if \"gblock\" in mod.name else mod.name.split(\"_\")[0]\n            rate = dropouts[key]\n            log_once = False\n            for layer in mod.layers:\n                if not isinstance(layer, kl.Dropout):\n                    continue\n                if layer.rate != rate:\n                    logger.debug(\"Updating dropout rate for %s from %s to %s\",\n                                 f\"{mod.name} - {layer.name}\", layer.rate, rate)\n                    if not log_once:\n                        logger.info(\"Updating Dropout Rate for '%s' from %s to %s\",\n                                    mod.name, layer.rate, rate)\n                        log_once = True\n                    layer.rate = rate\n                    updated = True\n        if updated:\n            logger.debug(\"Dropout rate updated. Cloning model\")\n            new_model = keras.models.clone_model(model)\n            new_model.set_weights(model.get_weights())\n            del model\n            model = new_model\n        return model\n\n    def _select_freeze_layers(self) -> list[str]:\n        \"\"\" Process the selected frozen layers and replace the `keras_encoder` option with the\n        actual keras model name\n\n        Returns\n        -------\n        list\n            The selected layers for weight freezing\n        \"\"\"\n        arch = self.config[\"enc_architecture\"]\n        layers = self.config[\"freeze_layers\"]\n        # EfficientNetV2 is inconsistent with other model's naming conventions\n        keras_name = _MODEL_MAPPING[arch].keras_name.replace(\"EfficientNetV2\", \"EfficientNetV2-\")\n        # CLIPv model is always called 'visual' regardless of weights/format loaded\n        keras_name = \"visual\" if arch.startswith(\"clipv_\") else keras_name\n\n        if \"keras_encoder\" not in self.config[\"freeze_layers\"]:\n            retval = layers\n        elif keras_name:\n            retval = [layer.replace(\"keras_encoder\", keras_name.lower()) for layer in layers]\n            logger.debug(\"Substituting 'keras_encoder' for '%s'\", arch)\n        else:\n            retval = [layer for layer in layers if layer != \"keras_encoder\"]\n            logger.debug(\"Removing 'keras_encoder' for '%s'\", arch)\n\n        return retval\n\n    def _get_input_shape(self) -> tuple[int, int, int]:\n        \"\"\" Obtain the input shape for the model.\n\n        Input shape is calculated from the selected Encoder's input size, scaled to the user\n        selected Input Scaling, rounded down to the nearest 16 pixels.\n\n        Notes\n        -----\n        Some models (NasNet) require the input size to be of a certain dimension if loading\n        imagenet weights. In these instances resize inputs and raise warning message\n\n        Returns\n        -------\n        tuple\n            The shape tuple for the input size to the Phaze-A model\n        \"\"\"\n        arch = self.config[\"enc_architecture\"]\n        enforce_size = _MODEL_MAPPING[arch].enforce_for_weights\n        default_size = _MODEL_MAPPING[arch].default_size\n        scaling = self.config[\"enc_scaling\"] / 100\n\n        min_size = _MODEL_MAPPING[arch].min_size\n        size = int(max(min_size, ((default_size * scaling) // 16) * 16))\n\n        if self.config[\"enc_load_weights\"] and enforce_size and scaling != 1.0:\n            logger.warning(\"%s requires input size to be %spx when loading imagenet weights. \"\n                           \"Adjusting input size from %spx to %spx\",\n                           arch, default_size, size, default_size)\n            retval = (default_size, default_size, 3)\n        else:\n            retval = (size, size, 3)\n\n        logger.debug(\"Encoder input set to: %s\", retval)\n        return retval\n\n    def _validate_encoder_architecture(self) -> None:\n        \"\"\" Validate that the requested architecture is a valid choice for the running system\n        configuration.\n\n        If the selection is not valid, an error is logged and system exits.\n        \"\"\"\n        arch = self.config[\"enc_architecture\"].lower()\n        model = _MODEL_MAPPING.get(arch)\n        if not model:\n            raise FaceswapError(f\"'{arch}' is not a valid choice for encoder architecture. Choose \"\n                                f\"one of {list(_MODEL_MAPPING.keys())}.\")\n\n        tf_ver = get_tf_version()\n        tf_min = model.tf_min\n        if tf_ver < tf_min:\n            raise FaceswapError(f\"{arch}' is not compatible with your version of Tensorflow. The \"\n                                f\"minimum version required is {tf_min} whilst you have version \"\n                                f\"{tf_ver} installed.\")\n\n    def build_model(self, inputs: list[tf.Tensor]) -> tf.keras.models.Model:\n        \"\"\" Create the model's structure.\n\n        Parameters\n        ----------\n        inputs: list\n            A list of input tensors for the model. This will be a list of 2 tensors of\n            shape :attr:`input_shape`, the first for side \"a\", the second for side \"b\".\n\n        Returns\n        -------\n        :class:`keras.models.Model`\n            The generated model\n        \"\"\"\n        # Create sub-Models\n        encoders = self._build_encoders(inputs)\n        inters = self._build_fully_connected(encoders)\n        g_blocks = self._build_g_blocks(inters)\n        decoders = self._build_decoders(g_blocks)\n\n        # Create Autoencoder\n        outputs = [decoders[\"a\"], decoders[\"b\"]]\n        autoencoder = keras.models.Model(inputs, outputs, name=self.model_name)\n        return autoencoder\n\n    def _build_encoders(self, inputs: list[tf.Tensor]) -> dict[str, tf.keras.models.Model]:\n        \"\"\" Build the encoders for Phaze-A\n\n        Parameters\n        ----------\n        inputs: list\n            A list of input tensors for the model. This will be a list of 2 tensors of\n            shape :attr:`input_shape`, the first for side \"a\", the second for side \"b\".\n\n        Returns\n        -------\n        dict\n            side as key ('a' or 'b'), encoder for side as value\n        \"\"\"\n        encoder = Encoder(self.input_shape, self.config)()\n        retval = {\"a\": encoder(inputs[0]), \"b\": encoder(inputs[1])}\n        logger.debug(\"Encoders: %s\", retval)\n        return retval\n\n    def _build_fully_connected(\n            self,\n            inputs: dict[str, tf.keras.models.Model]) -> dict[str, list[tf.keras.models.Model]]:\n        \"\"\" Build the fully connected layers for Phaze-A\n\n        Parameters\n        ----------\n        inputs: dict\n            The compiled encoder models that act as inputs to the fully connected layers\n\n        Returns\n        -------\n        dict\n            side as key ('a' or 'b'), fully connected model for side as value\n        \"\"\"\n        input_shapes = K.int_shape(inputs[\"a\"])[1:]\n\n        if self.config[\"split_fc\"]:\n            fc_a = FullyConnected(\"a\", input_shapes, self.config)()\n            inter_a = [fc_a(inputs[\"a\"])]\n            inter_b = [FullyConnected(\"b\", input_shapes, self.config)()(inputs[\"b\"])]\n        else:\n            fc_both = FullyConnected(\"both\", input_shapes, self.config)()\n            inter_a = [fc_both(inputs[\"a\"])]\n            inter_b = [fc_both(inputs[\"b\"])]\n\n        if self.config[\"shared_fc\"]:\n            if self.config[\"shared_fc\"] == \"full\":\n                fc_shared = FullyConnected(\"shared\", input_shapes, self.config)()\n            elif self.config[\"split_fc\"]:\n                fc_shared = fc_a\n            else:\n                fc_shared = fc_both\n            inter_a = [kl.Concatenate(name=\"inter_a\")([inter_a[0], fc_shared(inputs[\"a\"])])]\n            inter_b = [kl.Concatenate(name=\"inter_b\")([inter_b[0], fc_shared(inputs[\"b\"])])]\n\n        if self.config[\"enable_gblock\"]:\n            fc_gblock = FullyConnected(\"gblock\", input_shapes, self.config)()\n            inter_a.append(fc_gblock(inputs[\"a\"]))\n            inter_b.append(fc_gblock(inputs[\"b\"]))\n\n        retval = {\"a\": inter_a, \"b\": inter_b}\n        logger.debug(\"Fully Connected: %s\", retval)\n        return retval\n\n    def _build_g_blocks(\n                self,\n                inputs: dict[str, list[tf.keras.models.Model]]\n            ) -> dict[str, list[tf.keras.models.Model] | tf.keras.models.Model]:\n        \"\"\" Build the g-block layers for Phaze-A.\n\n        If a g-block has not been selected for this model, then the original `inters` models are\n        returned for passing straight to the decoder\n\n        Parameters\n        ----------\n        inputs: dict\n            The compiled inter models that act as inputs to the g_blocks\n\n        Returns\n        -------\n        dict\n            side as key ('a' or 'b'), g-block model for side as value. If g-block has been disabled\n            then the values will be the fully connected layers\n        \"\"\"\n        if not self.config[\"enable_gblock\"]:\n            logger.debug(\"No G-Block selected, returning Inters: %s\", inputs)\n            return inputs\n\n        input_shapes = [K.int_shape(inter)[1:] for inter in inputs[\"a\"]]\n        if self.config[\"split_gblock\"]:\n            retval = {\"a\": GBlock(\"a\", input_shapes, self.config)()(inputs[\"a\"]),\n                      \"b\": GBlock(\"b\", input_shapes, self.config)()(inputs[\"b\"])}\n        else:\n            g_block = GBlock(\"both\", input_shapes, self.config)()\n            retval = {\"a\": g_block((inputs[\"a\"])), \"b\": g_block((inputs[\"b\"]))}\n\n        logger.debug(\"G-Blocks: %s\", retval)\n        return retval\n\n    def _build_decoders(self,\n                        inputs: dict[str, list[tf.keras.models.Model] | tf.keras.models.Model]\n                        ) -> dict[str, tf.keras.models.Model]:\n        \"\"\" Build the encoders for Phaze-A\n\n        Parameters\n        ----------\n        inputs: dict\n            A dict of inputs to the decoder. This will either be g-block output (if g-block is\n            enabled) or fully connected layers output (if g-block is disabled).\n\n        Returns\n        -------\n        dict\n            side as key ('a' or 'b'), decoder for side as value\n        \"\"\"\n        input_ = inputs[\"a\"]\n        # If input is inters, shapes will be a list.\n        # There will only ever be 1 input. For inters: either inter out, or concatenate of inters\n        # For g-block, this only ever has one output\n        input_ = input_[0] if isinstance(input_, list) else input_\n\n        # If learning a mask and upscales have been placed into FC layer, then the mask will also\n        # come as an input\n        if self.config[\"learn_mask\"] and self.config[\"dec_upscales_in_fc\"]:\n            input_ = input_[0]\n\n        input_shape = K.int_shape(input_)[1:]\n\n        if self.config[\"split_decoders\"]:\n            retval = {\"a\": Decoder(\"a\", input_shape, self.config)()(inputs[\"a\"]),\n                      \"b\": Decoder(\"b\", input_shape, self.config)()(inputs[\"b\"])}\n        else:\n            decoder = Decoder(\"both\", input_shape, self.config)()\n            retval = {\"a\": decoder(inputs[\"a\"]), \"b\": decoder(inputs[\"b\"])}\n\n        logger.debug(\"Decoders: %s\", retval)\n        return retval\n\n\ndef _bottleneck(inputs: tf.Tensor, bottleneck: str, size: int, normalization: str) -> tf.Tensor:\n    \"\"\" The bottleneck fully connected layer. Can be called from Encoder or FullyConnected layers.\n\n    Parameters\n    ----------\n    inputs: tensor\n        The input to the bottleneck layer\n    bottleneck: str or ``None``\n        The type of layer to use for the bottleneck. ``None`` to not use a bottleneck\n    size: int\n        The number of nodes for the dense layer (if selected)\n    normalization: str\n        The normalization method to use prior to the bottleneck layer\n\n    Returns\n    -------\n    tensor\n        The output from the bottleneck\n    \"\"\"\n    norms = {\"layer\": kl.LayerNormalization,\n             \"rms\": RMSNormalization,\n             \"instance\": InstanceNormalization}\n    bottlenecks = {\"average_pooling\": kl.GlobalAveragePooling2D(),\n                   \"dense\": kl.Dense(size),\n                   \"max_pooling\": kl.GlobalMaxPooling2D()}\n    var_x = inputs\n    if normalization:\n        var_x = norms[normalization]()(var_x)\n    if bottleneck == \"dense\" and K.ndim(var_x) > 2:  # Flatten non-1D inputs for dense\n        var_x = kl.Flatten()(var_x)\n    if bottleneck != \"flatten\":\n        var_x = bottlenecks[bottleneck](var_x)\n    if K.ndim(var_x) > 2:\n        # Flatten prior to fc layers\n        var_x = kl.Flatten()(var_x)\n    return var_x\n\n\ndef _get_upscale_layer(method: T.Literal[\"resize_images\", \"subpixel\", \"upscale_dny\",\n                                         \"upscale_fast\", \"upscale_hybrid\", \"upsample2d\"],\n                       filters: int,\n                       activation: str | None = None,\n                       upsamples: int | None = None,\n                       interpolation: str | None = None) -> tf.keras.layers.Layer:\n    \"\"\" Obtain an instance of the requested upscale method.\n\n    Parameters\n    ----------\n    method: str\n        The user selected upscale method to use. One of `\"resize_images\"`, `\"subpixel\"`,\n        `\"upscale_dny\"`, `\"upscale_fast\"`, `\"upscale_hybrid\"`, `\"upsample2d\"`\n    filters: int\n        The number of filters to use in the upscale layer\n    activation: str, optional\n        The activation function to use in the upscale layer. ``None`` to use no activation.\n        Default: ``None``\n    upsamples: int, optional\n        Only used for UpSampling2D. If provided, then this is passed to the layer as the ``size``\n        parameter. Default: ``None``\n    interpolation: str, optional\n        Only used for UpSampling2D. If provided, then this is passed to the layer as the\n        ``interpolation`` parameter. Default: ``None``\n\n    Returns\n    -------\n    :class:`keras.layers.Layer`\n        The selected configured upscale layer\n    \"\"\"\n    if method == \"upsample2d\":\n        kwargs: dict[str, str | int] = {}\n        if upsamples:\n            kwargs[\"size\"] = upsamples\n        if interpolation:\n            kwargs[\"interpolation\"] = interpolation\n        return kl.UpSampling2D(**kwargs)\n    if method == \"subpixel\":\n        return UpscaleBlock(filters, activation=activation)\n    if method == \"upscale_fast\":\n        return Upscale2xBlock(filters, activation=activation, fast=True)\n    if method == \"upscale_hybrid\":\n        return Upscale2xBlock(filters, activation=activation, fast=False)\n    if method == \"upscale_dny\":\n        return UpscaleDNYBlock(filters, activation=activation)\n    return UpscaleResizeImagesBlock(filters, activation=activation)\n\n\ndef _get_curve(start_y: int,\n               end_y: int,\n               num_points: int,\n               scale: float,\n               mode: T.Literal[\"full\", \"cap_max\", \"cap_min\"] = \"full\") -> list[int]:\n    \"\"\" Obtain a curve.\n\n    For the given start and end y values, return the y co-ordinates of a curve for the given\n    number of points. The points are rounded down to the nearest 8.\n\n    Parameters\n    ----------\n    start_y: int\n        The y co-ordinate for the starting point of the curve\n    end_y: int\n        The y co-ordinate for the end point of the curve\n    num_points: int\n        The number of data points to plot on the x-axis\n    scale: float\n        The scale of the curve (from -.99 to 0.99)\n    slope_mode: str, optional\n        The method to generate the curve. One of `\"full\"`, `\"cap_max\"` or `\"cap_min\"`. `\"full\"`\n        mode generates a curve from the `\"start_y\"` to the `\"end_y\"` values. `\"cap_max\"` pads the\n        earlier points with the `\"start_y\"` value before filling out the remaining points at a\n        fixed divider to the `\"end_y\"` value. `\"cap_min\"` starts at the `\"start_y\" filling points\n        at a fixed divider until the `\"end_y\"` value is reached and pads the remaining points with\n        the `\"end_y\"` value. Default: `\"full\"`\n\n    Returns\n    -------\n    list\n        List of ints of points for the given curve\n     \"\"\"\n    scale = min(.99, max(-.99, scale))\n    logger.debug(\"Obtaining curve: (start_y: %s, end_y: %s, num_points: %s, scale: %s, mode: %s)\",\n                 start_y, end_y, num_points, scale, mode)\n    if mode == \"full\":\n        x_axis = np.linspace(0., 1., num=num_points)\n        y_axis = (x_axis - x_axis * scale) / (scale - abs(x_axis) * 2 * scale + 1)\n        y_axis = y_axis * (end_y - start_y) + start_y\n        retval = [int((y // 8) * 8) for y in y_axis]\n    else:\n        y_axis = [start_y]\n        scale = 1. - abs(scale)\n        for _ in range(num_points - 1):\n            current_value = max(end_y, int(((y_axis[-1] * scale) // 8) * 8))\n            y_axis.append(current_value)\n            if current_value == end_y:\n                break\n        pad = [start_y if mode == \"cap_max\" else end_y for _ in range(num_points - len(y_axis))]\n        retval = pad + y_axis if mode == \"cap_max\" else y_axis + pad\n    logger.debug(\"Returning curve: %s\", retval)\n    return retval\n\n\ndef _scale_dim(target_resolution: int, original_dim: int) -> int:\n    \"\"\" Scale a given `original_dim` so that it is a factor of the target resolution.\n\n    Parameters\n    ----------\n    target_resolution: int\n        The output resolution that is being targetted\n    original_dim: int\n        The dimension that needs to be checked for compatibility for upscaling to the\n        target resolution\n\n    Returns\n    -------\n    int\n        The highest dimension below or equal to `original_dim` that is a factor of the\n    target resolution.\n    \"\"\"\n    new_dim = target_resolution\n    while new_dim > original_dim:\n        next_dim = new_dim / 2\n        if not next_dim.is_integer():\n            break\n        new_dim = int(next_dim)\n    logger.debug(\"target_resolution: %s, original_dim: %s, new_dim: %s\",\n                 target_resolution, original_dim, new_dim)\n    return new_dim\n\n\nclass Encoder():  # pylint:disable=too-few-public-methods\n    \"\"\" Encoder. Uses one of pre-existing Keras/Faceswap models or custom encoder.\n\n    Parameters\n    ----------\n    input_shape: tuple\n        The shape tuple for the input tensor\n    config: dict\n        The model configuration options\n    \"\"\"\n    def __init__(self, input_shape: tuple[int, int, int], config: dict) -> None:\n        self.input_shape = input_shape\n        self._config = config\n        self._input_shape = input_shape\n\n    @property\n    def _model_kwargs(self) -> dict[str, dict[str, str | bool]]:\n        \"\"\" dict: Configuration option for architecture mapped to optional kwargs. \"\"\"\n        return {\"mobilenet\": {\"alpha\": self._config[\"mobilenet_width\"],\n                              \"depth_multiplier\": self._config[\"mobilenet_depth\"],\n                              \"dropout\": self._config[\"mobilenet_dropout\"]},\n                \"mobilenet_v2\": {\"alpha\": self._config[\"mobilenet_width\"]},\n                \"mobilenet_v3\": {\"alpha\": self._config[\"mobilenet_width\"],\n                                 \"minimalist\": self._config[\"mobilenet_minimalistic\"],\n                                 \"include_preprocessing\": False}}\n\n    @property\n    def _selected_model(self) -> tuple[_EncoderInfo, dict]:\n        \"\"\" tuple(dict, :class:`_EncoderInfo`): The selected encoder model and it's associated\n        keyword arguments \"\"\"\n        arch = self._config[\"enc_architecture\"]\n        model = _MODEL_MAPPING[arch]\n        kwargs = self._model_kwargs.get(arch, {})\n        if arch.startswith(\"efficientnet_v2\"):\n            kwargs[\"include_preprocessing\"] = False\n        return model, kwargs\n\n    def __call__(self) -> tf.keras.models.Model:\n        \"\"\" Create the Phaze-A Encoder Model.\n\n        Returns\n        -------\n        :class:`keras.models.Model`\n            The selected Encoder Model\n        \"\"\"\n        input_ = kl.Input(shape=self._input_shape)\n        var_x = input_\n\n        scaling = self._selected_model[0].scaling\n\n        if scaling:\n            #  Some models expect different scaling.\n            logger.debug(\"Scaling to %s for '%s'\", scaling, self._config[\"enc_architecture\"])\n            if scaling == (0, 255):\n                # models expecting inputs from 0 to 255.\n                var_x = var_x * 255.\n            if scaling == (-1, 1):\n                # models expecting inputs from -1 to 1.\n                var_x = var_x * 2.\n                var_x = var_x - 1.0\n\n        if (self._config[\"enc_architecture\"].startswith(\"efficientnet_b\")\n                and self._config[\"mixed_precision\"]):\n            # There is a bug in EfficientNet pre-processing where the normalized mean for the\n            # imagenet rgb values are not cast to float16 when mixed precision is enabled.\n            # We monkeypatch in a cast constant until the issue is resolved\n            # TODO revert if/when applying Imagenet Normalization works with mixed precision\n            # confirmed bugged: TF2.10\n            logger.debug(\"Patching efficientnet.IMAGENET_STDDEV_RGB to float16 constant\")\n            from keras.applications import efficientnet  # pylint:disable=import-outside-toplevel\n            setattr(efficientnet,\n                    \"IMAGENET_STDDEV_RGB\",\n                    K.constant(efficientnet.IMAGENET_STDDEV_RGB, dtype=\"float16\"))\n\n        var_x = self._get_encoder_model()(var_x)\n\n        if self._config[\"bottleneck_in_encoder\"]:\n            var_x = _bottleneck(var_x,\n                                self._config[\"bottleneck_type\"],\n                                self._config[\"bottleneck_size\"],\n                                self._config[\"bottleneck_norm\"])\n\n        return keras.models.Model(input_, var_x, name=\"encoder\")\n\n    def _get_encoder_model(self) -> tf.keras.models.Model:\n        \"\"\" Return the model defined by the selected architecture.\n\n        Returns\n        -------\n        :class:`keras.Model`\n            The selected keras model for the chosen encoder architecture\n        \"\"\"\n        model, kwargs = self._selected_model\n        if model.keras_name and self._config[\"enc_architecture\"].startswith(\"clipv_\"):\n            assert model.keras_name in T.get_args(TypeModelsViT)\n            kwargs[\"input_shape\"] = self._input_shape\n            kwargs[\"load_weights\"] = self._config[\"enc_load_weights\"]\n            retval = ViT(T.cast(TypeModelsViT, model.keras_name),\n                         input_size=self._input_shape[0],\n                         load_weights=self._config[\"enc_load_weights\"])()\n        elif model.keras_name:\n            kwargs[\"input_shape\"] = self._input_shape\n            kwargs[\"include_top\"] = False\n            kwargs[\"weights\"] = \"imagenet\" if self._config[\"enc_load_weights\"] else None\n            retval = getattr(kapp, model.keras_name)(**kwargs)\n        else:\n            retval = _EncoderFaceswap(self._config)\n        return retval\n\n\nclass _EncoderFaceswap():  # pylint:disable=too-few-public-methods\n    \"\"\" A configurable standard Faceswap encoder based off Original model.\n\n    Parameters\n    ----------\n    config: dict\n        The model configuration options\n    \"\"\"\n    def __init__(self, config: dict) -> None:\n        self._config = config\n        self._type = self._config[\"enc_architecture\"]\n        self._depth = config[f\"{self._type}_depth\"]\n        self._min_filters = config[\"fs_original_min_filters\"]\n        self._max_filters = config[\"fs_original_max_filters\"]\n        self._is_alt = config[\"fs_original_use_alt\"]\n        self._relu_alpha = 0.2 if self._is_alt else 0.1\n        self._kernel_size = 3 if self._is_alt else 5\n        self._strides = 1 if self._is_alt else 2\n\n    def __call__(self, inputs: tf.Tensor) -> tf.Tensor:\n        \"\"\" Call the original Faceswap Encoder\n\n        Parameters\n        ----------\n        inputs: tensor\n            The input tensor to the Faceswap Encoder\n\n        Returns\n        -------\n        tensor\n            The output tensor from the Faceswap Encoder\n        \"\"\"\n        var_x = inputs\n        filters = self._config[\"fs_original_min_filters\"]\n\n        if self._is_alt:\n            var_x = Conv2DBlock(filters,\n                                kernel_size=1,\n                                strides=self._strides,\n                                relu_alpha=self._relu_alpha)(var_x)\n\n        for i in range(self._depth):\n            name = f\"fs_{'dny_' if self._is_alt else ''}enc\"\n            var_x = Conv2DBlock(filters,\n                                kernel_size=self._kernel_size,\n                                strides=self._strides,\n                                relu_alpha=self._relu_alpha,\n                                name=f\"{name}_convblk_{i}\")(var_x)\n            filters = min(self._config[\"fs_original_max_filters\"], filters * 2)\n            if self._is_alt and i == self._depth - 1:\n                var_x = Conv2DBlock(filters,\n                                    kernel_size=4,\n                                    strides=self._strides,\n                                    padding=\"valid\",\n                                    relu_alpha=self._relu_alpha,\n                                    name=f\"{name}_convblk_{i}_1\")(var_x)\n            elif self._is_alt:\n                var_x = Conv2DBlock(filters,\n                                    kernel_size=self._kernel_size,\n                                    strides=self._strides,\n                                    relu_alpha=self._relu_alpha,\n                                    name=f\"{name}_convblk_{i}_1\")(var_x)\n                var_x = kl.MaxPool2D(2, name=f\"{name}_pool_{i}\")(var_x)\n        return var_x\n\n\nclass FullyConnected():  # pylint:disable=too-few-public-methods\n    \"\"\" Intermediate Fully Connected layers for Phaze-A Model.\n\n    Parameters\n    ----------\n    side: [\"a\", \"b\", \"both\", \"gblock\", \"shared\"]\n        The side of the model that the fully connected layers belong to. Used for naming\n    input_shape: tuple\n        The input shape for the fully connected layers\n    config: dict\n        The user configuration dictionary\n    \"\"\"\n    def __init__(self,\n                 side: T.Literal[\"a\", \"b\", \"both\", \"gblock\", \"shared\"],\n                 input_shape: tuple,\n                 config: dict) -> None:\n        logger.debug(\"Initializing: %s (side: %s, input_shape: %s)\",\n                     self.__class__.__name__, side, input_shape)\n        self._side = side\n        self._input_shape = input_shape\n        self._config = config\n        self._final_dims = self._config[\"fc_dimensions\"] * (self._config[\"fc_upsamples\"] + 1)\n        self._prefix = \"fc_gblock\" if self._side == \"gblock\" else \"fc\"\n\n        logger.debug(\"Initialized: %s (side: %s, min_nodes: %s, max_nodes: %s)\",\n                     self.__class__.__name__, self._side, self._min_nodes, self._max_nodes)\n\n    @property\n    def _min_nodes(self) -> int:\n        \"\"\" int: The number of nodes for the first Dense. For non g-block layers this will be the\n        given minimum filters multiplied by the dimensions squared. For g-block layers, this is the\n        given value \"\"\"\n        if self._side == \"gblock\":\n            return self._config[\"fc_gblock_min_nodes\"]\n        retval = self._scale_filters(self._config[\"fc_min_filters\"])\n        retval = int(retval * self._config[\"fc_dimensions\"] ** 2)\n        return retval\n\n    @property\n    def _max_nodes(self) -> int:\n        \"\"\" int: The number of nodes for the final Dense. For non g-block layers this will be the\n        given maximum filters multiplied by the dimensions squared. This number will be scaled down\n        if the final shape can not be mapped to the requested output size.\n\n        For g-block layers, this is the given config value.\n        \"\"\"\n        if self._side == \"gblock\":\n            return self._config[\"fc_gblock_max_nodes\"]\n        retval = self._scale_filters(self._config[\"fc_max_filters\"])\n        retval = int(retval * self._config[\"fc_dimensions\"] ** 2)\n        return retval\n\n    def _scale_filters(self, original_filters: int) -> int:\n        \"\"\" Scale the filters to be compatible with the model's selected output size.\n\n        Parameters\n        ----------\n        original_filters: int\n            The original user selected number of filters\n\n        Returns\n        -------\n        int\n            The number of filters scaled down for output size\n        \"\"\"\n        scaled_dim = _scale_dim(self._config[\"output_size\"], self._final_dims)\n        if scaled_dim == self._final_dims:\n            logger.debug(\"filters don't require scaling. Returning: %s\", original_filters)\n            return original_filters\n\n        flat = self._final_dims ** 2 * original_filters\n        modifier = self._final_dims ** 2 * scaled_dim ** 2\n        retval = int((flat // modifier) * modifier)\n        retval = int(retval / self._final_dims ** 2)\n        logger.debug(\"original_filters: %s, scaled_filters: %s\", original_filters, retval)\n        return retval\n\n    def _do_upsampling(self, inputs: tf.Tensor) -> tf.Tensor:\n        \"\"\" Perform the upsampling at the end of the fully connected layers.\n\n        Parameters\n        ----------\n        inputs: Tensor\n            The input to the upsample layers\n\n        Returns\n        -------\n        Tensor\n            The output from the upsample layers\n        \"\"\"\n        upsample_filts = self._scale_filters(self._config[\"fc_upsample_filters\"])\n        upsampler = self._config[\"fc_upsampler\"].lower()\n        num_upsamples = self._config[\"fc_upsamples\"]\n        var_x = inputs\n        if upsampler == \"upsample2d\" and num_upsamples > 1:\n            upscaler = _get_upscale_layer(upsampler,\n                                          upsample_filts,  # Not used but required\n                                          upsamples=2 ** num_upsamples,\n                                          interpolation=\"bilinear\")\n            var_x = upscaler(var_x)\n        else:\n            for _ in range(num_upsamples):\n                upscaler = _get_upscale_layer(upsampler,\n                                              upsample_filts,\n                                              activation=\"leakyrelu\")\n                var_x = upscaler(var_x)\n        if upsampler == \"upsample2d\":\n            var_x = kl.LeakyReLU(alpha=0.1)(var_x)\n        return var_x\n\n    def __call__(self) -> tf.keras.models.Model:\n        \"\"\" Call the intermediate layer.\n\n        Returns\n        -------\n        :class:`keras.models.Model`\n            The Fully connected model\n        \"\"\"\n        input_ = kl.Input(shape=self._input_shape)\n        var_x = input_\n\n        node_curve = _get_curve(self._min_nodes,\n                                self._max_nodes,\n                                self._config[f\"{self._prefix}_depth\"],\n                                self._config[f\"{self._prefix}_filter_slope\"])\n\n        if not self._config[\"bottleneck_in_encoder\"]:\n            var_x = _bottleneck(var_x,\n                                self._config[\"bottleneck_type\"],\n                                self._config[\"bottleneck_size\"],\n                                self._config[\"bottleneck_norm\"])\n\n        dropout = f\"{self._prefix}_dropout\"\n        for idx, nodes in enumerate(node_curve):\n            var_x = kl.Dropout(self._config[dropout], name=f\"{dropout}_{idx + 1}\")(var_x)\n            var_x = kl.Dense(nodes)(var_x)\n\n        if self._side != \"gblock\":\n            dim = self._config[\"fc_dimensions\"]\n            var_x = kl.Reshape((dim, dim, int(self._max_nodes / (dim ** 2))))(var_x)\n            var_x = self._do_upsampling(var_x)\n\n            num_upscales = self._config[\"dec_upscales_in_fc\"]\n            if num_upscales:\n                var_x = UpscaleBlocks(self._side,\n                                      self._config,\n                                      layer_indicies=(0, num_upscales))(var_x)\n\n        return keras.models.Model(input_, var_x, name=f\"fc_{self._side}\")\n\n\nclass UpscaleBlocks():  # pylint:disable=too-few-public-methods\n    \"\"\" Obtain a block of upscalers.\n\n    This class exists outside of the :class:`Decoder` model, as it is possible to place some of\n    the upscalers at the end of the Fully Connected Layers, so the upscale chain needs to be able\n    to be calculated by both the Fully Connected Layers and by the Decoder if required.\n\n    For this reason, the Upscale Filter list is created as a class attribute of the\n    :class:`UpscaleBlocks` layers for reference by either the Decoder or Fully Connected models\n\n    Parameters\n    ----------\n    side: [\"a\", \"b\", \"both\", \"shared\"]\n        The side of the model that the Decoder belongs to. Used for naming\n    config: dict\n        The user configuration dictionary\n    layer_indices: tuple, optional\n        The tuple indicies indicating the starting layer index and the ending layer index to\n        generate upscales for. Used for when splitting upscales between the Fully Connected Layers\n        and the Decoder. ``None`` will generate the full Upscale chain. An end index of -1 will\n        generate the layers from the starting index to the final upscale. Default: ``None``\n    \"\"\"\n    _filters: list[int] = []\n\n    def __init__(self,\n                 side: T.Literal[\"a\", \"b\", \"both\", \"shared\"],\n                 config: dict,\n                 layer_indicies: tuple[int, int] | None = None) -> None:\n        logger.debug(\"Initializing: %s (side: %s, layer_indicies: %s)\",\n                     self.__class__.__name__, side, layer_indicies)\n        self._side = side\n        self._config = config\n        self._is_dny = self._config[\"dec_upscale_method\"].lower() == \"upscale_dny\"\n        self._layer_indicies = layer_indicies\n        logger.debug(\"Initialized: %s\", self.__class__.__name__,)\n\n    def _reshape_for_output(self, inputs: tf.Tensor) -> tf.Tensor:\n        \"\"\" Reshape the input for arbitrary output sizes.\n\n        The number of filters in the input will have been scaled to the model output size allowing\n        us to scale the dimensions to the requested output size.\n\n        Parameters\n        ----------\n        inputs: tensor\n            The tensor that is to be reshaped\n\n        Returns\n        -------\n        tensor\n            The tensor shaped correctly to upscale to output size\n        \"\"\"\n        var_x = inputs\n        old_dim = K.int_shape(inputs)[1]\n        new_dim = _scale_dim(self._config[\"output_size\"], old_dim)\n        if new_dim != old_dim:\n            old_shape = K.int_shape(inputs)[1:]\n            new_shape = (new_dim, new_dim, np.prod(old_shape) // new_dim ** 2)\n            logger.debug(\"Reshaping tensor from %s to %s for output size %s\",\n                         K.int_shape(inputs)[1:], new_shape, self._config[\"output_size\"])\n            var_x = kl.Reshape(new_shape)(var_x)\n        return var_x\n\n    def _upscale_block(self,\n                       inputs: tf.Tensor,\n                       filters: int,\n                       skip_residual: bool = False,\n                       is_mask: bool = False) -> tf.Tensor:\n        \"\"\" Upscale block for Phaze-A Decoder.\n\n        Uses requested upscale method, adds requested regularization and activation function.\n\n        Parameters\n        ----------\n        inputs: tensor\n            The input tensor for the upscale block\n        filters: int\n            The number of filters to use for the upscale\n        skip_residual: bool, optional\n            ``True`` if a residual block should not be placed in the upscale block, otherwise\n            ``False``. Default ``False``\n        is_mask: bool, optional\n            ``True`` if the input is a mask. ``False`` if the input is a face. Default: ``False``\n\n        Returns\n        -------\n        tensor\n            The output tensor from the upscale block\n        \"\"\"\n        upscaler = _get_upscale_layer(self._config[\"dec_upscale_method\"].lower(),\n                                      filters,\n                                      activation=\"leakyrelu\",\n                                      upsamples=2,\n                                      interpolation=\"bilinear\")\n\n        var_x = upscaler(inputs)\n        if not is_mask and self._config[\"dec_gaussian\"]:\n            var_x = kl.GaussianNoise(1.0)(var_x)\n        if not is_mask and self._config[\"dec_res_blocks\"] and not skip_residual:\n            var_x = self._normalization(var_x)\n            var_x = kl.LeakyReLU(alpha=0.2)(var_x)\n            for _ in range(self._config[\"dec_res_blocks\"]):\n                var_x = ResidualBlock(filters)(var_x)\n        else:\n            var_x = self._normalization(var_x)\n            if not self._is_dny:\n                var_x = kl.LeakyReLU(alpha=0.1)(var_x)\n        return var_x\n\n    def _normalization(self, inputs: tf.Tensor) -> tf.Tensor:\n        \"\"\" Add a normalization layer if requested.\n\n        Parameters\n        ----------\n        inputs: tensor\n            The input tensor to apply normalization to.\n\n        Returns\n        --------\n        tensor\n            The tensor with any normalization applied\n        \"\"\"\n        if not self._config[\"dec_norm\"]:\n            return inputs\n        norms = {\"batch\": kl.BatchNormalization,\n                 \"group\": GroupNormalization,\n                 \"instance\": InstanceNormalization,\n                 \"layer\": kl.LayerNormalization,\n                 \"rms\": RMSNormalization}\n        return norms[self._config[\"dec_norm\"]]()(inputs)\n\n    def _dny_entry(self, inputs: tf.Tensor) -> tf.Tensor:\n        \"\"\" Entry convolutions for using the upscale_dny method.\n\n        Parameters\n        ----------\n        inputs: Tensor\n            The inputs to the dny entry block\n\n        Returns\n        -------\n        Tensor\n            The output from the dny entry block\n        \"\"\"\n        var_x = Conv2DBlock(self._config[\"dec_max_filters\"],\n                            kernel_size=4,\n                            strides=1,\n                            padding=\"same\",\n                            relu_alpha=0.2)(inputs)\n        var_x = Conv2DBlock(self._config[\"dec_max_filters\"],\n                            kernel_size=3,\n                            strides=1,\n                            padding=\"same\",\n                            relu_alpha=0.2)(var_x)\n        return var_x\n\n    def __call__(self, inputs: tf.Tensor | list[tf.Tensor]) -> tf.Tensor | list[tf.Tensor]:\n        \"\"\" Upscale Network.\n\n        Parameters\n        inputs: Tensor or list of tensors\n            Input tensor(s) to upscale block. This will be a single tensor if learn mask is not\n            selected or if this is the first call to the upscale blocks. If learn mask is selected\n            and this is not the first call to upscale blocks, then this will be a list of the face\n            and mask tensors.\n\n        Returns\n        -------\n         Tensor or list of tensors\n            The output of encoder blocks. Either a single tensor (if learn mask is not enabled) or\n            list of tensors (if learn mask is enabled)\n        \"\"\"\n        start_idx, end_idx = (0, None) if self._layer_indicies is None else self._layer_indicies\n        end_idx = None if end_idx == -1 else end_idx\n\n        if self._config[\"learn_mask\"] and start_idx == 0:\n            # Mask needs to be created\n            var_x = inputs\n            var_y = inputs\n        elif self._config[\"learn_mask\"]:\n            # Mask has already been created and is an input to upscale blocks\n            var_x, var_y = inputs\n        else:\n            # No mask required\n            var_x = inputs\n\n        if start_idx == 0:\n            var_x = self._reshape_for_output(var_x)\n\n            if self._config[\"learn_mask\"]:\n                var_y = self._reshape_for_output(var_y)\n\n            if self._is_dny:\n                var_x = self._dny_entry(var_x)\n            if self._is_dny and self._config[\"learn_mask\"]:\n                var_y = self._dny_entry(var_y)\n\n        # De-convolve\n        if not self._filters:\n            upscales = int(np.log2(self._config[\"output_size\"] / K.int_shape(var_x)[1]))\n            self._filters.extend(_get_curve(self._config[\"dec_max_filters\"],\n                                            self._config[\"dec_min_filters\"],\n                                            upscales,\n                                            self._config[\"dec_filter_slope\"],\n                                            mode=self._config[\"dec_slope_mode\"]))\n            logger.debug(\"Generated class filters: %s\", self._filters)\n\n        filters = self._filters[start_idx: end_idx]\n\n        for idx, filts in enumerate(filters):\n            skip_res = idx == len(filters) - 1 and self._config[\"dec_skip_last_residual\"]\n            var_x = self._upscale_block(var_x, filts, skip_residual=skip_res)\n            if self._config[\"learn_mask\"]:\n                var_y = self._upscale_block(var_y, filts, is_mask=True)\n        retval = [var_x, var_y] if self._config[\"learn_mask\"] else var_x\n        return retval\n\n\nclass GBlock():  # pylint:disable=too-few-public-methods\n    \"\"\" G-Block model, borrowing from Adain StyleGAN.\n\n    Parameters\n    ----------\n    side: [\"a\", \"b\", \"both\"]\n        The side of the model that the fully connected layers belong to. Used for naming\n    input_shapes: list or tuple\n        The shape tuples for the input to the G-Block. The first item is the input from each side's\n        fully connected model, the second item is the input shape from the combined fully connected\n        model.\n    config: dict\n        The user configuration dictionary\n    \"\"\"\n    def __init__(self,\n                 side: T.Literal[\"a\", \"b\", \"both\"],\n                 input_shapes: list | tuple,\n                 config: dict) -> None:\n        logger.debug(\"Initializing: %s (side: %s, input_shapes: %s)\",\n                     self.__class__.__name__, side, input_shapes)\n        self._side = side\n        self._config = config\n        self._inputs = [kl.Input(shape=shape) for shape in input_shapes]\n        self._dense_nodes = 512\n        self._dense_recursions = 3\n        logger.debug(\"Initialized: %s\", self.__class__.__name__)\n\n    @classmethod\n    def _g_block(cls,\n                 inputs: tf.Tensor,\n                 style: tf.Tensor,\n                 filters: int,\n                 recursions: int = 2) -> tf.Tensor:\n        \"\"\" G_block adapted from ADAIN StyleGAN.\n\n        Parameters\n        ----------\n        inputs: tensor\n            The input tensor to the G-Block model\n        style: tensor\n            The input combined 'style' tensor to the G-Block model\n        filters: int\n            The number of filters to use for the G-Block Convolutional layers\n        recursions: int, optional\n            The number of recursive Convolutions to process. Default: `2`\n\n        Returns\n        -------\n        tensor\n            The output tensor from the G-Block model\n        \"\"\"\n        var_x = inputs\n        for i in range(recursions):\n            styles = [kl.Reshape([1, 1, filters])(kl.Dense(filters)(style)) for _ in range(2)]\n            noise = kl.Conv2D(filters, 1, padding=\"same\")(kl.GaussianNoise(1.0)(var_x))\n\n            if i == recursions - 1:\n                var_x = kl.Conv2D(filters, 3, padding=\"same\")(var_x)\n\n            var_x = AdaInstanceNormalization(dtype=\"float32\")([var_x, *styles])\n            var_x = kl.Add()([var_x, noise])\n            var_x = kl.LeakyReLU(0.2)(var_x)\n\n        return var_x\n\n    def __call__(self) -> tf.keras.models.Model:\n        \"\"\" G-Block Network.\n\n        Returns\n        -------\n        :class:`keras.models.Model`\n            The G-Block model\n        \"\"\"\n        var_x, style = self._inputs\n        for i in range(self._dense_recursions):\n            style = kl.Dense(self._dense_nodes, kernel_initializer=\"he_normal\")(style)\n            if i != self._dense_recursions - 1:  # Don't add leakyReLu to final output\n                style = kl.LeakyReLU(0.1)(style)\n\n        # Scale g_block filters to side dense\n        g_filts = K.int_shape(var_x)[-1]\n        var_x = Conv2D(g_filts, 3, strides=1, padding=\"same\")(var_x)\n        var_x = kl.GaussianNoise(1.0)(var_x)\n        var_x = self._g_block(var_x, style, g_filts)\n        return keras.models.Model(self._inputs, var_x, name=f\"g_block_{self._side}\")\n\n\nclass Decoder():  # pylint:disable=too-few-public-methods\n    \"\"\" Decoder Network.\n\n    Parameters\n    ----------\n    side: [\"a\", \"b\", \"both\"]\n        The side of the model that the Decoder belongs to. Used for naming\n    input_shape: tuple\n        The shape tuple for the input to the decoder.\n    config: dict\n        The user configuration dictionary\n    \"\"\"\n    def __init__(self,\n                 side: T.Literal[\"a\", \"b\", \"both\"],\n                 input_shape: tuple[int, int, int],\n                 config: dict) -> None:\n        logger.debug(\"Initializing: %s (side: %s, input_shape: %s)\",\n                     self.__class__.__name__, side, input_shape)\n        self._side = side\n        self._input_shape = input_shape\n        self._config = config\n        logger.debug(\"Initialized: %s\", self.__class__.__name__,)\n\n    def __call__(self) -> tf.keras.models.Model:\n        \"\"\" Decoder Network.\n\n        Returns\n        -------\n        :class:`keras.models.Model`\n            The Decoder model\n        \"\"\"\n        inputs = kl.Input(shape=self._input_shape)\n\n        num_ups_in_fc = self._config[\"dec_upscales_in_fc\"]\n\n        if self._config[\"learn_mask\"] and num_ups_in_fc:\n            # Mask has already been created in FC and is an output of that model\n            inputs = [inputs, kl.Input(shape=self._input_shape)]\n\n        indicies = None if not num_ups_in_fc else (num_ups_in_fc, -1)\n        upscales = UpscaleBlocks(self._side,\n                                 self._config,\n                                 layer_indicies=indicies)(inputs)\n\n        if self._config[\"learn_mask\"]:\n            var_x, var_y = upscales\n        else:\n            var_x = upscales\n\n        outputs = [Conv2DOutput(3, self._config[\"dec_output_kernel\"], name=\"face_out\")(var_x)]\n        if self._config[\"learn_mask\"]:\n            outputs.append(Conv2DOutput(1,\n                                        self._config[\"dec_output_kernel\"],\n                                        name=\"mask_out\")(var_y))\n\n        return keras.models.Model(inputs, outputs=outputs, name=f\"decoder_{self._side}\")\n", "plugins/train/model/dlight_defaults.py": "#!/usr/bin/env python3\n\"\"\"\n    The default options for the faceswap Dfaker Model plugin.\n\n    Defaults files should be named <plugin_name>_defaults.py\n    Any items placed into this file will automatically get added to the relevant config .ini files\n    within the faceswap/config folder.\n\n    The following variables should be defined:\n        _HELPTEXT: A string describing what this plugin does\n        _DEFAULTS: A dictionary containing the options, defaults and meta information. The\n                   dictionary should be defined as:\n                       {<option_name>: {<metadata>}}\n\n                   <option_name> should always be lower text.\n                   <metadata> dictionary requirements are listed below.\n\n    The following keys are expected for the _DEFAULTS <metadata> dict:\n        datatype:  [required] A python type class. This limits the type of data that can be\n                   provided in the .ini file and ensures that the value is returned in the\n                   correct type to faceswap. Valid datatypes are: <class 'int'>, <class 'float'>,\n                   <class 'str'>, <class 'bool'>.\n        default:   [required] The default value for this option.\n        info:      [required] A string describing what this option does.\n        choices:   [optional] If this option's datatype is of <class 'str'> then valid\n                   selections can be defined here. This validates the option and also enables\n                   a combobox / radio option in the GUI.\n        gui_radio: [optional] If <choices> are defined, this indicates that the GUI should use\n                   radio buttons rather than a combobox to display this option.\n        min_max:   [partial] For <class 'int'> and <class 'float'> datatypes this is required\n                   otherwise it is ignored. Should be a tuple of min and max accepted values.\n                   This is used for controlling the GUI slider range. Values are not enforced.\n        rounding:  [partial] For <class 'int'> and <class 'float'> datatypes this is\n                   required otherwise it is ignored. Used for the GUI slider. For floats, this\n                   is the number of decimal places to display. For ints this is the step size.\n        fixed:     [optional] [train only]. Training configurations are fixed when the model is\n                   created, and then reloaded from the state file. Marking an item as fixed=False\n                   indicates that this value can be changed for existing models, and will override\n                   the value saved in the state file with the updated value in config. If not\n                   provided this will default to True.\n\"\"\"\n\n\n_HELPTEXT = (\"A lightweight, high resolution Dfaker variant \"\n             \"(Adapted from https://github.com/dfaker/df)\")\n\n\n_DEFAULTS = dict(\n    features=dict(\n        default=\"best\",\n        info=\"Higher settings will allow learning more features such as tatoos, piercing and \"\n             \"wrinkles.\\nStrongly affects VRAM usage.\",\n        datatype=str,\n        choices=[\"lowmem\", \"fair\", \"best\"],\n        group=\"settings\",\n        gui_radio=True,\n        fixed=True,\n    ),\n    details=dict(\n        default=\"good\",\n        info=\"Defines detail fidelity. Lower setting can appear 'rugged' while 'good' might take \"\n             \"a longer time to train.\\nAffects VRAM usage.\",\n        datatype=str,\n        choices=[\"fast\", \"good\"],\n        group=\"settings\",\n        gui_radio=True,\n        fixed=True,\n    ),\n    output_size=dict(\n        default=256,\n        info=\"Output image resolution (in pixels).\\nBe aware that larger resolution will increase \"\n             \"VRAM requirements.\\nNB: Must be either 128, 256, or 384.\",\n        datatype=int,\n        rounding=128,\n        min_max=(128, 384),\n        choices=[],\n        group=\"settings\",\n        gui_radio=False,\n        fixed=True,\n    ),\n)\n", "plugins/train/model/dfaker.py": "#!/usr/bin/env python3\n\"\"\" DFaker Model\n    Based on the dfaker model: https://github.com/dfaker \"\"\"\nimport logging\nimport sys\n\n# Ignore linting errors from Tensorflow's thoroughly broken import system\nfrom tensorflow.keras.initializers import RandomNormal  # pylint:disable=import-error\nfrom tensorflow.keras.layers import Input, LeakyReLU  # pylint:disable=import-error\nfrom tensorflow.keras.models import Model as KModel  # pylint:disable=import-error\n\nfrom lib.model.nn_blocks import Conv2DOutput, UpscaleBlock, ResidualBlock\nfrom .original import Model as OriginalModel\n\nlogger = logging.getLogger(__name__)\n\n\nclass Model(OriginalModel):\n    \"\"\" Dfaker Model \"\"\"\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self._output_size = self.config[\"output_size\"]\n        if self._output_size not in (128, 256):\n            logger.error(\"Dfaker output shape should be 128 or 256 px\")\n            sys.exit(1)\n        self.input_shape = (self._output_size // 2, self._output_size // 2, 3)\n        self.encoder_dim = 1024\n        self.kernel_initializer = RandomNormal(0, 0.02)\n\n    def decoder(self, side):\n        \"\"\" Decoder Network \"\"\"\n        input_ = Input(shape=(8, 8, 512))\n        var_x = input_\n\n        if self._output_size == 256:\n            var_x = UpscaleBlock(1024, activation=None)(var_x)\n            var_x = LeakyReLU(alpha=0.2)(var_x)\n            var_x = ResidualBlock(1024, kernel_initializer=self.kernel_initializer)(var_x)\n        var_x = UpscaleBlock(512, activation=None)(var_x)\n        var_x = LeakyReLU(alpha=0.2)(var_x)\n        var_x = ResidualBlock(512, kernel_initializer=self.kernel_initializer)(var_x)\n        var_x = UpscaleBlock(256, activation=None)(var_x)\n        var_x = LeakyReLU(alpha=0.2)(var_x)\n        var_x = ResidualBlock(256, kernel_initializer=self.kernel_initializer)(var_x)\n        var_x = UpscaleBlock(128, activation=None)(var_x)\n        var_x = LeakyReLU(alpha=0.2)(var_x)\n        var_x = ResidualBlock(128, kernel_initializer=self.kernel_initializer)(var_x)\n        var_x = UpscaleBlock(64, activation=\"leakyrelu\")(var_x)\n        var_x = Conv2DOutput(3, 5, name=f\"face_out_{side}\")(var_x)\n        outputs = [var_x]\n\n        if self.config.get(\"learn_mask\", False):\n            var_y = input_\n            if self._output_size == 256:\n                var_y = UpscaleBlock(1024, activation=\"leakyrelu\")(var_y)\n            var_y = UpscaleBlock(512, activation=\"leakyrelu\")(var_y)\n            var_y = UpscaleBlock(256, activation=\"leakyrelu\")(var_y)\n            var_y = UpscaleBlock(128, activation=\"leakyrelu\")(var_y)\n            var_y = UpscaleBlock(64, activation=\"leakyrelu\")(var_y)\n            var_y = Conv2DOutput(1, 5, name=f\"mask_out_{side}\")(var_y)\n            outputs.append(var_y)\n        return KModel([input_], outputs=outputs, name=f\"decoder_{side}\")\n", "plugins/train/model/original.py": "#!/usr/bin/env python3\n\"\"\" Original Model\nBased on the original https://www.reddit.com/r/deepfakes/ code sample + contributions.\n\nThis model is heavily documented as it acts as a template that other model plugins can be developed\nfrom.\n\"\"\"\n\n# Ignore linting errors from Tensorflow's thoroughly broken import system\nfrom tensorflow.keras.layers import Dense, Flatten, Reshape, Input  # noqa:E501  # pylint:disable=import-error\nfrom tensorflow.keras.models import Model as KModel  # pylint:disable=import-error\n\nfrom lib.model.nn_blocks import Conv2DOutput, Conv2DBlock, UpscaleBlock\nfrom ._base import ModelBase\n\n\nclass Model(ModelBase):\n    \"\"\" Original Faceswap Model.\n\n    This is the original faceswap model and acts as a template for plugin development.\n\n    All plugins must define the following attribute override after calling the parent's\n    :func:`__init__` method:\n\n        * :attr:`input_shape` (`tuple` or `list`): a tuple of ints defining the shape of the \\\n        faces that the model takes as input. If the input size is the same for both sides, this \\\n        can be a single 3 dimensional tuple. If the inputs have different sizes for \"A\" and \"B\" \\\n        this should be a list of 2 3 dimensional shape tuples, 1 for each side.\n\n    Any additional attributes used exclusively by this model should be defined here, but make sure\n    that you are not accidentally overriding any existing\n    :class:`~plugins.train.model._base.ModelBase` attributes.\n\n    Parameters\n    ----------\n    args: varies\n        The default command line arguments passed in from :class:`~scripts.train.Train` or\n        :class:`~scripts.train.Convert`\n    kwargs: varies\n        The default keyword arguments passed in from :class:`~scripts.train.Train` or\n        :class:`~scripts.train.Convert`\n    \"\"\"\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.input_shape = (64, 64, 3)\n        self.low_mem = self.config.get(\"lowmem\", False)\n        self.learn_mask = self.config[\"learn_mask\"]\n        self.encoder_dim = 512 if self.low_mem else 1024\n\n    def build_model(self, inputs):\n        \"\"\" Create the model's structure.\n\n        This function is automatically called immediately after :func:`__init__` has been called if\n        a new model is being created. It is ignored if an existing model is being loaded from disk\n        as the model structure will be defined in the saved model file.\n\n        The model's final structure is defined here.\n\n        For the original model, An encoder instance is defined, then the same instance is\n        referenced twice, one for each input \"A\" and \"B\" so that the same model is used for\n        both inputs.\n\n        2 Decoders are then defined (one for each side) with the encoder instances passed in as\n        input to the corresponding decoders.\n\n        The final output of the model should always call :class:`lib.model.nn_blocks.Conv2DOutput`\n        so that the correct data type is set for the final activation, to support Mixed Precision\n        Training. Failure to do so is likely to lead to issues when Mixed Precision is enabled.\n\n        Parameters\n        ----------\n        inputs: list\n            A list of input tensors for the model. This will be a list of 2 tensors of\n            shape :attr:`input_shape`, the first for side \"a\", the second for side \"b\".\n\n        Returns\n        -------\n        :class:`keras.models.Model`\n            See Keras documentation for the correct\n            structure, but note that parameter :attr:`name` is a required rather than an optional\n            argument in Faceswap. You should assign this to the attribute ``self.name`` that is\n            automatically generated from the plugin's filename.\n        \"\"\"\n        input_a = inputs[0]\n        input_b = inputs[1]\n\n        encoder = self.encoder()\n        encoder_a = [encoder(input_a)]\n        encoder_b = [encoder(input_b)]\n\n        outputs = [self.decoder(\"a\")(encoder_a), self.decoder(\"b\")(encoder_b)]\n\n        autoencoder = KModel(inputs, outputs, name=self.model_name)\n        return autoencoder\n\n    def encoder(self):\n        \"\"\" The original Faceswap Encoder Network.\n\n        The encoder for the original model has it's weights shared between both the \"A\" and \"B\"\n        side of the model, so only one instance is created :func:`build_model`. However this same\n        instance is then used twice (once for A and once for B) meaning that the weights get\n        shared.\n\n        Returns\n        -------\n        :class:`keras.models.Model`\n            The Keras encoder model, for sharing between inputs from both sides.\n        \"\"\"\n        input_ = Input(shape=self.input_shape)\n        var_x = input_\n        var_x = Conv2DBlock(128, activation=\"leakyrelu\")(var_x)\n        var_x = Conv2DBlock(256, activation=\"leakyrelu\")(var_x)\n        var_x = Conv2DBlock(512, activation=\"leakyrelu\")(var_x)\n        if not self.low_mem:\n            var_x = Conv2DBlock(1024, activation=\"leakyrelu\")(var_x)\n        var_x = Dense(self.encoder_dim)(Flatten()(var_x))\n        var_x = Dense(4 * 4 * 1024)(var_x)\n        var_x = Reshape((4, 4, 1024))(var_x)\n        var_x = UpscaleBlock(512, activation=\"leakyrelu\")(var_x)\n        return KModel(input_, var_x, name=\"encoder\")\n\n    def decoder(self, side):\n        \"\"\" The original Faceswap Decoder Network.\n\n        The decoders for the original model have separate weights for each side \"A\" and \"B\", so two\n        instances are created in :func:`build_model`, one for each side.\n\n        Parameters\n        ----------\n        side: str\n            Either `\"a` or `\"b\"`. This is used for naming the decoder model.\n\n        Returns\n        -------\n        :class:`keras.models.Model`\n            The Keras decoder model. This will be called twice, once for each side.\n        \"\"\"\n        input_ = Input(shape=(8, 8, 512))\n        var_x = input_\n        var_x = UpscaleBlock(256, activation=\"leakyrelu\")(var_x)\n        var_x = UpscaleBlock(128, activation=\"leakyrelu\")(var_x)\n        var_x = UpscaleBlock(64, activation=\"leakyrelu\")(var_x)\n        var_x = Conv2DOutput(3, 5, name=f\"face_out_{side}\")(var_x)\n        outputs = [var_x]\n\n        if self.learn_mask:\n            var_y = input_\n            var_y = UpscaleBlock(256, activation=\"leakyrelu\")(var_y)\n            var_y = UpscaleBlock(128, activation=\"leakyrelu\")(var_y)\n            var_y = UpscaleBlock(64, activation=\"leakyrelu\")(var_y)\n            var_y = Conv2DOutput(1, 5, name=f\"mask_out_{side}\")(var_y)\n            outputs.append(var_y)\n        return KModel(input_, outputs=outputs, name=f\"decoder_{side}\")\n\n    def _legacy_mapping(self):\n        \"\"\" The mapping of legacy separate model names to single model names \"\"\"\n        return {f\"{self.name}_encoder.h5\": \"encoder\",\n                f\"{self.name}_decoder_A.h5\": \"decoder_a\",\n                f\"{self.name}_decoder_B.h5\": \"decoder_b\"}\n", "plugins/train/model/villain_defaults.py": "#!/usr/bin/env python3\n\"\"\"\n    The default options for the faceswap Villain Model plugin.\n\n    Defaults files should be named <plugin_name>_defaults.py\n    Any items placed into this file will automatically get added to the relevant config .ini files\n    within the faceswap/config folder.\n\n    The following variables should be defined:\n        _HELPTEXT: A string describing what this plugin does\n        _DEFAULTS: A dictionary containing the options, defaults and meta information. The\n                   dictionary should be defined as:\n                       {<option_name>: {<metadata>}}\n\n                   <option_name> should always be lower text.\n                   <metadata> dictionary requirements are listed below.\n\n    The following keys are expected for the _DEFAULTS <metadata> dict:\n        datatype:  [required] A python type class. This limits the type of data that can be\n                   provided in the .ini file and ensures that the value is returned in the\n                   correct type to faceswap. Valid datatypes are: <class 'int'>, <class 'float'>,\n                   <class 'str'>, <class 'bool'>.\n        default:   [required] The default value for this option.\n        info:      [required] A string describing what this option does.\n        choices:   [optional] If this option's datatype is of <class 'str'> then valid\n                   selections can be defined here. This validates the option and also enables\n                   a combobox / radio option in the GUI.\n        gui_radio: [optional] If <choices> are defined, this indicates that the GUI should use\n                   radio buttons rather than a combobox to display this option.\n        min_max:   [partial] For <class 'int'> and <class 'float'> datatypes this is required\n                   otherwise it is ignored. Should be a tuple of min and max accepted values.\n                   This is used for controlling the GUI slider range. Values are not enforced.\n        rounding:  [partial] For <class 'int'> and <class 'float'> datatypes this is\n                   required otherwise it is ignored. Used for the GUI slider. For floats, this\n                   is the number of decimal places to display. For ints this is the step size.\n        fixed:     [optional] [train only]. Training configurations are fixed when the model is\n                   created, and then reloaded from the state file. Marking an item as fixed=False\n                   indicates that this value can be changed for existing models, and will override\n                   the value saved in the state file with the updated value in config. If not\n                   provided this will default to True.\n\"\"\"\n\n\n_HELPTEXT = (\n    \"A Higher resolution version of the Original Model by VillainGuy.\\n\"\n    \"Extremely VRAM heavy. Don't try to run this if you have a small GPU.\\n\"\n)\n\n\n_DEFAULTS = {\n    \"lowmem\": {\n        \"default\": False,\n        \"info\": \"Lower memory mode. Set to 'True' if having issues with VRAM useage.\\n\"\n                \"NB: Models with a changed lowmem mode are not compatible with each other.\",\n        \"datatype\": bool,\n        \"rounding\": None,\n        \"min_max\": None,\n        \"choices\": [],\n        \"gui_radio\": False,\n        \"group\": \"settings\",\n        \"fixed\": True,\n    },\n}\n", "plugins/train/model/phaze_a_defaults.py": "#!/usr/bin/env python3\n\"\"\"\n    The default options for the faceswap Phaze-A Model plugin.\n\n    Defaults files should be named <plugin_name>_defaults.py\n    Any items placed into this file will automatically get added to the relevant config .ini files\n    within the faceswap/config folder.\n\n    The following variables should be defined:\n        \"_HELPTEXT: A string describing what this plugin does\n        \"_DEFAULTS: A dictionary containing the options, defaults and meta information. The\n               \"   dictionary should be defined as:\n               \"       {<option_name>: {<metadata>}}\n\n               \"   <option_name> should always be lower text.\n               \"   <metadata> dictionary requirements are listed below.\n\n    The following keys are expected for the _DEFAULTS <metadata> dict:\n        \"datatype:  [required] A python type class. This limits the type of data that can be\n               \"   provided in the .ini file and ensures that the value is returned in the\n               \"   correct type to faceswap. Valid data types are: <class 'int'>, <class 'float'>,\n               \"   <class 'str'>, <class 'bool'>.\n        \"default:   [required] The default value for this option.\n        \"info:      [required] A string describing what this option does.\n        \"choices:   [optional] If this option's datatype is of <class 'str'> then valid\n               \"   selections can be defined here. This validates the option and also enables\n               \"   a combobox / radio option in the GUI.\n        \"gui_radio: [optional] If <choices> are defined, this indicates that the GUI should use\n               \"   radio buttons rather than a combobox to display this option.\n        \"min_max:   [partial] For <class 'int'> and <class 'float'> data types this is required\n               \"   otherwise it is ignored. Should be a tuple of min and max accepted values.\n               \"   This is used for controlling the GUI slider range. Values are not enforced.\n        \"rounding:  [partial] For <class 'int'> and <class 'float'> data types this is\n               \"   required otherwise it is ignored. Used for the GUI slider. For floats, this\n               \"   is the number of decimal places to display. For ints this is the step size.\n        \"fixed:     [optional] [train only]. Training configurations are fixed when the model is\n               \"   created, and then reloaded from the state file. Marking an item as fixed=False\n               \"   indicates that this value can be changed for existing models, and will override\n               \"   the value saved in the state file with the updated value in config. If not\n               \"   provided this will default to True.\n\"\"\"\n\n_HELPTEXT: str = (\n    \"Phaze-A Model by TorzDF, with thanks to BirbFakes.\\n\"\n    \"Allows for the experimentation of various standard Networks as the encoder and takes \"\n    \"inspiration from Nvidia's StyleGAN for the Decoder. It is highly recommended to research to \"\n    \"understand the parameters better.\")\n\n_ENCODERS: list[str] = sorted([\n    \"clipv_vit-b-16\", \"clipv_vit-b-32\", \"clipv_vit-l-14\", \"clipv_vit-l-14-336px\",\n    \"clipv_farl-b-16-16\", \"clipv_farl-b-16-64\",\n    \"densenet121\", \"densenet169\", \"densenet201\", \"efficientnet_b0\", \"efficientnet_b1\",\n    \"efficientnet_b2\", \"efficientnet_b3\", \"efficientnet_b4\", \"efficientnet_b5\", \"efficientnet_b6\",\n    \"efficientnet_b7\", \"efficientnet_v2_b0\", \"efficientnet_v2_b1\", \"efficientnet_v2_b2\",\n    \"efficientnet_v2_b3\", \"efficientnet_v2_l\", \"efficientnet_v2_m\", \"efficientnet_v2_s\",\n    \"inception_resnet_v2\", \"inception_v3\", \"mobilenet\", \"mobilenet_v2\", \"mobilenet_v3_large\",\n    \"mobilenet_v3_small\", \"nasnet_large\", \"nasnet_mobile\", \"resnet50\", \"resnet50_v2\", \"resnet101\",\n    \"resnet101_v2\", \"resnet152\", \"resnet152_v2\", \"vgg16\", \"vgg19\", \"xception\", \"fs_original\"])\n\n_DEFAULTS = {\n    # General\n    \"output_size\": {\n        \"default\": 128,\n        \"info\": (\n            \"Resolution (in pixels) of the output image to generate.\\n\"\n            \"BE AWARE Larger resolution will dramatically increase VRAM requirements.\"),\n        \"datatype\": int,\n        \"rounding\": 16,\n        \"min_max\": (64, 2048),\n        \"group\": \"general\",\n        \"fixed\": True},\n    \"shared_fc\": {\n        \"default\": \"none\",\n        \"info\": (\n            \"Whether to create a shared fully connected layer. This layer will have the same \"\n            \"structure as the fully connected layers used for each side of the model. A shared \"\n            \"fully connected layer looks for patterns that are common to both sides. NB: \"\n            \"Enabling this option only makes sense if 'split fc' is selected.\"\n            \"\\n\\tnone - Do not create a Fully Connected layer for shared data. (Original method)\"\n            \"\\n\\tfull - Create an exclusive Fully Connected layer for shared data. (IAE method)\"\n            \"\\n\\thalf - Use the 'fc_a' layer for shared data. This saves VRAM by re-using the \"\n            \"'A' side's fully connected model for the shared data. However, this will lead to \"\n            \"an 'unbalanced' model and can lead to more identity bleed (DFL method)\"),\n        \"datatype\": str,\n        \"choices\": [\"none\", \"full\", \"half\"],\n        \"gui_radio\": True,\n        \"group\": \"general\",\n        \"fixed\": True},\n    \"enable_gblock\": {\n        \"default\": True,\n        \"info\": (\n            \"Whether to enable the G-Block. If enabled, this will create a shared fully \"\n            \"connected layer (configurable in the 'G-Block hidden layers' section) to look for \"\n            \"patterns in the combined data, before feeding a block prior to the decoder for \"\n            \"merging this shared and combined data.\"\n            \"\\n\\tTrue - Use the G-Block in the Decoder. A combined fully connected layer will be \"\n            \"created to feed this block which can be configured below.\"\n            \"\\n\\tFalse - Don't use the G-Block in the decoder. No combined fully connected layer \"\n            \"will be created.\"),\n        \"datatype\": bool,\n        \"group\": \"general\",\n        \"fixed\": True},\n    \"split_fc\": {\n        \"default\": True,\n        \"info\": (\n            \"Whether to use a single shared Fully Connected layer or separate Fully Connected \"\n            \"layers for each side.\"\n            \"\\n\\tTrue - Use separate Fully Connected layers for Face A and Face B. This is more \"\n            \"similar to the 'IAE' style of model.\"\n            \"\\n\\tFalse - Use combined Fully Connected layers for both sides. This is more \"\n            \"similar to the original Faceswap architecture.\"),\n        \"datatype\": bool,\n        \"group\": \"general\",\n        \"fixed\": True},\n    \"split_gblock\": {\n        \"default\": False,\n        \"info\": (\n            \"If the G-Block is enabled, Whether to use a single G-Block shared between both \"\n            \"sides, or whether to have a separate G-Block (one for each side). NB: The Fully \"\n            \"Connected layer that feeds the G-Block will always be shared.\"\n            \"\\n\\tTrue - Use separate G-Blocks for Face A and Face B.\"\n            \"\\n\\tFalse - Use a combined G-Block layers for both sides.\"),\n        \"datatype\": bool,\n        \"group\": \"general\",\n        \"fixed\": True},\n    \"split_decoders\": {\n        \"default\": False,\n        \"info\": (\n            \"Whether to use a single decoder or split decoders.\"\n            \"\\n\\tTrue - Use a separate decoder for Face A and Face B. This is more similar to \"\n            \"the original Faceswap architecture.\"\n            \"\\n\\tFalse - Use a combined Decoder. This is more similar to 'IAE' style \"\n            \"architecture.\"),\n        \"datatype\": bool,\n        \"group\": \"general\",\n        \"fixed\": True},\n\n    # Encoder\n    \"enc_architecture\": {\n        \"default\": \"fs_original\",\n        \"info\": (\n            \"The encoder architecture to use. See the relevant config sections for specific \"\n            \"architecture tweaking.\\nNB: For keras based pre-built models, the global \"\n            \"initializers and padding options will be ignored for the selected encoder.\"\n            \"\\n\\n\\tCLIPv: This is an implementation of the Visual encoder from the CLIP \"\n            \"transformer. The ViT weights are trained on imagenet whilst the FaRL weights are \"\n            \"trained on face related tasks. All have a default input size of 224px except for \"\n            \"ViT-L-14-336px that has an input size of 336px. Ref: Learning Transferable Visual \"\n            \"Models From Natural Language Supervision (2021): https://arxiv.org/abs/2103.00020\"\n            \"\\n\\n\\tdensenet: (32px -224px). Ref: Densely Connected Convolutional Networks \"\n            \"(2016): https://arxiv.org/abs/1608.06993?source=post_page\"\n            \"\\n\\n\\tefficientnet: [Tensorflow 2.3+ only] EfficientNet has numerous variants (B0 - \"\n            \"B8) that increases the model width, depth and dimensional space at each step. The \"\n            \"minimum input resolution is 32px for all variants. The maximum input resolution for \"\n            \"each variant is: b0: 224px, b1: 240px, b2: 260px, b3: 300px, b4: 380px, b5: 456px, \"\n            \"b6: 528px, b7 600px. Ref: Rethinking Model Scaling for Convolutional Neural \"\n            \"Networks (2020): https://arxiv.org/abs/1905.11946\"\n            \"\\n\\n\\tefficientnet_v2: [Tensorflow 2.8+ only] EfficientNetV2 is the follow up to \"\n            \"efficientnet. It has numerous variants (B0 - B3 and Small, Medium and Large) that \"\n            \"increases the model width, depth and dimensional space at each step. The minimum \"\n            \"input resolution is 32px for all variants. The maximum input resolution for each \"\n            \"variant is: b0: 224px, b1: 240px, b2: 260px, b3: 300px, s: 384px, m: 480px, l: \"\n            \"480px. Ref: EfficientNetV2: Smaller Models and Faster Training (2021): \"\n            \"https://arxiv.org/abs/2104.00298\"\n            \"\\n\\n\\tfs_original: (32px - 1024px). A configurable variant of the original facewap \"\n            \"encoder. ImageNet weights cannot be loaded for this model. Additional parameters \"\n            \"can be configured with the 'fs_enc' options. A version of this encoder is used in \"\n            \"the following models: Original, Original (lowmem), Dfaker, DFL-H128, DFL-SAE, IAE, \"\n            \"Lightweight.\"\n            \"\\n\\n\\tinception_resnet_v2: (75px - 299px). Ref: Inception-ResNet and the Impact of \"\n            \"Residual Connections on Learning (2016): https://arxiv.org/abs/1602.07261\"\n            \"\\n\\n\\tinceptionV3: (75px - 299px). Ref: Rethinking the Inception Architecture for \"\n            \"Computer Vision (2015): https://arxiv.org/abs/1512.00567\"\n            \"\\n\\n\\tmobilenet: (32px - 224px). Additional MobileNet parameters can be set with \"\n            \"the 'mobilenet' options. Ref: MobileNets: Efficient Convolutional Neural Networks \"\n            \"for Mobile Vision Applications (2017): https://arxiv.org/abs/1704.04861\"\n            \"\\n\\n\\tmobilenet_v2: (32px - 224px). Additional MobileNet parameters can be set with \"\n            \"the 'mobilenet' options. Ref: MobileNetV2: Inverted Residuals and Linear \"\n            \"Bottlenecks (2018): https://arxiv.org/abs/1801.04381\"\n            \"\\n\\n\\tmobilenet_v3: (32px - 224px). Additional MobileNet parameters can be set with \"\n            \"the 'mobilenet' options. Ref: Searching for MobileNetV3 (2019): \"\n            \"https://arxiv.org/pdf/1905.02244.pdf\"\n            \"\\n\\n\\tnasnet: (32px - 331px (large) or 224px (mobile)). Ref: Learning Transferable \"\n            \"Architectures for Scalable Image Recognition (2017): \"\n            \"https://arxiv.org/abs/1707.07012\"\n            \"\\n\\n\\tresnet: (32px - 224px). Deep Residual Learning for Image Recognition (2015): \"\n            \"https://arxiv.org/abs/1512.03385\"\n            \"\\n\\n\\tvgg: (32px - 224px). Very Deep Convolutional Networks for Large-Scale Image \"\n            \"Recognition (2014): https://arxiv.org/abs/1409.1556\"\n            \"\\n\\n\\txception: (71px - 229px). Ref: Deep Learning with Depthwise Separable \"\n            \"Convolutions (2017): https://arxiv.org/abs/1409.1556.\\n\"),\n        \"datatype\": str,\n        \"choices\": _ENCODERS,\n        \"gui_radio\": False,\n        \"group\": \"encoder\",\n        \"fixed\": True},\n    \"enc_scaling\": {\n        \"default\": 7,\n        \"info\": (\n            \"Input scaling for the encoder. Some of the encoders have large input sizes, which \"\n            \"often are not helpful for Faceswap. This setting scales the dimensional space that \"\n            \"the encoder works in. For example an encoder with a maximum input size of 224px \"\n            \"will be input an image of 112px at 50%% scaling. See the Architecture tooltip for \"\n            \"the minimum and maximum sizes for each encoder. NB: The input size will be rounded \"\n            \"down to the nearest 16 pixels.\"),\n        \"datatype\": int,\n        \"min_max\": (0, 200),\n        \"rounding\": 1,\n        \"group\": \"encoder\",\n        \"fixed\": True},\n    \"enc_load_weights\": {\n        \"default\": True,\n        \"info\": (\n            \"Load pre-trained weights trained on ImageNet data. Only available for non-\"\n            \"Faceswap encoders (i.e. those not beginning with 'fs'). NB: If you use the global \"\n            \"'load weights' option and have selected to load weights from a previous model's \"\n            \"'encoder' or 'keras_encoder' then the weights loaded here will be replaced by the \"\n            \"weights loaded from your saved model.\"),\n        \"datatype\": bool,\n        \"group\": \"encoder\",\n        \"fixed\": True},\n\n    # Bottleneck\n    \"bottleneck_type\": {\n        \"default\": \"dense\",\n        \"info\": (\n            \"The type of layer to use for the bottleneck.\"\n            \"\\n\\taverage_pooling: Use a Global Average Pooling 2D layer for the bottleneck.\"\n            \"\\n\\tdense: Use a Dense layer for the bottleneck (the traditional Faceswap method). \"\n            \"You can set the size of the Dense layer with the 'bottleneck_size' parameter.\"\n            \"\\n\\tmax_pooling: Use a Global Max Pooling 2D layer for the bottleneck.\"\n            \"\\n\\flatten: Don't use a bottleneck at all. Some encoders output in a size that make \"\n            \"a bottleneck unnecessary. This option flattens the output from the encoder, with no \"\n            \"further operations\"),\n        \"datatype\": str,\n        \"group\": \"bottleneck\",\n        \"gui_radio\": True,\n        \"choices\": [\"average_pooling\", \"dense\", \"max_pooling\", \"flatten\"],\n        \"fixed\": True},\n    \"bottleneck_norm\": {\n        \"default\": \"none\",\n        \"info\": (\n            \"Apply a normalization layer after encoder output and prior to the bottleneck.\"\n            \"\\n\\tnone - Do not apply a normalization layer\"\n            \"\\n\\tinstance - Apply Instance Normalization\"\n            \"\\n\\tlayer - Apply Layer Normalization (Ba et al., 2016)\"\n            \"\\n\\trms - Apply Root Mean Squared Layer Normalization (Zhang et al., 2019). A \"\n            \"simplified version of Layer Normalization with reduced overhead.\"),\n        \"datatype\": str,\n        \"gui_radio\": True,\n        \"choices\": [\"none\", \"instance\", \"layer\", \"rms\"],\n        \"group\": \"bottleneck\",\n        \"fixed\": True},\n    \"bottleneck_size\": {\n        \"default\": 1024,\n        \"info\": (\n            \"If using a Dense layer for the bottleneck, then this is the number of nodes to \"\n            \"use.\"),\n        \"datatype\": int,\n        \"rounding\": 128,\n        \"min_max\": (128, 4096),\n        \"group\": \"bottleneck\",\n        \"fixed\": True},\n    \"bottleneck_in_encoder\": {\n        \"default\": True,\n        \"info\": (\n            \"Whether to place the bottleneck in the Encoder or to place it with the other \"\n            \"hidden layers. Placing the bottleneck in the encoder means that both sides will \"\n            \"share the same bottleneck. Placing it with the other fully connected layers means \"\n            \"that each fully connected layer will each get their own bottleneck. This may be \"\n            \"combined or split depending on your overall architecture configuration settings.\"),\n        \"datatype\": bool,\n        \"group\": \"bottleneck\",\n        \"fixed\": True},\n\n    # Intermediate Layers\n    \"fc_depth\": {\n        \"default\": 1,\n        \"info\": (\n            \"The number of consecutive Dense (fully connected) layers to include in each \"\n            \"side's intermediate layer.\"),\n        \"datatype\": int,\n        \"rounding\": 1,\n        \"min_max\": (0, 16),\n        \"group\": \"hidden layers\",\n        \"fixed\": True},\n    \"fc_min_filters\": {\n        \"default\": 1024,\n        \"info\": (\n            \"The number of filters to use for the initial fully connected layer. The number of \"\n            \"nodes actually used is: fc_min_filters x fc_dimensions x fc_dimensions.\\nNB: This \"\n            \"value may be scaled down, depending on output resolution.\"),\n        \"datatype\": int,\n        \"rounding\": 16,\n        \"min_max\": (16, 5120),\n        \"group\": \"hidden layers\",\n        \"fixed\": True},\n    \"fc_max_filters\": {\n        \"default\": 1024,\n        \"info\": (\n            \"This is the number of filters to be used in the final reshape layer at the end of \"\n            \"the fully connected layers. The actual number of nodes used for the final fully \"\n            \"connected layer is: fc_min_filters x fc_dimensions x fc_dimensions.\\nNB: This value \"\n            \"may be scaled down, depending on output resolution.\"),\n        \"datatype\": int,\n        \"rounding\": 64,\n        \"min_max\": (128, 5120),\n        \"group\": \"hidden layers\",\n        \"fixed\": True},\n    \"fc_dimensions\": {\n        \"default\": 4,\n        \"info\": (\n            \"The height and width dimension for the final reshape layer at the end of the \"\n            \"fully connected layers.\\nNB: The total number of nodes within the final fully \"\n            \"connected layer will be: fc_dimensions x fc_dimensions x fc_max_filters.\"),\n        \"datatype\": int,\n        \"rounding\": 1,\n        \"min_max\": (1, 16),\n        \"group\": \"hidden layers\",\n        \"fixed\": True},\n    \"fc_filter_slope\": {\n        \"default\": -0.5,\n        \"info\": (\n            \"The rate that the filters move from the minimum number of filters to the maximum \"\n            \"number of filters. EG:\\n\"\n            \"Negative numbers will change the number of filters quicker at first and slow down \"\n            \"each layer.\\n\"\n            \"Positive numbers will change the number of filters slower at first but then speed \"\n            \"up each layer.\\n\"\n            \"0.0 - This will change at a linear rate (i.e. the same number of filters will be \"\n            \"changed at each layer).\"),\n        \"datatype\": float,\n        \"min_max\": (-.99, .99),\n        \"rounding\": 2,\n        \"group\": \"hidden layers\",\n        \"fixed\": True},\n    \"fc_dropout\": {\n        \"default\": 0.0,\n        \"info\": (\n            \"Dropout is a form of regularization that can prevent a model from over-fitting \"\n            \"and help to keep neurons 'alive'. 0.5 will dropout half the connections between each \"\n            \"fully connected layer, 0.25 will dropout a quarter of the connections etc. Set to \"\n            \"0.0 to disable.\"),\n        \"datatype\": float,\n        \"rounding\": 2,\n        \"min_max\": (0.0, 0.99),\n        \"group\": \"hidden layers\",\n        \"fixed\": False},\n    \"fc_upsampler\": {\n        \"default\": \"upsample2d\",\n        \"info\": (\n            \"The type of dimensional upsampling to perform at the end of the fully connected \"\n            \"layers, if upsamples > 0. The number of filters used for the upscale layers will be \"\n            \"the value given in 'fc_upsample_filters'.\"\n            \"\\n\\tupsample2d - A lightweight and VRAM friendly method. 'quick and dirty' but does \"\n            \"not learn any parameters\"\n            \"\\n\\tsubpixel - Sub-pixel upscaler using depth-to-space which may require more \"\n            \"VRAM.\"\n            \"\\n\\tresize_images - Uses the Keras resize_image function to save about half as much \"\n            \"vram as the heaviest methods.\"\n            \"\\n\\tupscale_fast - Developed by Andenixa. Focusses on speed to upscale, but \"\n            \"requires more VRAM.\"\n            \"\\n\\tupscale_hybrid - Developed by Andenixa. Uses a combination of PixelShuffler and \"\n            \"Upsampling2D to upscale, saving about 1/3rd of VRAM of the heaviest methods.\"),\n        \"datatype\": str,\n        \"choices\": [\"resize_images\", \"subpixel\", \"upscale_fast\", \"upscale_hybrid\", \"upsample2d\"],\n        \"group\": \"hidden layers\",\n        \"gui_radio\": False,\n        \"fixed\": True},\n    \"fc_upsamples\": {\n        \"default\": 1,\n        \"info\": (\n            \"Some upsampling can occur within the Fully Connected layers rather than in the \"\n            \"Decoder to increase the dimensional space. Set how many upscale layers should occur \"\n            \"within the Fully Connected layers.\"),\n        \"datatype\": int,\n        \"min_max\": (0, 4),\n        \"rounding\": 1,\n        \"group\": \"hidden layers\",\n        \"fixed\": True},\n    \"fc_upsample_filters\": {\n        \"default\": 512,\n        \"info\": (\n            \"If you have selected an upsampler which requires filters (i.e. any upsampler with \"\n            \"the exception of Upsampling2D), then this is the number of filters to be used for \"\n            \"the upsamplers within the fully connected layers,  NB: This value may be scaled \"\n            \"down, depending on output resolution. Also note, that this figure will dictate the \"\n            \"number of filters used for the G-Block, if selected.\"),\n        \"datatype\": int,\n        \"rounding\": 64,\n        \"min_max\": (128, 5120),\n        \"group\": \"hidden layers\",\n        \"fixed\": True},\n\n    # G-Block\n    \"fc_gblock_depth\": {\n        \"default\": 3,\n        \"info\": (\n            \"The number of consecutive Dense (fully connected) layers to include in the \"\n            \"G-Block shared layer.\"),\n        \"datatype\": int,\n        \"rounding\": 1,\n        \"min_max\": (1, 16),\n        \"group\": \"g-block hidden layers\",\n        \"fixed\": True},\n    \"fc_gblock_min_nodes\": {\n        \"default\": 512,\n        \"info\": \"The number of nodes to use for the initial G-Block shared fully connected layer.\",\n        \"datatype\": int,\n        \"rounding\": 64,\n        \"min_max\": (128, 5120),\n        \"group\": \"g-block hidden layers\",\n        \"fixed\": True},\n    \"fc_gblock_max_nodes\": {\n        \"default\": 512,\n        \"info\": \"The number of nodes to use for the final G-Block shared fully connected layer.\",\n        \"datatype\": int,\n        \"rounding\": 64,\n        \"min_max\": (128, 5120),\n        \"group\": \"g-block hidden layers\",\n        \"fixed\": True},\n    \"fc_gblock_filter_slope\": {\n        \"default\": -0.5,\n        \"info\": (\n            \"The rate that the filters move from the minimum number of filters to the maximum \"\n            \"number of filters for the G-Block shared layers. EG:\\n\"\n            \"Negative numbers will change the number of filters quicker at first and slow down \"\n            \"each layer.\\n\"\n            \"Positive numbers will change the number of filters slower at first but then speed \"\n            \"up each layer.\\n\"\n            \"0.0 - This will change at a linear rate (i.e. the same number of filters will be \"\n            \"changed at each layer).\"),\n        \"datatype\": float,\n        \"min_max\": (-.99, .99),\n        \"rounding\": 2,\n        \"group\": \"g-block hidden layers\",\n        \"fixed\": True},\n    \"fc_gblock_dropout\": {\n        \"default\": 0.0,\n        \"info\": (\n            \"Dropout is a regularization technique that can prevent a model from over-fitting \"\n            \"and help to keep neurons 'alive'. 0.5 will dropout half the connections between \"\n            \"each fully connected layer, 0.25 will dropout a quarter of the connections etc. Set \"\n            \"to 0.0 to disable.\"),\n        \"datatype\": float,\n        \"rounding\": 2,\n        \"min_max\": (0.0, 0.99),\n        \"group\": \"g-block hidden layers\",\n        \"fixed\": False},\n\n    # Decoder\n    \"dec_upscale_method\": {\n        \"default\": \"subpixel\",\n        \"info\": (\n            \"The method to use for the upscales within the decoder. Images are upscaled \"\n            \"multiple times within the decoder as the network learns to reconstruct the face.\"\n            \"\\n\\tsubpixel - Sub-pixel upscaler using depth-to-space which requires more \"\n            \"VRAM.\"\n            \"\\n\\tresize_images - Uses the Keras resize_image function to save about half as much \"\n            \"vram as the heaviest methods.\"\n            \"\\n\\tupscale_fast - Developed by Andenixa. Focusses on speed to upscale, but \"\n            \"requires more VRAM.\"\n            \"\\n\\tupscale_hybrid - Developed by Andenixa. Uses a combination of PixelShuffler and \"\n            \"Upsampling2D to upscale, saving about 1/3rd of VRAM of the heaviest methods.\"\n            \"\\n\\tupscale_dny - An alternative upscale implementation using Upsampling2D to \"\n            \"upsale.\"),\n        \"datatype\": str,\n        \"choices\": [\"subpixel\", \"resize_images\", \"upscale_fast\", \"upscale_hybrid\", \"upscale_dny\"],\n        \"gui_radio\": True,\n        \"group\": \"decoder\",\n        \"fixed\": True},\n    \"dec_upscales_in_fc\": {\n        \"default\": 0,\n        \"min_max\": (0, 6),\n        \"rounding\": 1,\n        \"info\": (\n            \"It is possible to place some of the upscales at the end of the fully connected \"\n            \"model. For models with split decoders, but a shared fully connected layer, this \"\n            \"would have the effect of saving some VRAM but possibly at the cost of introducing \"\n            \"artefacts. For models with a shared decoder but split fully connected layers, this \"\n            \"would have the effect of increasing VRAM usage by processing some of the upscales \"\n            \"for each side rather than together.\"),\n        \"datatype\": int,\n        \"group\": \"decoder\",\n        \"fixed\": True},\n    \"dec_norm\": {\n        \"default\": \"none\",\n        \"info\": (\n            \"Normalization to apply to apply after each upscale.\"\n            \"\\n\\tnone - Do not apply a normalization layer\"\n            \"\\n\\tbatch - Apply Batch Normalization\"\n            \"\\n\\tgroup - Apply Group Normalization\"\n            \"\\n\\tinstance - Apply Instance Normalization\"\n            \"\\n\\tlayer - Apply Layer Normalization (Ba et al., 2016)\"\n            \"\\n\\trms - Apply Root Mean Squared Layer Normalization (Zhang et al., 2019). A \"\n            \"simplified version of Layer Normalization with reduced overhead.\"),\n        \"datatype\": str,\n        \"gui_radio\": True,\n        \"choices\": [\"none\", \"batch\", \"group\", \"instance\", \"layer\", \"rms\"],\n        \"group\": \"decoder\",\n        \"fixed\": True},\n    \"dec_min_filters\": {\n        \"default\": 64,\n        \"info\": (\n            \"The minimum number of filters to use in decoder upscalers (i.e. the number of \"\n            \"filters to use for the final upscale layer).\"),\n        \"datatype\": int,\n        \"min_max\": (16, 512),\n        \"rounding\": 16,\n        \"group\": \"decoder\",\n        \"fixed\": True},\n    \"dec_max_filters\": {\n        \"default\": 512,\n        \"info\": (\n            \"The maximum number of filters to use in decoder upscalers (i.e. the number of \"\n            \"filters to use for the first upscale layer).\"),\n        \"datatype\": int,\n        \"min_max\": (256, 5120),\n        \"rounding\": 64,\n        \"group\": \"decoder\",\n        \"fixed\": True},\n    \"dec_slope_mode\": {\n        \"default\": \"full\",\n        \"info\": (\n            \"Alters the action of the filter slope.\\n\"\n            \"\\n\\tfull: The number of filters at each upscale layer will reduce from the chosen \"\n            \"max_filters at the first layer to the chosen min_filters at the last layer as \"\n            \"dictated by the dec_filter_slope.\"\n            \"\\n\\tcap_max: The filters will decline at a fixed rate from each upscale to the next \"\n            \"based on the filter_slope setting. If there are more upscales than filters, \"\n            \"then the earliest upscales will be capped at the max_filter value until the filters \"\n            \"can reduce to the min_filters value at the final upscale. (EG: 512 -> 512 -> 512 -> \"\n            \"256 -> 128 -> 64).\"\n            \"\\n\\tcap_min: The filters will decline at a fixed rate from each upscale to the next \"\n            \"based on the filter_slope setting. If there are more upscales than filters, then \"\n            \"the earliest upscales will drop their filters until the min_filter value is met and \"\n            \"repeat the min_filter value for the remaining upscales. (EG: 512 -> 256 -> 128 -> \"\n            \"64 -> 64 -> 64).\"),\n        \"choices\": [\"full\", \"cap_max\", \"cap_min\"],\n        \"group\": \"decoder\",\n        \"fixed\": True,\n        \"gui_radio\": True},\n    \"dec_filter_slope\": {\n        \"default\": -0.45,\n        \"info\": (\n            \"The rate that the filters reduce at each upscale layer.\\n\"\n            \"\\n\\tFull Slope Mode: Negative numbers will drop the number of filters quicker at \"\n            \"first and slow down each upscale. Positive numbers will drop the number of filters \"\n            \"slower at first but then speed up each upscale. A value of 0.0 will reduce at a \"\n            \"linear rate (i.e. the same number of filters will be reduced at each upscale).\\n\"\n            \"\\n\\tCap Min/Max Slope Mode: Only positive values will work here. Negative values \"\n            \"will automatically be converted to their positive counterpart. A value of 0.5 will \"\n            \"halve the number of filters at each upscale until the minimum value is reached. A \"\n            \"value of 0.33 will be reduce the number of filters by a third until the minimum \"\n            \"value is reached etc.\"),\n        \"datatype\": float,\n        \"min_max\": (-.99, .99),\n        \"rounding\": 2,\n        \"group\": \"decoder\",\n        \"fixed\": True},\n    \"dec_res_blocks\": {\n        \"default\": 1,\n        \"info\": (\n            \"The number of Residual Blocks to apply to each upscale layer. Set to 0 to disable \"\n            \"residual blocks entirely.\"),\n        \"datatype\": int,\n        \"rounding\": 1,\n        \"min_max\": (0, 8),\n        \"group\": \"decoder\",\n        \"fixed\": True},\n    \"dec_output_kernel\": {\n        \"default\": 5,\n        \"info\": \"The kernel size to apply to the final Convolution layer.\",\n        \"datatype\": int,\n        \"rounding\": 2,\n        \"min_max\": (1, 9),\n        \"group\": \"decoder\",\n        \"fixed\": True},\n    \"dec_gaussian\": {\n        \"default\": True,\n        \"info\": (\n            \"Gaussian Noise acts as a regularization technique for preventing overfitting of \"\n            \"data.\"\n            \"\\n\\tTrue - Apply a Gaussian Noise layer to each upscale.\"\n            \"\\n\\tFalse - Don't apply a Gaussian Noise layer to each upscale.\"),\n        \"datatype\": bool,\n        \"group\": \"decoder\",\n        \"fixed\": True},\n    \"dec_skip_last_residual\": {\n        \"default\": True,\n        \"info\": (\n            \"If Residual blocks have been enabled, enabling this option will not apply a \"\n            \"Residual block to the final upscaler.\"\n            \"\\n\\tTrue - Don't apply a Residual block to the final upscale.\"\n            \"\\n\\tFalse - Apply a Residual block to all upscale layers.\"),\n        \"datatype\": bool,\n        \"group\": \"decoder\",\n        \"fixed\": True},\n\n    # Weight management\n    \"freeze_layers\": {\n        \"default\": \"keras_encoder\",\n        \"info\": (\n            \"If the command line option 'freeze-weights' is enabled, then the layers indicated \"\n            \"here will be frozen the next time the model starts up. NB: Not all architectures \"\n            \"contain all of the layers listed here, so any layers marked for freezing that are \"\n            \"not within your chosen architecture will be ignored. EG:\\n If 'split fc' has \"\n            \"been selected, then 'fc_a' and 'fc_b' are available for freezing. If it has \"\n            \"not been selected then 'fc_both' is available for freezing.\"),\n        \"datatype\": list,\n        \"choices\": [\"encoder\", \"keras_encoder\", \"fc_a\", \"fc_b\", \"fc_both\", \"fc_shared\",\n                    \"fc_gblock\", \"g_block_a\", \"g_block_b\", \"g_block_both\", \"decoder_a\",\n                    \"decoder_b\", \"decoder_both\"],\n        \"group\": \"weights\",\n        \"fixed\": False},\n    \"load_layers\": {\n        \"default\": \"encoder\",\n        \"info\": (\n            \"If the command line option 'load-weights' is populated, then the layers indicated \"\n            \"here will be loaded from the given weights file if starting a new model. NB Not all \"\n            \"architectures contain all of the layers listed here, so any layers marked for \"\n            \"loading that are not within your chosen architecture will be ignored. EG:\\n If \"\n            \"'split fc' has been selected, then 'fc_a' and 'fc_b' are available for loading. If \"\n            \"it has not been selected then 'fc_both' is available for loading.\"),\n        \"datatype\": list,\n        \"choices\": [\"encoder\", \"fc_a\", \"fc_b\", \"fc_both\", \"fc_shared\", \"fc_gblock\", \"g_block_a\",\n                    \"g_block_b\", \"g_block_both\", \"decoder_a\", \"decoder_b\", \"decoder_both\"],\n        \"group\": \"weights\",\n        \"fixed\": True},\n\n    # # SPECIFIC ENCODER SETTINGS # #\n    # Faceswap Original\n    \"fs_original_depth\": {\n        \"default\": 4,\n        \"info\": \"Faceswap Encoder only: The number of convolutions to perform within the encoder.\",\n        \"datatype\": int,\n        \"min_max\": (2, 10),\n        \"rounding\": 1,\n        \"group\": \"faceswap encoder configuration\",\n        \"fixed\": True},\n    \"fs_original_min_filters\": {\n        \"default\": 128,\n        \"info\": (\n            \"Faceswap Encoder only: The minumum number of filters to use for encoder \"\n            \"convolutions. (i.e. the number of filters to use for the first encoder layer).\"),\n        \"datatype\": int,\n        \"min_max\": (16, 2048),\n        \"rounding\": 64,\n        \"group\": \"faceswap encoder configuration\",\n        \"fixed\": True},\n    \"fs_original_max_filters\": {\n        \"default\": 1024,\n        \"info\": (\n            \"Faceswap Encoder only: The maximum number of filters to use for encoder \"\n            \"convolutions. (i.e. the number of filters to use for the final encoder layer).\"),\n        \"datatype\": int,\n        \"min_max\": (256, 8192),\n        \"rounding\": 128,\n        \"group\": \"faceswap encoder configuration\",\n        \"fixed\": True},\n    \"fs_original_use_alt\": {\n        \"default\": False,\n        \"info\": (\n            \"Use a slightly alternate version of the Faceswap Encoder.\"\n            \"\\n\\tTrue - Use the alternate variation of the Faceswap Encoder.\"\n            \"\\n\\tFalse - Use the original Faceswap Encoder.\"),\n        \"datatype\": bool,\n        \"group\": \"faceswap encoder configuration\",\n        \"fixed\": True},\n\n    # MobileNet\n    \"mobilenet_width\": {\n        \"default\": 1.0,\n        \"info\": (\n            \"The width multiplier for mobilenet encoders. Controls the width of the \"\n            \"network. Values less than 1.0 proportionally decrease the number of filters within \"\n            \"each layer. Values greater than 1.0 proportionally increase the number of filters \"\n            \"within each layer. 1.0 is the default number of layers used within the paper.\\n\"\n            \"NB: This option is ignored for any non-mobilenet encoders.\\n\"\n            \"NB: If loading ImageNet weights, then for MobilenetV1 only values of '0.25', \"\n            \"'0.5', '0.75' or '1.0 can be selected. For MobilenetV2 only values of '0.35', \"\n            \"'0.50', '0.75', '1.0', '1.3' or '1.4' can be selected. For mobilenet_v3 only values \"\n            \"of '0.75' or '1.0' can be selected\"),\n        \"datatype\": float,\n        \"min_max\": (0.1, 2.0),\n        \"rounding\": 2,\n        \"group\": \"mobilenet encoder configuration\",\n        \"fixed\": True},\n    \"mobilenet_depth\": {\n        \"default\": 1,\n        \"info\": (\n            \"The depth multiplier for MobilenetV1 encoder. This is the depth multiplier \"\n            \"for depthwise convolution (known as the resolution multiplier within the original \"\n            \"paper).\\n\"\n            \"NB: This option is only used for MobilenetV1 and is ignored for all other \"\n            \"encoders.\\n\"\n            \"NB: If loading ImageNet weights, this must be set to 1.\"),\n        \"datatype\": int,\n        \"min_max\": (1, 10),\n        \"rounding\": 1,\n        \"group\": \"mobilenet encoder configuration\",\n        \"fixed\": True},\n    \"mobilenet_dropout\": {\n        \"default\": 0.001,\n        \"info\": (\n            \"The dropout rate for MobilenetV1 encoder.\\n\"\n            \"NB: This option is only used for MobilenetV1 and is ignored for all other \"\n            \"encoders.\"),\n        \"datatype\": float,\n        \"min_max\": (0.001, 2.0),\n        \"rounding\": 3,\n        \"group\": \"mobilenet encoder configuration\",\n        \"fixed\": True},\n    \"mobilenet_minimalistic\": {\n        \"default\": False,\n        \"info\": (\n            \"Use a minimilist version of MobilenetV3.\\n\"\n            \"In addition to large and small models MobilenetV3 also contains so-called \"\n            \"minimalistic models, these models have the same per-layer dimensions characteristic \"\n            \"as MobilenetV3 however, they don't utilize any of the advanced blocks \"\n            \"(squeeze-and-excite units, hard-swish, and 5x5 convolutions). While these models \"\n            \"are less efficient on CPU, they are much more performant on GPU/DSP.\\n\"\n            \"NB: This option is only used for MobilenetV3 and is ignored for all other \"\n            \"encoders.\\n\"),\n        \"datatype\": bool,\n        \"group\": \"mobilenet encoder configuration\",\n        \"fixed\": True},\n    }\n", "plugins/train/model/iae.py": "#!/usr/bin/env python3\n\"\"\" Improved autoencoder for faceswap \"\"\"\n\n# Ignore linting errors from Tensorflow's thoroughly broken import system\nfrom tensorflow.keras.layers import Concatenate, Dense, Flatten, Input, Reshape  # noqa:E501  # pylint:disable=import-error\nfrom tensorflow.keras.models import Model as KModel  # pylint:disable=import-error\n\nfrom lib.model.nn_blocks import Conv2DOutput, Conv2DBlock, UpscaleBlock\n\nfrom ._base import ModelBase\n\n\nclass Model(ModelBase):\n    \"\"\" Improved Autoencoder Model \"\"\"\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.input_shape = (64, 64, 3)\n        self.encoder_dim = 1024\n\n    def build_model(self, inputs):\n        \"\"\" Build the IAE Model \"\"\"\n        encoder = self.encoder()\n        decoder = self.decoder()\n        inter_a = self.intermediate(\"a\")\n        inter_b = self.intermediate(\"b\")\n        inter_both = self.intermediate(\"both\")\n\n        encoder_a = encoder(inputs[0])\n        encoder_b = encoder(inputs[1])\n\n        outputs = [decoder(Concatenate()([inter_a(encoder_a), inter_both(encoder_a)])),\n                   decoder(Concatenate()([inter_b(encoder_b), inter_both(encoder_b)]))]\n\n        autoencoder = KModel(inputs, outputs, name=self.model_name)\n        return autoencoder\n\n    def encoder(self):\n        \"\"\" Encoder Network \"\"\"\n        input_ = Input(shape=self.input_shape)\n        var_x = input_\n        var_x = Conv2DBlock(128, activation=\"leakyrelu\")(var_x)\n        var_x = Conv2DBlock(256, activation=\"leakyrelu\")(var_x)\n        var_x = Conv2DBlock(512, activation=\"leakyrelu\")(var_x)\n        var_x = Conv2DBlock(1024, activation=\"leakyrelu\")(var_x)\n        var_x = Flatten()(var_x)\n        return KModel(input_, var_x, name=\"encoder\")\n\n    def intermediate(self, side):\n        \"\"\" Intermediate Network \"\"\"\n        input_ = Input(shape=(4 * 4 * 1024, ))\n        var_x = Dense(self.encoder_dim)(input_)\n        var_x = Dense(4 * 4 * int(self.encoder_dim/2))(var_x)\n        var_x = Reshape((4, 4, int(self.encoder_dim/2)))(var_x)\n        return KModel(input_, var_x, name=f\"inter_{side}\")\n\n    def decoder(self):\n        \"\"\" Decoder Network \"\"\"\n        input_ = Input(shape=(4, 4, self.encoder_dim))\n        var_x = input_\n        var_x = UpscaleBlock(512, activation=\"leakyrelu\")(var_x)\n        var_x = UpscaleBlock(256, activation=\"leakyrelu\")(var_x)\n        var_x = UpscaleBlock(128, activation=\"leakyrelu\")(var_x)\n        var_x = UpscaleBlock(64, activation=\"leakyrelu\")(var_x)\n        var_x = Conv2DOutput(3, 5, name=\"face_out\")(var_x)\n        outputs = [var_x]\n\n        if self.config.get(\"learn_mask\", False):\n            var_y = input_\n            var_y = UpscaleBlock(512, activation=\"leakyrelu\")(var_y)\n            var_y = UpscaleBlock(256, activation=\"leakyrelu\")(var_y)\n            var_y = UpscaleBlock(128, activation=\"leakyrelu\")(var_y)\n            var_y = UpscaleBlock(64, activation=\"leakyrelu\")(var_y)\n            var_y = Conv2DOutput(1, 5, name=\"mask_out\")(var_y)\n            outputs.append(var_y)\n        return KModel(input_, outputs=outputs, name=\"decoder\")\n\n    def _legacy_mapping(self):\n        \"\"\" The mapping of legacy separate model names to single model names \"\"\"\n        return {f\"{self.name}_encoder.h5\": \"encoder\",\n                f\"{self.name}_intermediate_A.h5\": \"inter_a\",\n                f\"{self.name}_intermediate_B.h5\": \"inter_b\",\n                f\"{self.name}_inter.h5\": \"inter_both\",\n                f\"{self.name}_decoder.h5\": \"decoder\"}\n", "plugins/train/model/dfl_sae_defaults.py": "#!/usr/bin/env python3\n\"\"\"\n    The default options for the faceswap Dfl_SAE Model plugin.\n\n    Defaults files should be named <plugin_name>_defaults.py\n    Any items placed into this file will automatically get added to the relevant config .ini files\n    within the faceswap/config folder.\n\n    The following variables should be defined:\n        _HELPTEXT: A string describing what this plugin does\n        _DEFAULTS: A dictionary containing the options, defaults and meta information. The\n                   dictionary should be defined as:\n                       {<option_name>: {<metadata>}}\n\n                   <option_name> should always be lower text.\n                   <metadata> dictionary requirements are listed below.\n\n    The following keys are expected for the _DEFAULTS <metadata> dict:\n        datatype:  [required] A python type class. This limits the type of data that can be\n                   provided in the .ini file and ensures that the value is returned in the\n                   correct type to faceswap. Valid datatypes are: <class 'int'>, <class 'float'>,\n                   <class 'str'>, <class 'bool'>.\n        default:   [required] The default value for this option.\n        info:      [required] A string describing what this option does.\n        choices:   [optional] If this option's datatype is of <class 'str'> then valid\n                   selections can be defined here. This validates the option and also enables\n                   a combobox / radio option in the GUI.\n        gui_radio: [optional] If <choices> are defined, this indicates that the GUI should use\n                   radio buttons rather than a combobox to display this option.\n        min_max:   [partial] For <class 'int'> and <class 'float'> datatypes this is required\n                   otherwise it is ignored. Should be a tuple of min and max accepted values.\n                   This is used for controlling the GUI slider range. Values are not enforced.\n        rounding:  [partial] For <class 'int'> and <class 'float'> datatypes this is\n                   required otherwise it is ignored. Used for the GUI slider. For floats, this\n                   is the number of decimal places to display. For ints this is the step size.\n        fixed:     [optional] [train only]. Training configurations are fixed when the model is\n                   created, and then reloaded from the state file. Marking an item as fixed=False\n                   indicates that this value can be changed for existing models, and will override\n                   the value saved in the state file with the updated value in config. If not\n                   provided this will default to True.\n\"\"\"\n\n\n_HELPTEXT = \"DFL SAE Model (Adapted from https://github.com/iperov/DeepFaceLab)\"\n\n\n_DEFAULTS = dict(\n    input_size=dict(\n        default=128,\n        info=\"Resolution (in pixels) of the input image to train on.\\n\"\n             \"BE AWARE Larger resolution will dramatically increase VRAM requirements.\\n\"\n             \"\\nMust be divisible by 16.\",\n        datatype=int,\n        rounding=16,\n        min_max=(64, 256),\n        group=\"size\",\n        fixed=True),\n    architecture=dict(\n        default=\"df\",\n        info=\"Model architecture:\"\n             \"\\n\\t'df': Keeps the faces more natural.\"\n             \"\\n\\t'liae': Can help fix overly different face shapes.\",\n        datatype=str,\n        choices=[\"df\", \"liae\"],\n        gui_radio=True,\n        fixed=True,\n        group=\"network\"),\n    autoencoder_dims=dict(\n        default=0,\n        info=\"Face information is stored in AutoEncoder dimensions. If there are not enough \"\n             \"dimensions then certain facial features may not be recognized.\"\n             \"\\nHigher number of dimensions are better, but require more VRAM.\"\n             \"\\nSet to 0 to use the architecture defaults (256 for liae, 512 for df).\",\n        datatype=int,\n        rounding=32,\n        min_max=(0, 1024),\n        fixed=True,\n        group=\"network\"),\n    encoder_dims=dict(\n        default=42,\n        info=\"Encoder dimensions per channel. Higher number of encoder dimensions will help \"\n             \"the model to recognize more facial features, but will require more VRAM.\",\n        datatype=int,\n        rounding=1,\n        min_max=(21, 85),\n        fixed=True,\n        group=\"network\"),\n    decoder_dims=dict(\n        default=21,\n        info=\"Decoder dimensions per channel. Higher number of decoder dimensions will help \"\n             \"the model to improve details, but will require more VRAM.\",\n        datatype=int,\n        rounding=1,\n        min_max=(10, 85),\n        fixed=True,\n        group=\"network\"),\n    multiscale_decoder=dict(\n        default=False,\n        info=\"Multiscale decoder can help to obtain better details.\",\n        datatype=bool,\n        fixed=True,\n        group=\"network\"))\n", "plugins/train/model/dfaker_defaults.py": "#!/usr/bin/env python3\n\"\"\"\n    The default options for the faceswap Dfl_SAE Model plugin.\n\n    Defaults files should be named <plugin_name>_defaults.py\n    Any items placed into this file will automatically get added to the relevant config .ini files\n    within the faceswap/config folder.\n\n    The following variables should be defined:\n        _HELPTEXT: A string describing what this plugin does\n        _DEFAULTS: A dictionary containing the options, defaults and meta information. The\n                   dictionary should be defined as:\n                       {<option_name>: {<metadata>}}\n\n                   <option_name> should always be lower text.\n                   <metadata> dictionary requirements are listed below.\n\n    The following keys are expected for the _DEFAULTS <metadata> dict:\n        datatype:  [required] A python type class. This limits the type of data that can be\n                   provided in the .ini file and ensures that the value is returned in the\n                   correct type to faceswap. Valid data types are: <class 'int'>, <class 'float'>,\n                   <class 'str'>, <class 'bool'>.\n        default:   [required] The default value for this option.\n        info:      [required] A string describing what this option does.\n        choices:   [optional] If this option's datatype is of <class 'str'> then valid\n                   selections can be defined here. This validates the option and also enables\n                   a combobox / radio option in the GUI.\n        gui_radio: [optional] If <choices> are defined, this indicates that the GUI should use\n                   radio buttons rather than a combobox to display this option.\n        min_max:   [partial] For <class 'int'> and <class 'float'> data types this is required\n                   otherwise it is ignored. Should be a tuple of min and max accepted values.\n                   This is used for controlling the GUI slider range. Values are not enforced.\n        rounding:  [partial] For <class 'int'> and <class 'float'> data types this is\n                   required otherwise it is ignored. Used for the GUI slider. For floats, this\n                   is the number of decimal places to display. For ints this is the step size.\n        fixed:     [optional] [train only]. Training configurations are fixed when the model is\n                   created, and then reloaded from the state file. Marking an item as fixed=False\n                   indicates that this value can be changed for existing models, and will override\n                   the value saved in the state file with the updated value in config. If not\n                   provided this will default to True.\n\"\"\"\n\n\n_HELPTEXT = \"Dfaker Model (Adapted from https://github.com/dfaker/df)\"\n\n\n_DEFAULTS = dict(\n    output_size=dict(\n        default=128,\n        info=\"Resolution (in pixels) of the output image to generate on.\\n\"\n             \"BE AWARE Larger resolution will dramatically increase VRAM requirements.\\n\"\n             \"Must be 128 or 256.\",\n        datatype=int,\n        rounding=128,\n        min_max=(128, 256),\n        group=\"size\",\n        fixed=True))\n", "plugins/train/model/villain.py": "#!/usr/bin/env python3\n\"\"\" Original - VillainGuy model\n    Based on the original https://www.reddit.com/r/deepfakes/ code sample + contributions\n    Adapted from a model by VillainGuy (https://github.com/VillainGuy) \"\"\"\n\n# Ignore linting errors from Tensorflow's thoroughly broken import system\nfrom tensorflow.keras.initializers import RandomNormal  # pylint:disable=import-error\nfrom tensorflow.keras.layers import add, Dense, Flatten, Input, LeakyReLU, Reshape  # noqa:E501  # pylint:disable=import-error\nfrom tensorflow.keras.models import Model as KModel  # pylint:disable=import-error\n\nfrom lib.model.layers import PixelShuffler\nfrom lib.model.nn_blocks import (Conv2DOutput, Conv2DBlock, ResidualBlock, SeparableConv2DBlock,\n                                 UpscaleBlock)\n\nfrom .original import Model as OriginalModel\n\n\nclass Model(OriginalModel):\n    \"\"\" Villain Faceswap Model \"\"\"\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.input_shape = (128, 128, 3)\n        self.encoder_dim = 512 if self.low_mem else 1024\n        self.kernel_initializer = RandomNormal(0, 0.02)\n\n    def encoder(self):\n        \"\"\" Encoder Network \"\"\"\n        kwargs = {\"kernel_initializer\": self.kernel_initializer}\n        input_ = Input(shape=self.input_shape)\n        in_conv_filters = self.input_shape[0]\n        if self.input_shape[0] > 128:\n            in_conv_filters = 128 + (self.input_shape[0] - 128) // 4\n        dense_shape = self.input_shape[0] // 16\n\n        var_x = Conv2DBlock(in_conv_filters, activation=None, **kwargs)(input_)\n        tmp_x = var_x\n\n        var_x = LeakyReLU(alpha=0.2)(var_x)\n        res_cycles = 8 if self.config.get(\"lowmem\", False) else 16\n        for _ in range(res_cycles):\n            nn_x = ResidualBlock(in_conv_filters, **kwargs)(var_x)\n            var_x = nn_x\n        # consider adding scale before this layer to scale the residual chain\n        tmp_x = LeakyReLU(alpha=0.1)(tmp_x)\n        var_x = add([var_x, tmp_x])\n        var_x = Conv2DBlock(128, activation=\"leakyrelu\", **kwargs)(var_x)\n        var_x = PixelShuffler()(var_x)\n        var_x = Conv2DBlock(128, activation=\"leakyrelu\", **kwargs)(var_x)\n        var_x = PixelShuffler()(var_x)\n        var_x = Conv2DBlock(128, activation=\"leakyrelu\", **kwargs)(var_x)\n        var_x = SeparableConv2DBlock(256, **kwargs)(var_x)\n        var_x = Conv2DBlock(512, activation=\"leakyrelu\", **kwargs)(var_x)\n        if not self.config.get(\"lowmem\", False):\n            var_x = SeparableConv2DBlock(1024, **kwargs)(var_x)\n\n        var_x = Dense(self.encoder_dim, **kwargs)(Flatten()(var_x))\n        var_x = Dense(dense_shape * dense_shape * 1024, **kwargs)(var_x)\n        var_x = Reshape((dense_shape, dense_shape, 1024))(var_x)\n        var_x = UpscaleBlock(512, activation=\"leakyrelu\", **kwargs)(var_x)\n        return KModel(input_, var_x, name=\"encoder\")\n\n    def decoder(self, side):\n        \"\"\" Decoder Network \"\"\"\n        kwargs = {\"kernel_initializer\": self.kernel_initializer}\n        decoder_shape = self.input_shape[0] // 8\n        input_ = Input(shape=(decoder_shape, decoder_shape, 512))\n\n        var_x = input_\n        var_x = UpscaleBlock(512, activation=None, **kwargs)(var_x)\n        var_x = LeakyReLU(alpha=0.2)(var_x)\n        var_x = ResidualBlock(512, **kwargs)(var_x)\n        var_x = UpscaleBlock(256, activation=None, **kwargs)(var_x)\n        var_x = LeakyReLU(alpha=0.2)(var_x)\n        var_x = ResidualBlock(256, **kwargs)(var_x)\n        var_x = UpscaleBlock(self.input_shape[0], activation=None, **kwargs)(var_x)\n        var_x = LeakyReLU(alpha=0.2)(var_x)\n        var_x = ResidualBlock(self.input_shape[0], **kwargs)(var_x)\n        var_x = Conv2DOutput(3, 5, name=f\"face_out_{side}\")(var_x)\n        outputs = [var_x]\n\n        if self.config.get(\"learn_mask\", False):\n            var_y = input_\n            var_y = UpscaleBlock(512, activation=\"leakyrelu\")(var_y)\n            var_y = UpscaleBlock(256, activation=\"leakyrelu\")(var_y)\n            var_y = UpscaleBlock(self.input_shape[0], activation=\"leakyrelu\")(var_y)\n            var_y = Conv2DOutput(1, 5, name=f\"mask_out_{side}\")(var_y)\n            outputs.append(var_y)\n        return KModel(input_, outputs=outputs, name=f\"decoder_{side}\")\n", "plugins/train/model/realface.py": "#!/usr/bin/env python3\n\"\"\" RealFaceRC1, codenamed 'Pegasus'\n    Based on the original https://www.reddit.com/r/deepfakes/\n    code sample + contributions\n    Major thanks goes to BryanLyon as it vastly powered by his ideas and insights.\n    Without him it would not be possible to come up with the model.\n    Additional thanks: Birb - source of inspiration, great Encoder ideas\n                       Kvrooman - additional counseling on auto-encoders and practical advice\n    \"\"\"\nimport logging\nimport sys\n\n# Ignore linting errors from Tensorflow's thoroughly broken import system\nfrom tensorflow.keras.initializers import RandomNormal  # pylint:disable=import-error\nfrom tensorflow.keras.layers import Dense, Flatten, Input, LeakyReLU, Reshape  # noqa:E501  # pylint:disable=import-error\nfrom tensorflow.keras.models import Model as KModel  # pylint:disable=import-error\n\nfrom lib.model.nn_blocks import Conv2DOutput, Conv2DBlock, ResidualBlock, UpscaleBlock\nfrom ._base import ModelBase\n\nlogger = logging.getLogger(__name__)\n\n\nclass Model(ModelBase):\n    \"\"\" RealFace(tm) Faceswap Model \"\"\"\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.input_shape = (self.config[\"input_size\"], self.config[\"input_size\"], 3)\n        self.check_input_output()\n        self.dense_width, self.upscalers_no = self.get_dense_width_upscalers_numbers()\n        self.kernel_initializer = RandomNormal(0, 0.02)\n\n    @property\n    def downscalers_no(self):\n        \"\"\" Number of downscale blocks. Don't change! \"\"\"\n        return 4\n\n    @property\n    def _downscale_ratio(self):\n        \"\"\" Downscale Ratio \"\"\"\n        return 2**self.downscalers_no\n\n    @property\n    def dense_filters(self):\n        \"\"\" Dense Filters. Don't change! \"\"\"\n        return (int(1024 - (self.dense_width - 4) * 64) // 16) * 16\n\n    def check_input_output(self):\n        \"\"\" Confirm valid input and output sized have been provided \"\"\"\n        if not 64 <= self.config[\"input_size\"] <= 128 or self.config[\"input_size\"] % 16 != 0:\n            logger.error(\"Config error: input_size must be between 64 and 128 and be divisible by \"\n                         \"16.\")\n            sys.exit(1)\n        if not 64 <= self.config[\"output_size\"] <= 256 or self.config[\"output_size\"] % 32 != 0:\n            logger.error(\"Config error: output_size must be between 64 and 256 and be divisible \"\n                         \"by 32.\")\n            sys.exit(1)\n        logger.debug(\"Input and output sizes are valid\")\n\n    def get_dense_width_upscalers_numbers(self):\n        \"\"\" Return the dense width and number of upscale blocks \"\"\"\n        output_size = self.config[\"output_size\"]\n        sides = [(output_size // 2**n, n) for n in [4, 5] if (output_size // 2**n) < 10]\n        closest = min([x * self._downscale_ratio for x, _ in sides],\n                      key=lambda x: abs(x - self.config[\"input_size\"]))\n        dense_width, upscalers_no = [(s, n) for s, n in sides\n                                     if s * self._downscale_ratio == closest][0]\n        logger.debug(\"dense_width: %s, upscalers_no: %s\", dense_width, upscalers_no)\n        return dense_width, upscalers_no\n\n    def build_model(self, inputs):\n        \"\"\" Build the RealFace model. \"\"\"\n        encoder = self.encoder()\n        encoder_a = encoder(inputs[0])\n        encoder_b = encoder(inputs[1])\n\n        outputs = [self.decoder_a()(encoder_a), self.decoder_b()(encoder_b)]\n\n        autoencoder = KModel(inputs, outputs, name=self.model_name)\n        return autoencoder\n\n    def encoder(self):\n        \"\"\" RealFace Encoder Network \"\"\"\n        input_ = Input(shape=self.input_shape)\n        var_x = input_\n\n        encoder_complexity = self.config[\"complexity_encoder\"]\n\n        for idx in range(self.downscalers_no - 1):\n            var_x = Conv2DBlock(encoder_complexity * 2**idx, activation=None)(var_x)\n            var_x = LeakyReLU(alpha=0.2)(var_x)\n            var_x = ResidualBlock(encoder_complexity * 2**idx, use_bias=True)(var_x)\n            var_x = ResidualBlock(encoder_complexity * 2**idx, use_bias=True)(var_x)\n\n        var_x = Conv2DBlock(encoder_complexity * 2**(idx + 1), activation=\"leakyrelu\")(var_x)\n\n        return KModel(input_, var_x, name=\"encoder\")\n\n    def decoder_b(self):\n        \"\"\" RealFace Decoder Network \"\"\"\n        input_filters = self.config[\"complexity_encoder\"] * 2**(self.downscalers_no-1)\n        input_width = self.config[\"input_size\"] // self._downscale_ratio\n        input_ = Input(shape=(input_width, input_width, input_filters))\n\n        var_xy = input_\n\n        var_xy = Dense(self.config[\"dense_nodes\"])(Flatten()(var_xy))\n        var_xy = Dense(self.dense_width * self.dense_width * self.dense_filters)(var_xy)\n        var_xy = Reshape((self.dense_width, self.dense_width, self.dense_filters))(var_xy)\n        var_xy = UpscaleBlock(self.dense_filters, activation=None)(var_xy)\n\n        var_x = var_xy\n        var_x = LeakyReLU(alpha=0.2)(var_x)\n        var_x = ResidualBlock(self.dense_filters, use_bias=False)(var_x)\n\n        decoder_b_complexity = self.config[\"complexity_decoder\"]\n        for idx in range(self.upscalers_no - 2):\n            var_x = UpscaleBlock(decoder_b_complexity // 2**idx, activation=None)(var_x)\n            var_x = LeakyReLU(alpha=0.2)(var_x)\n            var_x = ResidualBlock(decoder_b_complexity // 2**idx, use_bias=False)(var_x)\n            var_x = ResidualBlock(decoder_b_complexity // 2**idx, use_bias=True)(var_x)\n        var_x = UpscaleBlock(decoder_b_complexity // 2**(idx + 1), activation=\"leakyrelu\")(var_x)\n\n        var_x = Conv2DOutput(3, 5, name=\"face_out_b\")(var_x)\n\n        outputs = [var_x]\n\n        if self.config.get(\"learn_mask\", False):\n            var_y = var_xy\n            var_y = LeakyReLU(alpha=0.1)(var_y)\n\n            mask_b_complexity = 384\n            for idx in range(self.upscalers_no-2):\n                var_y = UpscaleBlock(mask_b_complexity // 2**idx, activation=\"leakyrelu\")(var_y)\n            var_y = UpscaleBlock(mask_b_complexity // 2**(idx + 1), activation=\"leakyrelu\")(var_y)\n\n            var_y = Conv2DOutput(1, 5, name=\"mask_out_b\")(var_y)\n\n            outputs += [var_y]\n\n        return KModel(input_, outputs=outputs, name=\"decoder_b\")\n\n    def decoder_a(self):\n        \"\"\" RealFace Decoder (A) Network \"\"\"\n        input_filters = self.config[\"complexity_encoder\"] * 2**(self.downscalers_no-1)\n        input_width = self.config[\"input_size\"] // self._downscale_ratio\n        input_ = Input(shape=(input_width, input_width, input_filters))\n\n        var_xy = input_\n\n        dense_nodes = int(self.config[\"dense_nodes\"]/1.5)\n        dense_filters = int(self.dense_filters/1.5)\n\n        var_xy = Dense(dense_nodes)(Flatten()(var_xy))\n        var_xy = Dense(self.dense_width * self.dense_width * dense_filters)(var_xy)\n        var_xy = Reshape((self.dense_width, self.dense_width, dense_filters))(var_xy)\n\n        var_xy = UpscaleBlock(dense_filters, activation=None)(var_xy)\n\n        var_x = var_xy\n        var_x = LeakyReLU(alpha=0.2)(var_x)\n        var_x = ResidualBlock(dense_filters, use_bias=False)(var_x)\n\n        decoder_a_complexity = int(self.config[\"complexity_decoder\"] / 1.5)\n        for idx in range(self.upscalers_no-2):\n            var_x = UpscaleBlock(decoder_a_complexity // 2**idx, activation=\"leakyrelu\")(var_x)\n        var_x = UpscaleBlock(decoder_a_complexity // 2**(idx + 1), activation=\"leakyrelu\")(var_x)\n\n        var_x = Conv2DOutput(3, 5, name=\"face_out_a\")(var_x)\n\n        outputs = [var_x]\n\n        if self.config.get(\"learn_mask\", False):\n            var_y = var_xy\n            var_y = LeakyReLU(alpha=0.1)(var_y)\n\n            mask_a_complexity = 384\n            for idx in range(self.upscalers_no-2):\n                var_y = UpscaleBlock(mask_a_complexity // 2**idx, activation=\"leakyrelu\")(var_y)\n            var_y = UpscaleBlock(mask_a_complexity // 2**(idx + 1), activation=\"leakyrelu\")(var_y)\n\n            var_y = Conv2DOutput(1, 5, name=\"mask_out_a\")(var_y)\n\n            outputs += [var_y]\n\n        return KModel(input_, outputs=outputs, name=\"decoder_a\")\n\n    def _legacy_mapping(self):\n        \"\"\" The mapping of legacy separate model names to single model names \"\"\"\n        return {f\"{self.name}_encoder.h5\": \"encoder\",\n                f\"{self.name}_decoder_A.h5\": \"decoder_a\",\n                f\"{self.name}_decoder_B.h5\": \"decoder_b\"}\n", "plugins/train/model/realface_defaults.py": "#!/usr/bin/env python3\n\"\"\"\n    The default options for the faceswap Realface Model plugin.\n\n    Defaults files should be named <plugin_name>_defaults.py\n    Any items placed into this file will automatically get added to the relevant config .ini files\n    within the faceswap/config folder.\n\n    The following variables should be defined:\n        _HELPTEXT: A string describing what this plugin does\n        _DEFAULTS: A dictionary containing the options, defaults and meta information. The\n                   dictionary should be defined as:\n                       {<option_name>: {<metadata>}}\n\n                   <option_name> should always be lower text.\n                   <metadata> dictionary requirements are listed below.\n\n    The following keys are expected for the _DEFAULTS <metadata> dict:\n        datatype:  [required] A python type class. This limits the type of data that can be\n                   provided in the .ini file and ensures that the value is returned in the\n                   correct type to faceswap. Valid datatypes are: <class 'int'>, <class 'float'>,\n                   <class 'str'>, <class 'bool'>.\n        default:   [required] The default value for this option.\n        info:      [required] A string describing what this option does.\n        choices:   [optional] If this option's datatype is of <class 'str'> then valid\n                   selections can be defined here. This validates the option and also enables\n                   a combobox / radio option in the GUI.\n        gui_radio: [optional] If <choices> are defined, this indicates that the GUI should use\n                   radio buttons rather than a combobox to display this option.\n        min_max:   [partial] For <class 'int'> and <class 'float'> datatypes this is required\n                   otherwise it is ignored. Should be a tuple of min and max accepted values.\n                   This is used for controlling the GUI slider range. Values are not enforced.\n        rounding:  [partial] For <class 'int'> and <class 'float'> datatypes this is\n                   required otherwise it is ignored. Used for the GUI slider. For floats, this\n                   is the number of decimal places to display. For ints this is the step size.\n        fixed:     [optional] [train only]. Training configurations are fixed when the model is\n                   created, and then reloaded from the state file. Marking an item as fixed=False\n                   indicates that this value can be changed for existing models, and will override\n                   the value saved in the state file with the updated value in config. If not\n                   provided this will default to True.\n\"\"\"\n\n\n_HELPTEXT = (\n    \"An extra detailed variant of Original model.\\n\"\n    \"Incorporates ideas from Bryanlyon and inspiration from the Villain model.\\n\"\n    \"Requires about 6GB-8GB of VRAM (batchsize 8-16).\\n\"\n)\n\n\n_DEFAULTS = {\n    \"input_size\": {\n        \"default\": 64,\n        \"info\": \"Resolution (in pixels) of the input image to train on.\\n\"\n                \"BE AWARE Larger resolution will dramatically increase VRAM requirements.\\n\"\n                \"Higher resolutions may increase prediction accuracy, but does not effect the \"\n                \"resulting output size.\\nMust be between 64 and 128 and be divisible by 16.\",\n        \"datatype\": int,\n        \"rounding\": 16,\n        \"min_max\": (64, 128),\n        \"choices\": [],\n        \"gui_radio\": False,\n        \"fixed\": True,\n        \"group\": \"size\"\n    },\n    \"output_size\": {\n        \"default\": 128,\n        \"info\": \"Output image resolution (in pixels).\\nBe aware that larger resolution will \"\n                \"increase VRAM requirements.\\nNB: Must be between 64 and 256 and be divisible \"\n                \"by 16.\",\n        \"datatype\": int,\n        \"rounding\": 16,\n        \"min_max\": (64, 256),\n        \"choices\": [],\n        \"gui_radio\": False,\n        \"fixed\": True,\n        \"group\": \"size\"\n    },\n    \"dense_nodes\": {\n        \"default\": 1536,\n        \"info\": \"Number of nodes for decoder. Might affect your model's ability to learn in \"\n                \"general.\\nNote that: Lower values will affect the ability to predict \"\n                \"details.\",\n        \"datatype\": int,\n        \"rounding\": 64,\n        \"min_max\": (768, 2048),\n        \"choices\": [],\n        \"gui_radio\": False,\n        \"fixed\": True,\n        \"group\": \"network\"\n    },\n    \"complexity_encoder\": {\n        \"default\": 128,\n        \"info\": \"Encoder Convolution Layer Complexity. sensible ranges: 128 to 150.\",\n        \"datatype\": int,\n        \"rounding\": 4,\n        \"min_max\": (96, 160),\n        \"choices\": [],\n        \"gui_radio\": False,\n        \"fixed\": True,\n        \"group\": \"network\"\n    },\n    \"complexity_decoder\": {\n        \"default\": 512,\n        \"info\": \"Decoder Complexity.\",\n        \"datatype\": int,\n        \"rounding\": 4,\n        \"min_max\": (512, 544),\n        \"choices\": [],\n        \"gui_radio\": False,\n        \"fixed\": True,\n        \"group\": \"network\"\n    },\n}\n", "plugins/train/model/lightweight.py": "#!/usr/bin/env python3\n\"\"\" Lightweight Model by torzdf\n    An extremely limited model for training on low-end graphics cards\n    Based on the original https://www.reddit.com/r/deepfakes/\n    code sample + contributions \"\"\"\n\nfrom tensorflow.keras.models import Model as KModel  # pylint:disable=import-error\n\nfrom lib.model.nn_blocks import Conv2DOutput, Conv2DBlock, UpscaleBlock\nfrom .original import Model as OriginalModel, Dense, Flatten, Input, Reshape\n\n\nclass Model(OriginalModel):\n    \"\"\" Lightweight Model for ~2GB Graphics Cards \"\"\"\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.encoder_dim = 512\n\n    def encoder(self):\n        \"\"\" Encoder Network \"\"\"\n        input_ = Input(shape=self.input_shape)\n        var_x = input_\n        var_x = Conv2DBlock(128, activation=\"leakyrelu\")(var_x)\n        var_x = Conv2DBlock(256, activation=\"leakyrelu\")(var_x)\n        var_x = Conv2DBlock(512, activation=\"leakyrelu\")(var_x)\n        var_x = Dense(self.encoder_dim)(Flatten()(var_x))\n        var_x = Dense(4 * 4 * 512)(var_x)\n        var_x = Reshape((4, 4, 512))(var_x)\n        var_x = UpscaleBlock(256, activation=\"leakyrelu\")(var_x)\n        return KModel(input_, var_x, name=\"encoder\")\n\n    def decoder(self, side):\n        \"\"\" Decoder Network \"\"\"\n        input_ = Input(shape=(8, 8, 256))\n        var_x = input_\n        var_x = UpscaleBlock(512, activation=\"leakyrelu\")(var_x)\n        var_x = UpscaleBlock(256, activation=\"leakyrelu\")(var_x)\n        var_x = UpscaleBlock(128, activation=\"leakyrelu\")(var_x)\n        var_x = Conv2DOutput(3, 5, activation=\"sigmoid\", name=f\"face_out_{side}\")(var_x)\n        outputs = [var_x]\n\n        if self.config.get(\"learn_mask\", False):\n            var_y = input_\n            var_y = UpscaleBlock(512, activation=\"leakyrelu\")(var_y)\n            var_y = UpscaleBlock(256, activation=\"leakyrelu\")(var_y)\n            var_y = UpscaleBlock(128, activation=\"leakyrelu\")(var_y)\n            var_y = Conv2DOutput(1, 5,\n                                 activation=\"sigmoid\",\n                                 name=f\"mask_out_{side}\")(var_y)\n            outputs.append(var_y)\n        return KModel(input_, outputs=outputs, name=f\"decoder_{side}\")\n", "plugins/train/model/unbalanced_defaults.py": "#!/usr/bin/env python3\n\"\"\"\n    The default options for the faceswap Unbalanced Model plugin.\n\n    Defaults files should be named <plugin_name>_defaults.py\n    Any items placed into this file will automatically get added to the relevant config .ini files\n    within the faceswap/config folder.\n\n    The following variables should be defined:\n        _HELPTEXT: A string describing what this plugin does\n        _DEFAULTS: A dictionary containing the options, defaults and meta information. The\n                   dictionary should be defined as:\n                       {<option_name>: {<metadata>}}\n\n                   <option_name> should always be lower text.\n                   <metadata> dictionary requirements are listed below.\n\n    The following keys are expected for the _DEFAULTS <metadata> dict:\n        datatype:  [required] A python type class. This limits the type of data that can be\n                   provided in the .ini file and ensures that the value is returned in the\n                   correct type to faceswap. Valid datatypes are: <class 'int'>, <class 'float'>,\n                   <class 'str'>, <class 'bool'>.\n        default:   [required] The default value for this option.\n        info:      [required] A string describing what this option does.\n        choices:   [optional] If this option's datatype is of <class 'str'> then valid\n                   selections can be defined here. This validates the option and also enables\n                   a combobox / radio option in the GUI.\n        gui_radio: [optional] If <choices> are defined, this indicates that the GUI should use\n                   radio buttons rather than a combobox to display this option.\n        min_max:   [partial] For <class 'int'> and <class 'float'> datatypes this is required\n                   otherwise it is ignored. Should be a tuple of min and max accepted values.\n                   This is used for controlling the GUI slider range. Values are not enforced.\n        rounding:  [partial] For <class 'int'> and <class 'float'> datatypes this is\n                   required otherwise it is ignored. Used for the GUI slider. For floats, this\n                   is the number of decimal places to display. For ints this is the step size.\n        fixed:     [optional] [train only]. Training configurations are fixed when the model is\n                   created, and then reloaded from the state file. Marking an item as fixed=False\n                   indicates that this value can be changed for existing models, and will override\n                   the value saved in the state file with the updated value in config. If not\n                   provided this will default to True.\n\"\"\"\n\n\n_HELPTEXT = (\n    \"An unbalanced model with adjustable input size options.\\n\"\n    \"This is an unbalanced model so b>a swaps may not work well\\n\"\n)\n\n\n_DEFAULTS = dict(\n    input_size=dict(\n        default=128,\n        info=\"Resolution (in pixels) of the image to train on.\\n\"\n             \"BE AWARE Larger resolution will dramatically increaseVRAM requirements.\\n\"\n             \"Make sure your resolution is divisible by 64 (e.g. 64, 128, 256 etc.).\\n\"\n             \"NB: Your faceset must be at least 1.6x larger than your required input \"\n             \"size.\\n(e.g. 160 is the maximum input size for a 256x256 faceset).\",\n        datatype=int,\n        rounding=64,\n        min_max=(64, 512),\n        choices=[],\n        gui_radio=False,\n        group=\"size\",\n        fixed=True),\n    lowmem=dict(\n        default=False,\n        info=\"Lower memory mode. Set to 'True' if having issues with VRAM useage.\\n\"\n             \"NB: Models with a changed lowmem mode are not compatible with each other.\\n\"\n             \"NB: lowmem will override cutom nodes and complexity settings.\",\n        datatype=bool,\n        rounding=None,\n        min_max=None,\n        choices=[],\n        gui_radio=False,\n        group=\"settings\",\n        fixed=True),\n    nodes=dict(\n        default=1024,\n        info=\"Number of nodes for decoder. Don't change this unless you know what you are doing!\",\n        datatype=int,\n        rounding=64,\n        min_max=(512, 4096),\n        choices=[],\n        gui_radio=False,\n        fixed=True,\n        group=\"network\"),\n    complexity_encoder=dict(\n        default=128,\n        info=\"Encoder Convolution Layer Complexity. sensible ranges: 128 to 160.\",\n        datatype=int,\n        rounding=16,\n        min_max=(64, 1024),\n        choices=[],\n        gui_radio=False,\n        fixed=True,\n        group=\"network\"),\n    complexity_decoder_a=dict(\n        default=384,\n        info=\"Decoder A Complexity.\",\n        datatype=int,\n        rounding=16,\n        min_max=(64, 1024),\n        choices=[],\n        gui_radio=False,\n        fixed=True,\n        group=\"network\"),\n    complexity_decoder_b=dict(\n        default=512,\n        info=\"Decoder B Complexity.\",\n        datatype=int,\n        rounding=16,\n        min_max=(64, 1024),\n        choices=[],\n        gui_radio=False,\n        fixed=True,\n        group=\"network\"))\n", "plugins/train/model/dfl_h128.py": "#!/usr/bin/env python3\n\"\"\" DeepFaceLab H128 Model\n    Based on https://github.com/iperov/DeepFaceLab\n\"\"\"\n\n# Ignore linting errors from Tensorflow's thoroughly broken import system\nfrom tensorflow.keras.layers import Dense, Flatten, Input, Reshape  # noqa:E501  # pylint:disable=import-error\nfrom tensorflow.keras.models import Model as KModel  # pylint:disable=import-error\n\nfrom lib.model.nn_blocks import Conv2DOutput, Conv2DBlock, UpscaleBlock\nfrom .original import Model as OriginalModel\n\n\nclass Model(OriginalModel):\n    \"\"\" H128 Model from DFL \"\"\"\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.input_shape = (128, 128, 3)\n        self.encoder_dim = 256 if self.config[\"lowmem\"] else 512\n\n    def encoder(self):\n        \"\"\" DFL H128 Encoder \"\"\"\n        input_ = Input(shape=self.input_shape)\n        var_x = Conv2DBlock(128, activation=\"leakyrelu\")(input_)\n        var_x = Conv2DBlock(256, activation=\"leakyrelu\")(var_x)\n        var_x = Conv2DBlock(512, activation=\"leakyrelu\")(var_x)\n        var_x = Conv2DBlock(1024, activation=\"leakyrelu\")(var_x)\n        var_x = Dense(self.encoder_dim)(Flatten()(var_x))\n        var_x = Dense(8 * 8 * self.encoder_dim)(var_x)\n        var_x = Reshape((8, 8, self.encoder_dim))(var_x)\n        var_x = UpscaleBlock(self.encoder_dim, activation=\"leakyrelu\")(var_x)\n        return KModel(input_, var_x, name=\"encoder\")\n\n    def decoder(self, side):\n        \"\"\" DFL H128 Decoder \"\"\"\n        input_ = Input(shape=(16, 16, self.encoder_dim))\n        var_x = input_\n        var_x = UpscaleBlock(self.encoder_dim, activation=\"leakyrelu\")(var_x)\n        var_x = UpscaleBlock(self.encoder_dim // 2, activation=\"leakyrelu\")(var_x)\n        var_x = UpscaleBlock(self.encoder_dim // 4, activation=\"leakyrelu\")(var_x)\n        var_x = Conv2DOutput(3, 5, name=f\"face_out_{side}\")(var_x)\n        outputs = [var_x]\n\n        if self.config.get(\"learn_mask\", False):\n            var_y = input_\n            var_y = UpscaleBlock(self.encoder_dim, activation=\"leakyrelu\")(var_y)\n            var_y = UpscaleBlock(self.encoder_dim // 2, activation=\"leakyrelu\")(var_y)\n            var_y = UpscaleBlock(self.encoder_dim // 4, activation=\"leakyrelu\")(var_y)\n            var_y = Conv2DOutput(1, 5, name=f\"mask_out_{side}\")(var_y)\n            outputs.append(var_y)\n        return KModel(input_, outputs=outputs, name=f\"decoder_{side}\")\n", "plugins/train/model/dfl_h128_defaults.py": "#!/usr/bin/env python3\n\"\"\"\n    The default options for the faceswap Dfl_H128 Model plugin.\n\n    Defaults files should be named <plugin_name>_defaults.py\n    Any items placed into this file will automatically get added to the relevant config .ini files\n    within the faceswap/config folder.\n\n    The following variables should be defined:\n        _HELPTEXT: A string describing what this plugin does\n        _DEFAULTS: A dictionary containing the options, defaults and meta information. The\n                   dictionary should be defined as:\n                       {<option_name>: {<metadata>}}\n\n                   <option_name> should always be lower text.\n                   <metadata> dictionary requirements are listed below.\n\n    The following keys are expected for the _DEFAULTS <metadata> dict:\n        datatype:  [required] A python type class. This limits the type of data that can be\n                   provided in the .ini file and ensures that the value is returned in the\n                   correct type to faceswap. Valid datatypes are: <class 'int'>, <class 'float'>,\n                   <class 'str'>, <class 'bool'>.\n        default:   [required] The default value for this option.\n        info:      [required] A string describing what this option does.\n        choices:   [optional] If this option's datatype is of <class 'str'> then valid\n                   selections can be defined here. This validates the option and also enables\n                   a combobox / radio option in the GUI.\n        gui_radio: [optional] If <choices> are defined, this indicates that the GUI should use\n                   radio buttons rather than a combobox to display this option.\n        min_max:   [partial] For <class 'int'> and <class 'float'> datatypes this is required\n                   otherwise it is ignored. Should be a tuple of min and max accepted values.\n                   This is used for controlling the GUI slider range. Values are not enforced.\n        rounding:  [partial] For <class 'int'> and <class 'float'> datatypes this is\n                   required otherwise it is ignored. Used for the GUI slider. For floats, this\n                   is the number of decimal places to display. For ints this is the step size.\n        fixed:     [optional] [train only]. Training configurations are fixed when the model is\n                   created, and then reloaded from the state file. Marking an item as fixed=False\n                   indicates that this value can be changed for existing models, and will override\n                   the value saved in the state file with the updated value in config. If not\n                   provided this will default to True.\n\"\"\"\n\n\n_HELPTEXT = \"DFL H128 Model (Adapted from https://github.com/iperov/DeepFaceLab)\"\n\n\n_DEFAULTS = {\n    \"lowmem\": {\n        \"default\": False,\n        \"info\": \"Lower memory mode. Set to 'True' if having issues with VRAM useage.\\n\"\n                \"NB: Models with a changed lowmem mode are not compatible with each other.\",\n        \"datatype\": bool,\n        \"rounding\": None,\n        \"min_max\": None,\n        \"choices\": [],\n        \"gui_radio\": False,\n        \"group\": \"settings\",\n        \"fixed\": True,\n    },\n}\n", "plugins/train/model/dfl_sae.py": "#!/usr/bin/env python3\n\"\"\" DeepFaceLab SAE Model\n    Based on https://github.com/iperov/DeepFaceLab\n\"\"\"\nimport logging\n\nimport numpy as np\n\n# Ignore linting errors from Tensorflow's thoroughly broken import system\nfrom tensorflow.keras.layers import Concatenate, Dense, Flatten, Input, LeakyReLU, Reshape  # noqa:E501  # pylint:disable=import-error\nfrom tensorflow.keras.models import Model as KModel  # pylint:disable=import-error\n\nfrom lib.model.nn_blocks import Conv2DOutput, Conv2DBlock, ResidualBlock, UpscaleBlock\n\nfrom ._base import ModelBase\n\nlogger = logging.getLogger(__name__)\n\n\nclass Model(ModelBase):\n    \"\"\" SAE Model from DFL \"\"\"\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.input_shape = (self.config[\"input_size\"], self.config[\"input_size\"], 3)\n        self.architecture = self.config[\"architecture\"].lower()\n        self.use_mask = self.config.get(\"learn_mask\", False)\n        self.multiscale_count = 3 if self.config[\"multiscale_decoder\"] else 1\n        self.encoder_dim = self.config[\"encoder_dims\"]\n        self.decoder_dim = self.config[\"decoder_dims\"]\n\n        self._patch_weights_management()\n\n    @property\n    def model_name(self):\n        \"\"\" str: The name of the keras model. Varies depending on selected architecture. \"\"\"\n        return f\"{self.name}_{self.architecture}\"\n\n    @property\n    def ae_dims(self):\n        \"\"\" Set the Autoencoder Dimensions or set to default \"\"\"\n        retval = self.config[\"autoencoder_dims\"]\n        if retval == 0:\n            retval = 256 if self.architecture == \"liae\" else 512\n        return retval\n\n    def _patch_weights_management(self):\n        \"\"\" Patch in the correct encoder name into the config dictionary for freezing and loading\n        weights based on architecture.\n        \"\"\"\n        self.config[\"freeze_layers\"] = [f\"encoder_{self.architecture}\"]\n        self.config[\"load_layers\"] = [f\"encoder_{self.architecture}\"]\n        logger.debug(\"Patched encoder layers to config: %s\",\n                     {k: v for k, v in self.config.items()\n                      if k in (\"freeze_layers\", \"load_layers\")})\n\n    def build_model(self, inputs):\n        \"\"\" Build the DFL-SAE Model \"\"\"\n        encoder = getattr(self, f\"encoder_{self.architecture}\")()\n        enc_output_shape = encoder.output_shape[1:]\n        encoder_a = encoder(inputs[0])\n        encoder_b = encoder(inputs[1])\n\n        if self.architecture == \"liae\":\n            inter_both = self.inter_liae(\"both\", enc_output_shape)\n            int_output_shape = (np.array(inter_both.output_shape[1:]) * (1, 1, 2)).tolist()\n\n            inter_a = Concatenate()([inter_both(encoder_a), inter_both(encoder_a)])\n            inter_b = Concatenate()([self.inter_liae(\"b\", enc_output_shape)(encoder_b),\n                                     inter_both(encoder_b)])\n\n            decoder = self.decoder(\"both\", int_output_shape)\n            outputs = [decoder(inter_a), decoder(inter_b)]\n        else:\n            outputs = [self.decoder(\"a\", enc_output_shape)(encoder_a),\n                       self.decoder(\"b\", enc_output_shape)(encoder_b)]\n        autoencoder = KModel(inputs, outputs, name=self.model_name)\n        return autoencoder\n\n    def encoder_df(self):\n        \"\"\" DFL SAE DF Encoder Network\"\"\"\n        input_ = Input(shape=self.input_shape)\n        dims = self.input_shape[-1] * self.encoder_dim\n        lowest_dense_res = self.input_shape[0] // 16\n        var_x = Conv2DBlock(dims, activation=\"leakyrelu\")(input_)\n        var_x = Conv2DBlock(dims * 2, activation=\"leakyrelu\")(var_x)\n        var_x = Conv2DBlock(dims * 4, activation=\"leakyrelu\")(var_x)\n        var_x = Conv2DBlock(dims * 8, activation=\"leakyrelu\")(var_x)\n        var_x = Dense(self.ae_dims)(Flatten()(var_x))\n        var_x = Dense(lowest_dense_res * lowest_dense_res * self.ae_dims)(var_x)\n        var_x = Reshape((lowest_dense_res, lowest_dense_res, self.ae_dims))(var_x)\n        var_x = UpscaleBlock(self.ae_dims, activation=\"leakyrelu\")(var_x)\n        return KModel(input_, var_x, name=\"encoder_df\")\n\n    def encoder_liae(self):\n        \"\"\" DFL SAE LIAE Encoder Network \"\"\"\n        input_ = Input(shape=self.input_shape)\n        dims = self.input_shape[-1] * self.encoder_dim\n        var_x = Conv2DBlock(dims, activation=\"leakyrelu\")(input_)\n        var_x = Conv2DBlock(dims * 2, activation=\"leakyrelu\")(var_x)\n        var_x = Conv2DBlock(dims * 4, activation=\"leakyrelu\")(var_x)\n        var_x = Conv2DBlock(dims * 8, activation=\"leakyrelu\")(var_x)\n        var_x = Flatten()(var_x)\n        return KModel(input_, var_x, name=\"encoder_liae\")\n\n    def inter_liae(self, side, input_shape):\n        \"\"\" DFL SAE LIAE Intermediate Network \"\"\"\n        input_ = Input(shape=input_shape)\n        lowest_dense_res = self.input_shape[0] // 16\n        var_x = input_\n        var_x = Dense(self.ae_dims)(var_x)\n        var_x = Dense(lowest_dense_res * lowest_dense_res * self.ae_dims * 2)(var_x)\n        var_x = Reshape((lowest_dense_res, lowest_dense_res, self.ae_dims * 2))(var_x)\n        var_x = UpscaleBlock(self.ae_dims * 2, activation=\"leakyrelu\")(var_x)\n        return KModel(input_, var_x, name=f\"intermediate_{side}\")\n\n    def decoder(self, side, input_shape):\n        \"\"\" DFL SAE Decoder Network\"\"\"\n        input_ = Input(shape=input_shape)\n        outputs = []\n\n        dims = self.input_shape[-1] * self.decoder_dim\n        var_x = input_\n\n        var_x1 = UpscaleBlock(dims * 8, activation=None)(var_x)\n        var_x1 = LeakyReLU(alpha=0.2)(var_x1)\n        var_x1 = ResidualBlock(dims * 8)(var_x1)\n        var_x1 = ResidualBlock(dims * 8)(var_x1)\n        if self.multiscale_count >= 3:\n            outputs.append(Conv2DOutput(3, 5, name=f\"face_out_32_{side}\")(var_x1))\n\n        var_x2 = UpscaleBlock(dims * 4, activation=None)(var_x1)\n        var_x2 = LeakyReLU(alpha=0.2)(var_x2)\n        var_x2 = ResidualBlock(dims * 4)(var_x2)\n        var_x2 = ResidualBlock(dims * 4)(var_x2)\n        if self.multiscale_count >= 2:\n            outputs.append(Conv2DOutput(3, 5, name=f\"face_out_64_{side}\")(var_x2))\n\n        var_x3 = UpscaleBlock(dims * 2, activation=None)(var_x2)\n        var_x3 = LeakyReLU(alpha=0.2)(var_x3)\n        var_x3 = ResidualBlock(dims * 2)(var_x3)\n        var_x3 = ResidualBlock(dims * 2)(var_x3)\n\n        outputs.append(Conv2DOutput(3, 5, name=f\"face_out_128_{side}\")(var_x3))\n\n        if self.use_mask:\n            var_y = input_\n            var_y = UpscaleBlock(self.decoder_dim * 8, activation=\"leakyrelu\")(var_y)\n            var_y = UpscaleBlock(self.decoder_dim * 4, activation=\"leakyrelu\")(var_y)\n            var_y = UpscaleBlock(self.decoder_dim * 2, activation=\"leakyrelu\")(var_y)\n            var_y = Conv2DOutput(1, 5, name=f\"mask_out_{side}\")(var_y)\n            outputs.append(var_y)\n        return KModel(input_, outputs=outputs, name=f\"decoder_{side}\")\n\n    def _legacy_mapping(self):\n        \"\"\" The mapping of legacy separate model names to single model names \"\"\"\n        mappings = {\"df\": {f\"{self.name}_encoder.h5\": \"encoder_df\",\n                           f\"{self.name}_decoder_A.h5\": \"decoder_a\",\n                           f\"{self.name}_decoder_B.h5\": \"decoder_b\"},\n                    \"liae\": {f\"{self.name}_encoder.h5\": \"encoder_liae\",\n                             f\"{self.name}_intermediate_B.h5\": \"intermediate_both\",\n                             f\"{self.name}_intermediate.h5\": \"intermediate_b\",\n                             f\"{self.name}_decoder.h5\": \"decoder_both\"}}\n        return mappings[self.config[\"architecture\"]]\n", "plugins/train/model/__init__.py": "", "plugins/train/model/dlight.py": "#!/usr/bin/env python3\n\"\"\" A lightweight variant of DFaker Model\n    By AnDenix, 2018-2019\n    Based on the dfaker model: https://github.com/dfaker\n\n    Acknowledgments:\n    kvrooman for numerous insights and invaluable aid\n    DeepHomage for lots of testing\n    \"\"\"\nimport logging\n\n# Ignore linting errors from Tensorflow's thoroughly broken import system\nfrom tensorflow.keras.layers import (  # pylint:disable=import-error\n    AveragePooling2D, BatchNormalization, Concatenate, Dense, Dropout, Flatten, Input, Reshape,\n    LeakyReLU, UpSampling2D)\nfrom tensorflow.keras.models import Model as KModel  # pylint:disable=import-error\n\nfrom lib.model.nn_blocks import (Conv2DOutput, Conv2DBlock, ResidualBlock, UpscaleBlock,\n                                 Upscale2xBlock)\nfrom lib.utils import FaceswapError\n\nfrom ._base import ModelBase\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass Model(ModelBase):\n    \"\"\" DLight Autoencoder Model \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.input_shape = (128, 128, 3)\n\n        self.features = {\"lowmem\": 0, \"fair\": 1, \"best\": 2}[self.config[\"features\"]]\n        self.encoder_filters = 64 if self.features > 0 else 48\n\n        bonum_fortunam = 128\n        self.encoder_dim = {0: 512 + bonum_fortunam,\n                            1: 1024 + bonum_fortunam,\n                            2: 1536 + bonum_fortunam}[self.features]\n        self.details = {\"fast\": 0, \"good\": 1}[self.config[\"details\"]]\n        try:\n            self.upscale_ratio = {128: 2,\n                                  256: 4,\n                                  384: 6}[self.config[\"output_size\"]]\n        except KeyError as err:\n            logger.error(\"Config error: output_size must be one of: 128, 256, or 384.\")\n            raise FaceswapError(\"Config error: output_size must be one of: \"\n                                \"128, 256, or 384.\") from err\n\n        logger.debug(\"output_size: %s, features: %s, encoder_filters: %s, encoder_dim: %s, \"\n                     \" details: %s, upscale_ratio: %s\", self.config[\"output_size\"], self.features,\n                     self.encoder_filters, self.encoder_dim, self.details, self.upscale_ratio)\n\n    def build_model(self, inputs):\n        \"\"\" Build the Dlight Model. \"\"\"\n        encoder = self.encoder()\n        encoder_a = encoder(inputs[0])\n        encoder_b = encoder(inputs[1])\n\n        decoder_b = self.decoder_b if self.details > 0 else self.decoder_b_fast\n\n        outputs = [self.decoder_a()(encoder_a), decoder_b()(encoder_b)]\n\n        autoencoder = KModel(inputs, outputs, name=self.model_name)\n        return autoencoder\n\n    def encoder(self):\n        \"\"\" DeLight Encoder Network \"\"\"\n        input_ = Input(shape=self.input_shape)\n        var_x = input_\n\n        var_x1 = Conv2DBlock(self.encoder_filters // 2, activation=\"leakyrelu\")(var_x)\n        var_x2 = AveragePooling2D()(var_x)\n        var_x2 = LeakyReLU(0.1)(var_x2)\n        var_x = Concatenate()([var_x1, var_x2])\n\n        var_x1 = Conv2DBlock(self.encoder_filters, activation=\"leakyrelu\")(var_x)\n        var_x2 = AveragePooling2D()(var_x)\n        var_x2 = LeakyReLU(0.1)(var_x2)\n        var_x = Concatenate()([var_x1, var_x2])\n\n        var_x1 = Conv2DBlock(self.encoder_filters * 2, activation=\"leakyrelu\")(var_x)\n        var_x2 = AveragePooling2D()(var_x)\n        var_x2 = LeakyReLU(0.1)(var_x2)\n        var_x = Concatenate()([var_x1, var_x2])\n\n        var_x1 = Conv2DBlock(self.encoder_filters * 4, activation=\"leakyrelu\")(var_x)\n        var_x2 = AveragePooling2D()(var_x)\n        var_x2 = LeakyReLU(0.1)(var_x2)\n        var_x = Concatenate()([var_x1, var_x2])\n\n        var_x1 = Conv2DBlock(self.encoder_filters * 8, activation=\"leakyrelu\")(var_x)\n        var_x2 = AveragePooling2D()(var_x)\n        var_x2 = LeakyReLU(0.1)(var_x2)\n        var_x = Concatenate()([var_x1, var_x2])\n\n        var_x = Dense(self.encoder_dim)(Flatten()(var_x))\n        var_x = Dropout(0.05)(var_x)\n        var_x = Dense(4 * 4 * 1024)(var_x)\n        var_x = Dropout(0.05)(var_x)\n        var_x = Reshape((4, 4, 1024))(var_x)\n\n        return KModel(input_, var_x, name=\"encoder\")\n\n    def decoder_a(self):\n        \"\"\" DeLight Decoder A(old face) Network \"\"\"\n        input_ = Input(shape=(4, 4, 1024))\n        dec_a_complexity = 256\n        mask_complexity = 128\n\n        var_xy = input_\n        var_xy = UpSampling2D(self.upscale_ratio, interpolation='bilinear')(var_xy)\n\n        var_x = var_xy\n        var_x = Upscale2xBlock(dec_a_complexity, activation=\"leakyrelu\", fast=False)(var_x)\n        var_x = Upscale2xBlock(dec_a_complexity // 2, activation=\"leakyrelu\", fast=False)(var_x)\n        var_x = Upscale2xBlock(dec_a_complexity // 4, activation=\"leakyrelu\", fast=False)(var_x)\n        var_x = Upscale2xBlock(dec_a_complexity // 8, activation=\"leakyrelu\", fast=False)(var_x)\n\n        var_x = Conv2DOutput(3, 5, name=\"face_out\")(var_x)\n\n        outputs = [var_x]\n\n        if self.config.get(\"learn_mask\", False):\n            var_y = var_xy  # mask decoder\n            var_y = Upscale2xBlock(mask_complexity, activation=\"leakyrelu\", fast=False)(var_y)\n            var_y = Upscale2xBlock(mask_complexity // 2, activation=\"leakyrelu\", fast=False)(var_y)\n            var_y = Upscale2xBlock(mask_complexity // 4, activation=\"leakyrelu\", fast=False)(var_y)\n            var_y = Upscale2xBlock(mask_complexity // 8, activation=\"leakyrelu\", fast=False)(var_y)\n\n            var_y = Conv2DOutput(1, 5, name=\"mask_out\")(var_y)\n\n            outputs.append(var_y)\n\n        return KModel([input_], outputs=outputs, name=\"decoder_a\")\n\n    def decoder_b_fast(self):\n        \"\"\" DeLight Fast Decoder B(new face) Network  \"\"\"\n        input_ = Input(shape=(4, 4, 1024))\n\n        dec_b_complexity = 512\n        mask_complexity = 128\n\n        var_xy = input_\n\n        var_xy = UpscaleBlock(512, scale_factor=self.upscale_ratio, activation=\"leakyrelu\")(var_xy)\n        var_x = var_xy\n\n        var_x = Upscale2xBlock(dec_b_complexity, activation=\"leakyrelu\", fast=True)(var_x)\n        var_x = Upscale2xBlock(dec_b_complexity // 2, activation=\"leakyrelu\", fast=True)(var_x)\n        var_x = Upscale2xBlock(dec_b_complexity // 4, activation=\"leakyrelu\", fast=True)(var_x)\n        var_x = Upscale2xBlock(dec_b_complexity // 8, activation=\"leakyrelu\", fast=True)(var_x)\n\n        var_x = Conv2DOutput(3, 5, name=\"face_out\")(var_x)\n\n        outputs = [var_x]\n\n        if self.config.get(\"learn_mask\", False):\n            var_y = var_xy  # mask decoder\n\n            var_y = Upscale2xBlock(mask_complexity, activation=\"leakyrelu\", fast=False)(var_y)\n            var_y = Upscale2xBlock(mask_complexity // 2, activation=\"leakyrelu\", fast=False)(var_y)\n            var_y = Upscale2xBlock(mask_complexity // 4, activation=\"leakyrelu\", fast=False)(var_y)\n            var_y = Upscale2xBlock(mask_complexity // 8, activation=\"leakyrelu\", fast=False)(var_y)\n\n            var_y = Conv2DOutput(1, 5, name=\"mask_out\")(var_y)\n\n            outputs.append(var_y)\n\n        return KModel([input_], outputs=outputs, name=\"decoder_b_fast\")\n\n    def decoder_b(self):\n        \"\"\" DeLight Decoder B(new face) Network  \"\"\"\n        input_ = Input(shape=(4, 4, 1024))\n\n        dec_b_complexity = 512\n        mask_complexity = 128\n\n        var_xy = input_\n\n        var_xy = Upscale2xBlock(512,\n                                scale_factor=self.upscale_ratio,\n                                activation=None,\n                                fast=False)(var_xy)\n        var_x = var_xy\n\n        var_x = LeakyReLU(alpha=0.2)(var_x)\n        var_x = ResidualBlock(512, use_bias=True)(var_x)\n        var_x = ResidualBlock(512, use_bias=False)(var_x)\n        var_x = ResidualBlock(512, use_bias=False)(var_x)\n        var_x = Upscale2xBlock(dec_b_complexity, activation=None, fast=False)(var_x)\n        var_x = LeakyReLU(alpha=0.2)(var_x)\n        var_x = ResidualBlock(dec_b_complexity, use_bias=True)(var_x)\n        var_x = ResidualBlock(dec_b_complexity, use_bias=False)(var_x)\n        var_x = BatchNormalization()(var_x)\n        var_x = Upscale2xBlock(dec_b_complexity // 2, activation=None, fast=False)(var_x)\n        var_x = LeakyReLU(alpha=0.2)(var_x)\n        var_x = ResidualBlock(dec_b_complexity // 2, use_bias=True)(var_x)\n        var_x = Upscale2xBlock(dec_b_complexity // 4, activation=None, fast=False)(var_x)\n        var_x = LeakyReLU(alpha=0.2)(var_x)\n        var_x = ResidualBlock(dec_b_complexity // 4, use_bias=False)(var_x)\n        var_x = BatchNormalization()(var_x)\n        var_x = Upscale2xBlock(dec_b_complexity // 8, activation=\"leakyrelu\", fast=False)(var_x)\n\n        var_x = Conv2DOutput(3, 5, name=\"face_out\")(var_x)\n\n        outputs = [var_x]\n\n        if self.config.get(\"learn_mask\", False):\n            var_y = var_xy  # mask decoder\n            var_y = LeakyReLU(alpha=0.1)(var_y)\n\n            var_y = Upscale2xBlock(mask_complexity, activation=\"leakyrelu\", fast=False)(var_y)\n            var_y = Upscale2xBlock(mask_complexity // 2, activation=\"leakyrelu\", fast=False)(var_y)\n            var_y = Upscale2xBlock(mask_complexity // 4, activation=\"leakyrelu\", fast=False)(var_y)\n            var_y = Upscale2xBlock(mask_complexity // 8, activation=\"leakyrelu\", fast=False)(var_y)\n\n            var_y = Conv2DOutput(1, 5, name=\"mask_out\")(var_y)\n\n            outputs.append(var_y)\n\n        return KModel([input_], outputs=outputs, name=\"decoder_b\")\n\n    def _legacy_mapping(self):\n        \"\"\" The mapping of legacy separate model names to single model names \"\"\"\n        decoder_b = \"decoder_b\" if self.details > 0 else \"decoder_b_fast\"\n        return {f\"{self.name}_encoder.h5\": \"encoder\",\n                f\"{self.name}_decoder_A.h5\": \"decoder_a\",\n                f\"{self.name}_decoder_B.h5\": decoder_b}\n", "plugins/train/model/_base/model.py": "#!/usr/bin/env python3\n\"\"\"\nBase class for Models. ALL Models should at least inherit from this class.\n\nSee :mod:`~plugins.train.model.original` for an annotated example for how to create model plugins.\n\"\"\"\nfrom __future__ import annotations\nimport logging\nimport os\nimport sys\nimport time\nimport typing as T\n\nfrom collections import OrderedDict\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom lib.serializer import get_serializer\nfrom lib.model.nn_blocks import set_config as set_nnblock_config\nfrom lib.utils import FaceswapError\nfrom plugins.train._config import Config\n\nfrom .io import IO, get_all_sub_models, Weights\nfrom .settings import Loss, Optimizer, Settings\n\nif T.TYPE_CHECKING:\n    import argparse\n    from lib.config import ConfigValueType\n\nkeras = tf.keras\nK = tf.keras.backend\n\n\nlogger = logging.getLogger(__name__)\n_CONFIG: dict[str, ConfigValueType] = {}\n\n\nclass ModelBase():\n    \"\"\" Base class that all model plugins should inherit from.\n\n    Parameters\n    ----------\n    model_dir: str\n        The full path to the model save location\n    arguments: :class:`argparse.Namespace`\n        The arguments that were passed to the train or convert process as generated from\n        Faceswap's command line arguments\n    predict: bool, optional\n        ``True`` if the model is being loaded for inference, ``False`` if the model is being loaded\n        for training. Default: ``False``\n\n    Attributes\n    ----------\n    input_shape: tuple or list\n        A `tuple` of `ints` defining the shape of the faces that the model takes as input. This\n        should be overridden by model plugins in their :func:`__init__` function. If the input size\n        is the same for both sides of the model, then this can be a single 3 dimensional `tuple`.\n        If the inputs have different sizes for `\"A\"` and `\"B\"` this should be a `list` of 2 3\n        dimensional shape `tuples`, 1 for each side respectively.\n    trainer: str\n        Currently there is only one trainer available (`\"original\"`), so at present this attribute\n        can be ignored. If/when more trainers are added, then this attribute should be overridden\n        with the trainer name that a model requires in the model plugin's\n        :func:`__init__` function.\n    \"\"\"\n    def __init__(self,\n                 model_dir: str,\n                 arguments: argparse.Namespace,\n                 predict: bool = False) -> None:\n        logger.debug(\"Initializing ModelBase (%s): (model_dir: '%s', arguments: %s, predict: %s)\",\n                     self.__class__.__name__, model_dir, arguments, predict)\n\n        # Input shape must be set within the plugin after initializing\n        self.input_shape: tuple[int, ...] = ()\n        self.trainer = \"original\"  # Override for plugin specific trainer\n        self.color_order: T.Literal[\"bgr\", \"rgb\"] = \"bgr\"  # Override for image color channel order\n\n        self._args = arguments\n        self._is_predict = predict\n        self._model: tf.keras.models.Model | None = None\n\n        self._configfile = arguments.configfile if hasattr(arguments, \"configfile\") else None\n        self._load_config()\n\n        if self.config[\"penalized_mask_loss\"] and self.config[\"mask_type\"] is None:\n            raise FaceswapError(\"Penalized Mask Loss has been selected but you have not chosen a \"\n                                \"Mask to use. Please select a mask or disable Penalized Mask \"\n                                \"Loss.\")\n\n        if self.config[\"learn_mask\"] and self.config[\"mask_type\"] is None:\n            raise FaceswapError(\"'Learn Mask' has been selected but you have not chosen a Mask to \"\n                                \"use. Please select a mask or disable 'Learn Mask'.\")\n\n        self._mixed_precision = self.config[\"mixed_precision\"]\n        self._io = IO(self, model_dir, self._is_predict, self.config[\"save_optimizer\"])\n        self._check_multiple_models()\n\n        self._state = State(model_dir,\n                            self.name,\n                            self._config_changeable_items,\n                            False if self._is_predict else self._args.no_logs)\n        self._settings = Settings(self._args,\n                                  self._mixed_precision,\n                                  self.config[\"allow_growth\"],\n                                  self._is_predict)\n        self._loss = Loss(self.config, self.color_order)\n\n        logger.debug(\"Initialized ModelBase (%s)\", self.__class__.__name__)\n\n    @property\n    def model(self) -> tf.keras.models.Model:\n        \"\"\":class:`Keras.models.Model`: The compiled model for this plugin. \"\"\"\n        return self._model\n\n    @property\n    def command_line_arguments(self) -> argparse.Namespace:\n        \"\"\" :class:`argparse.Namespace`: The command line arguments passed to the model plugin from\n        either the train or convert script \"\"\"\n        return self._args\n\n    @property\n    def coverage_ratio(self) -> float:\n        \"\"\" float: The ratio of the training image to crop out and train on as defined in user\n        configuration options.\n\n        NB: The coverage ratio is a raw float, but will be applied to integer pixel images.\n\n        To ensure consistent rounding and guaranteed even image size, the calculation for coverage\n        should always be: :math:`(original_size * coverage_ratio // 2) * 2`\n        \"\"\"\n        return self.config.get(\"coverage\", 62.5) / 100\n\n    @property\n    def io(self) -> IO:  # pylint:disable=invalid-name\n        \"\"\" :class:`~plugins.train.model.io.IO`: Input/Output operations for the model \"\"\"\n        return self._io\n\n    @property\n    def config(self) -> dict:\n        \"\"\" dict: The configuration dictionary for current plugin, as set by the user's\n        configuration settings. \"\"\"\n        global _CONFIG  # pylint:disable=global-statement\n        if not _CONFIG:\n            model_name = self._config_section\n            logger.debug(\"Loading config for: %s\", model_name)\n            _CONFIG = Config(model_name, configfile=self._configfile).config_dict\n        return _CONFIG\n\n    @property\n    def name(self) -> str:\n        \"\"\" str: The name of this model based on the plugin name. \"\"\"\n        _name = sys.modules[self.__module__].__file__\n        assert isinstance(_name, str)\n        return os.path.splitext(os.path.basename(_name))[0].lower()\n\n    @property\n    def model_name(self) -> str:\n        \"\"\" str: The name of the keras model. Generally this will be the same as :attr:`name`\n        but some plugins will override this when they contain multiple architectures \"\"\"\n        return self.name\n\n    @property\n    def input_shapes(self) -> list[tuple[None, int, int, int]]:\n        \"\"\" list: A flattened list corresponding to all of the inputs to the model. \"\"\"\n        shapes = [T.cast(tuple[None, int, int, int], K.int_shape(inputs))\n                  for inputs in self.model.inputs]\n        return shapes\n\n    @property\n    def output_shapes(self) -> list[tuple[None, int, int, int]]:\n        \"\"\" list: A flattened list corresponding to all of the outputs of the model. \"\"\"\n        shapes = [T.cast(tuple[None, int, int, int], K.int_shape(output))\n                  for output in self.model.outputs]\n        return shapes\n\n    @property\n    def iterations(self) -> int:\n        \"\"\" int: The total number of iterations that the model has trained. \"\"\"\n        return self._state.iterations\n\n    # Private properties\n    @property\n    def _config_section(self) -> str:\n        \"\"\" str: The section name for the current plugin for loading configuration options from the\n        config file. \"\"\"\n        return \".\".join(self.__module__.split(\".\")[-2:])\n\n    @property\n    def _config_changeable_items(self) -> dict:\n        \"\"\" dict: The configuration options that can be updated after the model has already been\n            created. \"\"\"\n        return Config(self._config_section, configfile=self._configfile).changeable_items\n\n    @property\n    def state(self) -> \"State\":\n        \"\"\":class:`State`: The state settings for the current plugin. \"\"\"\n        return self._state\n\n    def _load_config(self) -> None:\n        \"\"\" Load the global config for reference in :attr:`config` and set the faceswap blocks\n        configuration options in `lib.model.nn_blocks` \"\"\"\n        global _CONFIG  # pylint:disable=global-statement\n        if not _CONFIG:\n            model_name = self._config_section\n            logger.debug(\"Loading config for: %s\", model_name)\n            _CONFIG = Config(model_name, configfile=self._configfile).config_dict\n\n        nn_block_keys = ['icnr_init', 'conv_aware_init', 'reflect_padding']\n        set_nnblock_config({key: _CONFIG.pop(key)\n                            for key in nn_block_keys})\n\n    def _check_multiple_models(self) -> None:\n        \"\"\" Check whether multiple models exist in the model folder, and that no models exist that\n        were trained with a different plugin than the requested plugin.\n\n        Raises\n        ------\n        FaceswapError\n            If multiple model files, or models for a different plugin from that requested exists\n            within the model folder\n        \"\"\"\n        multiple_models = self._io.multiple_models_in_folder\n        if multiple_models is None:\n            logger.debug(\"Contents of model folder are valid\")\n            return\n\n        if len(multiple_models) == 1:\n            msg = (f\"You have requested to train with the '{self.name}' plugin, but a model file \"\n                   f\"for the '{multiple_models[0]}' plugin already exists in the folder \"\n                   f\"'{self.io.model_dir}'.\\nPlease select a different model folder.\")\n        else:\n            ptypes = \"', '\".join(multiple_models)\n            msg = (f\"There are multiple plugin types ('{ptypes}') stored in the model folder '\"\n                   f\"{self.io.model_dir}'. This is not supported.\\nPlease split the model files \"\n                   \"into their own folders before proceeding\")\n        raise FaceswapError(msg)\n\n    def build(self) -> None:\n        \"\"\" Build the model and assign to :attr:`model`.\n\n        Within the defined strategy scope, either builds the model from scratch or loads an\n        existing model if one exists.\n\n        If running inference, then the model is built only for the required side to perform the\n        swap function, otherwise  the model is then compiled with the optimizer and chosen\n        loss function(s).\n\n        Finally, a model summary is outputted to the logger at verbose level.\n        \"\"\"\n        self._update_legacy_models()\n        is_summary = hasattr(self._args, \"summary\") and self._args.summary\n        with self._settings.strategy_scope():\n            if self._io.model_exists:\n                model = self.io.load()\n                if self._is_predict:\n                    inference = _Inference(model, self._args.swap_model)\n                    self._model = inference.model\n                else:\n                    self._model = model\n            else:\n                self._validate_input_shape()\n                inputs = self._get_inputs()\n                if not self._settings.use_mixed_precision and not is_summary:\n                    # Store layer names which can be switched to mixed precision\n                    model, mp_layers = self._settings.get_mixed_precision_layers(self.build_model,\n                                                                                 inputs)\n                    self._state.add_mixed_precision_layers(mp_layers)\n                    self._model = model\n                else:\n                    self._model = self.build_model(inputs)\n            if not is_summary and not self._is_predict:\n                self._compile_model()\n            self._output_summary()\n\n    def _update_legacy_models(self) -> None:\n        \"\"\" Load weights from legacy split models into new unified model, archiving old model files\n        to a new folder. \"\"\"\n        legacy_mapping = self._legacy_mapping()  # pylint:disable=assignment-from-none\n        if legacy_mapping is None:\n            return\n\n        if not all(os.path.isfile(os.path.join(self.io.model_dir, fname))\n                   for fname in legacy_mapping):\n            return\n        archive_dir = f\"{self.io.model_dir}_TF1_Archived\"\n        if os.path.exists(archive_dir):\n            raise FaceswapError(\"We need to update your model files for use with Tensorflow 2.x, \"\n                                \"but the archive folder already exists. Please remove the \"\n                                f\"following folder to continue: '{archive_dir}'\")\n\n        logger.info(\"Updating legacy models for Tensorflow 2.x\")\n        logger.info(\"Your Tensorflow 1.x models will be archived in the following location: '%s'\",\n                    archive_dir)\n        os.rename(self.io.model_dir, archive_dir)\n        os.mkdir(self.io.model_dir)\n        new_model = self.build_model(self._get_inputs())\n        for model_name, layer_name in legacy_mapping.items():\n            old_model: tf.keras.models.Model = keras.models.load_model(\n                os.path.join(archive_dir, model_name),\n                compile=False)\n            layer = [layer for layer in new_model.layers if layer.name == layer_name]\n            if not layer:\n                logger.warning(\"Skipping legacy weights from '%s'...\", model_name)\n                continue\n            klayer: tf.keras.layers.Layer = layer[0]\n            logger.info(\"Updating legacy weights from '%s'...\", model_name)\n            klayer.set_weights(old_model.get_weights())\n        filename = self._io.filename\n        logger.info(\"Saving Tensorflow 2.x model to '%s'\", filename)\n        new_model.save(filename)\n        # Penalized Loss and Learn Mask used to be disabled automatically if a mask wasn't\n        # selected, so disable it if enabled, but mask_type is None\n        if self.config[\"mask_type\"] is None:\n            self.config[\"penalized_mask_loss\"] = False\n            self.config[\"learn_mask\"] = False\n            self.config[\"eye_multiplier\"] = 1\n            self.config[\"mouth_multiplier\"] = 1\n        self._state.save()\n\n    def _validate_input_shape(self) -> None:\n        \"\"\" Validate that the input shape is either a single shape tuple of 3 dimensions or\n        a list of 2 shape tuples of 3 dimensions. \"\"\"\n        assert len(self.input_shape) == 3, \"Input shape should be a 3 dimensional shape tuple\"\n\n    def _get_inputs(self) -> list[tf.keras.layers.Input]:\n        \"\"\" Obtain the standardized inputs for the model.\n\n        The inputs will be returned for the \"A\" and \"B\" sides in the shape as defined by\n        :attr:`input_shape`.\n\n        Returns\n        -------\n        list\n            A list of :class:`keras.layers.Input` tensors. This will be a list of 2 tensors (one\n            for each side) each of shapes :attr:`input_shape`.\n        \"\"\"\n        logger.debug(\"Getting inputs\")\n        input_shapes = [self.input_shape, self.input_shape]\n        inputs = [keras.layers.Input(shape=shape, name=f\"face_in_{side}\")\n                  for side, shape in zip((\"a\", \"b\"), input_shapes)]\n        logger.debug(\"inputs: %s\", inputs)\n        return inputs\n\n    def build_model(self, inputs: list[tf.keras.layers.Input]) -> tf.keras.models.Model:\n        \"\"\" Override for Model Specific autoencoder builds.\n\n        Parameters\n        ----------\n        inputs: list\n            A list of :class:`keras.layers.Input` tensors. This will be a list of 2 tensors (one\n            for each side) each of shapes :attr:`input_shape`.\n\n        Returns\n        -------\n        :class:`keras.models.Model`\n            See Keras documentation for the correct structure, but note that parameter :attr:`name`\n            is a required rather than an optional argument in Faceswap. You should assign this to\n            the attribute ``self.name`` that is automatically generated from the plugin's filename.\n        \"\"\"\n        raise NotImplementedError\n\n    def _output_summary(self) -> None:\n        \"\"\" Output the summary of the model and all sub-models to the verbose logger. \"\"\"\n        if hasattr(self._args, \"summary\") and self._args.summary:\n            print_fn = None  # Print straight to stdout\n        else:\n            # print to logger\n            print_fn = lambda x: logger.verbose(\"%s\", x)  #type:ignore[attr-defined]  # noqa[E731]  # pylint:disable=C3001\n        for idx, model in enumerate(get_all_sub_models(self.model)):\n            if idx == 0:\n                parent = model\n                continue\n            model.summary(line_length=100, print_fn=print_fn)\n        parent.summary(line_length=100, print_fn=print_fn)\n\n    def _compile_model(self) -> None:\n        \"\"\" Compile the model to include the Optimizer and Loss Function(s). \"\"\"\n        logger.debug(\"Compiling Model\")\n\n        if self.state.model_needs_rebuild:\n            self._model = self._settings.check_model_precision(self._model, self._state)\n\n        optimizer = Optimizer(self.config[\"optimizer\"],\n                              self.config[\"learning_rate\"],\n                              self.config[\"autoclip\"],\n                              10 ** int(self.config[\"epsilon_exponent\"])).optimizer\n        if self._settings.use_mixed_precision:\n            optimizer = self._settings.loss_scale_optimizer(optimizer)\n\n        weights = Weights(self)\n        weights.load(self._io.model_exists)\n        weights.freeze()\n\n        self._loss.configure(self.model)\n        self.model.compile(optimizer=optimizer, loss=self._loss.functions)\n        self._state.add_session_loss_names(self._loss.names)\n        logger.debug(\"Compiled Model: %s\", self.model)\n\n    def _legacy_mapping(self) -> dict | None:\n        \"\"\" The mapping of separate model files to single model layers for transferring of legacy\n        weights.\n\n        Returns\n        -------\n        dict or ``None``\n            Dictionary of original H5 filenames for legacy models mapped to new layer names or\n            ``None`` if the model did not exist in Faceswap prior to Tensorflow 2\n        \"\"\"\n        return None\n\n    def add_history(self, loss: list[float]) -> None:\n        \"\"\" Add the current iteration's loss history to :attr:`_io.history`.\n\n        Called from the trainer after each iteration, for tracking loss drop over time between\n        save iterations.\n\n        Parameters\n        ----------\n        loss: list\n            The loss values for the A and B side for the current iteration. This should be the\n            collated loss values for each side.\n        \"\"\"\n        self._io.history[0].append(loss[0])\n        self._io.history[1].append(loss[1])\n\n\nclass State():\n    \"\"\" Holds state information relating to the plugin's saved model.\n\n    Parameters\n    ----------\n    model_dir: str\n        The full path to the model save location\n    model_name: str\n        The name of the model plugin\n    config_changeable_items: dict\n        Configuration options that can be altered when resuming a model, and their current values\n    no_logs: bool\n        ``True`` if Tensorboard logs should not be generated, otherwise ``False``\n    \"\"\"\n    def __init__(self,\n                 model_dir: str,\n                 model_name: str,\n                 config_changeable_items: dict,\n                 no_logs: bool) -> None:\n        logger.debug(\"Initializing %s: (model_dir: '%s', model_name: '%s', \"\n                     \"config_changeable_items: '%s', no_logs: %s\", self.__class__.__name__,\n                     model_dir, model_name, config_changeable_items, no_logs)\n        self._serializer = get_serializer(\"json\")\n        filename = f\"{model_name}_state.{self._serializer.file_extension}\"\n        self._filename = os.path.join(model_dir, filename)\n        self._name = model_name\n        self._iterations = 0\n        self._mixed_precision_layers: list[str] = []\n        self._rebuild_model = False\n        self._sessions: dict[int, dict] = {}\n        self._lowest_avg_loss: dict[str, float] = {}\n        self._config: dict[str, ConfigValueType] = {}\n        self._load(config_changeable_items)\n        self._session_id = self._new_session_id()\n        self._create_new_session(no_logs, config_changeable_items)\n        logger.debug(\"Initialized %s:\", self.__class__.__name__)\n\n    @property\n    def filename(self) -> str:\n        \"\"\" str: Full path to the state filename \"\"\"\n        return self._filename\n\n    @property\n    def loss_names(self) -> list[str]:\n        \"\"\" list: The loss names for the current session \"\"\"\n        return self._sessions[self._session_id][\"loss_names\"]\n\n    @property\n    def current_session(self) -> dict:\n        \"\"\" dict: The state dictionary for the current :attr:`session_id`. \"\"\"\n        return self._sessions[self._session_id]\n\n    @property\n    def iterations(self) -> int:\n        \"\"\" int: The total number of iterations that the model has trained. \"\"\"\n        return self._iterations\n\n    @property\n    def lowest_avg_loss(self) -> dict:\n        \"\"\"dict: The lowest average save interval loss seen for each side. \"\"\"\n        return self._lowest_avg_loss\n\n    @property\n    def session_id(self) -> int:\n        \"\"\" int: The current training session id. \"\"\"\n        return self._session_id\n\n    @property\n    def sessions(self) -> dict[int, dict[str, T.Any]]:\n        \"\"\" dict[int, dict[str, Any]]: The session information for each session in the state\n        file \"\"\"\n        return {int(k): v for k, v in self._sessions.items()}\n\n    @property\n    def mixed_precision_layers(self) -> list[str]:\n        \"\"\"list: Layers that can be switched between mixed-float16 and float32. \"\"\"\n        return self._mixed_precision_layers\n\n    @property\n    def model_needs_rebuild(self) -> bool:\n        \"\"\"bool: ``True`` if mixed precision policy has changed so model needs to be rebuilt\n        otherwise ``False`` \"\"\"\n        return self._rebuild_model\n\n    def _new_session_id(self) -> int:\n        \"\"\" Generate a new session id. Returns 1 if this is a new model, or the last session id + 1\n        if it is a pre-existing model.\n\n        Returns\n        -------\n        int\n            The newly generated session id\n        \"\"\"\n        if not self._sessions:\n            session_id = 1\n        else:\n            session_id = max(int(key) for key in self._sessions.keys()) + 1\n        logger.debug(session_id)\n        return session_id\n\n    def _create_new_session(self, no_logs: bool, config_changeable_items: dict) -> None:\n        \"\"\" Initialize a new session, creating the dictionary entry for the session in\n        :attr:`_sessions`.\n\n        Parameters\n        ----------\n        no_logs: bool\n            ``True`` if Tensorboard logs should not be generated, otherwise ``False``\n        config_changeable_items: dict\n            Configuration options that can be altered when resuming a model, and their current\n            values\n        \"\"\"\n        logger.debug(\"Creating new session. id: %s\", self._session_id)\n        self._sessions[self._session_id] = {\"timestamp\": time.time(),\n                                            \"no_logs\": no_logs,\n                                            \"loss_names\": [],\n                                            \"batchsize\": 0,\n                                            \"iterations\": 0,\n                                            \"config\": config_changeable_items}\n\n    def update_session_config(self, key: str, value: T.Any) -> None:\n        \"\"\" Update a configuration item of the currently loaded session.\n\n        Parameters\n        ----------\n        key: str\n            The configuration item to update for the current session\n        value: any\n            The value to update to\n        \"\"\"\n        old_val = self.current_session[\"config\"][key]\n        assert isinstance(value, type(old_val))\n        logger.debug(\"Updating configuration item '%s' from '%s' to '%s'\", key, old_val, value)\n        self.current_session[\"config\"][key] = value\n\n    def add_session_loss_names(self, loss_names: list[str]) -> None:\n        \"\"\" Add the session loss names to the sessions dictionary.\n\n        The loss names are used for Tensorboard logging\n\n        Parameters\n        ----------\n        loss_names: list\n            The list of loss names for this session.\n        \"\"\"\n        logger.debug(\"Adding session loss_names: %s\", loss_names)\n        self._sessions[self._session_id][\"loss_names\"] = loss_names\n\n    def add_session_batchsize(self, batch_size: int) -> None:\n        \"\"\" Add the session batch size to the sessions dictionary.\n\n        Parameters\n        ----------\n        batch_size: int\n            The batch size for the current training session\n        \"\"\"\n        logger.debug(\"Adding session batch size: %s\", batch_size)\n        self._sessions[self._session_id][\"batchsize\"] = batch_size\n\n    def increment_iterations(self) -> None:\n        \"\"\" Increment :attr:`iterations` and session iterations by 1. \"\"\"\n        self._iterations += 1\n        self._sessions[self._session_id][\"iterations\"] += 1\n\n    def add_mixed_precision_layers(self, layers: list[str]) -> None:\n        \"\"\" Add the list of model's layers that are compatible for mixed precision to the\n        state dictionary \"\"\"\n        logger.debug(\"Storing mixed precision layers: %s\", layers)\n        self._mixed_precision_layers = layers\n\n    def _load(self, config_changeable_items: dict) -> None:\n        \"\"\" Load a state file and set the serialized values to the class instance.\n\n        Updates the model's config with the values stored in the state file.\n\n        Parameters\n        ----------\n        config_changeable_items: dict\n            Configuration options that can be altered when resuming a model, and their current\n            values\n        \"\"\"\n        logger.debug(\"Loading State\")\n        if not os.path.exists(self._filename):\n            logger.info(\"No existing state file found. Generating.\")\n            return\n        state = self._serializer.load(self._filename)\n        self._name = state.get(\"name\", self._name)\n        self._sessions = state.get(\"sessions\", {})\n        self._lowest_avg_loss = state.get(\"lowest_avg_loss\", {})\n        self._iterations = state.get(\"iterations\", 0)\n        self._mixed_precision_layers = state.get(\"mixed_precision_layers\", [])\n        self._config = state.get(\"config\", {})\n        logger.debug(\"Loaded state: %s\", state)\n        self._replace_config(config_changeable_items)\n\n    def save(self) -> None:\n        \"\"\" Save the state values to the serialized state file. \"\"\"\n        logger.debug(\"Saving State\")\n        state = {\"name\": self._name,\n                 \"sessions\": self._sessions,\n                 \"lowest_avg_loss\": self._lowest_avg_loss,\n                 \"iterations\": self._iterations,\n                 \"mixed_precision_layers\": self._mixed_precision_layers,\n                 \"config\": _CONFIG}\n        self._serializer.save(self._filename, state)\n        logger.debug(\"Saved State\")\n\n    def _replace_config(self, config_changeable_items) -> None:\n        \"\"\" Replace the loaded config with the one contained within the state file.\n\n        Check for any `fixed`=``False`` parameter changes and log info changes.\n\n        Update any legacy config items to their current versions.\n\n        Parameters\n        ----------\n        config_changeable_items: dict\n            Configuration options that can be altered when resuming a model, and their current\n            values\n        \"\"\"\n        global _CONFIG  # pylint:disable=global-statement\n        if _CONFIG is None:\n            return\n        legacy_update = self._update_legacy_config()\n        # Add any new items to state config for legacy purposes where the new default may be\n        # detrimental to an existing model.\n        legacy_defaults: dict[str, str | int | bool] = {\"centering\": \"legacy\",\n                                                        \"mask_loss_function\": \"mse\",\n                                                        \"l2_reg_term\": 100,\n                                                        \"optimizer\": \"adam\",\n                                                        \"mixed_precision\": False}\n        for key, val in _CONFIG.items():\n            if key not in self._config.keys():\n                setting: ConfigValueType = legacy_defaults.get(key, val)\n                logger.info(\"Adding new config item to state file: '%s': '%s'\", key, setting)\n                self._config[key] = setting\n        self._update_changed_config_items(config_changeable_items)\n        logger.debug(\"Replacing config. Old config: %s\", _CONFIG)\n        _CONFIG = self._config\n        if legacy_update:\n            self.save()\n        logger.debug(\"Replaced config. New config: %s\", _CONFIG)\n        logger.info(\"Using configuration saved in state file\")\n\n    def _update_legacy_config(self) -> bool:\n        \"\"\" Legacy updates for new config additions.\n\n        When new config items are added to the Faceswap code, existing model state files need to be\n        updated to handle these new items.\n\n        Current existing legacy update items:\n\n            * loss - If old `dssim_loss` is ``true`` set new `loss_function` to `ssim` otherwise\n            set it to `mae`. Remove old `dssim_loss` item\n\n            * l2_reg_term - If this exists, set loss_function_2 to ``mse`` and loss_weight_2 to\n            the value held in the old ``l2_reg_term`` item\n\n            * masks - If `learn_mask` does not exist then it is set to ``True`` if `mask_type` is\n            not ``None`` otherwise it is set to ``False``.\n\n            * masks type - Replace removed masks 'dfl_full' and 'facehull' with `components` mask\n\n            * clipnorm - Only existed in 2 models (DFL-SAE + Unbalanced). Replaced with global\n            option autoclip\n\n        Returns\n        -------\n        bool\n            ``True`` if legacy items exist and state file has been updated, otherwise ``False``\n        \"\"\"\n        logger.debug(\"Checking for legacy state file update\")\n        priors = [\"dssim_loss\", \"mask_type\", \"mask_type\", \"l2_reg_term\", \"clipnorm\"]\n        new_items = [\"loss_function\", \"learn_mask\", \"mask_type\", \"loss_function_2\",\n                     \"autoclip\"]\n        updated = False\n        for old, new in zip(priors, new_items):\n            if old not in self._config:\n                logger.debug(\"Legacy item '%s' not in config. Skipping update\", old)\n                continue\n\n            # dssim_loss > loss_function\n            if old == \"dssim_loss\":\n                self._config[new] = \"ssim\" if self._config[old] else \"mae\"\n                del self._config[old]\n                updated = True\n                logger.info(\"Updated config from legacy dssim format. New config loss \"\n                            \"function: '%s'\", self._config[new])\n                continue\n\n            # Add learn mask option and set to True if model has \"penalized_mask_loss\" specified\n            if old == \"mask_type\" and new == \"learn_mask\" and new not in self._config:\n                self._config[new] = self._config[\"mask_type\"] is not None\n                updated = True\n                logger.info(\"Added new 'learn_mask' config item for this model. Value set to: %s\",\n                            self._config[new])\n                continue\n\n            # Replace removed masks with most similar equivalent\n            if old == \"mask_type\" and new == \"mask_type\" and self._config[old] in (\"facehull\",\n                                                                                   \"dfl_full\"):\n                old_mask = self._config[old]\n                self._config[new] = \"components\"\n                updated = True\n                logger.info(\"Updated 'mask_type' from '%s' to '%s' for this model\",\n                            old_mask, self._config[new])\n\n            # Replace l2_reg_term with the correct loss_2_function and update the value of\n            # loss_2_weight\n            if old == \"l2_reg_term\":\n                self._config[new] = \"mse\"\n                self._config[\"loss_weight_2\"] = self._config[old]\n                del self._config[old]\n                updated = True\n                logger.info(\"Updated config from legacy 'l2_reg_term' to 'loss_function_2'\")\n\n            # Replace clipnorm with correct gradient clipping type and value\n            if old == \"clipnorm\":\n                self._config[new] = self._config[old]\n                del self._config[old]\n                updated = True\n                logger.info(\"Updated config from legacy '%s' to '%s'\", old, new)\n\n        logger.debug(\"State file updated for legacy config: %s\", updated)\n        return updated\n\n    def _update_changed_config_items(self, config_changeable_items: dict) -> None:\n        \"\"\" Update any parameters which are not fixed and have been changed.\n\n        Set the :attr:`model_needs_rebuild` to ``True`` if mixed precision state has changed\n\n        Parameters\n        ----------\n        config_changeable_items: dict\n            Configuration options that can be altered when resuming a model, and their current\n            values\n        \"\"\"\n        rebuild_tasks = [\"mixed_precision\"]\n        if not config_changeable_items:\n            logger.debug(\"No changeable parameters have been updated\")\n            return\n        for key, val in config_changeable_items.items():\n            old_val = self._config[key]\n            if old_val == val:\n                continue\n            self._config[key] = val\n            logger.info(\"Config item: '%s' has been updated from '%s' to '%s'\", key, old_val, val)\n            self._rebuild_model = self._rebuild_model or key in rebuild_tasks\n\n\nclass _Inference():  # pylint:disable=too-few-public-methods\n    \"\"\" Calculates required layers and compiles a saved model for inference.\n\n    Parameters\n    ----------\n    saved_model: :class:`keras.models.Model`\n        The saved trained Faceswap model\n    switch_sides: bool\n        ``True`` if the swap should be performed \"B\" > \"A\" ``False`` if the swap should be\n        \"A\" > \"B\"\n    \"\"\"\n    def __init__(self, saved_model: tf.keras.models.Model, switch_sides: bool) -> None:\n        logger.debug(\"Initializing: %s (saved_model: %s, switch_sides: %s)\",\n                     self.__class__.__name__, saved_model, switch_sides)\n        self._config = saved_model.get_config()\n\n        self._input_idx = 1 if switch_sides else 0\n        self._output_idx = 0 if switch_sides else 1\n\n        self._input_names = [inp[0] for inp in self._config[\"input_layers\"]]\n        self._model = self._make_inference_model(saved_model)\n        logger.debug(\"Initialized: %s\", self.__class__.__name__)\n\n    @property\n    def model(self) -> tf.keras.models.Model:\n        \"\"\" :class:`keras.models.Model`: The Faceswap model, compiled for inference. \"\"\"\n        return self._model\n\n    def _get_nodes(self, nodes: np.ndarray) -> list[tuple[str, int]]:\n        \"\"\" Given in input list of nodes from a :attr:`keras.models.Model.get_config` dictionary,\n        filters the layer name(s) and output index of the node, splitting to the correct output\n        index in the event of multiple inputs.\n\n        Parameters\n        ----------\n        nodes: list\n            A node entry from the :attr:`keras.models.Model.get_config` dictionary\n\n        Returns\n        -------\n        list\n            The (node name, output index) for each node passed in\n        \"\"\"\n        anodes = np.array(nodes, dtype=\"object\")[..., :3]\n        num_layers = anodes.shape[0]\n        anodes = anodes[self._output_idx] if num_layers == 2 else anodes[0]\n\n        # Probably better checks for this, but this occurs when DNY preset is used and learn\n        # mask is enabled (i.e. the mask is created in fully connected layers)\n        anodes = anodes.squeeze() if anodes.ndim == 3 else anodes\n\n        retval = [(node[0], node[2]) for node in anodes]\n        return retval\n\n    def _make_inference_model(self, saved_model: tf.keras.models.Model) -> tf.keras.models.Model:\n        \"\"\" Extract the sub-models from the saved model that are required for inference.\n\n        Parameters\n        ----------\n        saved_model: :class:`keras.models.Model`\n            The saved trained Faceswap model\n\n        Returns\n        -------\n        :class:`keras.models.Model`\n            The model compiled for inference\n        \"\"\"\n        logger.debug(\"Compiling inference model. saved_model: %s\", saved_model)\n        struct = self._get_filtered_structure()\n        model_inputs = self._get_inputs(saved_model.inputs)\n        compiled_layers: dict[str, tf.keras.layers.Layer] = {}\n        for layer in saved_model.layers:\n            if layer.name not in struct:\n                logger.debug(\"Skipping unused layer: '%s'\", layer.name)\n                continue\n            inbound = struct[layer.name]\n            logger.debug(\"Processing layer '%s': (layer: %s, inbound_nodes: %s)\",\n                         layer.name, layer, inbound)\n            if not inbound:\n                model = model_inputs\n                logger.debug(\"Adding model inputs %s: %s\", layer.name, model)\n            else:\n                layer_inputs = []\n                for inp in inbound:\n                    inbound_layer = compiled_layers[inp[0]]\n                    if isinstance(inbound_layer, list) and len(inbound_layer) > 1:\n                        # Multi output inputs\n                        inbound_output_idx = inp[1]\n                        next_input = inbound_layer[inbound_output_idx]\n                        logger.debug(\"Selecting output index %s from multi output inbound layer: \"\n                                     \"%s (using: %s)\", inbound_output_idx, inbound_layer,\n                                     next_input)\n                    else:\n                        next_input = inbound_layer\n\n                    layer_inputs.append(next_input)\n\n                logger.debug(\"Compiling layer '%s': layer inputs: %s\", layer.name, layer_inputs)\n                model = layer(layer_inputs)\n            compiled_layers[layer.name] = model\n            retval = keras.models.Model(model_inputs, model, name=f\"{saved_model.name}_inference\")\n        logger.debug(\"Compiled inference model '%s': %s\", retval.name, retval)\n        return retval\n\n    def _get_filtered_structure(self) -> OrderedDict:\n        \"\"\" Obtain the structure of the inference model.\n\n        This parses the model config (in reverse) to obtain the required layers for an inference\n        model.\n\n        Returns\n        -------\n        :class:`collections.OrderedDict`\n            The layer name as key with the input name and output index as value.\n        \"\"\"\n        # Filter output layer\n        out = np.array(self._config[\"output_layers\"], dtype=\"object\")\n        if out.ndim == 2:\n            out = np.expand_dims(out, axis=1)  # Needs to be expanded for _get_nodes\n        outputs = self._get_nodes(out)\n\n        # Iterate backwards from the required output to get the reversed model structure\n        current_layers = [outputs[0]]\n        next_layers = []\n        struct = OrderedDict()\n        drop_input = self._input_names[abs(self._input_idx - 1)]\n        switch_input = self._input_names[self._input_idx]\n        while True:\n            layer_info = current_layers.pop(0)\n            current_layer = next(lyr for lyr in self._config[\"layers\"]\n                                 if lyr[\"name\"] == layer_info[0])\n            inbound = current_layer[\"inbound_nodes\"]\n\n            if not inbound:\n                break\n\n            inbound_info = self._get_nodes(inbound)\n\n            if any(inb[0] == drop_input for inb in inbound_info):  # Switch inputs\n                inbound_info = [(switch_input if inb[0] == drop_input else inb[0], inb[1])\n                                for inb in inbound_info]\n            struct[layer_info[0]] = inbound_info\n            next_layers.extend(inbound_info)\n\n            if not current_layers:\n                current_layers = next_layers\n                next_layers = []\n\n        struct[switch_input] = []  # Add the input layer\n        logger.debug(\"Model structure: %s\", struct)\n        return struct\n\n    def _get_inputs(self, inputs: list) -> list:\n        \"\"\" Obtain the inputs for the requested swap direction.\n\n        Parameters\n        ----------\n        inputs: list\n            The full list of input tensors to the saved faceswap training model\n\n        Returns\n        -------\n        list\n            List of input tensors to feed the model for the requested swap direction\n        \"\"\"\n        input_split = len(inputs) // 2\n        start_idx = input_split * self._input_idx\n        retval = inputs[start_idx: start_idx + input_split]\n        logger.debug(\"model inputs: %s, input_split: %s, start_idx: %s, inference_inputs: %s\",\n                     inputs, input_split, start_idx, retval)\n        return retval\n", "plugins/train/model/_base/settings.py": "#!/usr/bin/env python3\n\"\"\"\nSettings for the model base plugins.\n\nThe objects in this module should not be called directly, but are called from\n:class:`~plugins.train.model._base.ModelBase`\n\nHandles configuration of model plugins for:\n    - Loss configuration\n    - Optimizer settings\n    - General global model configuration settings\n\"\"\"\nfrom __future__ import annotations\nfrom dataclasses import dataclass, field\nimport logging\nimport platform\nimport typing as T\n\nfrom contextlib import nullcontext\n\nimport tensorflow as tf\n# Ignore linting errors from Tensorflow's thoroughly broken import system\nfrom tensorflow.keras import losses as k_losses  # pylint:disable=import-error\nimport tensorflow.keras.mixed_precision as mixedprecision  # noqa pylint:disable=import-error\n\nfrom lib.model import losses, optimizers\nfrom lib.model.autoclip import AutoClipper\nfrom lib.utils import get_backend\n\nif T.TYPE_CHECKING:\n    from collections.abc import Callable\n    from contextlib import AbstractContextManager as ContextManager\n    from argparse import Namespace\n    from .model import State\n\nkeras = tf.keras\nK = keras.backend\n\nlogger = logging.getLogger(__name__)\n\n\n@dataclass\nclass LossClass:\n    \"\"\" Typing class for holding loss functions.\n\n    Parameters\n    ----------\n    function: Callable\n        The function that takes in the true/predicted images and returns the loss\n    init: bool, Optional\n        Whether the loss object ``True`` needs to be initialized (i.e. it's a class) or\n        ``False`` it does not require initialization (i.e. it's a function).\n        Default ``True``\n    kwargs: dict\n        Any keyword arguments to supply to the loss function at initialization.\n    \"\"\"\n    function: Callable[[tf.Tensor, tf.Tensor], tf.Tensor] | T.Any = k_losses.mae\n    init: bool = True\n    kwargs: dict[str, T.Any] = field(default_factory=dict)\n\n\nclass Loss():\n    \"\"\" Holds loss names and functions for an Autoencoder.\n\n    Parameters\n    ----------\n    config: dict\n        The configuration options for the current model plugin\n    color_order: str\n        Color order of the model. One of `\"BGR\"` or `\"RGB\"`\n    \"\"\"\n    def __init__(self, config: dict, color_order: T.Literal[\"bgr\", \"rgb\"]) -> None:\n        logger.debug(\"Initializing %s: (color_order: %s)\", self.__class__.__name__, color_order)\n        self._config = config\n        self._mask_channels = self._get_mask_channels()\n        self._inputs: list[tf.keras.layers.Layer] = []\n        self._names: list[str] = []\n        self._funcs: dict[str, Callable] = {}\n\n        self._loss_dict = {\"ffl\": LossClass(function=losses.FocalFrequencyLoss),\n                           \"flip\": LossClass(function=losses.LDRFLIPLoss,\n                                             kwargs={\"color_order\": color_order}),\n                           \"gmsd\": LossClass(function=losses.GMSDLoss),\n                           \"l_inf_norm\": LossClass(function=losses.LInfNorm),\n                           \"laploss\": LossClass(function=losses.LaplacianPyramidLoss),\n                           \"logcosh\": LossClass(function=k_losses.logcosh, init=False),\n                           \"lpips_alex\": LossClass(function=losses.LPIPSLoss,\n                                                   kwargs={\"trunk_network\": \"alex\"}),\n                           \"lpips_squeeze\": LossClass(function=losses.LPIPSLoss,\n                                                      kwargs={\"trunk_network\": \"squeeze\"}),\n                           \"lpips_vgg16\": LossClass(function=losses.LPIPSLoss,\n                                                    kwargs={\"trunk_network\": \"vgg16\"}),\n                           \"ms_ssim\": LossClass(function=losses.MSSIMLoss),\n                           \"mae\": LossClass(function=k_losses.mean_absolute_error, init=False),\n                           \"mse\": LossClass(function=k_losses.mean_squared_error, init=False),\n                           \"pixel_gradient_diff\": LossClass(function=losses.GradientLoss),\n                           \"ssim\": LossClass(function=losses.DSSIMObjective),\n                           \"smooth_loss\": LossClass(function=losses.GeneralizedLoss)}\n\n        logger.debug(\"Initialized: %s\", self.__class__.__name__)\n\n    @property\n    def names(self) -> list[str]:\n        \"\"\" list: The list of loss names for the model. \"\"\"\n        return self._names\n\n    @property\n    def functions(self) -> dict:\n        \"\"\" dict: The loss functions that apply to each model output. \"\"\"\n        return self._funcs\n\n    @property\n    def _mask_inputs(self) -> list | None:\n        \"\"\" list: The list of input tensors to the model that contain the mask. Returns ``None``\n        if there is no mask input to the model. \"\"\"\n        mask_inputs = [inp for inp in self._inputs if inp.name.startswith(\"mask\")]\n        return None if not mask_inputs else mask_inputs\n\n    @property\n    def _mask_shapes(self) -> list[tuple] | None:\n        \"\"\" list: The list of shape tuples for the mask input tensors for the model. Returns\n        ``None`` if there is no mask input. \"\"\"\n        if self._mask_inputs is None:\n            return None\n        return [K.int_shape(mask_input) for mask_input in self._mask_inputs]\n\n    def configure(self, model: tf.keras.models.Model) -> None:\n        \"\"\" Configure the loss functions for the given inputs and outputs.\n\n        Parameters\n        ----------\n        model: :class:`keras.models.Model`\n            The model that is to be trained\n        \"\"\"\n        self._inputs = model.inputs\n        self._set_loss_names(model.outputs)\n        self._set_loss_functions(model.output_names)\n        self._names.insert(0, \"total\")\n\n    def _set_loss_names(self, outputs: list[tf.Tensor]) -> None:\n        \"\"\" Name the losses based on model output.\n\n        This is used for correct naming in the state file, for display purposes only.\n\n        Adds the loss names to :attr:`names`\n\n        Notes\n        -----\n        TODO Currently there is an issue in Tensorflow that wraps all outputs in an Identity layer\n        when running in Eager Execution mode, which means we cannot use the name of the output\n        layers to name the losses (https://github.com/tensorflow/tensorflow/issues/32180).\n        With this in mind, losses are named based on their shapes\n\n        Parameters\n        ----------\n        outputs: list\n            A list of output tensors from the model plugin\n        \"\"\"\n        # TODO Use output names if/when these are fixed upstream\n        split_outputs = [outputs[:len(outputs) // 2], outputs[len(outputs) // 2:]]\n        for side, side_output in zip((\"a\", \"b\"), split_outputs):\n            output_names = [output.name for output in side_output]\n            output_shapes = [K.int_shape(output)[1:] for output in side_output]\n            output_types = [\"mask\" if shape[-1] == 1 else \"face\" for shape in output_shapes]\n            logger.debug(\"side: %s, output names: %s, output_shapes: %s, output_types: %s\",\n                         side, output_names, output_shapes, output_types)\n            for idx, name in enumerate(output_types):\n                suffix = \"\" if output_types.count(name) == 1 else f\"_{idx}\"\n                self._names.append(f\"{name}_{side}{suffix}\")\n        logger.debug(self._names)\n\n    def _get_function(self, name: str) -> Callable[[tf.Tensor, tf.Tensor], tf.Tensor]:\n        \"\"\" Obtain the requested Loss function\n\n        Parameters\n        ----------\n        name: str\n            The name of the loss function from the training configuration file\n\n        Returns\n        -------\n        Keras Loss Function\n            The requested loss function\n        \"\"\"\n        func = self._loss_dict[name]\n        retval = func.function(**func.kwargs) if func.init else func.function  # type:ignore\n        logger.debug(\"Obtained loss function `%s` (%s)\", name, retval)\n        return retval\n\n    def _set_loss_functions(self, output_names: list[str]):\n        \"\"\" Set the loss functions and their associated weights.\n\n        Adds the loss functions to the :attr:`functions` dictionary.\n\n        Parameters\n        ----------\n        output_names: list\n            The output names from the model\n        \"\"\"\n        face_losses = [(lossname, self._config.get(f\"loss_weight_{k[-1]}\", 100))\n                       for k, lossname in sorted(self._config.items())\n                       if k.startswith(\"loss_function\")\n                       and self._config.get(f\"loss_weight_{k[-1]}\", 100) != 0\n                       and lossname is not None]\n\n        for name, output_name in zip(self._names, output_names):\n            if name.startswith(\"mask\"):\n                loss_func = self._get_function(self._config[\"mask_loss_function\"])\n            else:\n                loss_func = losses.LossWrapper()\n                for func, weight in face_losses:\n                    self._add_face_loss_function(loss_func, func, weight / 100.)\n\n            logger.debug(\"%s: (output_name: '%s', function: %s)\", name, output_name, loss_func)\n            self._funcs[output_name] = loss_func\n        logger.debug(\"functions: %s\", self._funcs)\n\n    def _add_face_loss_function(self,\n                                loss_wrapper: losses.LossWrapper,\n                                loss_function: str,\n                                weight: float) -> None:\n        \"\"\" Add the given face loss function at the given weight and apply any mouth and eye\n        multipliers\n\n        Parameters\n        ----------\n        loss_wrapper: :class:`lib.model.losses.LossWrapper`\n            The wrapper loss function that holds the face losses\n        loss_function: str\n            The loss function to add to the loss wrapper\n        weight: float\n            The amount of weight to apply to the given loss function\n        \"\"\"\n        logger.debug(\"Adding loss function: %s, weight: %s\", loss_function, weight)\n        loss_wrapper.add_loss(self._get_function(loss_function),\n                              weight=weight,\n                              mask_channel=self._mask_channels[0])\n\n        channel_idx = 1\n        for section in (\"eye_multiplier\", \"mouth_multiplier\"):\n            mask_channel = self._mask_channels[channel_idx]\n            multiplier = self._config[section] * 1.\n            if multiplier > 1.:\n                logger.debug(\"Adding section loss %s: %s\", section, multiplier)\n                loss_wrapper.add_loss(self._get_function(loss_function),\n                                      weight=weight * multiplier,\n                                      mask_channel=mask_channel)\n            channel_idx += 1\n\n    def _get_mask_channels(self) -> list[int]:\n        \"\"\" Obtain the channels from the face targets that the masks reside in from the training\n        data generator.\n\n        Returns\n        -------\n        list:\n            A list of channel indices that contain the mask for the corresponding config item\n        \"\"\"\n        eye_multiplier = self._config[\"eye_multiplier\"]\n        mouth_multiplier = self._config[\"mouth_multiplier\"]\n        if not self._config[\"penalized_mask_loss\"] and (eye_multiplier > 1 or\n                                                        mouth_multiplier > 1):\n            logger.warning(\"You have selected eye/mouth loss multipliers greater than 1x, but \"\n                           \"Penalized Mask Loss is disabled. Disabling all multipliers.\")\n            eye_multiplier = 1\n            mouth_multiplier = 1\n        uses_masks = (self._config[\"penalized_mask_loss\"],\n                      eye_multiplier > 1,\n                      mouth_multiplier > 1)\n        mask_channels = [-1 for _ in range(len(uses_masks))]\n        current_channel = 3\n        for idx, mask_required in enumerate(uses_masks):\n            if mask_required:\n                mask_channels[idx] = current_channel\n                current_channel += 1\n        logger.debug(\"uses_masks: %s, mask_channels: %s\", uses_masks, mask_channels)\n        return mask_channels\n\n\nclass Optimizer():  # pylint:disable=too-few-public-methods\n    \"\"\" Obtain the selected optimizer with the appropriate keyword arguments.\n\n    Parameters\n    ----------\n    optimizer: str\n        The selected optimizer name for the plugin\n    learning_rate: float\n        The selected learning rate to use\n    autoclip: bool\n        ``True`` if AutoClip should be enabled otherwise ``False``\n    epsilon: float\n        The value to use for the epsilon of the optimizer\n    \"\"\"\n    def __init__(self,\n                 optimizer: str,\n                 learning_rate: float,\n                 autoclip: bool,\n                 epsilon: float) -> None:\n        logger.debug(\"Initializing %s: (optimizer: %s, learning_rate: %s, autoclip: %s, \"\n                     \", epsilon: %s)\", self.__class__.__name__, optimizer, learning_rate,\n                     autoclip, epsilon)\n        valid_optimizers = {\"adabelief\": (optimizers.AdaBelief,\n                                          {\"beta_1\": 0.5, \"beta_2\": 0.99, \"epsilon\": epsilon}),\n                            \"adam\": (optimizers.Adam,\n                                     {\"beta_1\": 0.5, \"beta_2\": 0.99, \"epsilon\": epsilon}),\n                            \"nadam\": (optimizers.Nadam,\n                                      {\"beta_1\": 0.5, \"beta_2\": 0.99, \"epsilon\": epsilon}),\n                            \"rms-prop\": (optimizers.RMSprop, {\"epsilon\": epsilon})}\n        optimizer_info = valid_optimizers[optimizer]\n        self._optimizer: Callable = optimizer_info[0]\n        self._kwargs: dict[str, T.Any] = optimizer_info[1]\n\n        self._configure(learning_rate, autoclip)\n        logger.verbose(\"Using %s optimizer\", optimizer.title())  # type:ignore[attr-defined]\n        logger.debug(\"Initialized: %s\", self.__class__.__name__)\n\n    @property\n    def optimizer(self) -> tf.keras.optimizers.Optimizer:\n        \"\"\" :class:`keras.optimizers.Optimizer`: The requested optimizer. \"\"\"\n        return self._optimizer(**self._kwargs)\n\n    def _configure(self,\n                   learning_rate: float,\n                   autoclip: bool) -> None:\n        \"\"\" Configure the optimizer based on user settings.\n\n        Parameters\n        ----------\n        learning_rate: float\n            The selected learning rate to use\n        autoclip: bool\n            ``True`` if AutoClip should be enabled otherwise ``False``\n        \"\"\"\n        self._kwargs[\"learning_rate\"] = learning_rate\n        if not autoclip:\n            return\n\n        logger.info(\"Enabling AutoClip\")\n        self._kwargs[\"gradient_transformers\"] = [AutoClipper(10, history_size=10000)]\n        logger.debug(\"optimizer kwargs: %s\", self._kwargs)\n\n\nclass Settings():\n    \"\"\" Tensorflow core training settings.\n\n    Sets backend tensorflow settings prior to launching the model.\n\n    Tensorflow 2 uses distribution strategies for multi-GPU/system training. These are context\n    managers.\n\n    Parameters\n    ----------\n    arguments: :class:`argparse.Namespace`\n        The arguments that were passed to the train or convert process as generated from\n        Faceswap's command line arguments\n    mixed_precision: bool\n        ``True`` if Mixed Precision training should be used otherwise ``False``\n    allow_growth: bool\n        ``True`` if the Tensorflow allow_growth parameter should be set otherwise ``False``\n    is_predict: bool, optional\n        ``True`` if the model is being loaded for inference, ``False`` if the model is being loaded\n        for training. Default: ``False``\n    \"\"\"\n    def __init__(self,\n                 arguments: Namespace,\n                 mixed_precision: bool,\n                 allow_growth: bool,\n                 is_predict: bool) -> None:\n        logger.debug(\"Initializing %s: (arguments: %s, mixed_precision: %s, allow_growth: %s, \"\n                     \"is_predict: %s)\", self.__class__.__name__, arguments, mixed_precision,\n                     allow_growth, is_predict)\n        self._set_tf_settings(allow_growth, arguments.exclude_gpus)\n\n        use_mixed_precision = not is_predict and mixed_precision\n        self._use_mixed_precision = self._set_keras_mixed_precision(use_mixed_precision)\n        if self._use_mixed_precision:\n            logger.info(\"Enabling Mixed Precision Training.\")\n\n        if hasattr(arguments, \"distribution_strategy\"):\n            strategy = arguments.distribution_strategy\n        else:\n            strategy = \"default\"\n        self._strategy = self._get_strategy(strategy)\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    @property\n    def use_mixed_precision(self) -> bool:\n        \"\"\" bool: ``True`` if mixed precision training has been enabled, otherwise ``False``. \"\"\"\n        return self._use_mixed_precision\n\n    @classmethod\n    def loss_scale_optimizer(\n            cls,\n            optimizer: tf.keras.optimizers.Optimizer) -> mixedprecision.LossScaleOptimizer:\n        \"\"\" Optimize loss scaling for mixed precision training.\n\n        Parameters\n        ----------\n        optimizer: :class:`tf.keras.optimizers.Optimizer`\n            The optimizer instance to wrap\n\n        Returns\n        --------\n        :class:`tf.keras.mixed_precision.loss_scale_optimizer.LossScaleOptimizer`\n            The original optimizer with loss scaling applied\n        \"\"\"\n        return mixedprecision.LossScaleOptimizer(optimizer)  # pylint:disable=no-member\n\n    @classmethod\n    def _set_tf_settings(cls, allow_growth: bool, exclude_devices: list[int]) -> None:\n        \"\"\" Specify Devices to place operations on and Allow TensorFlow to manage VRAM growth.\n\n        Enables the Tensorflow allow_growth option if requested in the command line arguments\n\n        Parameters\n        ----------\n        allow_growth: bool\n            ``True`` if the Tensorflow allow_growth parameter should be set otherwise ``False``\n        exclude_devices: list or ``None``\n            List of GPU device indices that should not be made available to Tensorflow. Pass\n            ``None`` if all devices should be made available\n        \"\"\"\n        backend = get_backend()\n        if backend == \"cpu\":\n            logger.verbose(\"Hiding GPUs from Tensorflow\")  # type:ignore[attr-defined]\n            tf.config.set_visible_devices([], \"GPU\")\n            return\n\n        if not exclude_devices and not allow_growth:\n            logger.debug(\"Not setting any specific Tensorflow settings\")\n            return\n\n        gpus = tf.config.list_physical_devices('GPU')\n        if exclude_devices:\n            gpus = [gpu for idx, gpu in enumerate(gpus) if idx not in exclude_devices]\n            logger.debug(\"Filtering devices to: %s\", gpus)\n            tf.config.set_visible_devices(gpus, \"GPU\")\n\n        if allow_growth and backend == \"nvidia\":\n            logger.debug(\"Setting Tensorflow 'allow_growth' option\")\n            for gpu in gpus:\n                logger.info(\"Setting allow growth for GPU: %s\", gpu)\n                tf.config.experimental.set_memory_growth(gpu, True)\n            logger.debug(\"Set Tensorflow 'allow_growth' option\")\n\n    @classmethod\n    def _set_keras_mixed_precision(cls, use_mixed_precision: bool) -> bool:\n        \"\"\" Enable the Keras experimental Mixed Precision API.\n\n        Enables the Keras experimental Mixed Precision API if requested in the user configuration\n        file.\n\n        Parameters\n        ----------\n        use_mixed_precision: bool\n            ``True`` if experimental mixed precision support should be enabled for Nvidia GPUs\n            otherwise ``False``.\n\n        Returns\n        -------\n        bool\n            ``True`` if mixed precision has been enabled otherwise ``False``\n        \"\"\"\n        logger.debug(\"use_mixed_precision: %s\", use_mixed_precision)\n        if not use_mixed_precision:\n            policy = mixedprecision.Policy('float32')  # pylint:disable=no-member\n            mixedprecision.set_global_policy(policy)  # pylint:disable=no-member\n            logger.debug(\"Disabling mixed precision. (Compute dtype: %s, variable_dtype: %s)\",\n                         policy.compute_dtype, policy.variable_dtype)\n            return False\n\n        policy = mixedprecision.Policy('mixed_float16')  # pylint:disable=no-member\n        mixedprecision.set_global_policy(policy)  # pylint:disable=no-member\n        logger.debug(\"Enabled mixed precision. (Compute dtype: %s, variable_dtype: %s)\",\n                     policy.compute_dtype, policy.variable_dtype)\n        return True\n\n    def _get_strategy(self,\n                      strategy: T.Literal[\"default\", \"central-storage\", \"mirrored\"]\n                      ) -> tf.distribute.Strategy | None:\n        \"\"\" If we are running on Nvidia backend and the strategy is not ``None`` then return\n        the correct tensorflow distribution strategy, otherwise return ``None``.\n\n        Notes\n        -----\n        By default Tensorflow defaults mirrored strategy to use the Nvidia NCCL method for\n        reductions, however this is only available in Linux, so the method used falls back to\n        `Hierarchical Copy All Reduce` if the OS is not Linux.\n\n        Central Storage strategy is not compatible with Mixed Precision. However, in testing it\n        worked fine when using a single GPU, so we monkey-patch out the tests for Mixed-Precision\n        when using this strategy with a single GPU\n\n        Parameters\n        ----------\n        strategy: str\n            One of 'default', 'central-storage' or 'mirrored'.\n\n        Returns\n        -------\n        :class:`tensorflow.distribute.Strategy` or `None`\n            The request Tensorflow Strategy if the backend is Nvidia and the strategy is not\n            `\"Default\"` otherwise ``None``\n        \"\"\"\n        if get_backend() not in (\"nvidia\", \"directml\", \"rocm\"):\n            retval = None\n        elif strategy == \"mirrored\":\n            retval = self._get_mirrored_strategy()\n        elif strategy == \"central-storage\":\n            retval = self._get_central_storage_strategy()\n        else:\n            retval = tf.distribute.get_strategy()\n        logger.debug(\"Using strategy: %s\", retval)\n        return retval\n\n    @classmethod\n    def _get_mirrored_strategy(cls) -> tf.distribute.MirroredStrategy:\n        \"\"\" Obtain an instance of a Tensorflow Mirrored Strategy, setting the cross device\n        operations appropriate for the OS in use.\n\n        Returns\n        -------\n        :class:`tensorflow.distribute.MirroredStrategy`\n            The Mirrored Distribution Strategy object with correct cross device operations set\n        \"\"\"\n        if platform.system().lower() == \"linux\":\n            cross_device_ops = tf.distribute.NcclAllReduce()\n        else:\n            cross_device_ops = tf.distribute.HierarchicalCopyAllReduce()\n        logger.debug(\"cross_device_ops: %s\", cross_device_ops)\n        return tf.distribute.MirroredStrategy(cross_device_ops=cross_device_ops)\n\n    @classmethod\n    def _get_central_storage_strategy(cls) -> tf.distribute.experimental.CentralStorageStrategy:\n        \"\"\" Obtain an instance of a Tensorflow Central Storage Strategy. If the strategy is being\n        run on a single GPU then monkey patch Tensorflows mixed-precision strategy checks to pass\n        successfully.\n\n        Returns\n        -------\n        :class:`tensorflow.distribute.experimental.CentralStorageStrategy`\n            The Central Storage Distribution Strategy object\n        \"\"\"\n        gpus = tf.config.get_visible_devices(\"GPU\")\n        if len(gpus) == 1:\n            # TODO Remove these monkey patches when Strategy supports mixed-precision\n            from keras.mixed_precision import loss_scale_optimizer  # noqa pylint:disable=import-outside-toplevel\n\n            # Force a return of True on Loss Scale Optimizer Stategy check\n            loss_scale_optimizer.strategy_supports_loss_scaling = lambda: True\n\n            # As LossScaleOptimizer aggregates gradients internally, it passes `False` as the value\n            # for `experimental_aggregate_gradients` in `OptimizerV2.apply_gradients`. This causes\n            # the optimizer to fail when checking against this strategy. We could monkey patch\n            # `Optimizer.apply_gradients`, but it is a lot more code to check, so we just switch\n            # the `experimental_aggregate_gradients` back to `True`. In brief testing this does not\n            # appear to have a negative impact.\n            func = lambda s, grads, wvars, name: s._optimizer.apply_gradients(  # noqa pylint:disable=protected-access,unnecessary-lambda-assignment\n                 list(zip(grads, wvars.value)), name, experimental_aggregate_gradients=True)\n            loss_scale_optimizer.LossScaleOptimizer._apply_gradients = func  # noqa pylint:disable=protected-access\n\n        return tf.distribute.experimental.CentralStorageStrategy(parameter_device=\"/cpu:0\")\n\n    def _get_mixed_precision_layers(self, layers: list[dict]) -> list[str]:\n        \"\"\" Obtain the names of the layers in a mixed precision model that have their dtype policy\n        explicitly set to mixed-float16.\n\n        Parameters\n        ----------\n        layers: List\n            The list of layers that appear in a keras's model configuration `dict`\n\n        Returns\n        -------\n        list\n            A list of layer names within the model that are assigned a float16 policy\n        \"\"\"\n        retval = []\n        for layer in layers:\n            config = layer[\"config\"]\n\n            if layer[\"class_name\"] in (\"Functional\", \"Sequential\"):  # Recurse into sub-models\n                retval.extend(self._get_mixed_precision_layers(config[\"layers\"]))\n                continue\n\n            dtype = config[\"dtype\"]\n            if isinstance(dtype, dict) and dtype[\"config\"][\"name\"] == \"mixed_float16\":\n                logger.debug(\"Adding supported mixed precision layer: %s %s\", layer[\"name\"], dtype)\n                retval.append(layer[\"name\"])\n            else:\n                logger.debug(\"Skipping unsupported layer: %s %s\",\n                             layer.get(\"name\", f\"class_name: {layer['class_name']}\"), dtype)\n        return retval\n\n    def _switch_precision(self, layers: list[dict], compatible: list[str]) -> None:\n        \"\"\" Switch a model's datatype between mixed-float16 and float32.\n\n        Parameters\n        ----------\n        layers: List\n            The list of layers that appear in a keras's model configuration `dict`\n        compatible: List\n            A list of layer names that are compatible to have their datatype switched\n        \"\"\"\n        dtype = \"mixed_float16\" if self.use_mixed_precision else \"float32\"\n        policy = {\"class_name\": \"Policy\", \"config\": {\"name\": dtype}}\n\n        for layer in layers:\n            config = layer[\"config\"]\n\n            if layer[\"class_name\"] in [\"Functional\", \"Sequential\"]:  # Recurse into sub-models\n                self._switch_precision(config[\"layers\"], compatible)\n                continue\n\n            if layer[\"name\"] not in compatible:\n                logger.debug(\"Skipping incompatible layer: %s\", layer[\"name\"])\n                continue\n\n            logger.debug(\"Updating dtype for %s from: %s to: %s\",\n                         layer[\"name\"], config[\"dtype\"], policy)\n            config[\"dtype\"] = policy\n\n    def get_mixed_precision_layers(self,\n                                   build_func: Callable[[list[tf.keras.layers.Layer]],\n                                                        tf.keras.models.Model],\n                                   inputs: list[tf.keras.layers.Layer]\n                                   ) -> tuple[tf.keras.models.Model, list[str]]:\n        \"\"\" Get and store the mixed precision layers from a full precision enabled model.\n\n        Parameters\n        ----------\n        build_func: Callable\n            The function to be called to compile the newly created model\n        inputs:\n            The inputs to the model to be compiled\n\n        Returns\n        -------\n        model: :class:`tensorflow.keras.model`\n            The built model in fp32\n        list\n            The list of layer names within the full precision model that can be switched\n            to mixed precision\n        \"\"\"\n        logger.info(\"Storing Mixed Precision compatible layers. Please ignore any following \"\n                    \"warnings about using mixed precision.\")\n        self._set_keras_mixed_precision(True)\n        with tf.device(\"CPU\"):\n            model = build_func(inputs)\n            layers = self._get_mixed_precision_layers(model.get_config()[\"layers\"])\n\n        tf.keras.backend.clear_session()\n        self._set_keras_mixed_precision(False)\n\n        config = model.get_config()\n        self._switch_precision(config[\"layers\"], layers)\n        new_model = model.from_config(config)\n        del model\n        return new_model, layers\n\n    def check_model_precision(self,\n                              model: tf.keras.models.Model,\n                              state: \"State\") -> tf.keras.models.Model:\n        \"\"\" Check the model's precision.\n\n        If this is a new model, then\n        Rewrite an existing model's training precsion mode from mixed-float16 to float32 or\n        vice versa.\n\n        This is not easy to do in keras, so we edit the model's config to change the dtype policy\n        for compatible layers. Create a new model from this config, then port the weights from the\n        old model to the new model.\n\n        Parameters\n        ----------\n        model: :class:`keras.models.Model`\n            The original saved keras model to rewrite the dtype\n        state: ~:class:`plugins.train.model._base.model.State`\n            The State information for the model\n\n        Returns\n        -------\n        :class:`keras.models.Model`\n            The original model with the datatype updated\n        \"\"\"\n        if self.use_mixed_precision and not state.mixed_precision_layers:\n            # Switching to mixed precision on a model which was started in FP32 prior to the\n            # ability to switch between precisions on a saved model is not supported as we\n            # do not have the compatible layer names\n            logger.warning(\"Switching from Full Precision to Mixed Precision is not supported on \"\n                           \"older model files. Reverting to Full Precision.\")\n            return model\n\n        config = model.get_config()\n\n        if not self.use_mixed_precision and not state.mixed_precision_layers:\n            # Switched to Full Precision, get compatible layers from model if not already stored\n            state.add_mixed_precision_layers(self._get_mixed_precision_layers(config[\"layers\"]))\n\n        self._switch_precision(config[\"layers\"], state.mixed_precision_layers)\n\n        new_model = keras.models.Model().from_config(config)\n        new_model.set_weights(model.get_weights())\n        logger.info(\"Mixed precision has been updated from '%s' to '%s'\",\n                    not self.use_mixed_precision, self.use_mixed_precision)\n        del model\n        return new_model\n\n    def strategy_scope(self) -> ContextManager:\n        \"\"\" Return the strategy scope if we have set a strategy, otherwise return a null\n        context.\n\n        Returns\n        -------\n        :func:`tensorflow.python.distribute.Strategy.scope` or :func:`contextlib.nullcontext`\n            The tensorflow strategy scope if a strategy is valid in the current scenario. A null\n            context manager if the strategy is not valid in the current scenario\n        \"\"\"\n        retval = nullcontext() if self._strategy is None else self._strategy.scope()\n        logger.debug(\"Using strategy scope: %s\", retval)\n        return retval\n", "plugins/train/model/_base/io.py": "#!/usr/bin/env python3\n\"\"\"\nIO handling for the model base plugin.\n\nThe objects in this module should not be called directly, but are called from\n:class:`~plugins.train.model._base.ModelBase`\n\nThis module handles:\n    - The loading, saving and backing up of keras models to and from disk.\n    - The loading and freezing of weights for model plugins.\n\"\"\"\nfrom __future__ import annotations\nimport logging\nimport os\nimport sys\nimport typing as T\n\nimport tensorflow as tf\n\nfrom lib.model.backup_restore import Backup\nfrom lib.utils import FaceswapError\n\nif T.TYPE_CHECKING:\n    from .model import ModelBase\n\nkmodels = tf.keras.models\nlogger = logging.getLogger(__name__)\n\n\ndef get_all_sub_models(\n        model: tf.keras.models.Model,\n        models: list[tf.keras.models.Model] | None = None) -> list[tf.keras.models.Model]:\n    \"\"\" For a given model, return all sub-models that occur (recursively) as children.\n\n    Parameters\n    ----------\n    model: :class:`tensorflow.keras.models.Model`\n        A Keras model to scan for sub models\n    models: `None`\n        Do not provide this parameter. It is used for recursion\n\n    Returns\n    -------\n    list\n        A list of all :class:`tensorflow.keras.models.Model` objects found within the given model.\n        The provided model will always be returned in the first position\n    \"\"\"\n    if models is None:\n        models = [model]\n    else:\n        models.append(model)\n    for layer in model.layers:\n        if isinstance(layer, kmodels.Model):\n            get_all_sub_models(layer, models=models)\n    return models\n\n\nclass IO():\n    \"\"\" Model saving and loading functions.\n\n    Handles the loading and saving of the plugin model from disk as well as the model backup and\n    snapshot functions.\n\n    Parameters\n    ----------\n    plugin: :class:`Model`\n        The parent plugin class that owns the IO functions.\n    model_dir: str\n        The full path to the model save location\n    is_predict: bool\n        ``True`` if the model is being loaded for inference. ``False`` if the model is being loaded\n        for training.\n    save_optimizer: [\"never\", \"always\", \"exit\"]\n        When to save the optimizer weights. `\"never\"` never saves the optimizer weights. `\"always\"`\n        always saves the optimizer weights. `\"exit\"` only saves the optimizer weights on an exit\n        request.\n    \"\"\"\n    def __init__(self,\n                 plugin: ModelBase,\n                 model_dir: str,\n                 is_predict: bool,\n                 save_optimizer: T.Literal[\"never\", \"always\", \"exit\"]) -> None:\n        self._plugin = plugin\n        self._is_predict = is_predict\n        self._model_dir = model_dir\n        self._save_optimizer = save_optimizer\n        self._history: list[list[float]] = [[], []]  # Loss histories per save iteration\n        self._backup = Backup(self._model_dir, self._plugin.name)\n\n    @property\n    def model_dir(self) -> str:\n        \"\"\" str: The full path to the model folder \"\"\"\n        return self._model_dir\n\n    @property\n    def filename(self) -> str:\n        \"\"\"str: The filename for this model.\"\"\"\n        return os.path.join(self._model_dir, f\"{self._plugin.name}.h5\")\n\n    @property\n    def model_exists(self) -> bool:\n        \"\"\" bool: ``True`` if a model of the type being loaded exists within the model folder\n        location otherwise ``False``.\n        \"\"\"\n        return os.path.isfile(self.filename)\n\n    @property\n    def history(self) -> list[list[float]]:\n        \"\"\" list: list of loss histories per side for the current save iteration. \"\"\"\n        return self._history\n\n    @property\n    def multiple_models_in_folder(self) -> list[str] | None:\n        \"\"\" :list: or ``None`` If there are multiple model types in the requested folder, or model\n        types that don't correspond to the requested plugin type, then returns the list of plugin\n        names that exist in the folder, otherwise returns ``None`` \"\"\"\n        plugins = [fname.replace(\".h5\", \"\")\n                   for fname in os.listdir(self._model_dir)\n                   if fname.endswith(\".h5\")]\n        test_names = plugins + [self._plugin.name]\n        test = False if not test_names else os.path.commonprefix(test_names) == \"\"\n        retval = None if not test else plugins\n        logger.debug(\"plugin name: %s, plugins: %s, test result: %s, retval: %s\",\n                     self._plugin.name, plugins, test, retval)\n        return retval\n\n    def load(self) -> tf.keras.models.Model:\n        \"\"\" Loads the model from disk\n\n        If the predict function is to be called and the model cannot be found in the model folder\n        then an error is logged and the process exits.\n\n        When loading the model, the plugin model folder is scanned for custom layers which are\n        added to Keras' custom objects.\n\n        Returns\n        -------\n        :class:`tensorflow.keras.models.Model`\n            The saved model loaded from disk\n        \"\"\"\n        logger.debug(\"Loading model: %s\", self.filename)\n        if self._is_predict and not self.model_exists:\n            logger.error(\"Model could not be found in folder '%s'. Exiting\", self._model_dir)\n            sys.exit(1)\n\n        try:\n            model = kmodels.load_model(self.filename, compile=False)\n        except RuntimeError as err:\n            if \"unable to get link info\" in str(err).lower():\n                msg = (f\"Unable to load the model from '{self.filename}'. This may be a \"\n                       \"temporary error but most likely means that your model has corrupted.\\n\"\n                       \"You can try to load the model again but if the problem persists you \"\n                       \"should use the Restore Tool to restore your model from backup.\\n\"\n                       f\"Original error: {str(err)}\")\n                raise FaceswapError(msg) from err\n            raise err\n        except KeyError as err:\n            if \"unable to open object\" in str(err).lower():\n                msg = (f\"Unable to load the model from '{self.filename}'. This may be a \"\n                       \"temporary error but most likely means that your model has corrupted.\\n\"\n                       \"You can try to load the model again but if the problem persists you \"\n                       \"should use the Restore Tool to restore your model from backup.\\n\"\n                       f\"Original error: {str(err)}\")\n                raise FaceswapError(msg) from err\n            raise err\n\n        logger.info(\"Loaded model from disk: '%s'\", self.filename)\n        return model\n\n    def save(self,\n             is_exit: bool = False,\n             force_save_optimizer: bool = False) -> None:\n        \"\"\" Backup and save the model and state file.\n\n        Parameters\n        ----------\n        is_exit: bool, optional\n            ``True`` if the save request has come from an exit process request otherwise ``False``.\n            Default: ``False``\n        force_save_optimizer: bool, optional\n            ``True`` to force saving the optimizer weights with the model, otherwise ``False``.\n            Default:``False``\n\n        Notes\n        -----\n        The backup function actually backups the model from the previous save iteration rather than\n        the current save iteration. This is not a bug, but protection against long save times, as\n        models can get quite large, so renaming the current model file rather than copying it can\n        save substantial amount of time.\n        \"\"\"\n        logger.debug(\"Backing up and saving models\")\n        print(\"\")  # Insert a new line to avoid spamming the same row as loss output\n        save_averages = self._get_save_averages()\n        if save_averages and self._should_backup(save_averages):\n            self._backup.backup_model(self.filename)\n            self._backup.backup_model(self._plugin.state.filename)\n\n        include_optimizer = (force_save_optimizer or\n                             self._save_optimizer == \"always\" or\n                             (self._save_optimizer == \"exit\" and is_exit))\n\n        try:\n            self._plugin.model.save(self.filename, include_optimizer=include_optimizer)\n        except ValueError as err:\n            if include_optimizer and \"name already exists\" in str(err):\n                logger.warning(\"Due to a bug in older versions of Tensorflow, optimizer state \"\n                               \"cannot be saved for this model.\")\n                self._plugin.model.save(self.filename, include_optimizer=False)\n            else:\n                raise\n\n        self._plugin.state.save()\n\n        msg = \"[Saved optimizer state for Snapshot]\" if force_save_optimizer else \"[Saved model]\"\n        if save_averages:\n            lossmsg = [f\"face_{side}: {avg:.5f}\"\n                       for side, avg in zip((\"a\", \"b\"), save_averages)]\n            msg += f\" - Average loss since last save: {', '.join(lossmsg)}\"\n        logger.info(msg)\n\n    def _get_save_averages(self) -> list[float]:\n        \"\"\" Return the average loss since the last save iteration and reset historical loss \"\"\"\n        logger.debug(\"Getting save averages\")\n        if not all(loss for loss in self._history):\n            logger.debug(\"No loss in history\")\n            retval = []\n        else:\n            retval = [sum(loss) / len(loss) for loss in self._history]\n            self._history = [[], []]  # Reset historical loss\n        logger.debug(\"Average losses since last save: %s\", retval)\n        return retval\n\n    def _should_backup(self, save_averages: list[float]) -> bool:\n        \"\"\" Check whether the loss averages for this save iteration is the lowest that has been\n        seen.\n\n        This protects against model corruption by only backing up the model if both sides have\n        seen a total fall in loss.\n\n        Notes\n        -----\n        This is by no means a perfect system. If the model corrupts at an iteration close\n        to a save iteration, then the averages may still be pushed lower than a previous\n        save average, resulting in backing up a corrupted model.\n\n        Parameters\n        ----------\n        save_averages: list\n            The average loss for each side for this save iteration\n        \"\"\"\n        backup = True\n        for side, loss in zip((\"a\", \"b\"), save_averages):\n            if not self._plugin.state.lowest_avg_loss.get(side, None):\n                logger.debug(\"Set initial save iteration loss average for '%s': %s\", side, loss)\n                self._plugin.state.lowest_avg_loss[side] = loss\n                continue\n            backup = loss < self._plugin.state.lowest_avg_loss[side] if backup else backup\n\n        if backup:  # Update lowest loss values to the state file\n            # pylint:disable=unnecessary-comprehension\n            old_avgs = {key: val for key, val in self._plugin.state.lowest_avg_loss.items()}\n            self._plugin.state.lowest_avg_loss[\"a\"] = save_averages[0]\n            self._plugin.state.lowest_avg_loss[\"b\"] = save_averages[1]\n            logger.debug(\"Updated lowest historical save iteration averages from: %s to: %s\",\n                         old_avgs, self._plugin.state.lowest_avg_loss)\n\n        logger.debug(\"Should backup: %s\", backup)\n        return backup\n\n    def snapshot(self) -> None:\n        \"\"\" Perform a model snapshot.\n\n        Notes\n        -----\n        Snapshot function is called 1 iteration after the model was saved, so that it is built from\n        the latest save, hence iteration being reduced by 1.\n        \"\"\"\n        logger.debug(\"Performing snapshot. Iterations: %s\", self._plugin.iterations)\n        self._backup.snapshot_models(self._plugin.iterations - 1)\n        logger.debug(\"Performed snapshot\")\n\n\nclass Weights():\n    \"\"\" Handling of freezing and loading model weights\n\n    Parameters\n    ----------\n    plugin: :class:`Model`\n        The parent plugin class that owns the IO functions.\n    \"\"\"\n    def __init__(self, plugin: ModelBase) -> None:\n        logger.debug(\"Initializing %s: (plugin: %s)\", self.__class__.__name__, plugin)\n        self._model = plugin.model\n        self._name = plugin.model_name\n        self._do_freeze = plugin._args.freeze_weights\n        self._weights_file = self._check_weights_file(plugin._args.load_weights)\n\n        freeze_layers = plugin.config.get(\"freeze_layers\")  # Standardized config for freezing\n        load_layers = plugin.config.get(\"load_layers\")  # Standardized config for loading\n        self._freeze_layers = freeze_layers if freeze_layers else [\"encoder\"]  # No plugin config\n        self._load_layers = load_layers if load_layers else [\"encoder\"]  # No plugin config\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    @classmethod\n    def _check_weights_file(cls, weights_file: str) -> str | None:\n        \"\"\" Validate that we have a valid path to a .h5 file.\n\n        Parameters\n        ----------\n        weights_file: str\n            The full path to a weights file\n\n        Returns\n        -------\n        str\n            The full path to a weights file\n        \"\"\"\n        if not weights_file:\n            logger.debug(\"No weights file selected.\")\n            return None\n\n        msg = \"\"\n        if not os.path.exists(weights_file):\n            msg = f\"Load weights selected, but the path '{weights_file}' does not exist.\"\n        elif not os.path.splitext(weights_file)[-1].lower() == \".h5\":\n            msg = (f\"Load weights selected, but the path '{weights_file}' is not a valid Keras \"\n                   f\"model (.h5) file.\")\n\n        if msg:\n            msg += \" Please check and try again.\"\n            raise FaceswapError(msg)\n\n        logger.verbose(\"Using weights file: %s\", weights_file)  # type:ignore\n        return weights_file\n\n    def freeze(self) -> None:\n        \"\"\" If freeze has been selected in the cli arguments, then freeze those models indicated\n        in the plugin's configuration. \"\"\"\n        # Blanket unfreeze layers, as checking the value of :attr:`layer.trainable` appears to\n        # return ``True`` even when the weights have been frozen\n        for layer in get_all_sub_models(self._model):\n            layer.trainable = True\n\n        if not self._do_freeze:\n            logger.debug(\"Freeze weights deselected. Not freezing\")\n            return\n\n        for layer in get_all_sub_models(self._model):\n            if layer.name in self._freeze_layers:\n                logger.info(\"Freezing weights for '%s' in model '%s'\", layer.name, self._name)\n                layer.trainable = False\n                self._freeze_layers.remove(layer.name)\n        if self._freeze_layers:\n            logger.warning(\"The following layers were set to be frozen but do not exist in the \"\n                           \"model: %s\", self._freeze_layers)\n\n    def load(self, model_exists: bool) -> None:\n        \"\"\" Load weights for newly created models, or output warning for pre-existing models.\n\n        Parameters\n        ----------\n        model_exists: bool\n            ``True`` if a model pre-exists and is being resumed, ``False`` if this is a new model\n        \"\"\"\n        if not self._weights_file:\n            logger.debug(\"No weights file provided. Not loading weights.\")\n            return\n        if model_exists and self._weights_file:\n            logger.warning(\"Ignoring weights file '%s' as this model is resuming.\",\n                           self._weights_file)\n            return\n\n        weights_models = self._get_weights_model()\n        all_models = get_all_sub_models(self._model)\n\n        for model_name in self._load_layers:\n            sub_model = next((lyr for lyr in all_models if lyr.name == model_name), None)\n            sub_weights = next((lyr for lyr in weights_models if lyr.name == model_name), None)\n\n            if not sub_model or not sub_weights:\n                msg = f\"Skipping layer {model_name} as not in \"\n                msg += \"current_model.\" if not sub_model else f\"weights '{self._weights_file}.'\"\n                logger.warning(msg)\n                continue\n\n            logger.info(\"Loading weights for layer '%s'\", model_name)\n            skipped_ops = 0\n            loaded_ops = 0\n            for layer in sub_model.layers:\n                success = self._load_layer_weights(layer, sub_weights, model_name)\n                if success == 0:\n                    skipped_ops += 1\n                elif success == 1:\n                    loaded_ops += 1\n\n        del weights_models\n\n        if loaded_ops == 0:\n            raise FaceswapError(f\"No weights were succesfully loaded from your weights file: \"\n                                f\"'{self._weights_file}'. Please check and try again.\")\n        if skipped_ops > 0:\n            logger.warning(\"%s weight(s) were unable to be loaded for your model. This is most \"\n                           \"likely because the weights you are trying to load were trained with \"\n                           \"different settings than you have set for your current model.\",\n                           skipped_ops)\n\n    def _get_weights_model(self) -> list[tf.keras.models.Model]:\n        \"\"\" Obtain a list of all sub-models contained within the weights model.\n\n        Returns\n        -------\n        list\n            List of all models contained within the .h5 file\n\n        Raises\n        ------\n        FaceswapError\n            In the event of a failure to load the weights, or the weights belonging to a different\n            model\n        \"\"\"\n        retval = get_all_sub_models(kmodels.load_model(self._weights_file, compile=False))\n        if not retval:\n            raise FaceswapError(f\"Error loading weights file {self._weights_file}.\")\n\n        if retval[0].name != self._name:\n            raise FaceswapError(f\"You are attempting to load weights from a '{retval[0].name}' \"\n                                f\"model into a '{self._name}' model. This is not supported.\")\n        return retval\n\n    def _load_layer_weights(self,\n                            layer: tf.keras.layers.Layer,\n                            sub_weights: tf.keras.layers.Layer,\n                            model_name: str) -> T.Literal[-1, 0, 1]:\n        \"\"\" Load the weights for a single layer.\n\n        Parameters\n        ----------\n        layer: :class:`tensorflow.keras.layers.Layer`\n            The layer to set the weights for\n        sub_weights: list\n            The list of layers in the weights model to load weights from\n        model_name: str\n            The name of the current sub-model that is having it's weights loaded\n\n        Returns\n        -------\n        int\n            `-1` if the layer has no weights to load. `0` if weights loading was unsuccessful. `1`\n            if weights loading was successful\n        \"\"\"\n        old_weights = layer.get_weights()\n        if not old_weights:\n            logger.debug(\"Skipping layer without weights: %s\", layer.name)\n            return -1\n\n        layer_weights = next((lyr for lyr in sub_weights.layers\n                             if lyr.name == layer.name), None)\n        if not layer_weights:\n            logger.warning(\"The weights file '%s' for layer '%s' does not contain weights for \"\n                           \"'%s'. Skipping\", self._weights_file, model_name, layer.name)\n            return 0\n\n        new_weights = layer_weights.get_weights()\n        if old_weights[0].shape != new_weights[0].shape:\n            logger.warning(\"The weights for layer '%s' are of incompatible shapes. Skipping.\",\n                           layer.name)\n            return 0\n        logger.verbose(\"Setting weights for '%s'\", layer.name)  # type:ignore\n        layer.set_weights(layer_weights.get_weights())\n        return 1\n", "plugins/train/model/_base/__init__.py": "#!/usr/bin/env python3\n\"\"\" Base class for Models plugins ALL Models should at least inherit from this class. \"\"\"\n\nfrom .model import get_all_sub_models, ModelBase\n", "plugins/convert/_config.py": "#!/usr/bin/env python3\n\"\"\" Default configurations for convert \"\"\"\n\nimport logging\nimport os\n\nfrom lib.config import FaceswapConfig\n\nlogger = logging.getLogger(__name__)\n\n\nclass Config(FaceswapConfig):\n    \"\"\" Config File for Convert \"\"\"\n\n    def set_defaults(self):\n        \"\"\" Set the default values for config \"\"\"\n        self._defaults_from_plugin(os.path.dirname(__file__))\n", "plugins/convert/__init__.py": "", "plugins/convert/scaling/sharpen_defaults.py": "#!/usr/bin/env python3\n\"\"\"\n    The default options for the faceswap Sharpen Scaling plugin.\n\n    Defaults files should be named <plugin_name>_defaults.py\n    Any items placed into this file will automatically get added to the relevant config .ini files\n    within the faceswap/config folder.\n\n    The following variables should be defined:\n        _HELPTEXT: A string describing what this plugin does\n        _DEFAULTS: A dictionary containing the options, defaults and meta information. The\n                   dictionary should be defined as:\n                       {<option_name>: {<metadata>}}\n\n                   <option_name> should always be lower text.\n                   <metadata> dictionary requirements are listed below.\n\n    The following keys are expected for the _DEFAULTS <metadata> dict:\n        datatype:  [required] A python type class. This limits the type of data that can be\n                   provided in the .ini file and ensures that the value is returned in the\n                   correct type to faceswap. Valid datatypes are: <class 'int'>, <class 'float'>,\n                   <class 'str'>, <class 'bool'>.\n        default:   [required] The default value for this option.\n        info:      [required] A string describing what this option does.\n        choices:   [optional] If this option's datatype is of <class 'str'> then valid\n                   selections can be defined here. This validates the option and also enables\n                   a combobox / radio option in the GUI.\n        gui_radio: [optional] If <choices> are defined, this indicates that the GUI should use\n                   radio buttons rather than a combobox to display this option.\n        min_max:   [partial] For <class 'int'> and <class 'float'> datatypes this is required\n                   otherwise it is ignored. Should be a tuple of min and max accepted values.\n                   This is used for controlling the GUI slider range. Values are not enforced.\n        rounding:  [partial] For <class 'int'> and <class 'float'> datatypes this is\n                   required otherwise it is ignored. Used for the GUI slider. For floats, this\n                   is the number of decimal places to display. For ints this is the step size.\n        fixed:     [optional] [train only]. Training configurations are fixed when the model is\n                   created, and then reloaded from the state file. Marking an item as fixed=False\n                   indicates that this value can be changed for existing models, and will override\n                   the value saved in the state file with the updated value in config. If not\n                   provided this will default to True.\n\"\"\"\n\n\n_HELPTEXT = \"Options for sharpening the face after placement\"\n\n\n_DEFAULTS = dict(\n    method=dict(\n        default=\"none\",\n        info=\"The type of sharpening to use:\"\n             \"\\n\\t none: Don't perform any sharpening.\"\n             \"\\n\\t box: Fastest, but weakest method. Uses a box filter to assess edges.\"\n             \"\\n\\t gaussian: Slower, but better than box. Uses a gaussian filter to assess edges.\"\n             \"\\n\\t unsharp-mask: Slowest, but most tweakable. Uses the unsharp-mask method to \"\n             \"assess edges.\",\n        datatype=str,\n        rounding=None,\n        min_max=None,\n        choices=[\"none\", \"box\", \"gaussian\", \"unsharp_mask\"],\n        gui_radio=True,\n        group=\"sharpen type\",\n        fixed=True,\n    ),\n    amount=dict(\n        default=150,\n        info=\"Percentage that controls the magnitude of each overshoot (how much darker and how \"\n             \"much lighter the edge borders become).\\nThis can also be thought of as how much \"\n             \"contrast is added at the edges. It does not affect the width of the edge rims.\",\n        datatype=int,\n        rounding=1,\n        min_max=(100, 500),\n        choices=[],\n        gui_radio=False,\n        group=\"settings\",\n        fixed=True,\n    ),\n    radius=dict(\n        default=0.3,\n        info=\"Affects the size of the edges to be enhanced or how wide the edge rims become, so a \"\n             \"smaller radius enhances smaller-scale detail.\\nRadius is set as a percentage of the \"\n             \"final frame width and rounded to the nearest pixel. E.g for a 1280 width frame, a \"\n             \"0.6 percenatage will give a radius of 8px.\\nHigher radius values can cause halos at \"\n             \"the edges, a detectable faint light rim around objects. Fine detail needs a smaller \"\n             \"radius. \\nRadius and amount interact; reducing one allows more of the other.\",\n        datatype=float,\n        rounding=1,\n        min_max=(0.1, 5.0),\n        choices=[],\n        gui_radio=False,\n        group=\"settings\",\n        fixed=True,\n    ),\n    threshold=dict(\n        default=5.0,\n        info=\"[unsharp_mask only] Controls the minimal brightness change that will be sharpened \"\n             \"or how far apart adjacent tonal values have to be before the filter does anything.\\n\"\n             \"This lack of action is important to prevent smooth areas from becoming speckled. \"\n             \"The threshold setting can be used to sharpen more pronounced edges, while leaving \"\n             \"subtler edges untouched. \\nLow values should sharpen more because fewer areas are \"\n             \"excluded. \\nHigher threshold values exclude areas of lower contrast.\",\n        datatype=float,\n        rounding=1,\n        min_max=(1.0, 10.0),\n        choices=[],\n        gui_radio=False,\n        group=\"settings\",\n        fixed=True,\n    ),\n)\n", "plugins/convert/scaling/_base.py": "#!/usr/bin/env python3\n\"\"\" Parent class for scaling Adjustments for faceswap.py converter \"\"\"\n\nimport logging\nimport numpy as np\n\nfrom plugins.convert._config import Config\n\nlogger = logging.getLogger(__name__)\n\n\ndef get_config(plugin_name, configfile=None):\n    \"\"\" Return the config for the requested model \"\"\"\n    return Config(plugin_name, configfile=configfile).config_dict\n\n\nclass Adjustment():\n    \"\"\" Parent class for scaling adjustments \"\"\"\n    def __init__(self, configfile=None, config=None):\n        logger.debug(\"Initializing %s: (configfile: %s, config: %s)\",\n                     self.__class__.__name__, configfile, config)\n        self.config = self.set_config(configfile, config)\n        logger.debug(\"config: %s\", self.config)\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    def set_config(self, configfile, config):\n        \"\"\" Set the config to either global config or passed in config \"\"\"\n        section = \".\".join(self.__module__.split(\".\")[-2:])\n        if config is None:\n            logger.debug(\"Loading base config\")\n            retval = get_config(section, configfile=configfile)\n        else:\n            logger.debug(\"Loading passed in config\")\n            config.section = section\n            retval = config.config_dict\n            config.section = None\n        logger.debug(\"Config: %s\", retval)\n        return retval\n\n    def process(self, new_face):\n        \"\"\" Override for specific scaling adjustment process \"\"\"\n        raise NotImplementedError\n\n    def run(self, new_face):\n        \"\"\" Perform selected adjustment on face \"\"\"\n        logger.trace(\"Performing scaling adjustment\")\n        # Remove Mask for processing\n        reinsert_mask = False\n        if new_face.shape[2] == 4:\n            reinsert_mask = True\n            final_mask = new_face[:, :, -1]\n            new_face = new_face[:, :, :3]\n        new_face = self.process(new_face)\n        new_face = np.clip(new_face, 0.0, 1.0)\n        if reinsert_mask and new_face.shape[2] != 4:\n            # Reinsert Mask\n            new_face = np.concatenate((new_face, np.expand_dims(final_mask, axis=-1)), -1)\n        logger.trace(\"Performed scaling adjustment\")\n        return new_face\n", "plugins/convert/scaling/sharpen.py": "#!/usr/bin/env python3\n\"\"\" Sharpening for enlarged face for faceswap.py converter \"\"\"\nimport cv2\nimport numpy as np\n\nfrom ._base import Adjustment, logger\n\n\nclass Scaling(Adjustment):\n    \"\"\" Sharpening Adjustments for the face applied after warp to final frame \"\"\"\n\n    def process(self, new_face):\n        \"\"\" Sharpen using the requested technique \"\"\"\n        amount = self.config[\"amount\"] / 100.0\n        kernel_center = self.get_kernel_size(new_face, self.config[\"radius\"])\n        new_face = getattr(self, self.config[\"method\"])(new_face, kernel_center, amount)\n        return new_face\n\n    @staticmethod\n    def get_kernel_size(new_face, radius_percent):\n        \"\"\" Return the kernel size and central point for the given radius\n            relative to frame width \"\"\"\n        radius = max(1, round(new_face.shape[1] * radius_percent / 100))\n        kernel_size = int((radius * 2) + 1)\n        kernel_size = (kernel_size, kernel_size)\n        logger.trace(kernel_size)\n        return kernel_size, radius\n\n    @staticmethod\n    def box(new_face, kernel_center, amount):\n        \"\"\" Sharpen using box filter \"\"\"\n        kernel_size, center = kernel_center\n        kernel = np.zeros(kernel_size, dtype=\"float32\")\n        kernel[center, center] = 1.0\n        box_filter = np.ones(kernel_size, dtype=\"float32\") / kernel_size[0]**2\n        kernel = kernel + (kernel - box_filter) * amount\n        new_face = cv2.filter2D(new_face, -1, kernel)  # pylint:disable=no-member\n        return new_face\n\n    @staticmethod\n    def gaussian(new_face, kernel_center, amount):\n        \"\"\" Sharpen using gaussian filter \"\"\"\n        kernel_size = kernel_center[0]\n        blur = cv2.GaussianBlur(new_face, kernel_size, 0)  # pylint:disable=no-member\n        new_face = cv2.addWeighted(new_face,  # pylint:disable=no-member\n                                   1.0 + (0.5 * amount),\n                                   blur,\n                                   -(0.5 * amount),\n                                   0)\n        return new_face\n\n    def unsharp_mask(self, new_face, kernel_center, amount):\n        \"\"\" Sharpen using unsharp mask \"\"\"\n        kernel_size = kernel_center[0]\n        threshold = self.config[\"threshold\"] / 255.0\n        blur = cv2.GaussianBlur(new_face, kernel_size, 0)  # pylint:disable=no-member\n        low_contrast_mask = (abs(new_face - blur) < threshold).astype(\"float32\")\n        sharpened = (new_face * (1.0 + amount)) + (blur * -amount)\n        new_face = (new_face * (1.0 - low_contrast_mask)) + (sharpened * low_contrast_mask)\n        return new_face\n", "plugins/convert/scaling/__init__.py": "", "plugins/convert/color/color_transfer.py": "#!/usr/bin/env python3\n\"\"\" Color Transfer adjustment color matching adjustment plugin for faceswap.py converter\n    source: https://github.com/jrosebr1/color_transfer\n    The MIT License (MIT)\n\n    Copyright (c) 2014 Adrian Rosebrock, http://www.pyimagesearch.com\n\n    Permission is hereby granted, free of charge, to any person obtaining a copy\n    of this software and associated documentation files (the \"Software\"), to deal\n    in the Software without restriction, including without limitation the rights\n    to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n    copies of the Software, and to permit persons to whom the Software is\n    furnished to do so, subject to the following conditions:\n\n    The above copyright notice and this permission notice shall be included in\n    all copies or substantial portions of the Software.\n\n    THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n    IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n    FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n    AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n    LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n    OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n    THE SOFTWARE. \"\"\"\n\nimport cv2\nimport numpy as np\nfrom ._base import Adjustment\n\n\nclass Color(Adjustment):\n    \"\"\"\n    Transfers the color distribution from the source to the target\n    image using the mean and standard deviations of the L*a*b*\n    color space.\n\n    This implementation is (loosely) based on to the \"Color Transfer\n    between Images\" paper by Reinhard et al., 2001.\n    \"\"\"\n\n    def process(self, old_face, new_face, raw_mask):\n        \"\"\"\n        Parameters\n        ----------\n        source: NumPy array\n            OpenCV image in BGR color space (the source image)\n        target: NumPy array\n            OpenCV image in BGR color space (the target image)\n        clip: Should components of L*a*b* image be scaled by np.clip before\n            converting back to BGR color space?\n            If False then components will be min-max scaled appropriately.\n            Clipping will keep target image brightness truer to the input.\n            Scaling will adjust image brightness to avoid washed out portions\n            in the resulting color transfer that can be caused by clipping.\n        preserve_paper: Should color transfer strictly follow methodology\n            layed out in original paper? The method does not always produce\n            aesthetically pleasing results.\n            If False then L*a*b* components will scaled using the reciprocal of\n            the scaling factor proposed in the paper.  This method seems to produce\n            more consistently aesthetically pleasing results\n\n        Returns\n        -------\n        transfer: NumPy array\n            OpenCV image (w, h, 3) NumPy array (uint8)\n        \"\"\"\n        clip = self.config.get(\"clip\", True)\n        preserve_paper = self.config.get(\"preserve_paper\", True)\n\n        # convert the images from the RGB to L*ab* color space, being\n        # sure to utilizing the floating point data type (note: OpenCV\n        # expects floats to be 32-bit, so use that instead of 64-bit)\n        source = cv2.cvtColor(  # pylint:disable=no-member\n            np.rint(old_face * raw_mask * 255.0).astype(\"uint8\"),\n            cv2.COLOR_BGR2LAB).astype(\"float32\")  # pylint:disable=no-member\n        target = cv2.cvtColor(  # pylint:disable=no-member\n            np.rint(new_face * raw_mask * 255.0).astype(\"uint8\"),\n            cv2.COLOR_BGR2LAB).astype(\"float32\")  # pylint:disable=no-member\n        # compute color statistics for the source and target images\n        (l_mean_src, l_std_src,\n         a_mean_src, a_std_src,\n         b_mean_src, b_std_src) = self.image_stats(source)\n        (l_mean_tar, l_std_tar,\n         a_mean_tar, a_std_tar,\n         b_mean_tar, b_std_tar) = self.image_stats(target)\n\n        # subtract the means from the target image\n        (light, col_a, col_b) = cv2.split(target)  # pylint:disable=no-member\n        light -= l_mean_tar\n        col_a -= a_mean_tar\n        col_b -= b_mean_tar\n\n        if preserve_paper:\n            # scale by the standard deviations using paper proposed factor\n            light = (l_std_tar / l_std_src) * light\n            col_a = (a_std_tar / a_std_src) * col_a\n            col_b = (b_std_tar / b_std_src) * col_b\n        else:\n            # scale by the standard deviations using reciprocal of paper proposed factor\n            light = (l_std_src / l_std_tar) * light\n            col_a = (a_std_src / a_std_tar) * col_a\n            col_b = (b_std_src / b_std_tar) * col_b\n\n        # add in the source mean\n        light += l_mean_src\n        col_a += a_mean_src\n        col_b += b_mean_src\n\n        # clip/scale the pixel intensities to [0, 255] if they fall\n        # outside this range\n        light = self._scale_array(light, clip=clip)\n        col_a = self._scale_array(col_a, clip=clip)\n        col_b = self._scale_array(col_b, clip=clip)\n\n        # merge the channels together and convert back to the RGB color\n        # space, being sure to utilize the 8-bit unsigned integer data\n        # type\n        transfer = cv2.merge([light, col_a, col_b])  # pylint:disable=no-member\n        transfer = cv2.cvtColor(  # pylint:disable=no-member\n            transfer.astype(\"uint8\"),\n            cv2.COLOR_LAB2BGR).astype(\"float32\") / 255.0  # pylint:disable=no-member\n        background = new_face * (1 - raw_mask)\n        merged = transfer + background\n        # return the color transferred image\n        return merged\n\n    @staticmethod\n    def image_stats(image):\n        \"\"\"\n        Parameters\n        ----------\n\n        image: NumPy array\n            OpenCV image in L*a*b* color space\n\n        Returns\n        -------\n        Tuple of mean and standard deviations for the L*, a*, and b*\n        channels, respectively\n        \"\"\"\n        # compute the mean and standard deviation of each channel\n        (light, col_a, col_b) = cv2.split(image)  # pylint:disable=no-member\n        (l_mean, l_std) = (light.mean(), light.std())\n        (a_mean, a_std) = (col_a.mean(), col_a.std())\n        (b_mean, b_std) = (col_b.mean(), col_b.std())\n\n        # return the color statistics\n        return (l_mean, l_std, a_mean, a_std, b_mean, b_std)\n\n    @staticmethod\n    def _min_max_scale(arr, new_range=(0, 255)):\n        \"\"\"\n        Perform min-max scaling to a NumPy array\n\n        Parameters\n        ----------\n        arr: NumPy array to be scaled to [new_min, new_max] range\n        new_range: tuple of form (min, max) specifying range of\n            transformed array\n\n        Returns\n        -------\n        NumPy array that has been scaled to be in\n        [new_range[0], new_range[1]] range\n        \"\"\"\n        # get array's current min and max\n        arr_min = arr.min()\n        arr_max = arr.max()\n\n        # check if scaling needs to be done to be in new_range\n        if arr_min < new_range[0] or arr_max > new_range[1]:\n            # perform min-max scaling\n            scaled = (new_range[1] - new_range[0]) * (arr - arr_min) / (arr_max -\n                                                                        arr_min) + new_range[0]\n        else:\n            # return array if already in range\n            scaled = arr\n\n        return scaled\n\n    def _scale_array(self, arr, clip=True):\n        \"\"\"\n        Trim NumPy array values to be in [0, 255] range with option of\n        clipping or scaling.\n\n        Parameters\n        ----------\n        arr: array to be trimmed to [0, 255] range\n        clip: should array be scaled by np.clip? if False then input\n            array will be min-max scaled to range\n            [max([arr.min(), 0]), min([arr.max(), 255])]\n\n        Returns\n        -------\n        NumPy array that has been scaled to be in [0, 255] range\n        \"\"\"\n        if clip:\n            scaled = np.clip(arr, 0, 255)\n        else:\n            scale_range = (max([arr.min(), 0]), min([arr.max(), 255]))\n            scaled = self._min_max_scale(arr, new_range=scale_range)\n\n        return scaled\n", "plugins/convert/color/color_transfer_defaults.py": "#!/usr/bin/env python3\n\"\"\"\n    The default options for the faceswap Color_Transfer Color plugin.\n\n    Defaults files should be named <plugin_name>_defaults.py\n    Any items placed into this file will automatically get added to the relevant config .ini files\n    within the faceswap/config folder.\n\n    The following variables should be defined:\n        _HELPTEXT: A string describing what this plugin does\n        _DEFAULTS: A dictionary containing the options, defaults and meta information. The\n                   dictionary should be defined as:\n                       {<option_name>: {<metadata>}}\n\n                   <option_name> should always be lower text.\n                   <metadata> dictionary requirements are listed below.\n\n    The following keys are expected for the _DEFAULTS <metadata> dict:\n        datatype:  [required] A python type class. This limits the type of data that can be\n                   provided in the .ini file and ensures that the value is returned in the\n                   correct type to faceswap. Valid datatypes are: <class 'int'>, <class 'float'>,\n                   <class 'str'>, <class 'bool'>.\n        default:   [required] The default value for this option.\n        info:      [required] A string describing what this option does.\n        choices:   [optional] If this option's datatype is of <class 'str'> then valid\n                   selections can be defined here. This validates the option and also enables\n                   a combobox / radio option in the GUI.\n        gui_radio: [optional] If <choices> are defined, this indicates that the GUI should use\n                   radio buttons rather than a combobox to display this option.\n        min_max:   [partial] For <class 'int'> and <class 'float'> datatypes this is required\n                   otherwise it is ignored. Should be a tuple of min and max accepted values.\n                   This is used for controlling the GUI slider range. Values are not enforced.\n        rounding:  [partial] For <class 'int'> and <class 'float'> datatypes this is\n                   required otherwise it is ignored. Used for the GUI slider. For floats, this\n                   is the number of decimal places to display. For ints this is the step size.\n        fixed:     [optional] [train only]. Training configurations are fixed when the model is\n                   created, and then reloaded from the state file. Marking an item as fixed=False\n                   indicates that this value can be changed for existing models, and will override\n                   the value saved in the state file with the updated value in config. If not\n                   provided this will default to True.\n\"\"\"\n\n\n_HELPTEXT = (\n    \"Options for transfering the color distribution from the source to the target image using the \"\n    \"mean and standard deviations of the L*a*b* color space.\\nThis implementation is (loosely) \"\n    \"based on the 'Color Transfer between Images' paper by Reinhard et al., 2001. matching the \"\n    \"histograms between the source and destination faces.\"\n)\n\n\n_DEFAULTS = dict(\n    clip=dict(\n        default=True,\n        info=\"Should components of L*a*b* image be scaled by np.clip before converting back to \"\n             \"BGR color space?\\nIf False then components will be min-max scaled appropriately.\\n\"\n             \"Clipping will keep target image brightness truer to the input.\\nScaling will adjust \"\n             \"image brightness to avoid washed out portions in the resulting color transfer that \"\n             \"can be caused by clipping.\",\n        datatype=bool,\n        group=\"method\",\n        rounding=None,\n        min_max=None,\n        choices=[],\n        gui_radio=False,\n        fixed=True,\n    ),\n    preserve_paper=dict(\n        default=True,\n        info=\"Should color transfer strictly follow methodology layed out in original paper?\\nThe \"\n             \"method does not always produce aesthetically pleasing results.\\nIf False then \"\n             \"L*a*b* components will be scaled using the reciprocal of the scaling factor \"\n             \"proposed in the paper. This method seems to produce more consistently aesthetically \"\n             \"pleasing results.\",\n        datatype=bool,\n        group=\"method\",\n        rounding=None,\n        min_max=None,\n        choices=[],\n        gui_radio=False,\n        fixed=True,\n    ),\n)\n", "plugins/convert/color/match_hist.py": "#!/usr/bin/env python3\n\"\"\" Match histogram colour adjustment color matching adjustment plugin\n    for faceswap.py converter \"\"\"\n\nimport numpy as np\nfrom ._base import Adjustment\n\n\nclass Color(Adjustment):\n    \"\"\" Match the histogram of the color intensity of each channel \"\"\"\n\n    def process(self, old_face, new_face, raw_mask):\n        mask_indices = np.nonzero(raw_mask.squeeze())\n        new_face = [self.hist_match(old_face[:, :, c],\n                                    new_face[:, :, c],\n                                    mask_indices,\n                                    self.config[\"threshold\"] / 100)\n                    for c in range(3)]\n        new_face = np.stack(new_face, axis=-1)\n        return new_face\n\n    @staticmethod\n    def hist_match(old_channel, new_channel, mask_indices, threshold):\n        \"\"\"  Construct the histogram of the color intensity of a channel\n             for the swap and the original. Match the histogram of the original\n             by interpolation\n        \"\"\"\n        if mask_indices[0].size == 0:\n            return new_channel\n\n        old_masked = old_channel[mask_indices]\n        new_masked = new_channel[mask_indices]\n        _, bin_idx, s_counts = np.unique(new_masked, return_inverse=True, return_counts=True)\n        t_values, t_counts = np.unique(old_masked, return_counts=True)\n        s_quants = np.cumsum(s_counts, dtype='float32')\n        t_quants = np.cumsum(t_counts, dtype='float32')\n        s_quants = threshold * s_quants / s_quants[-1]  # cdf\n        t_quants /= t_quants[-1]  # cdf\n        interp_s_values = np.interp(s_quants, t_quants, t_values)\n        new_channel[mask_indices] = interp_s_values[bin_idx]\n        return new_channel\n", "plugins/convert/color/match_hist_defaults.py": "#!/usr/bin/env python3\n\"\"\"\n    The default options for the faceswap Match_Hist Color plugin.\n\n    Defaults files should be named <plugin_name>_defaults.py\n    Any items placed into this file will automatically get added to the relevant config .ini files\n    within the faceswap/config folder.\n\n    The following variables should be defined:\n        _HELPTEXT: A string describing what this plugin does\n        _DEFAULTS: A dictionary containing the options, defaults and meta information. The\n                   dictionary should be defined as:\n                       {<option_name>: {<metadata>}}\n\n                   <option_name> should always be lower text.\n                   <metadata> dictionary requirements are listed below.\n\n    The following keys are expected for the _DEFAULTS <metadata> dict:\n        datatype:  [required] A python type class. This limits the type of data that can be\n                   provided in the .ini file and ensures that the value is returned in the\n                   correct type to faceswap. Valid datatypes are: <class 'int'>, <class 'float'>,\n                   <class 'str'>, <class 'bool'>.\n        default:   [required] The default value for this option.\n        info:      [required] A string describing what this option does.\n        choices:   [optional] If this option's datatype is of <class 'str'> then valid\n                   selections can be defined here. This validates the option and also enables\n                   a combobox / radio option in the GUI.\n        gui_radio: [optional] If <choices> are defined, this indicates that the GUI should use\n                   radio buttons rather than a combobox to display this option.\n        min_max:   [partial] For <class 'int'> and <class 'float'> datatypes this is required\n                   otherwise it is ignored. Should be a tuple of min and max accepted values.\n                   This is used for controlling the GUI slider range. Values are not enforced.\n        rounding:  [partial] For <class 'int'> and <class 'float'> datatypes this is\n                   required otherwise it is ignored. Used for the GUI slider. For floats, this\n                   is the number of decimal places to display. For ints this is the step size.\n        fixed:     [optional] [train only]. Training configurations are fixed when the model is\n                   created, and then reloaded from the state file. Marking an item as fixed=False\n                   indicates that this value can be changed for existing models, and will override\n                   the value saved in the state file with the updated value in config. If not\n                   provided this will default to True.\n\"\"\"\n\n\n_HELPTEXT = \"Options for matching the histograms between the source and destination faces\"\n\n\n_DEFAULTS = dict(\n    threshold=dict(\n        default=99.0,\n        info=\"Adjust the threshold for histogram matching. Can reduce extreme colors leaking in \"\n             \"by filtering out colors at the extreme ends of the histogram spectrum.\",\n        datatype=float,\n        rounding=1,\n        min_max=(90.0, 100.0),\n        choices=[],\n        gui_radio=False,\n        group=\"settings\",\n        fixed=True,\n    )\n)\n", "plugins/convert/color/seamless_clone.py": "#!/usr/bin/env python3\n\"\"\" Seamless clone adjustment plugin for faceswap.py converter\n    NB: This probably isn't the best place for this, but it is independent of\n        color adjustments and does not have a natural home, so here for now\n        and called as an extra plugin from lib/convert.py\n\"\"\"\n\nimport cv2\nimport numpy as np\nfrom ._base import Adjustment\n\n\nclass Color(Adjustment):\n    \"\"\" Seamless clone the swapped face into the old face with cv2\n        NB: This probably isn't the best place for this, but it doesn't work well and\n        and does not have a natural home, so here for now.\n    \"\"\"\n\n    def process(self, old_face, new_face, raw_mask):\n        height, width, _ = old_face.shape\n        height = height // 2\n        width = width // 2\n\n        y_indices, x_indices, _ = np.nonzero(raw_mask)\n        y_crop = slice(np.min(y_indices), np.max(y_indices))\n        x_crop = slice(np.min(x_indices), np.max(x_indices))\n        y_center = int(np.rint((np.max(y_indices) + np.min(y_indices)) / 2 + height))\n        x_center = int(np.rint((np.max(x_indices) + np.min(x_indices)) / 2 + width))\n\n        insertion = np.rint(new_face[y_crop, x_crop] * 255.0).astype(\"uint8\")\n        insertion_mask = np.rint(raw_mask[y_crop, x_crop] * 255.0).astype(\"uint8\")\n        insertion_mask[insertion_mask != 0] = 255\n        prior = np.rint(np.pad(old_face * 255.0,\n                               ((height, height), (width, width), (0, 0)),\n                               'constant')).astype(\"uint8\")\n\n        blended = cv2.seamlessClone(insertion,  # pylint:disable=no-member\n                                    prior,\n                                    insertion_mask,\n                                    (x_center, y_center),\n                                    cv2.NORMAL_CLONE)  # pylint:disable=no-member\n        blended = blended[height:-height, width:-width]\n\n        return blended.astype(\"float32\") / 255.0\n", "plugins/convert/color/_base.py": "#!/usr/bin/env python3\n\"\"\" Parent class for color Adjustments for faceswap.py converter \"\"\"\n\nimport logging\nimport numpy as np\n\nfrom plugins.convert._config import Config\n\nlogger = logging.getLogger(__name__)\n\n\ndef get_config(plugin_name, configfile=None):\n    \"\"\" Return the config for the requested model \"\"\"\n    return Config(plugin_name, configfile=configfile).config_dict\n\n\nclass Adjustment():\n    \"\"\" Parent class for adjustments \"\"\"\n    def __init__(self, configfile=None, config=None):\n        logger.debug(\"Initializing %s: (configfile: %s, config: %s)\",\n                     self.__class__.__name__, configfile, config)\n        self.config = self.set_config(configfile, config)\n        logger.debug(\"config: %s\", self.config)\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    def set_config(self, configfile, config):\n        \"\"\" Set the config to either global config or passed in config \"\"\"\n        section = \".\".join(self.__module__.split(\".\")[-2:])\n        if config is None:\n            retval = get_config(section, configfile)\n        else:\n            config.section = section\n            retval = config.config_dict\n            config.section = None\n        logger.debug(\"Config: %s\", retval)\n        return retval\n\n    def process(self, old_face, new_face, raw_mask):\n        \"\"\" Override for specific color adjustment process \"\"\"\n        raise NotImplementedError\n\n    def run(self, old_face, new_face, raw_mask):\n        \"\"\" Perform selected adjustment on face \"\"\"\n        logger.trace(\"Performing color adjustment\")\n        # Remove Mask for processing\n        reinsert_mask = False\n        if new_face.shape[2] == 4:\n            reinsert_mask = True\n            final_mask = new_face[:, :, -1]\n            new_face = new_face[:, :, :3]\n        new_face = self.process(old_face, new_face, raw_mask)\n        new_face = np.clip(new_face, 0.0, 1.0)\n        if reinsert_mask and new_face.shape[2] != 4:\n            # Reinsert Mask\n            new_face = np.concatenate((new_face, np.expand_dims(final_mask, axis=-1)), -1)\n        logger.trace(\"Performed color adjustment\")\n        return new_face\n", "plugins/convert/color/manual_balance_defaults.py": "#!/usr/bin/env python3\n\"\"\"\n    The default options for the faceswap Manual_Balance Color plugin.\n\n    Defaults files should be named <plugin_name>_defaults.py\n    Any items placed into this file will automatically get added to the relevant config .ini files\n    within the faceswap/config folder.\n\n    The following variables should be defined:\n        _HELPTEXT: A string describing what this plugin does\n        _DEFAULTS: A dictionary containing the options, defaults and meta information. The\n                   dictionary should be defined as:\n                       {<option_name>: {<metadata>}}\n\n                   <option_name> should always be lower text.\n                   <metadata> dictionary requirements are listed below.\n\n    The following keys are expected for the _DEFAULTS <metadata> dict:\n        datatype:  [required] A python type class. This limits the type of data that can be\n                   provided in the .ini file and ensures that the value is returned in the\n                   correct type to faceswap. Valid datatypes are: <class 'int'>, <class 'float'>,\n                   <class 'str'>, <class 'bool'>.\n        default:   [required] The default value for this option.\n        info:      [required] A string describing what this option does.\n        choices:   [optional] If this option's datatype is of <class 'str'> then valid\n                   selections can be defined here. This validates the option and also enables\n                   a combobox / radio option in the GUI.\n        gui_radio: [optional] If <choices> are defined, this indicates that the GUI should use\n                   radio buttons rather than a combobox to display this option.\n        min_max:   [partial] For <class 'int'> and <class 'float'> datatypes this is required\n                   otherwise it is ignored. Should be a tuple of min and max accepted values.\n                   This is used for controlling the GUI slider range. Values are not enforced.\n        rounding:  [partial] For <class 'int'> and <class 'float'> datatypes this is\n                   required otherwise it is ignored. Used for the GUI slider. For floats, this\n                   is the number of decimal places to display. For ints this is the step size.\n        fixed:     [optional] [train only]. Training configurations are fixed when the model is\n                   created, and then reloaded from the state file. Marking an item as fixed=False\n                   indicates that this value can be changed for existing models, and will override\n                   the value saved in the state file with the updated value in config. If not\n                   provided this will default to True.\n\"\"\"\n\n\n_HELPTEXT = \"Options for manually altering the balance of colors of the swapped face\"\n\n\n_DEFAULTS = {\n    \"colorspace\": {\n        \"default\": \"HSV\",\n        \"info\": \"The colorspace to use for adjustment: The three adjustment sliders will \"\n                \"effect the image differently depending on which colorspace is selected:\"\n                \"\\n\\t RGB: Red, Green, Blue. An additive colorspace where colors are obtained \"\n                \"by a linear combination of Red, Green, and Blue values. The three channels \"\n                \"are correlated by the amount of light hitting the surface. In RGB color \"\n                \"space the color information is separated into three channels but the same \"\n                \"three channels also encode brightness information.\"\n                \"\\n\\t HSV: Hue, Saturation, Value. Hue - Dominant wavelength. Saturation - \"\n                \"Purity / shades of color. Value - Intensity. Best thing is that it uses only \"\n                \"one channel to describe color (H), making it very intuitive to specify color.\"\n                \"\\n\\t LAB: Lightness, A, B. Lightness - Intensity. A - Color range from green \"\n                \"to magenta. B - Color range from blue to yellow. The L channel is \"\n                \"independent of color information and encodes brightness only. The other two \"\n                \"channels encode color.\"\n                \"\\n\\t YCrCb: Y - Luminance or Luma component obtained from RGB after gamma \"\n                \"correction. Cr - how far is the red component from Luma. Cb - how far is the \"\n                \"blue component from Luma. Separates the luminance and chrominance components \"\n                \"into different channels.\",\n        \"datatype\": str,\n        \"rounding\": None,\n        \"min_max\": None,\n        \"group\": \"color balance\",\n        \"choices\": [\"RGB\", \"HSV\", \"LAB\", \"YCrCb\"],\n        \"gui_radio\": True,\n        \"fixed\": True,\n    },\n    \"balance_1\": {\n        \"default\": 0.0,\n        \"info\": \"Balance of channel 1:\"\n                \"\\n\\tRGB: Red\"\n                \"\\n\\tHSV: Hue\"\n                \"\\n\\tLAB: Lightness\"\n                \"\\n\\tYCrCb: Luma\",\n        \"datatype\": float,\n        \"rounding\": 1,\n        \"min_max\": (-100.0, 100.0),\n        \"choices\": [],\n        \"group\": \"color balance\",\n        \"gui_radio\": False,\n        \"fixed\": True,\n    },\n    \"balance_2\": {\n        \"default\": 0.0,\n        \"info\": \"Balance of channel 2:\"\n                \"\\n\\tRGB: Green\"\n                \"\\n\\tHSV: Saturation\"\n                \"\\n\\tLAB: Green > Magenta\"\n                \"\\n\\tYCrCb: Distance of red from Luma\",\n        \"datatype\": float,\n        \"rounding\": 1,\n        \"min_max\": (-100.0, 100.0),\n        \"choices\": [],\n        \"gui_radio\": False,\n        \"group\": \"color balance\",\n        \"fixed\": True,\n    },\n    \"balance_3\": {\n        \"default\": 0.0,\n        \"info\": \"Balance of channel 3:\"\n                \"\\n\\tRGB: Blue\"\n                \"\\n\\tHSV: Intensity\"\n                \"\\n\\tLAB: Blue > Yellow\"\n                \"\\n\\tYCrCb: Distance of blue from Luma\",\n        \"datatype\": float,\n        \"rounding\": 1,\n        \"min_max\": (-100.0, 100.0),\n        \"choices\": [],\n        \"gui_radio\": False,\n        \"group\": \"color balance\",\n        \"fixed\": True,\n    },\n    \"contrast\": {\n        \"default\": 0.0,\n        \"info\": \"Amount of contrast applied.\",\n        \"datatype\": float,\n        \"rounding\": 1,\n        \"min_max\": (-100.0, 100.0),\n        \"choices\": [],\n        \"gui_radio\": False,\n        \"group\": \"brightness contrast\",\n        \"fixed\": True,\n    },\n    \"brightness\": {\n        \"default\": 0.0,\n        \"info\": \"Amount of brighness applied.\",\n        \"datatype\": float,\n        \"rounding\": 1,\n        \"min_max\": (-100.0, 100.0),\n        \"choices\": [],\n        \"gui_radio\": False,\n        \"group\": \"brightness contrast\",\n        \"fixed\": True,\n    },\n}\n", "plugins/convert/color/__init__.py": "", "plugins/convert/color/manual_balance.py": "#!/usr/bin/env python3\n\"\"\" Manual Balance colour adjustment plugin for faceswap.py converter \"\"\"\n\nimport cv2\nimport numpy as np\nfrom ._base import Adjustment\n\n\nclass Color(Adjustment):\n    \"\"\" Adjust the mean of the color channels to be the same for the swap and old frame \"\"\"\n\n    def process(self, old_face, new_face, raw_mask):\n        image = self.convert_colorspace(new_face * 255.0)\n        adjustment = np.array([self.config[\"balance_1\"] / 100.0,\n                               self.config[\"balance_2\"] / 100.0,\n                               self.config[\"balance_3\"] / 100.0]).astype(\"float32\")\n        for idx in range(3):\n            if adjustment[idx] >= 0:\n                image[:, :, idx] = ((1 - image[:, :, idx]) * adjustment[idx]) + image[:, :, idx]\n            else:\n                image[:, :, idx] = image[:, :, idx] * (1 + adjustment[idx])\n\n        image = self.convert_colorspace(image * 255.0, to_bgr=True)\n        image = self.adjust_contrast(image)\n        return image\n\n    def adjust_contrast(self, image):\n        \"\"\"\n        Adjust image contrast and brightness.\n        \"\"\"\n        contrast = max(-126, int(round(self.config[\"contrast\"] * 1.27)))\n        brightness = max(-126, int(round(self.config[\"brightness\"] * 1.27)))\n\n        if not contrast and not brightness:\n            return image\n\n        image = np.rint(image * 255.0).astype(\"uint8\")\n        image = np.clip(image * (contrast/127+1) - contrast + brightness, 0, 255)\n        image = np.clip(np.divide(image, 255, dtype=np.float32), .0, 1.0)\n        return image\n\n    def convert_colorspace(self, new_face, to_bgr=False):\n        \"\"\" Convert colorspace based on mode or back to bgr \"\"\"\n        mode = self.config[\"colorspace\"].lower()\n        colorspace = \"YCrCb\" if mode == \"ycrcb\" else mode.upper()\n        conversion = f\"{colorspace}2BGR\" if to_bgr else f\"BGR2{colorspace}\"\n        image = cv2.cvtColor(new_face.astype(\"uint8\"),  # pylint:disable=no-member\n                             getattr(cv2, f\"COLOR_{conversion}\")).astype(\"float32\") / 255.0\n        return image\n", "plugins/convert/color/avg_color.py": "#!/usr/bin/env python3\n\"\"\" Average colour adjustment color matching adjustment plugin for faceswap.py converter \"\"\"\n\nimport numpy as np\nfrom ._base import Adjustment\n\n\nclass Color(Adjustment):\n    \"\"\" Adjust the mean of the color channels to be the same for the swap and old frame \"\"\"\n\n    def process(self,\n                old_face: np.ndarray,\n                new_face: np.ndarray,\n                raw_mask: np.ndarray) -> np.ndarray:\n        \"\"\" Adjust the mean of the original face and the new face to be the same\n\n        Parameters\n        ----------\n        old_face: :class:`numpy.ndarray`\n            The original face\n        new_face: :class:`numpy.ndarray`\n            The Faceswap generated face\n        raw_mask: :class:`numpy.ndarray`\n            A raw mask for including the face area only\n\n        Returns\n        -------\n        :class:`numpy.ndarray`\n            The adjusted face patch\n        \"\"\"\n        for _ in [0, 1]:\n            diff = old_face - new_face\n            if np.any(raw_mask):\n                avg_diff = np.sum(diff * raw_mask, axis=(0, 1))\n                adjustment = avg_diff / np.sum(raw_mask, axis=(0, 1))\n            else:\n                adjustment = diff\n            new_face += adjustment\n        return new_face\n", "plugins/convert/writer/opencv.py": "#!/usr/bin/env python3\n\"\"\" Image output writer for faceswap.py converter\n    Uses cv2 for writing as in testing this was a lot faster than both Pillow and ImageIO\n\"\"\"\nimport cv2\nimport numpy as np\n\nfrom ._base import Output, logger\n\n\nclass Writer(Output):\n    \"\"\" Images output writer using cv2\n\n    Parameters\n    ----------\n    output_folder: str\n        The full path to the output folder where the converted media should be saved\n    configfile: str, optional\n        The full path to a custom configuration ini file. If ``None`` is passed\n        then the file is loaded from the default location. Default: ``None``.\n    \"\"\"\n    def __init__(self, output_folder: str, **kwargs) -> None:\n        super().__init__(output_folder, **kwargs)\n        self._extension = f\".{self.config['format']}\"\n        self._check_transparency_format()\n        self._separate_mask = self.config[\"draw_transparent\"] and self.config[\"separate_mask\"]\n        self._args = self._get_save_args()\n\n    def _check_transparency_format(self) -> None:\n        \"\"\" Make sure that the output format is correct if draw_transparent is selected \"\"\"\n        transparent = self.config[\"draw_transparent\"]\n        if not transparent or (transparent and self.config[\"format\"] == \"png\"):\n            return\n        logger.warning(\"Draw Transparent selected, but the requested format does not support \"\n                       \"transparency. Changing output format to 'png'\")\n        self.config[\"format\"] = \"png\"\n\n    def _get_save_args(self) -> tuple[int, ...]:\n        \"\"\" Obtain the save parameters for the file format.\n\n        Returns\n        -------\n        tuple\n            The OpenCV specific arguments for the selected file format\n         \"\"\"\n        filetype = self.config[\"format\"]\n        args: tuple[int, ...] = tuple()\n        if filetype == \"jpg\" and self.config[\"jpg_quality\"] > 0:\n            args = (cv2.IMWRITE_JPEG_QUALITY,\n                    self.config[\"jpg_quality\"])\n        if filetype == \"png\" and self.config[\"png_compress_level\"] > -1:\n            args = (cv2.IMWRITE_PNG_COMPRESSION,\n                    self.config[\"png_compress_level\"])\n        logger.debug(args)\n        return args\n\n    def write(self, filename: str, image: list[bytes]) -> None:\n        \"\"\" Write out the pre-encoded image to disk. If separate mask has been selected, write out\n        the encoded mask to a sub-folder in the output directory.\n\n        Parameters\n        ----------\n        filename: str\n            The full path to write out the image to.\n        image: list\n            List of :class:`bytes` objects of length 1 (containing just the image to write out)\n            or length 2 (containing the image and mask to write out)\n        \"\"\"\n        logger.trace(\"Outputting: (filename: '%s'\", filename)  # type:ignore\n        filenames = self.output_filename(filename, self._separate_mask)\n        for fname, img in zip(filenames, image):\n            try:\n                with open(fname, \"wb\") as outfile:\n                    outfile.write(img)\n            except Exception as err:  # pylint:disable=broad-except\n                logger.error(\"Failed to save image '%s'. Original Error: %s\", filename, err)\n\n    def pre_encode(self, image: np.ndarray, **kwargs) -> list[bytes]:\n        \"\"\" Pre_encode the image in lib/convert.py threads as it is a LOT quicker.\n\n        Parameters\n        ----------\n        image: :class:`numpy.ndarray`\n            A 3 or 4 channel BGR swapped frame\n\n        Returns\n        -------\n        list\n            List of :class:`bytes` objects ready for writing. The list will be of length 1 with\n            image bytes object as the only member unless separate mask has been requested, in which\n            case it will be length 2 with the image in position 0 and mask in position 1\n         \"\"\"\n        logger.trace(\"Pre-encoding image\")  # type:ignore\n        retval = []\n\n        if self._separate_mask:\n            mask = image[..., -1]\n            image = image[..., :3]\n\n            retval.append(cv2.imencode(self._extension,\n                                       mask,\n                                       self._args)[1])\n\n        retval.insert(0, cv2.imencode(self._extension,\n                                      image,\n                                      self._args)[1])\n        return retval\n\n    def close(self) -> None:\n        \"\"\" Does nothing as OpenCV writer does not need a close method \"\"\"\n        return\n", "plugins/convert/writer/ffmpeg_defaults.py": "#!/usr/bin/env python3\n\"\"\"\n    The default options for the faceswap Ffmpeg Writer plugin.\n\n    Defaults files should be named <plugin_name>_defaults.py\n    Any items placed into this file will automatically get added to the relevant config .ini files\n    within the faceswap/config folder.\n\n    The following variables should be defined:\n        _HELPTEXT: A string describing what this plugin does\n        _DEFAULTS: A dictionary containing the options, defaults and meta information. The\n                   dictionary should be defined as:\n                       {<option_name>: {<metadata>}}\n\n                   <option_name> should always be lower text.\n                   <metadata> dictionary requirements are listed below.\n\n    The following keys are expected for the _DEFAULTS <metadata> dict:\n        datatype:  [required] A python type class. This limits the type of data that can be\n                   provided in the .ini file and ensures that the value is returned in the\n                   correct type to faceswap. Valid datatypes are: <class 'int'>, <class 'float'>,\n                   <class 'str'>, <class 'bool'>.\n        default:   [required] The default value for this option.\n        info:      [required] A string describing what this option does.\n        choices:   [optional] If this option's datatype is of <class 'str'> then valid\n                   selections can be defined here. This validates the option and also enables\n                   a combobox / radio option in the GUI.\n        gui_radio: [optional] If <choices> are defined, this indicates that the GUI should use\n                   radio buttons rather than a combobox to display this option.\n        min_max:   [partial] For <class 'int'> and <class 'float'> datatypes this is required\n                   otherwise it is ignored. Should be a tuple of min and max accepted values.\n                   This is used for controlling the GUI slider range. Values are not enforced.\n        rounding:  [partial] For <class 'int'> and <class 'float'> datatypes this is\n                   required otherwise it is ignored. Used for the GUI slider. For floats, this\n                   is the number of decimal places to display. For ints this is the step size.\n        fixed:     [optional] [train only]. Training configurations are fixed when the model is\n                   created, and then reloaded from the state file. Marking an item as fixed=False\n                   indicates that this value can be changed for existing models, and will override\n                   the value saved in the state file with the updated value in config. If not\n                   provided this will default to True.\n\"\"\"\n\n\n_HELPTEXT = \"Options for encoding converted frames to video.\"\n\n\n_DEFAULTS = dict(\n    container=dict(\n        default=\"mp4\",\n        info=\"Video container to use.\",\n        datatype=str,\n        rounding=None,\n        min_max=None,\n        choices=[\"avi\", \"flv\", \"mkv\", \"mov\", \"mp4\", \"mpeg\", \"webm\"],\n        group=\"codec\",\n        gui_radio=True,\n    ),\n    codec=dict(\n        default=\"libx264\",\n        info=\"Video codec to use:\"\n             \"\\n\\t libx264: H.264. A widely supported and commonly used codec.\"\n             \"\\n\\t libx265: H.265 / HEVC video encoder application library.\",\n        datatype=str,\n        rounding=None,\n        min_max=None,\n        choices=[\"libx264\", \"libx265\"],\n        group=\"codec\",\n        gui_radio=True,\n    ),\n    crf=dict(\n        default=23,\n        info=\"Constant Rate Factor:  0 is lossless and 51 is worst quality possible. A \"\n             \"lower value generally leads to higher quality, and a subjectively sane range \"\n             \"is 17-28. Consider 17 or 18 to be visually lossless or nearly so; it should \"\n             \"look the same or nearly the same as the input but it isn't technically \"\n             \"lossless.\\nThe range is exponential, so increasing the CRF value +6 results \"\n             \"in roughly half the bitrate / file size, while -6 leads to roughly twice the \"\n             \"bitrate.\",\n        datatype=int,\n        rounding=1,\n        min_max=(0, 51),\n        choices=[],\n        gui_radio=False,\n        group=\"quality\",\n    ),\n    preset=dict(\n        default=\"medium\",\n        info=\"A preset is a collection of options that will provide a certain encoding \"\n             \"speed to compression ratio.\\nA slower preset will provide better compression \"\n             \"(compression is quality per filesize).\\nUse the slowest preset that you have \"\n             \"patience for.\",\n        datatype=str,\n        rounding=None,\n        min_max=None,\n        choices=[\"ultrafast\", \"superfast\", \"veryfast\", \"faster\", \"fast\", \"medium\", \"slow\",\n                 \"slower\", \"veryslow\"],\n        gui_radio=True,\n        group=\"quality\",\n    ),\n    tune=dict(\n        default=\"none\",\n        info=\"Change settings based upon the specifics of your input:\"\n             \"\\n\\t none: Don't perform any additional tuning.\"\n             \"\\n\\t film: [H.264 only] Use for high quality movie content; lowers deblocking.\"\n             \"\\n\\t animation: [H.264 only] Good for cartoons; uses higher deblocking and more \"\n             \"reference frames.\"\n             \"\\n\\t grain: Preserves the grain structure in old, grainy film material.\"\n             \"\\n\\t stillimage: [H.264 only] Good for slideshow-like content.\"\n             \"\\n\\t fastdecode: Allows faster decoding by disabling certain filters.\"\n             \"\\n\\t zerolatency: Good for fast encoding and low-latency streaming.\",\n        datatype=str,\n        rounding=None,\n        min_max=None,\n        choices=[\"none\", \"film\", \"animation\", \"grain\", \"stillimage\", \"fastdecode\", \"zerolatency\"],\n        gui_radio=False,\n        group=\"settings\",\n    ),\n    profile=dict(\n        default=\"auto\",\n        info=\"[H.264 Only] Limit the output to a specific H.264 profile. Don't change this \"\n             \"unless your target device only supports a certain profile.\",\n        datatype=str,\n        rounding=None,\n        min_max=None,\n        choices=[\"auto\", \"baseline\", \"main\", \"high\", \"high10\", \"high422\", \"high444\"],\n        gui_radio=False,\n        group=\"settings\",\n    ),\n    level=dict(\n        default=\"auto\",\n        info=\"[H.264 Only] Set the encoder level, Don't change this unless your target \"\n             \"device only supports a certain level.\",\n        datatype=str,\n        rounding=None,\n        min_max=None,\n        choices=[\"auto\", \"1\", \"1b\", \"1.1\", \"1.2\", \"1.3\", \"2\", \"2.1\", \"2.2\", \"3\", \"3.1\", \"3.2\", \"4\",\n                 \"4.1\", \"4.2\", \"5\", \"5.1\", \"5.2\", \"6\", \"6.1\", \"6.2\"],\n        gui_radio=False,\n        group=\"settings\",\n    ),\n    skip_mux=dict(\n        default=False,\n        info=\"Skip muxing audio to the final video output. This will result in a video without an \"\n             \"audio track.\",\n        datatype=bool,\n        group=\"settings\",\n    ),\n)\n", "plugins/convert/writer/opencv_defaults.py": "#!/usr/bin/env python3\n\"\"\"\n    The default options for the faceswap Opencv Writer plugin.\n\n    Defaults files should be named <plugin_name>_defaults.py\n    Any items placed into this file will automatically get added to the relevant config .ini files\n    within the faceswap/config folder.\n\n    The following variables should be defined:\n        _HELPTEXT: A string describing what this plugin does\n        _DEFAULTS: A dictionary containing the options, defaults and meta information. The\n                   dictionary should be defined as:\n                       {<option_name>: {<metadata>}}\n\n                   <option_name> should always be lower text.\n                   <metadata> dictionary requirements are listed below.\n\n    The following keys are expected for the _DEFAULTS <metadata> dict:\n        datatype:  [required] A python type class. This limits the type of data that can be\n                   provided in the .ini file and ensures that the value is returned in the\n                   correct type to faceswap. Valid datatypes are: <class 'int'>, <class 'float'>,\n                   <class 'str'>, <class 'bool'>.\n        default:   [required] The default value for this option.\n        info:      [required] A string describing what this option does.\n        choices:   [optional] If this option's datatype is of <class 'str'> then valid\n                   selections can be defined here. This validates the option and also enables\n                   a combobox / radio option in the GUI.\n        gui_radio: [optional] If <choices> are defined, this indicates that the GUI should use\n                   radio buttons rather than a combobox to display this option.\n        min_max:   [partial] For <class 'int'> and <class 'float'> datatypes this is required\n                   otherwise it is ignored. Should be a tuple of min and max accepted values.\n                   This is used for controlling the GUI slider range. Values are not enforced.\n        rounding:  [partial] For <class 'int'> and <class 'float'> datatypes this is\n                   required otherwise it is ignored. Used for the GUI slider. For floats, this\n                   is the number of decimal places to display. For ints this is the step size.\n        fixed:     [optional] [train only]. Training configurations are fixed when the model is\n                   created, and then reloaded from the state file. Marking an item as fixed=False\n                   indicates that this value can be changed for existing models, and will override\n                   the value saved in the state file with the updated value in config. If not\n                   provided this will default to True.\n\"\"\"\n\n\n_HELPTEXT = (\n    \"Options for outputting converted frames to a series of images using OpenCV\\n\"\n    \"OpenCV can be faster than other image writers, but lacks some configuration \"\n    \"options and formats.\"\n)\n\n\n_DEFAULTS = dict(\n    format=dict(\n        default=\"png\",\n        info=\"Image format to use:\"\n             \"\\n\\t bmp: Windows bitmap\"\n             \"\\n\\t jpg: JPEG format\"\n             \"\\n\\t jp2: JPEG 2000 format\"\n             \"\\n\\t png: Portable Network Graphics\"\n             \"\\n\\t ppm: Portable Pixmap Format\",\n        datatype=str,\n        rounding=None,\n        min_max=None,\n        choices=[\"bmp\", \"jpg\", \"jp2\", \"png\", \"ppm\"],\n        group=\"format\",\n        gui_radio=True,\n        fixed=True,\n    ),\n    draw_transparent=dict(\n        default=False,\n        info=\"Place the swapped face on a transparent layer rather than the original frame.\\nNB: \"\n             \"This is only compatible with images saved in png format. If an incompatible format \"\n             \"is selected then the image will be saved as a png.\",\n        datatype=bool,\n        rounding=None,\n        min_max=None,\n        choices=[],\n        group=\"format\",\n        gui_radio=False,\n        fixed=True,\n    ),\n    separate_mask=dict(\n        default=False,\n        info=\"Seperate the mask into its own single channel image. This only applies when \"\n             \"'draw-transparent' is selected. If enabled, the RGB image will be saved into the \"\n             \"selected output folder whilst the masks will be saved into a sub-folder named \"\n             \"`masks`. If not enabled then the mask will be included in the alpha-channel of the \"\n             \"RGBA output.\",\n        datatype=bool,\n        rounding=None,\n        min_max=None,\n        choices=[],\n        group=\"format\",\n        gui_radio=False,\n        fixed=True,\n    ),\n    jpg_quality=dict(\n        default=75,\n        info=\"[jpg only] Set the jpg quality. 1 is worst 95 is best. Higher quality leads to \"\n             \"larger file sizes.\",\n        datatype=int,\n        rounding=1,\n        min_max=(1, 95),\n        choices=[],\n        group=\"compression\",\n        gui_radio=False,\n        fixed=True,\n    ),\n    png_compress_level=dict(\n        default=3,\n        info=\"[png only] ZLIB compression level, 1 gives best speed, 9 gives best compression, 0 \"\n             \"gives no compression at all.\",\n        datatype=int,\n        rounding=1,\n        min_max=(0, 9),\n        choices=[],\n        group=\"compression\",\n        gui_radio=False,\n        fixed=True,\n    ),\n)\n", "plugins/convert/writer/patch_defaults.py": "#!/usr/bin/env python3\n\"\"\"\n    The default options for the faceswap patch Writer plugin.\n\n    Defaults files should be named <plugin_name>_defaults.py\n    Any items placed into this file will automatically get added to the relevant config .ini files\n    within the faceswap/config folder.\n\n    The following variables should be defined:\n        _HELPTEXT: A string describing what this plugin does\n        _DEFAULTS: A dictionary containing the options, defaults and meta information. The\n                   dictionary should be defined as:\n                       {<option_name>: {<metadata>}}\n\n                   <option_name> should always be lower text.\n                   <metadata> dictionary requirements are listed below.\n\n    The following keys are expected for the _DEFAULTS <metadata> dict:\n        datatype:  [required] A python type class. This limits the type of data that can be\n                   provided in the .ini file and ensures that the value is returned in the\n                   correct type to faceswap. Valid datatypes are: <class 'int'>, <class 'float'>,\n                   <class 'str'>, <class 'bool'>.\n        default:   [required] The default value for this option.\n        info:      [required] A string describing what this option does.\n        choices:   [optional] If this option's datatype is of <class 'str'> then valid\n                   selections can be defined here. This validates the option and also enables\n                   a combobox / radio option in the GUI.\n        gui_radio: [optional] If <choices> are defined, this indicates that the GUI should use\n                   radio buttons rather than a combobox to display this option.\n        min_max:   [partial] For <class 'int'> and <class 'float'> datatypes this is required\n                   otherwise it is ignored. Should be a tuple of min and max accepted values.\n                   This is used for controlling the GUI slider range. Values are not enforced.\n        rounding:  [partial] For <class 'int'> and <class 'float'> datatypes this is\n                   required otherwise it is ignored. Used for the GUI slider. For floats, this\n                   is the number of decimal places to display. For ints this is the step size.\n        fixed:     [optional] [train only]. Training configurations are fixed when the model is\n                   created, and then reloaded from the state file. Marking an item as fixed=False\n                   indicates that this value can be changed for existing models, and will override\n                   the value saved in the state file with the updated value in config. If not\n                   provided this will default to True.\n\"\"\"\n\n_HELPTEXT = (\n    \"Options for outputting the raw converted face patches from faceswap\\n\"\n    \"The raw face patches are output along with the transformation matrix, per face, to \"\n    \"transform the face back into the original frame in external tools\"\n)\n\n_DEFAULTS = {\n    \"start_index\": {\n        \"default\": \"0\",\n        \"info\": \"The starting frame number for the first output frame.\",\n        \"datatype\": str,\n        \"choices\": [\"0\", \"1\"],\n        \"group\": \"file_naming\",\n        \"gui_radio\": True,\n    },\n    \"index_offset\": {\n        \"default\": 0,\n        \"info\": \"How much to offset the frame numbering by.\",\n        \"datatype\": int,\n        \"rounding\": 1,\n        \"min_max\": (0, 1000),\n        \"group\": \"file_naming\",\n    },\n    \"number_padding\": {\n        \"default\": 6,\n        \"info\": \"Length to pad the frame numbers by.\",\n        \"datatype\": int,\n        \"rounding\": 6,\n        \"min_max\": (0, 10),\n        \"group\": \"file_naming\",\n    },\n    \"include_filename\": {\n        \"default\": True,\n        \"info\": \"Prefix the filename of the original frame to each face patch's output filename.\",\n        \"datatype\": bool,\n        \"group\": \"file_naming\",\n    },\n    \"face_index_location\": {\n        \"default\": \"before\",\n        \"info\": \"For frames that contain multiple faces, where the face index should appear in \"\n                \"the filename:\"\n                \"\\n\\t before: places the face index before the frame number.\"\n                \"\\n\\t after: places the face index after the frame number.\",\n        \"datatype\": str,\n        \"choices\": [\"before\", \"after\"],\n        \"group\": \"file_naming\",\n        \"gui_radio\": True,\n    },\n    \"origin\": {\n        \"default\": \"bottom-left\",\n        \"info\": \"The origin (0, 0) location of the software that patches will be imported into. \"\n                \"This impacts the transformation matrix that is supplied with the image patch. \"\n                \"Setting the correct origin here will make importing into the external tool \"\n                \"simpler.\"\n                \"\\n\\t top-left: The origin (0, 0) of the external canvas is at the top left \"\n                \"corner.\"\n                \"\\n\\t bottom-left: The origin (0, 0) of the external canvas is at the bottom \"\n                \"left corner.\"\n                \"\\n\\t top-right: The origin (0, 0) of the external canvas is at the top right \"\n                \"corner.\"\n                \"\\n\\t bottom-right: The origin (0, 0) of the external canvas is at the bottom \"\n                \"right corner.\",\n        \"datatype\": str,\n        \"choices\": [\"top-left\", \"bottom-left\", \"top-right\", \"bottom-right\"],\n        \"group\": \"output\",\n        \"gui_radio\": True\n    },\n    \"empty_frames\": {\n        \"default\": \"blank\",\n        \"info\": \"How to handle the output of frames without faces:\"\n                \"\\n\\t skip: skips any frames that do not have a face within it. This will lead to \"\n                \"gaps within the final image sequence.\"\n                \"\\n\\t blank: outputs a blank (empty) face patch for any frames without faces. \"\n                \"There will be no gaps within the final image sequence, as those gaps will be \"\n                \"padded with empty face patches\",\n        \"datatype\": str,\n        \"choices\": [\"skip\", \"blank\"],\n        \"group\": \"output\",\n        \"gui_radio\": True,\n    },\n    \"json_output\": {\n        \"default\": False,\n        \"info\": \"The transformation matrix, and other associated metadata, is output within the \"\n                \"face images EXIF fields. Some external tools can read this data, others cannot.\"\n                \"enable this option to output a json file which contains this same metadata \"\n                \"mapped to each output face patch's filename.\",\n        \"datatype\": bool,\n        \"group\": \"output\"\n    },\n    \"separate_mask\": {\n        \"default\": False,\n        \"info\": \"Seperate the mask into its own single channel patch. If enabled, the RGB image \"\n                \"will be saved into the selected output folder whilst the masks will be saved \"\n                \"into a sub-folder named `masks`. If not enabled then the mask will be included \"\n                \"in the alpha-channel of the RGBA output.\",\n        \"datatype\": bool,\n        \"group\": \"output\",\n    },\n    \"bit_depth\": {\n        \"default\": \"16\",\n        \"info\": \"The bit-depth for the output images:\"\n                \"\\n\\t 8: 8-bit unsigned - Supported by all formats.\"\n                \"\\n\\t 16: 16-bit unsigned - Supported by all formats.\"\n                \"\\n\\t 32: 32-bit float - Supported by Tiff only.\",\n        \"datatype\": str,\n        \"choices\": [\"8\", \"16\", \"32\"],\n        \"group\": \"format\",\n        \"gui_radio\": True,\n    },\n    \"format\": {\n        \"default\": \"png\",\n        \"info\": \"File format to save as.\"\n                \"\\n\\t png: PNG file format. Transformation matrix is written to the custom iTxt \"\n                \"header field 'faceswap'\"\n                \"\\n\\t tiff: TIFF file format. Transformation matrix is written to the \"\n                \"'image_description' header field\",\n        \"datatype\": str,\n        \"choices\": [\"png\", \"tiff\"],\n        \"group\": \"format\",\n        \"gui_radio\": True\n    },\n    \"png_compress_level\": {\n        \"default\": 3,\n        \"info\": \"ZLIB compression level, 1 gives best speed, 9 gives best compression, 0 gives no \"\n                \"compression at all.\",\n        \"datatype\": int,\n        \"rounding\": 1,\n        \"min_max\": (0, 9),\n        \"group\": \"format\",\n    },\n    \"tiff_compression_method\": {\n        \"default\": \"lzw\",\n        \"info\": \"The compression method to use for Tiff files. Note: For 32bit output, SGILOG \"\n                \"compression will always be used regardless of what is selected here.\",\n        \"datatype\": str,\n        \"choices\": [\"none\", \"lzw\", \"deflate\"],\n        \"group\": \"format\",\n        \"gui_radio\": True\n    },\n}\n", "plugins/convert/writer/patch.py": "#!/usr/bin/env python3\n\"\"\" Face patch output writer for faceswap.py converter\n    Extracts the swapped Face Patch from faceswap rather than the final composited frame along with\n    the transformation matrix for re-inserting the face into the origial frame\n\"\"\"\nimport json\nimport logging\nimport re\n\nimport os\nimport cv2\nimport numpy as np\n\nfrom lib.image import encode_image, png_read_meta, tiff_read_meta\nfrom ._base import Output\n\nlogger = logging.getLogger(__name__)\n\n\nclass Writer(Output):\n    \"\"\" Face patch writer for outputting swapped face patches and transformation matrices\n\n    Parameters\n    ----------\n    output_folder: str\n        The full path to the output folder where the face patches should besaved\n    patch_size: int\n        The size of the face patch output from the model\n    configfile: str, optional\n        The full path to a custom configuration ini file. If ``None`` is passed\n        then the file is loaded from the default location. Default: ``None``.\n    \"\"\"\n    def __init__(self, output_folder: str, patch_size: int, **kwargs) -> None:\n        logger.debug(\"patch_size: %s\", patch_size)\n        super().__init__(output_folder, **kwargs)\n        self._extension = {\"png\": \".png\", \"tiff\": \".tif\"}[self.config[\"format\"]]\n        self._separate_mask = self.config[\"separate_mask\"]\n        self._fname_split = re.compile(\"[^0-9a-zA-Z]\")\n\n        if self._extension == \".png\" and self.config[\"bit_depth\"] not in (\"8\", \"16\"):\n            logger.warning(\"Patch Writer: Bit Depth '%s' is unsupported for format '%s'. \"\n                           \"Updating to '16'\", self.config[\"bit_depth\"], self.config[\"format\"])\n            self.config[\"bit_depth\"] = \"16\"\n\n        self._dtype = {\"8\": np.uint8, \"16\": np.uint16, \"32\": np.float32}[self.config[\"bit_depth\"]]\n        self._multiplier = {\"8\": 255., \"16\": 65535., \"32\": 1.}[self.config[\"bit_depth\"]]\n\n        self._dummy_patch = np.zeros((1, patch_size, patch_size, 4), dtype=np.float32)\n\n        tl_box = np.array([[0, 0], [patch_size, 0], [patch_size, patch_size], [0, patch_size]],\n                          dtype=np.float32)\n        self._patch_corner = {\"top-left\": tl_box[0],\n                              \"top-right\": tl_box[1],\n                              \"bottom-right\": tl_box[2],\n                              \"bottom-left\": tl_box[3]}[self.config[\"origin\"]].copy()\n        self._box = tl_box\n        if self.config[\"origin\"] in (\"top-right\", \"bottom-left\"):\n            self._box[[1, 3], :] = self._box[[3, 1], :]  # keep clockwise from 0,0\n\n        self._args = self._get_save_args()\n        self._matrices: dict[str, dict[str, list[list[float]]]] = {}\n\n    def _get_save_args(self) -> tuple[int, ...]:\n        \"\"\" Obtain the save parameters for the file format.\n\n        Returns\n        -------\n        tuple\n            The OpenCV specific arguments for the selected file format\n         \"\"\"\n        args: tuple[int, ...] = tuple()\n        if self._extension == \".png\" and self.config[\"png_compress_level\"] > -1:\n            args = (cv2.IMWRITE_PNG_COMPRESSION, self.config[\"png_compress_level\"])\n        if self._extension == \".tif\" and self.config[\"bit_depth\"] != \"32\":\n            tiff_methods = {\"none\": 1, \"lzw\": 5, \"deflate\": 8}\n            method = self.config[\"tiff_compression_method\"]\n            method = \"none\" if method is None else method\n            args = (cv2.IMWRITE_TIFF_COMPRESSION, tiff_methods[method])\n        logger.debug(args)\n        return args\n\n    def _get_new_filename(self, filename: str, face_index: int) -> str:\n        \"\"\" Obtain the filename for the output file based on the frame's filename and the user\n        selected naming options\n\n        Parameters\n        ----------\n        filename: str\n            The original frame's filename\n        face_index: int\n            The index of the face within the frame\n\n        Returns\n        -------\n        str\n            The new filename for naming the output face patch\n        \"\"\"\n        face_idx = str(face_index).rjust(2, \"0\")\n        fname, ext = os.path.splitext(filename)\n        fname = os.path.basename(fname)\n\n        split_fname = self._fname_split.split(fname)\n        if split_fname and split_fname[-1].isdigit():\n            i_frame_no = (int(split_fname[-1]) +\n                          (int(self.config[\"start_index\"]) - 1) +\n                          self.config[\"index_offset\"])\n            frame_no = f\".{str(i_frame_no).rjust(self.config['number_padding'], '0')}\"\n            base_fname = fname[:-len(split_fname[-1]) - 1]\n        else:\n            frame_no = \"\"\n            base_fname = fname\n\n        retval = \"\"\n        if self.config[\"include_filename\"]:\n            retval += base_fname\n        if self.config[\"face_index_location\"] == \"before\":\n            retval = f\"{retval}_{face_idx}\"\n        retval += frame_no\n        if self.config[\"face_index_location\"] == \"after\":\n            retval = f\"{retval}.{face_idx}\"\n        retval += ext\n        logger.trace(\"source filename: '%s', output filename: '%s'\",  # type:ignore[attr-defined]\n                     filename, retval)\n        return retval\n\n    def write(self, filename: str, image: list[list[bytes]]) -> None:\n        \"\"\" Write out the pre-encoded image to disk. If separate mask has been selected, write out\n        the encoded mask to a sub-folder in the output directory.\n\n        Parameters\n        ----------\n        filename: str\n            The full path to write out the image to.\n        image: list[list[bytes]]\n            List of list of :class:`bytes` objects of containing all swapped faces from a frame to\n            write out. The inner list will be of length 1 (mask included in the alpha channel) or\n            length 2 (mask to write out separately)\n        \"\"\"\n        logger.trace(\"Outputting: (filename: '%s')\", filename)  # type:ignore[attr-defined]\n\n        read_func = png_read_meta if self._extension == \".png\" else tiff_read_meta\n        for idx, face in enumerate(image):\n            new_filename = self._get_new_filename(filename, idx)\n            filenames = self.output_filename(new_filename, self._separate_mask)\n            for fname, img in zip(filenames, face):\n                try:\n                    with open(fname, \"wb\") as outfile:\n                        outfile.write(img)\n                except Exception as err:  # pylint:disable=broad-except\n                    logger.error(\"Failed to save image '%s'. Original Error: %s\", filename, err)\n                if not self.config[\"json_output\"]:\n                    continue\n                mat = read_func(img)\n                self._matrices[os.path.splitext(os.path.basename(fname))[0]] = mat\n\n    @classmethod\n    def _get_inverse_matrices(cls, matrices: np.ndarray) -> np.ndarray:\n        \"\"\" Obtain the inverse matrices for the given matrices. If ``None`` is supplied return a\n        dummy transformation matrix that performs no action\n\n        Parameters\n        ----------\n        matrices : :class:`numpy.ndarray`\n            The original transform matrices that the inverse needs to be calculated for\n\n        Returns\n        -------\n        :class:`numpy.ndarray`\n            The inverse transformation matrices\n        \"\"\"\n        if not np.any(matrices):\n            return np.array([[[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]]], dtype=np.float32)\n\n        identity = np.array([[[0., 0., 1.]]], dtype=np.float32)\n        mat = np.concatenate([matrices, np.repeat(identity, matrices.shape[0], axis=0)], axis=1)\n        retval = np.linalg.inv(mat)\n        logger.trace(\"matrix: %s, inverse: %s\", mat, retval)  # type:ignore[attr-defined]\n        return retval\n\n    def _adjust_to_origin(self, matrices: np.ndarray, canvas_size: tuple[int, int]) -> None:\n        \"\"\" Adjust the transformation matrix to use the correct target coordinates system. The\n        matrix adjustment is done in place, so this does not return a value\n\n        Parameters\n        ----------\n        matrices: :class:`numpy.ndarray`\n            The transformation matrices to be adjusted\n        canvas_size: tuple[int, int]\n            The size of the canvas width, height) that the transformation matrix applies to.\n        \"\"\"\n        if self.config[\"origin\"] == \"top-left\":\n            return\n\n        for mat in matrices:\n            og_cnr = cv2.transform(self._patch_corner[None, None], mat[:2, ...]).squeeze()\n            x_shift, y_shift = og_cnr\n            if self.config[\"origin\"].split(\"-\")[-1] == \"right\":\n                x_shift = canvas_size[0] - x_shift\n            if self.config[\"origin\"].split(\"-\")[0] == \"bottom\":\n                y_shift = canvas_size[1] - y_shift\n            mat[:2, 2] = [x_shift, y_shift]\n\n        if self.config[\"origin\"] in (\"top-right\", \"bottom-left\"):\n            matrices[..., :2, :2] *= [[[1, -1], [-1, 1]]]  # switch shear\n\n    def _get_roi(self, matrices: np.ndarray) -> np.ndarray:\n        \"\"\" Obtain the (x, y) ROI points of the patch in the original frame. Points are returned\n        in clockwise order from the origin location\n\n        Parameters\n        ----------\n        matrices: :class:`numpy.ndarray`\n            The transformation matrices for the current frame\n\n        Returns\n        -------\n        np.ndarray\n            The ROI of the patches in original frame co-ordinates in clockwise order from the\n            origin point\n        \"\"\"\n        retval = [cv2.transform(np.expand_dims(self._box, axis=1), mat[:2, ...]).squeeze()\n                  for mat in matrices]\n        return np.array(retval, dtype=np.float32)\n\n    def pre_encode(self, image: np.ndarray, **kwargs) -> list[list[bytes]]:\n        \"\"\" Pre_encode the image in lib/convert.py threads as it is a LOT quicker.\n\n        Parameters\n        ----------\n        image: :class:`numpy.ndarray`\n            A 3 or 4 channel BGR swapped face batch as float32\n        canvas_size: tuple[int, int]\n            The size of the canvas (x, y) that the transformation matrix applies to.\n        matrices: :class:`numpy.ndarray`, optional\n            The transformation matrices for extracting the face patches from the original frame.\n            Must be provided if an image is provided, otherwise ``None`` to insert a dummy matrix\n\n        Returns\n        -------\n        list\n            List of :class:`bytes` objects ready for writing. The list will be of length 1 with\n            image bytes object as the only member unless separate mask has been requested, in which\n            case it will be length 2 with the image in position 0 and mask in position 1\n         \"\"\"\n        logger.trace(\"Pre-encoding image\")  # type:ignore[attr-defined]\n        retval = []\n        canvas_size: tuple[int, int] = kwargs.get(\"canvas_size\", (1, 1))\n        matrices: np.ndarray = kwargs.get(\"matrices\", np.array([]))\n\n        if not np.any(image) and self.config[\"empty_frames\"] == \"blank\":\n            image = self._dummy_patch\n\n        matrices = self._get_inverse_matrices(matrices)\n        self._adjust_to_origin(matrices, canvas_size)\n        rois = self._get_roi(matrices)\n        patches = (image * self._multiplier).astype(self._dtype)\n\n        for patch, matrix, roi in zip(patches, matrices, rois):\n            this_face = []\n            mat = json.dumps({\"transform_matrix\": matrix.tolist(), \"roi\": roi.tolist()},\n                             ensure_ascii=True).encode(\"ascii\")\n            if self._separate_mask:\n                mask = patch[..., -1]\n                face = patch[..., :3]\n\n                this_face.append(encode_image(mask,\n                                              self._extension,\n                                              encoding_args=self._args,\n                                              metadata=mat))\n            else:\n                face = patch\n\n            this_face.insert(0, encode_image(face,\n                                             self._extension,\n                                             encoding_args=self._args,\n                                             metadata=mat))\n            retval.append(this_face)\n        return retval\n\n    def close(self) -> None:\n        \"\"\" Outputs json file if requested \"\"\"\n        if not self.config[\"json_output\"]:\n            return\n        fname = os.path.join(self.output_folder, \"matrices.json\")\n        with open(fname, \"w\", encoding=\"utf-8\") as ofile:\n            json.dump(self._matrices, ofile, indent=2, sort_keys=True)\n        logger.info(\"Patch matrices written to: '%s'\", fname)\n", "plugins/convert/writer/pillow_defaults.py": "#!/usr/bin/env python3\n\"\"\"\n    The default options for the faceswap Pillow Writer plugin.\n\n    Defaults files should be named <plugin_name>_defaults.py\n    Any items placed into this file will automatically get added to the relevant config .ini files\n    within the faceswap/config folder.\n\n    The following variables should be defined:\n        _HELPTEXT: A string describing what this plugin does\n        _DEFAULTS: A dict(ionary containing the options, defaults and meta information. The\n                   dict(ionary should be defined as:\n                       {<option_name>: {<metadata>}}\n\n                   <option_name> should always be lower text.\n                   <metadata> dict(ionary requirements are listed below.\n\n    The following keys are expected for the _DEFAULTS <metadata> dict(:\n        datatype:  [required] A python type class. This limits the type of data that can be\n                   provided in the .ini file and ensures that the value is returned in the\n                   correct type to faceswap. Valid datatypes are: <class 'int'>, <class 'float'>,\n                   <class 'str'>, <class 'bool'>.\n        default:   [required] The default value for this option.\n        info:      [required] A string describing what this option does.\n        choices:   [optional] If this option's datatype is of <class 'str'> then valid\n                   selections can be defined here. This validates the option and also enables\n                   a combobox / radio option in the GUI.\n        gui_radio: [optional] If <choices> are defined, this indicates that the GUI should use\n                   radio buttons rather than a combobox to display this option.\n        min_max:   [partial] For <class 'int'> and <class 'float'> datatypes this is required\n                   otherwise it is ignored. Should be a tuple of min and max accepted values.\n                   This is used for controlling the GUI slider range. Values are not enforced.\n        rounding:  [partial] For <class 'int'> and <class 'float'> datatypes this is\n                   required otherwise it is ignored. Used for the GUI slider. For floats, this\n                   is the number of decimal places to display. For ints this is the step size.\n        fixed:     [optional] [train only]. Training configurations are fixed when the model is\n                   created, and then reloaded from the state file. Marking an item as fixed=False\n                   indicates that this value can be changed for existing models, and will override\n                   the value saved in the state file with the updated value in config. If not\n                   provided this will default to True.\n\"\"\"\n\n\n_HELPTEXT = (\n    \"Options for outputting converted frames to a series of images using Pillow\\n\"\n    \"Pillow is more feature rich than OpenCV but can be slower.\"\n)\n\n\n_DEFAULTS = dict(\n    format=dict(\n        default=\"png\",\n        info=\"Image format to use:\"\n             \"\\n\\t bmp: Windows bitmap\"\n             \"\\n\\t gif: Graphics Interchange Format (NB: Not animated)\"\n             \"\\n\\t jpg: JPEG format\"\n             \"\\n\\t jp2: JPEG 2000 format\"\n             \"\\n\\t png: Portable Network Graphics\"\n             \"\\n\\t ppm: Portable Pixmap Format\"\n             \"\\n\\t tif: Tag Image File Format\",\n        datatype=str,\n        rounding=None,\n        min_max=None,\n        choices=[\"bmp\", \"gif\", \"jpg\", \"jp2\", \"png\", \"ppm\", \"tif\"],\n        group=\"format\",\n        gui_radio=True,\n        fixed=True,\n    ),\n    draw_transparent=dict(\n        default=False,\n        info=\"Place the swapped face on a transparent layer rather than the original frame.\\nNB: \"\n             \"This is only compatible with images saved in png or tif format. If an incompatible \"\n             \"format is selected then the image will be saved as a png.\",\n        datatype=bool,\n        rounding=None,\n        min_max=None,\n        choices=[],\n        group=\"format\",\n        gui_radio=False,\n        fixed=True,\n    ),\n    separate_mask=dict(\n        default=False,\n        info=\"Seperate the mask into its own single channel image. This only applies when \"\n             \"'draw-transparent' is selected. If enabled, the RGB image will be saved into the \"\n             \"selected output folder whilst the masks will be saved into a sub-folder named \"\n             \"`masks`. If not enabled then the mask will be included in the alpha-channel of the \"\n             \"RGBA output.\",\n        datatype=bool,\n        rounding=None,\n        min_max=None,\n        choices=[],\n        group=\"format\",\n        gui_radio=False,\n        fixed=True,\n    ),\n    optimize=dict(\n        default=False,\n        info=\"[gif, jpg and png only] If enabled, indicates that the encoder should make an extra \"\n             \"pass over the image in order to select optimal encoder settings.\",\n        datatype=bool,\n        rounding=None,\n        min_max=None,\n        choices=[],\n        group=\"settings\",\n        gui_radio=False,\n        fixed=True,\n    ),\n    gif_interlace=dict(\n        default=True,\n        info=\"[gif only] Set whether to save the gif as interlaced or not.\",\n        datatype=bool,\n        rounding=None,\n        min_max=None,\n        choices=[],\n        group=\"settings\",\n        gui_radio=False,\n        fixed=True,\n    ),\n    jpg_quality=dict(\n        default=75,\n        info=\"[jpg only] Set the jpg quality. 1 is worst 95 is best. Higher quality leads to \"\n             \"larger file sizes.\",\n        datatype=int,\n        rounding=1,\n        min_max=(1, 95),\n        choices=[],\n        group=\"compression\",\n        gui_radio=False,\n        fixed=True,\n    ),\n    png_compress_level=dict(\n        default=3,\n        info=\"[png only] ZLIB compression level, 1 gives best speed, 9 gives best compression, 0 \"\n             \"gives no compression at all. When optimize option is set to True this has no effect \"\n             \"(it is set to 9 regardless of a value passed).\",\n        datatype=int,\n        rounding=1,\n        min_max=(0, 9),\n        choices=[],\n        group=\"compression\",\n        gui_radio=False,\n        fixed=True,\n    ),\n    tif_compression=dict(\n        default=\"tiff_deflate\",\n        info=\"[tif only] The desired compression method for the file.\",\n        datatype=str,\n        rounding=None,\n        min_max=None,\n        choices=[\"none\", \"tiff_ccitt\", \"group3\", \"group4\", \"tiff_jpeg\", \"tiff_adobe_deflate\",\n                 \"tiff_thunderscan\", \"tiff_deflate\", \"tiff_sgilog\", \"tiff_sgilog24\",\n                 \"tiff_raw_16\"],\n        group=\"compression\",\n        gui_radio=False,\n        fixed=True,\n    ),\n)\n", "plugins/convert/writer/gif_defaults.py": "#!/usr/bin/env python3\n\"\"\"\n    The default options for the faceswap Gif Writer plugin.\n\n    Defaults files should be named <plugin_name>_defaults.py\n    Any items placed into this file will automatically get added to the relevant config .ini files\n    within the faceswap/config folder.\n\n    The following variables should be defined:\n        _HELPTEXT: A string describing what this plugin does\n        _DEFAULTS: A dictionary containing the options, defaults and meta information. The\n                   dictionary should be defined as:\n                       {<option_name>: {<metadata>}}\n\n                   <option_name> should always be lower text.\n                   <metadata> dictionary requirements are listed below.\n\n    The following keys are expected for the _DEFAULTS <metadata> dict:\n        datatype:  [required] A python type class. This limits the type of data that can be\n                   provided in the .ini file and ensures that the value is returned in the\n                   correct type to faceswap. Valid datatypes are: <class 'int'>, <class 'float'>,\n                   <class 'str'>, <class 'bool'>.\n        default:   [required] The default value for this option.\n        info:      [required] A string describing what this option does.\n        choices:   [optional] If this option's datatype is of <class 'str'> then valid\n                   selections can be defined here. This validates the option and also enables\n                   a combobox / radio option in the GUI.\n        gui_radio: [optional] If <choices> are defined, this indicates that the GUI should use\n                   radio buttons rather than a combobox to display this option.\n        min_max:   [partial] For <class 'int'> and <class 'float'> datatypes this is required\n                   otherwise it is ignored. Should be a tuple of min and max accepted values.\n                   This is used for controlling the GUI slider range. Values are not enforced.\n        rounding:  [partial] For <class 'int'> and <class 'float'> datatypes this is\n                   required otherwise it is ignored. Used for the GUI slider. For floats, this\n                   is the number of decimal places to display. For ints this is the step size.\n        fixed:     [optional] [train only]. Training configurations are fixed when the model is\n                   created, and then reloaded from the state file. Marking an item as fixed=False\n                   indicates that this value can be changed for existing models, and will override\n                   the value saved in the state file with the updated value in config. If not\n                   provided this will default to True.\n\"\"\"\n\n\n_HELPTEXT = \"Options for outputting converted frames to an animated gif.\"\n\n\n_DEFAULTS = dict(\n    fps=dict(\n        default=25,\n        info=\"Frames per Second.\",\n        datatype=int,\n        rounding=1,\n        min_max=(1, 60),\n        choices=[],\n        group=\"settings\",\n        gui_radio=False,\n        fixed=True,\n    ),\n    loop=dict(\n        default=0,\n        info=\"The number of iterations. Set to 0 to loop indefinitely.\",\n        datatype=int,\n        rounding=1,\n        min_max=(0, 100),\n        choices=[],\n        group=\"settings\",\n        gui_radio=False,\n        fixed=True,\n    ),\n    palettesize=dict(\n        default=\"256\",\n        info=\"The number of colors to quantize the image to. Is rounded to the nearest power of \"\n             \"two.\",\n        datatype=str,\n        rounding=None,\n        min_max=None,\n        choices=[\"2\", \"4\", \"8\", \"16\", \"32\", \"64\", \"128\", \"256\"],\n        group=\"settings\",\n        gui_radio=False,\n        fixed=True,\n    ),\n    subrectangles=dict(\n        default=False,\n        info=\"If True, will try and optimize the GIF by storing only the rectangular parts of \"\n             \"each frame that change with respect to the previous.\",\n        datatype=bool,\n        rounding=None,\n        min_max=None,\n        choices=[],\n        group=\"settings\",\n        gui_radio=False,\n        fixed=True,\n    ),\n)\n", "plugins/convert/writer/_base.py": "#!/usr/bin/env python3\n\"\"\" Parent class for output writers for faceswap.py converter \"\"\"\n\nimport logging\nimport os\nimport re\nimport typing as T\n\nimport numpy as np\n\nfrom plugins.convert._config import Config\n\nlogger = logging.getLogger(__name__)\n\n\ndef get_config(plugin_name: str, configfile: str | None = None) -> dict:\n    \"\"\" Obtain the configuration settings for the writer plugin.\n\n    Parameters\n    ----------\n    plugin_name: str\n        The name of the convert plugin to return configuration settings for\n    configfile: str, optional\n        The full path to a custom configuration ini file. If ``None`` is passed\n        then the file is loaded from the default location. Default: ``None``.\n\n    Returns\n    -------\n    dict\n        The requested configuration dictionary\n    \"\"\"\n    return Config(plugin_name, configfile=configfile).config_dict\n\n\nclass Output():\n    \"\"\" Parent class for writer plugins.\n\n    Parameters\n    ----------\n    output_folder: str\n        The full path to the output folder where the converted media should be saved\n    configfile: str, optional\n        The full path to a custom configuration ini file. If ``None`` is passed\n        then the file is loaded from the default location. Default: ``None``.\n    \"\"\"\n    def __init__(self, output_folder: str, configfile: str | None = None) -> None:\n        logger.debug(\"Initializing %s: (output_folder: '%s')\",\n                     self.__class__.__name__, output_folder)\n        self.config: dict = get_config(\".\".join(self.__module__.split(\".\")[-2:]),\n                                       configfile=configfile)\n        logger.debug(\"config: %s\", self.config)\n        self.output_folder: str = output_folder\n\n        # For creating subfolders when separate mask is selected\n        self._subfolders_created: bool = False\n\n        # Methods for making sure frames are written out in frame order\n        self.re_search = re.compile(r\"(\\d+)(?=\\.\\w+$)\")  # Identify frame numbers\n        self.cache: dict = {}  # Cache for when frames must be written in correct order\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    @property\n    def is_stream(self) -> bool:\n        \"\"\" bool: Whether the writer outputs a stream or a series images.\n\n        Writers that write to a stream have a frame_order paramater to dictate\n        the order in which frames should be written out (eg. gif/ffmpeg) \"\"\"\n        retval = hasattr(self, \"_frame_order\")\n        return retval\n\n    @classmethod\n    def _set_frame_order(cls,\n                         total_count: int,\n                         frame_ranges: list[tuple[int, int]] | None) -> list[int]:\n        \"\"\" Obtain the full list of frames to be converted in order.\n\n        Used for FFMPEG and Gif writers to ensure correct frame order\n\n        Parameters\n        ----------\n        total_count: int\n            The total number of frames to be converted\n        frame_ranges: list or ``None``\n            List of tuples for starting and end values of each frame range to be converted or\n            ``None`` if all frames are to be converted\n\n        Returns\n        -------\n        list\n            Full list of all frame indices to be converted\n        \"\"\"\n        if frame_ranges is None:\n            retval = list(range(1, total_count + 1))\n        else:\n            retval = []\n            for rng in frame_ranges:\n                retval.extend(list(range(rng[0], rng[1] + 1)))\n        logger.debug(\"frame_order: %s\", retval)\n        return retval\n\n    def output_filename(self, filename: str, separate_mask: bool = False) -> list[str]:\n        \"\"\" Obtain the full path for the output file, including the correct extension, for the\n        given input filename.\n\n        NB: The plugin must have a config item 'format' that contains the file extension to use\n        this method.\n\n        Parameters\n        ----------\n        filename: str\n            The input frame filename to generate the output file name for\n        separate_mask: bool, optional\n            ``True`` if the mask should be saved out to a sub-folder otherwise ``False``\n\n        Returns\n        -------\n        list\n            The full path for the output converted frame to be saved to in position 1. The full\n            path for the mask to be output to in position 2 (if requested)\n        \"\"\"\n        filename = os.path.splitext(os.path.basename(filename))[0]\n        out_filename = f\"{filename}.{self.config['format']}\"\n        retval = [os.path.join(self.output_folder, out_filename)]\n        if separate_mask:\n            retval.append(os.path.join(self.output_folder, \"masks\", out_filename))\n\n        if separate_mask and not self._subfolders_created:\n            locations = [os.path.dirname(loc) for loc in retval]\n            logger.debug(\"Creating sub-folders: %s\", locations)\n            for location in locations:\n                os.makedirs(location, exist_ok=True)\n\n        logger.trace(\"in filename: '%s', out filename: '%s'\", filename, retval)  # type:ignore\n        return retval\n\n    def cache_frame(self, filename: str, image: np.ndarray) -> None:\n        \"\"\" Add the incoming converted frame to the cache ready for writing out.\n\n        Used for ffmpeg and gif writers to ensure that the frames are written out in the correct\n        order.\n\n        Parameters\n        ----------\n        filename: str\n            The filename of the incoming frame, where the frame index can be extracted from\n        image: class:`numpy.ndarray`\n            The converted frame corresponding to the given filename\n        \"\"\"\n        re_frame = re.search(self.re_search, filename)\n        assert re_frame is not None\n        frame_no = int(re_frame.group())\n        self.cache[frame_no] = image\n        logger.trace(\"Added to cache. Frame no: %s\", frame_no)  # type: ignore\n        logger.trace(\"Current cache: %s\", sorted(self.cache.keys()))  # type:ignore\n\n    def write(self, filename: str, image: T.Any) -> None:\n        \"\"\" Override for specific frame writing method.\n\n        Parameters\n        ----------\n        filename: str\n            The incoming frame filename.\n        image: Any\n            The converted image to be written. Could be a numpy array, a bytes encoded image or\n            any other plugin specific format\n        \"\"\"\n        raise NotImplementedError\n\n    def pre_encode(self, image: np.ndarray, **kwargs) -> T.Any:  # pylint:disable=unused-argument\n        \"\"\" Some writer plugins support the pre-encoding of images prior to saving out. As\n        patching is done in multiple threads, but writing is done in a single thread, it can\n        speed up the process to do any pre-encoding as part of the converter process.\n\n        If the writer supports pre-encoding then override this to pre-encode the image in\n        :mod:`lib.convert` to speed up saving.\n\n        Parameters\n        ----------\n        image: :class:`numpy.ndarray`\n            The converted image that is to be run through the pre-encoding function\n\n        Returns\n        -------\n        Any or ``None``\n            If ``None`` then the writer does not support pre-encoding, otherwise return output of\n            the plugin specific pre-enccode function\n        \"\"\"\n        return None\n\n    def close(self) -> None:\n        \"\"\" Override for specific converted frame writing close methods \"\"\"\n        raise NotImplementedError\n", "plugins/convert/writer/pillow.py": "#!/usr/bin/env python3\n\"\"\" Image output writer for faceswap.py converter \"\"\"\nfrom io import BytesIO\nfrom PIL import Image\n\nimport numpy as np\n\nfrom ._base import Output, logger\n\n\nclass Writer(Output):\n    \"\"\" Images output writer using Pillow\n\n    Parameters\n    ----------\n    output_folder: str\n        The full path to the output folder where the converted media should be saved\n    configfile: str, optional\n        The full path to a custom configuration ini file. If ``None`` is passed\n        then the file is loaded from the default location. Default: ``None``.\n    \"\"\"\n    def __init__(self, output_folder: str, **kwargs) -> None:\n        super().__init__(output_folder, **kwargs)\n        self._check_transparency_format()\n        # Correct format namings for writing to byte stream\n        self._format_dict = {\"jpg\": \"JPEG\", \"jp2\": \"JPEG 2000\", \"tif\": \"TIFF\"}\n        self._separate_mask = self.config[\"draw_transparent\"] and self.config[\"separate_mask\"]\n        self._kwargs = self._get_save_kwargs()\n\n    def _check_transparency_format(self) -> None:\n        \"\"\" Make sure that the output format is correct if draw_transparent is selected \"\"\"\n        transparent = self.config[\"draw_transparent\"]\n        if not transparent or (transparent and self.config[\"format\"] in (\"png\", \"tif\")):\n            return\n        logger.warning(\"Draw Transparent selected, but the requested format does not support \"\n                       \"transparency. Changing output format to 'png'\")\n        self.config[\"format\"] = \"png\"\n\n    def _get_save_kwargs(self) -> dict[str, bool | int | str]:\n        \"\"\" Return the save parameters for the file format\n\n        Returns\n        -------\n        dict\n            The specific keyword arguments for the selected file format\n        \"\"\"\n        filetype = self.config[\"format\"]\n        kwargs = {}\n        if filetype in (\"gif\", \"jpg\", \"png\"):\n            kwargs[\"optimize\"] = self.config[\"optimize\"]\n        if filetype == \"gif\":\n            kwargs[\"interlace\"] = self.config[\"gif_interlace\"]\n        if filetype == \"png\":\n            kwargs[\"compress_level\"] = self.config[\"png_compress_level\"]\n        if filetype == \"tif\":\n            kwargs[\"compression\"] = self.config[\"tif_compression\"]\n        logger.debug(kwargs)\n        return kwargs\n\n    def write(self, filename: str, image: list[BytesIO]) -> None:\n        \"\"\" Write out the pre-encoded image to disk. If separate mask has been selected, write out\n        the encoded mask to a sub-folder in the output directory.\n\n        Parameters\n        ----------\n        filename: str\n            The full path to write out the image to.\n        image: list\n            List of :class:`BytesIO` objects of length 1 (containing just the image to write out)\n            or length 2 (containing the image and mask to write out)\n        \"\"\"\n        logger.trace(\"Outputting: (filename: '%s'\", filename)  # type:ignore\n        filenames = self.output_filename(filename, self._separate_mask)\n        try:\n            for fname, img in zip(filenames, image):\n                with open(fname, \"wb\") as outfile:\n                    outfile.write(img.read())\n        except Exception as err:  # pylint:disable=broad-except\n            logger.error(\"Failed to save image '%s'. Original Error: %s\", filename, err)\n\n    def pre_encode(self, image: np.ndarray, **kwargs) -> list[BytesIO]:\n        \"\"\" Pre_encode the image in lib/convert.py threads as it is a LOT quicker\n\n        Parameters\n        ----------\n        image: :class:`numpy.ndarray`\n            A 3 or 4 channel BGR swapped frame\n\n        Returns\n        -------\n        list\n            List of :class:`BytesIO` objects ready for writing. The list will be of length 1 with\n            image bytes object as the only member unless separate mask has been requested, in which\n            case it will be length 2 with the image in position 0 and mask in position 1\n         \"\"\"\n        logger.trace(\"Pre-encoding image\")  # type:ignore\n\n        if self._separate_mask:\n            encoded_mask = self._encode_image(image[..., -1])\n            image = image[..., :3]\n\n        rgb = [2, 1, 0, 3] if image.shape[2] == 4 else [2, 1, 0]\n        encoded_image = self._encode_image(image[..., rgb])\n\n        retval = [encoded_image]\n\n        if self._separate_mask:\n            retval.append(encoded_mask)\n\n        return retval\n\n    def _encode_image(self, image: np.ndarray) -> BytesIO:\n        \"\"\" Encode an image in the correct format as a bytes object for saving\n\n        Parameters\n        ----------\n        image: :class:`np.ndarray`\n            The single channel mask to encode for saving\n\n        Returns\n        -------\n        :class:`BytesIO`\n            The image as a bytes object ready for writing to disk\n        \"\"\"\n        fmt = self._format_dict.get(self.config[\"format\"], self.config[\"format\"].upper())\n        encoded = BytesIO()\n        out_image = Image.fromarray(image)\n        out_image.save(encoded, fmt, **self._kwargs)\n        encoded.seek(0)\n        return encoded\n\n    def close(self) -> None:\n        \"\"\" Does nothing as Pillow writer does not need a close method \"\"\"\n        return\n", "plugins/convert/writer/__init__.py": "", "plugins/convert/writer/gif.py": "#!/usr/bin/env python3\n\"\"\" Animated GIF writer for faceswap.py converter \"\"\"\nfrom __future__ import annotations\nimport os\nimport typing as T\n\nimport cv2\nimport imageio\n\nfrom ._base import Output, logger\n\nif T.TYPE_CHECKING:\n    from imageio.core import format as im_format  # noqa:F401\n\n\nclass Writer(Output):\n    \"\"\" GIF output writer using imageio.\n\n\n    Parameters\n    ----------\n    output_folder: str\n        The folder to save the output gif to\n    total_count: int\n        The total number of frames to be converted\n    frame_ranges: list or ``None``\n        List of tuples for starting and end values of each frame range to be converted or ``None``\n        if all frames are to be converted\n    kwargs: dict\n        Any additional standard :class:`plugins.convert.writer._base.Output` key word arguments.\n    \"\"\"\n    def __init__(self,\n                 output_folder: str,\n                 total_count: int,\n                 frame_ranges: list[tuple[int, int]] | None,\n                 **kwargs) -> None:\n        logger.debug(\"total_count: %s, frame_ranges: %s\", total_count, frame_ranges)\n        super().__init__(output_folder, **kwargs)\n        self._frame_order: list[int] = self._set_frame_order(total_count, frame_ranges)\n        # Fix dims on 1st received frame\n        self._output_dimensions: tuple[int, int] | None = None\n        # Need to know dimensions of first frame, so set writer then\n        self._writer: imageio.plugins.pillowmulti.GIFFormat.Writer | None = None\n        self._gif_file: str | None = None  # Set filename based on first file seen\n\n    @property\n    def _gif_params(self) -> dict:\n        \"\"\" dict: The selected gif plugin configuration options. \"\"\"\n        kwargs = {key: int(val) for key, val in self.config.items()}\n        logger.debug(kwargs)\n        return kwargs\n\n    def _get_writer(self) -> im_format.Format.Writer:\n        \"\"\" Obtain the GIF writer with the requested GIF encoding options.\n\n        Returns\n        -------\n        :class:`imageio.plugins.pillowmulti.GIFFormat.Writer`\n            The imageio GIF writer\n        \"\"\"\n        logger.debug(\"writer config: %s\", self.config)\n        assert self._gif_file is not None\n        return imageio.get_writer(self._gif_file,\n                                  mode=\"i\",\n                                  **self._gif_params)\n\n    def write(self, filename: str, image) -> None:\n        \"\"\" Frames come from the pool in arbitrary order, so frames are cached for writing out\n        in the correct order.\n\n        Parameters\n        ----------\n        filename: str\n            The incoming frame filename.\n        image: :class:`numpy.ndarray`\n            The converted image to be written\n        \"\"\"\n        logger.trace(\"Received frame: (filename: '%s', shape: %s\",  # type: ignore\n                     filename, image.shape)\n        if not self._gif_file:\n            self._set_gif_filename(filename)\n            self._set_dimensions(image.shape[:2])\n            self._writer = self._get_writer()\n        if (image.shape[1], image.shape[0]) != self._output_dimensions:\n            image = cv2.resize(image, self._output_dimensions)  # pylint:disable=no-member\n        self.cache_frame(filename, image)\n        self._save_from_cache()\n\n    def _set_gif_filename(self, filename: str) -> None:\n        \"\"\" Set the full path to GIF output file to :attr:`_gif_file`\n\n        The filename is the created from the source filename of the first input image received with\n        `\"_converted\"` appended to the end and a .gif extension. If a file already exists with the\n        given filename, then `\"_1\"` is appended to the end of the filename. This number iterates\n        until a valid filename that does not exist is found.\n\n        Parameters\n        ----------\n        filename: str\n            The incoming frame filename.\n        \"\"\"\n\n        logger.debug(\"sample filename: '%s'\", filename)\n        filename = os.path.splitext(os.path.basename(filename))[0]\n        snip = len(filename)\n        for char in list(filename[::-1]):\n            if not char.isdigit() and char not in (\"_\", \"-\"):\n                break\n            snip -= 1\n        filename = filename[:snip]\n\n        idx = 0\n        while True:\n            out_file = f\"{filename}_converted{'' if idx == 0 else f'_{idx}'}.gif\"\n            retval = os.path.join(self.output_folder, out_file)\n            if not os.path.exists(retval):\n                break\n            idx += 1\n\n        self._gif_file = retval\n        logger.info(\"Outputting to: '%s'\", self._gif_file)\n\n    def _set_dimensions(self, frame_dims: tuple[int, int]) -> None:\n        \"\"\" Set the attribute :attr:`_output_dimensions` based on the first frame received. This\n        protects against different sized images coming in and ensure all images get written to the\n        Gif at the sema dimensions. \"\"\"\n        logger.debug(\"input dimensions: %s\", frame_dims)\n        self._output_dimensions = (frame_dims[1], frame_dims[0])\n        logger.debug(\"Set dimensions: %s\", self._output_dimensions)\n\n    def _save_from_cache(self) -> None:\n        \"\"\" Writes any consecutive frames to the GIF container that are ready to be output\n        from the cache. \"\"\"\n        assert self._writer is not None\n        while self._frame_order:\n            if self._frame_order[0] not in self.cache:\n                logger.trace(\"Next frame not ready. Continuing\")  # type: ignore\n                break\n            save_no = self._frame_order.pop(0)\n            save_image = self.cache.pop(save_no)\n            logger.trace(\"Rendering from cache. Frame no: %s\", save_no)  # type: ignore\n            self._writer.append_data(save_image[:, :, ::-1])\n        logger.trace(\"Current cache size: %s\", len(self.cache))  # type: ignore\n\n    def close(self) -> None:\n        \"\"\" Close the GIF writer on completion. \"\"\"\n        if self._writer is not None:\n            self._writer.close()\n", "plugins/convert/writer/ffmpeg.py": "#!/usr/bin/env python3\n\"\"\" Video output writer for faceswap.py converter \"\"\"\nfrom __future__ import annotations\nimport os\nimport typing as T\n\nfrom math import ceil\nfrom subprocess import CalledProcessError, check_output, STDOUT\n\nimport imageio\nimport imageio_ffmpeg as im_ffm\nimport numpy as np\n\nfrom ._base import Output, logger\n\nif T.TYPE_CHECKING:\n    from collections.abc import Generator\n\n\nclass Writer(Output):\n    \"\"\" Video output writer using imageio-ffmpeg.\n\n    Parameters\n    ----------\n    output_folder: str\n        The folder to save the output video to\n    total_count: int\n        The total number of frames to be converted\n    frame_ranges: list or ``None``\n        List of tuples for starting and end values of each frame range to be converted or ``None``\n        if all frames are to be converted\n    source_video: str\n        The full path to the source video for obtaining fps and audio\n    kwargs: dict\n        Any additional standard :class:`plugins.convert.writer._base.Output` key word arguments.\n    \"\"\"\n    def __init__(self,\n                 output_folder: str,\n                 total_count: int,\n                 frame_ranges: list[tuple[int, int]] | None,\n                 source_video: str,\n                 **kwargs) -> None:\n        super().__init__(output_folder, **kwargs)\n        logger.debug(\"total_count: %s, frame_ranges: %s, source_video: '%s'\",\n                     total_count, frame_ranges, source_video)\n        self._source_video: str = source_video\n        self._output_filename: str = self._get_output_filename()\n        self._frame_ranges: list[tuple[int, int]] | None = frame_ranges\n        self._frame_order: list[int] = self._set_frame_order(total_count, frame_ranges)\n        self._output_dimensions: str | None = None  # Fix dims on 1st received frame\n        # Need to know dimensions of first frame, so set writer then\n        self._writer: Generator[None, np.ndarray, None] | None = None\n\n    @property\n    def _valid_tunes(self) -> dict:\n        \"\"\" dict: Valid tune selections for libx264 and libx265 codecs. \"\"\"\n        return {\"libx264\": [\"film\", \"animation\", \"grain\", \"stillimage\", \"fastdecode\",\n                            \"zerolatency\"],\n                \"libx265\": [\"grain\", \"fastdecode\", \"zerolatency\"]}\n\n    @property\n    def _video_fps(self) -> float:\n        \"\"\" float: The fps of the source video. \"\"\"\n        reader = imageio.get_reader(self._source_video, \"ffmpeg\")  # type:ignore[arg-type]\n        retval = reader.get_meta_data()[\"fps\"]\n        reader.close()\n        logger.debug(retval)\n        return retval\n\n    @property\n    def _output_params(self) -> list[str]:\n        \"\"\" list: The FFMPEG Output parameters \"\"\"\n        codec = self.config[\"codec\"]\n        tune = self.config[\"tune\"]\n        # Force all frames to the same size\n        output_args = [\"-vf\", f\"scale={self._output_dimensions}\"]\n\n        output_args.extend([\"-crf\", str(self.config[\"crf\"])])\n        output_args.extend([\"-preset\", self.config[\"preset\"]])\n\n        if tune is not None and tune in self._valid_tunes[codec]:\n            output_args.extend([\"-tune\", tune])\n\n        if codec == \"libx264\" and self.config[\"profile\"] != \"auto\":\n            output_args.extend([\"-profile:v\", self.config[\"profile\"]])\n\n        if codec == \"libx264\" and self.config[\"level\"] != \"auto\":\n            output_args.extend([\"-level\", self.config[\"level\"]])\n\n        logger.debug(output_args)\n        return output_args\n\n    @property\n    def _audio_codec(self) -> str | None:\n        \"\"\" str or ``None``: The audio codec to use. This will either be ``\"copy\"`` (the default)\n        or ``None`` if skip muxing has been selected in configuration options, or if frame ranges\n        have been passed in the command line arguments. \"\"\"\n        retval: str | None = \"copy\"\n        if self.config[\"skip_mux\"]:\n            logger.info(\"Skipping audio muxing due to configuration settings.\")\n            retval = None\n        elif self._frame_ranges is not None:\n            logger.warning(\"Muxing audio is not supported for limited frame ranges.\"\n                           \"The output video will be created but you will need to mux audio \"\n                           \"manually.\")\n            retval = None\n        elif not self._test_for_audio_stream():\n            logger.warning(\"No audio stream could be found in the source video '%s'. Muxing audio \"\n                           \"will be disabled.\", self._source_video)\n            retval = None\n        logger.debug(\"Audio codec: %s\", retval)\n        return retval\n\n    def _test_for_audio_stream(self) -> bool:\n        \"\"\" Check whether the source video file contains an audio stream.\n\n        If we attempt to mux audio from a source video that does not contain an audio stream\n        ffmpeg will crash faceswap in a fairly ugly manner.\n\n        Returns\n        -------\n        bool\n            ``True`` if an audio stream is found in the source video file, otherwise ``False``\n\n        Raises\n        ------\n        ValueError\n            If a subprocess error is raised scanning the input video file\n        \"\"\"\n        exe = im_ffm.get_ffmpeg_exe()\n        cmd = [exe, \"-hide_banner\", \"-i\", self._source_video, \"-f\", \"ffmetadata\", \"-\"]\n\n        try:\n            out = check_output(cmd, stderr=STDOUT)\n        except CalledProcessError as err:\n            err_out = err.output.decode(errors=\"ignore\")\n            msg = f\"Error checking audio stream. Status: {err.returncode}\\n{err_out}\"\n            raise ValueError(msg) from err\n\n        retval = False\n        for line in out.splitlines():\n            if not line.strip().startswith(b\"Stream #\"):\n                continue\n            logger.debug(\"scanning Stream line: %s\", line.decode(errors=\"ignore\").strip())\n            if b\"Audio\" in line:\n                retval = True\n                break\n        logger.debug(\"Audio found: %s\", retval)\n        return retval\n\n    def _get_output_filename(self) -> str:\n        \"\"\" Return full path to video output file.\n\n        The filename is the same as the input video with `\"_converted\"` appended to the end. The\n        file extension is as selected in the plugin settings. If a file already exists with the\n        given filename, then `\"_1\"` is appended to the end of the filename. This number iterates\n        until a valid filename that does not exist is found.\n\n        Returns\n        -------\n        str\n            The full path to the output video filename\n        \"\"\"\n        filename = os.path.basename(self._source_video)\n        filename = os.path.splitext(filename)[0]\n        ext = self.config[\"container\"]\n        idx = 0\n        while True:\n            out_file = f\"{filename}_converted{'' if idx == 0 else f'_{idx}'}.{ext}\"\n            retval = os.path.join(self.output_folder, out_file)\n            if not os.path.exists(retval):\n                break\n            idx += 1\n        logger.info(\"Outputting to: '%s'\", retval)\n        return retval\n\n    def _get_writer(self, frame_dims: tuple[int, int]) -> Generator[None, np.ndarray, None]:\n        \"\"\" Add the requested encoding options and return the writer.\n\n        Parameters\n        ----------\n        frame_dims: tuple\n            The (rows, colums) shape of the input image\n\n        Returns\n        -------\n        generator\n            The imageio ffmpeg writer\n        \"\"\"\n        audio_codec = self._audio_codec\n        audio_path = None if audio_codec is None else self._source_video\n        logger.debug(\"writer config: %s, audio_path: '%s'\", self.config, audio_path)\n\n        retval = im_ffm.write_frames(self._output_filename,\n                                     size=(frame_dims[1], frame_dims[0]),\n                                     fps=self._video_fps,\n                                     quality=None,\n                                     codec=self.config[\"codec\"],\n                                     macro_block_size=8,\n                                     ffmpeg_log_level=\"error\",\n                                     ffmpeg_timeout=10,\n                                     output_params=self._output_params,\n                                     audio_path=audio_path,\n                                     audio_codec=audio_codec)\n        logger.debug(\"FFMPEG Writer created: %s\", retval)\n        retval.send(None)\n\n        return retval\n\n    def write(self, filename: str, image: np.ndarray) -> None:\n        \"\"\" Frames come from the pool in arbitrary order, so frames are cached for writing out\n        in the correct order.\n\n        Parameters\n        ----------\n        filename: str\n            The incoming frame filename.\n        image: :class:`numpy.ndarray`\n            The converted image to be written\n        \"\"\"\n        logger.trace(\"Received frame: (filename: '%s', shape: %s\",  # type:ignore[attr-defined]\n                     filename, image.shape)\n        if not self._output_dimensions:\n            input_dims = T.cast(tuple[int, int], image.shape[:2])\n            self._set_dimensions(input_dims)\n            self._writer = self._get_writer(input_dims)\n        self.cache_frame(filename, image)\n        self._save_from_cache()\n\n    def _set_dimensions(self, frame_dims: tuple[int, int]) -> None:\n        \"\"\" Set the attribute :attr:`_output_dimensions` based on the first frame received.\n        This protects against different sized images coming in and ensures all images are written\n        to ffmpeg at the same size. Dimensions are mapped to a macro block size 8.\n\n        Parameters\n        ----------\n        frame_dims: tuple\n            The (rows, colums) shape of the input image\n        \"\"\"\n        logger.debug(\"input dimensions: %s\", frame_dims)\n        self._output_dimensions = (f\"{int(ceil(frame_dims[1] / 8) * 8)}:\"\n                                   f\"{int(ceil(frame_dims[0] / 8) * 8)}\")\n        logger.debug(\"Set dimensions: %s\", self._output_dimensions)\n\n    def _save_from_cache(self) -> None:\n        \"\"\" Writes any consecutive frames to the video container that are ready to be output\n        from the cache. \"\"\"\n        assert self._writer is not None\n        while self._frame_order:\n            if self._frame_order[0] not in self.cache:\n                logger.trace(\"Next frame not ready. Continuing\")  # type:ignore[attr-defined]\n                break\n            save_no = self._frame_order.pop(0)\n            save_image = self.cache.pop(save_no)\n            logger.trace(\"Rendering from cache. Frame no: %s\",  # type:ignore[attr-defined]\n                         save_no)\n            self._writer.send(np.ascontiguousarray(save_image[:, :, ::-1]))\n        logger.trace(\"Current cache size: %s\", len(self.cache))  # type:ignore[attr-defined]\n\n    def close(self) -> None:\n        \"\"\" Close the ffmpeg writer and mux the audio \"\"\"\n        if self._writer is not None:\n            self._writer.close()\n", "plugins/convert/mask/mask_blend.py": "#!/usr/bin/env python3\n\"\"\" Plugin to blend the edges of the face between the swap and the original face. \"\"\"\nimport logging\nimport typing as T\n\nimport cv2\nimport numpy as np\n\nfrom lib.align import BlurMask, DetectedFace\nfrom lib.config import FaceswapConfig\nfrom plugins.convert._config import Config\n\nlogger = logging.getLogger(__name__)\n\n\nclass Mask():\n    \"\"\" Manipulations to perform to the mask that is to be applied to the output of the Faceswap\n    model.\n\n    Parameters\n    ----------\n    mask_type: str\n        The mask type to use for this plugin\n    output_size: int\n        The size of the output from the Faceswap model.\n    coverage_ratio: float\n        The coverage ratio that the Faceswap model was trained at.\n    configfile: str, Optional\n        Optional location of custom configuration ``ini`` file. If ``None`` then use the default\n        config location. Default: ``None``\n    config: :class:`lib.config.FaceswapConfig`, Optional\n        Optional pre-loaded :class:`lib.config.FaceswapConfig`. If passed, then this will be used\n        over any configuration on disk. If ``None`` then it is ignored. Default: ``None``\n\n    \"\"\"\n    def __init__(self,\n                 mask_type: str,\n                 output_size: int,\n                 coverage_ratio: float,\n                 configfile: str | None = None,\n                 config: FaceswapConfig | None = None) -> None:\n        logger.debug(\"Initializing %s: (mask_type: '%s', output_size: %s, coverage_ratio: %s, \"\n                     \"configfile: %s, config: %s)\", self.__class__.__name__, mask_type,\n                     coverage_ratio, output_size, configfile, config)\n        self._mask_type = mask_type\n        self._config = self._set_config(configfile, config)\n        logger.debug(\"config: %s\", self._config)\n\n        self._coverage_ratio = coverage_ratio\n        self._box = self._get_box(output_size)\n\n        erode_types = [f\"erosion{f}\" for f in [\"\", \"_left\", \"_top\", \"_right\", \"_bottom\"]]\n        self._erodes = [self._config.get(erode, 0) / 100 for erode in erode_types]\n        self._do_erode = any(amount != 0 for amount in self._erodes)\n\n    def _set_config(self,\n                    configfile: str | None,\n                    config: FaceswapConfig | None) -> dict:\n        \"\"\" Set the correct configuration for the plugin based on whether a config file\n        or pre-loaded config has been passed in.\n\n        Parameters\n        ----------\n        configfile: str\n            Location of custom configuration ``ini`` file. If ``None`` then use the\n            default config location\n        config: :class:`lib.config.FaceswapConfig`\n            Pre-loaded :class:`lib.config.FaceswapConfig`. If passed, then this will be\n            used over any configuration on disk. If ``None`` then it is ignored.\n\n        Returns\n        -------\n        dict\n            The configuration in dictionary form for the given from\n            :attr:`lib.config.FaceswapConfig.config_dict`\n        \"\"\"\n        section = \".\".join(self.__module__.split(\".\")[-2:])\n        if config is None:\n            retval = Config(section, configfile=configfile).config_dict\n        else:\n            config.section = section\n            retval = config.config_dict\n            config.section = None\n        logger.debug(\"Config: %s\", retval)\n        return retval\n\n    def _get_box(self, output_size: int) -> np.ndarray:\n        \"\"\" Apply a gradient overlay to the edge of the swap box to smooth out any hard areas\n        that where the face intersects with the edge of the swap area.\n\n        Gradient is created from 1/16th distance from the edge of the face box and uses the\n        parameters as provided for mask blend settings\n\n        Parameters\n        ----------\n        output_size: int\n            The size of the box that contains the swapped face\n\n        Returns\n        -------\n        :class:`numpy.ndarray`\n            The box mask\n        \"\"\"\n        box = np.zeros((output_size, output_size, 1), dtype=\"float32\")\n        edge = (output_size // 32) + 1\n        box[edge:-edge, edge:-edge] = 1.0\n\n        if self._config[\"type\"] is not None:\n            box = BlurMask(\"gaussian\",\n                           box,\n                           6,\n                           is_ratio=True).blurred\n        return box\n\n    def run(self,\n            detected_face: DetectedFace,\n            source_offset: np.ndarray,\n            target_offset: np.ndarray,\n            centering: T.Literal[\"legacy\", \"face\", \"head\"],\n            predicted_mask: np.ndarray | None = None) -> tuple[np.ndarray, np.ndarray]:\n        \"\"\" Obtain the requested mask type and perform any defined mask manipulations.\n\n        Parameters\n        ----------\n        detected_face: :class:`lib.align.DetectedFace`\n            The DetectedFace object as returned from :class:`scripts.convert.Predictor`.\n        source_offset: :class:`numpy.ndarray`\n            The (x, y) offset for the mask at its stored centering\n        target_offset: :class:`numpy.ndarray`\n            The (x, y) offset for the mask at the requested target centering\n        centering: [`\"legacy\"`, `\"face\"`, `\"head\"`]\n            The centering to obtain the mask for\n        predicted_mask: :class:`numpy.ndarray`, optional\n            The predicted mask as output from the Faceswap Model, if the model was trained\n            with a mask, otherwise ``None``. Default: ``None``.\n\n        Returns\n        -------\n        mask: :class:`numpy.ndarray`\n            The mask with all requested manipulations applied\n        raw_mask: :class:`numpy.ndarray`\n            The mask with no erosion/dilation applied\n        \"\"\"\n        logger.trace(\"Performing mask adjustment: (detected_face: %s, \"  # type: ignore\n                     \"source_offset: %s, target_offset: %s, centering: '%s', predicted_mask: %s\",\n                     detected_face, source_offset, target_offset, centering,\n                     predicted_mask is not None)\n        mask = self._get_mask(detected_face,\n                              predicted_mask,\n                              centering,\n                              source_offset,\n                              target_offset)\n        raw_mask = mask.copy()\n\n        if self._mask_type != \"none\":\n            out = self._erode(mask) if self._do_erode else mask\n            out = np.minimum(out, self._box)\n        else:\n            out = mask\n\n        logger.trace(  # type: ignore\n            \"mask shape: %s, raw_mask shape: %s\", mask.shape, raw_mask.shape)\n        return out, raw_mask\n\n    def _get_mask(self,\n                  detected_face: DetectedFace,\n                  predicted_mask: np.ndarray | None,\n                  centering: T.Literal[\"legacy\", \"face\", \"head\"],\n                  source_offset: np.ndarray,\n                  target_offset: np.ndarray) -> np.ndarray:\n        \"\"\" Return the requested mask with any requested blurring applied.\n\n        Parameters\n        ----------\n        detected_face: :class:`lib.align.DetectedFace`\n            The DetectedFace object as returned from :class:`scripts.convert.Predictor`.\n        predicted_mask: :class:`numpy.ndarray`\n            The predicted mask as output from the Faceswap Model if the model was trained\n            with a mask, otherwise ``None``\n        centering: [`\"legacy\"`, `\"face\"`, `\"head\"`]\n            The centering to obtain the mask for\n        source_offset: :class:`numpy.ndarray`\n            The (x, y) offset for the mask at its stored centering\n        target_offset: :class:`numpy.ndarray`\n            The (x, y) offset for the mask at the requested target centering\n\n        Returns\n        -------\n        :class:`numpy.ndarray`\n            The requested mask.\n        \"\"\"\n        if self._mask_type == \"none\":\n            mask = np.ones_like(self._box)  # Return a dummy mask if not using a mask\n        elif self._mask_type == \"predicted\" and predicted_mask is not None:\n            mask = self._process_predicted_mask(predicted_mask)\n        else:\n            mask = self._get_stored_mask(detected_face, centering, source_offset, target_offset)\n\n        logger.trace(mask.shape)  # type: ignore\n        return mask\n\n    def _process_predicted_mask(self, mask: np.ndarray) -> np.ndarray:\n        \"\"\" Process blurring of the predicted mask\n\n        Parameters\n        ----------\n        mask: :class:`numpy.ndarray`\n            The predicted mask as output from the Faceswap Model\n\n        Returns\n        ------\n        :class:`numpy.ndarray`\n            The processed predicted mask\n        \"\"\"\n        blur_type = self._config[\"type\"].lower()\n        if blur_type is not None:\n            mask = BlurMask(blur_type,\n                            mask,\n                            self._config[\"kernel_size\"],\n                            passes=self._config[\"passes\"]).blurred\n        return mask\n\n    def _get_stored_mask(self,\n                         detected_face: DetectedFace,\n                         centering: T.Literal[\"legacy\", \"face\", \"head\"],\n                         source_offset: np.ndarray,\n                         target_offset: np.ndarray) -> np.ndarray:\n        \"\"\" get the requested stored mask from the detected face object.\n\n        Parameters\n        ----------\n        detected_face: :class:`lib.align.DetectedFace`\n            The DetectedFace object as returned from :class:`scripts.convert.Predictor`.\n        centering: [`\"legacy\"`, `\"face\"`, `\"head\"`]\n            The centering to obtain the mask for\n        source_offset: :class:`numpy.ndarray`\n            The (x, y) offset for the mask at its stored centering\n        target_offset: :class:`numpy.ndarray`\n            The (x, y) offset for the mask at the requested target centering\n\n        Returns\n        -------\n        :class:`numpy.ndarray`\n            The mask sized to Faceswap model output with any requested blurring applied.\n        \"\"\"\n        mask = detected_face.mask[self._mask_type]\n        mask.set_blur_and_threshold(blur_kernel=self._config[\"kernel_size\"],\n                                    blur_type=self._config[\"type\"],\n                                    blur_passes=self._config[\"passes\"],\n                                    threshold=self._config[\"threshold\"])\n        mask.set_sub_crop(source_offset, target_offset, centering, self._coverage_ratio)\n        face_mask = mask.mask\n        mask_size = face_mask.shape[0]\n        face_size = self._box.shape[0]\n        if mask_size != face_size:\n            interp = cv2.INTER_CUBIC if mask_size < face_size else cv2.INTER_AREA\n            face_mask = cv2.resize(face_mask,\n                                   self._box.shape[:2],\n                                   interpolation=interp)[..., None].astype(\"float32\") / 255.\n        else:\n            face_mask = face_mask.astype(\"float32\") / 255.\n        return face_mask\n\n    # MASK MANIPULATIONS\n    def _erode(self, mask: np.ndarray) -> np.ndarray:\n        \"\"\" Erode or dilate mask the mask based on configuration options.\n\n        Parameters\n        ----------\n        mask: :class:`numpy.ndarray`\n            The mask to be eroded or dilated\n\n        Returns\n        -------\n        :class:`numpy.ndarray`\n            The mask with erosion/dilation applied\n        \"\"\"\n        kernels = self._get_erosion_kernels(mask)\n        if not any(k.any() for k in kernels):\n            return mask  # No kernels could be created from selected input res\n        eroded = mask\n        for idx, (kernel, ratio) in enumerate(zip(kernels, self._erodes)):\n            if not kernel.any():\n                continue\n            anchor = [-1, -1]\n            if idx > 0:\n                pos = 1 if idx % 2 == 0 else 0\n                if ratio > 0:\n                    val = max(kernel.shape) - 1 if idx < 3 else 0\n                else:\n                    val = 0 if idx < 3 else max(kernel.shape) - 1\n                anchor[pos] = val\n\n            func = cv2.erode if ratio > 0 else cv2.dilate\n            eroded = func(eroded, kernel, iterations=1, anchor=anchor)\n\n        return eroded[..., None]\n\n    def _get_erosion_kernels(self, mask: np.ndarray) -> list[np.ndarray]:\n        \"\"\" Get the erosion kernels for each of the center, left, top right and bottom erosions.\n\n        An approximation is made based on the number of positive pixels within the mask to create\n        an ellipse to act as kernel.\n\n        Parameters\n        ----------\n        mask: :class:`numpy.ndarray`\n            The mask to be eroded or dilated\n\n        Returns\n        -------\n        list\n            The erosion kernels to be used for erosion/dilation\n        \"\"\"\n        mask_radius = np.sqrt(np.sum(mask)) / 2\n        kernel_sizes = [max(0, int(abs(ratio * mask_radius))) for ratio in self._erodes]\n        kernels = []\n        for idx, size in enumerate(kernel_sizes):\n            kernel = [size, size]\n            shape = cv2.MORPH_ELLIPSE if idx == 0 else cv2.MORPH_RECT\n            if idx > 1:\n                pos = 0 if idx % 2 == 0 else 1\n                kernel[pos] = 1  # Set x/y to 1px based on whether eroding top/bottom, left/right\n            kernels.append(cv2.getStructuringElement(shape, kernel) if size else np.array(0))\n        logger.trace(\"Erosion kernels: %s\", [k.shape for k in kernels])  # type: ignore\n        return kernels\n", "plugins/convert/mask/mask_blend_defaults.py": "#!/usr/bin/env python3\n\"\"\"\n    The default options for the faceswap Mask_Blend Mask plugin.\n\n    Defaults files should be named <plugin_name>_defaults.py\n    Any items placed into this file will automatically get added to the relevant config .ini files\n    within the faceswap/config folder.\n\n    The following variables should be defined:\n        _HELPTEXT: A string describing what this plugin does\n        _DEFAULTS: A dictionary containing the options, defaults and meta information. The\n                   dictionary should be defined as:\n                       {<option_name>: {<metadata>}}\n\n                   <option_name> should always be lower text.\n                   <metadata> dictionary requirements are listed below.\n\n    The following keys are expected for the _DEFAULTS <metadata> dict:\n        datatype:  [required] A python type class. This limits the type of data that can be\n                   provided in the .ini file and ensures that the value is returned in the\n                   correct type to faceswap. Valid data types are: <class 'int'>, <class 'float'>,\n                   <class 'str'>, <class 'bool'>.\n        default:   [required] The default value for this option.\n        info:      [required] A string describing what this option does.\n        choices:   [optional] If this option's datatype is of <class 'str'> then valid\n                   selections can be defined here. This validates the option and also enables\n                   a combobox / radio option in the GUI.\n        gui_radio: [optional] If <choices> are defined, this indicates that the GUI should use\n                   radio buttons rather than a combobox to display this option.\n        min_max:   [partial] For <class 'int'> and <class 'float'> data types this is required\n                   otherwise it is ignored. Should be a tuple of min and max accepted values.\n                   This is used for controlling the GUI slider range. Values are not enforced.\n        rounding:  [partial] For <class 'int'> and <class 'float'> data types this is\n                   required otherwise it is ignored. Used for the GUI slider. For floats, this\n                   is the number of decimal places to display. For ints this is the step size.\n        fixed:     [optional] [train only]. Training configurations are fixed when the model is\n                   created, and then reloaded from the state file. Marking an item as fixed=False\n                   indicates that this value can be changed for existing models, and will override\n                   the value saved in the state file with the updated value in config. If not\n                   provided this will default to True.\n\"\"\"\n\n\n_HELPTEXT = \"Options for blending the edges between the mask and the background image\"\n\n\n_DEFAULTS = dict(\n    type=dict(\n        default=\"normalized\",\n        info=\"The type of blending to use:\"\n             \"\\n\\t gaussian: Blend with Gaussian filter. Slower, but often better than Normalized\"\n             \"\\n\\t normalized: Blend with Normalized box filter. Faster than Gaussian\"\n             \"\\n\\t none: Don't perform blending\",\n        datatype=str,\n        rounding=None,\n        min_max=None,\n        choices=[\"gaussian\", \"normalized\", \"none\"],\n        gui_radio=True,\n        group=\"Blending type\",\n        fixed=True,\n    ),\n    kernel_size=dict(\n        default=3,\n        info=\"The kernel size dictates how much blending should occur.\\n\"\n             \"The size is the diameter of the kernel in pixels (calculated from a 128px mask). \"\n             \"This value should be odd, if an even number is passed in then it will be rounded to \"\n             \"the next odd number. Higher sizes means more blending.\",\n        datatype=int,\n        rounding=1,\n        min_max=(1, 9),\n        choices=[],\n        gui_radio=False,\n        group=\"settings\",\n        fixed=True,\n    ),\n    passes=dict(\n        default=4,\n        info=\"The number of passes to perform. Additional passes of the blending algorithm can \"\n             \"improve smoothing at a time cost. This is more useful for 'box' type blending.\\n\"\n             \"Additional passes have exponentially less effect so it's not worth setting this too \"\n             \"high.\",\n        datatype=int,\n        rounding=1,\n        min_max=(1, 8),\n        choices=[],\n        gui_radio=False,\n        group=\"settings\",\n        fixed=True,\n    ),\n    threshold=dict(\n        default=4,\n        info=\"Sets pixels that are near white to white and near black to black. Set to 0 for off.\",\n        datatype=int,\n        rounding=1,\n        min_max=(0, 50),\n        choices=[],\n        gui_radio=False,\n        group=\"settings\",\n        fixed=True,\n    ),\n    erosion=dict(\n        default=0.0,\n        info=\"Apply erosion to the whole of the face mask.\\n\"\n             \"Erosion kernel size as a percentage of the mask radius area.\\n\"\n             \"Positive values apply erosion which reduces the size of the swapped area.\\n\"\n             \"Negative values apply dilation which increases the swapped area.\",\n        datatype=float,\n        rounding=1,\n        min_max=(-100.0, 100.0),\n        choices=[],\n        gui_radio=False,\n        group=\"settings\",\n        fixed=True,\n    ),\n    erosion_top=dict(\n        default=0.0,\n        info=\"Apply erosion to the top part of the mask only.\\n\"\n             \"Positive values apply erosion which pulls the mask into the center.\\n\"\n             \"Negative values apply dilation which pushes the mask away from the center.\",\n        datatype=float,\n        rounding=1,\n        min_max=(-100.0, 100.0),\n        choices=[],\n        gui_radio=False,\n        group=\"settings\",\n        fixed=True,\n    ),\n    erosion_bottom=dict(\n        default=0.0,\n        info=\"Apply erosion to the bottom part of the mask only.\\n\"\n             \"Positive values apply erosion which pulls the mask into the center.\\n\"\n             \"Negative values apply dilation which pushes the mask away from the center.\",\n        datatype=float,\n        rounding=1,\n        min_max=(-100.0, 100.0),\n        choices=[],\n        gui_radio=False,\n        group=\"settings\",\n        fixed=True,\n    ),\n    erosion_left=dict(\n        default=0.0,\n        info=\"Apply erosion to the left part of the mask only.\\n\"\n             \"Positive values apply erosion which pulls the mask into the center.\\n\"\n             \"Negative values apply dilation which pushes the mask away from the center.\",\n        datatype=float,\n        rounding=1,\n        min_max=(-100.0, 100.0),\n        choices=[],\n        gui_radio=False,\n        group=\"settings\",\n        fixed=True,\n    ),\n    erosion_right=dict(\n        default=0.0,\n        info=\"Apply erosion to the right part of the mask only.\\n\"\n             \"Positive values apply erosion which pulls the mask into the center.\\n\"\n             \"Negative values apply dilation which pushes the mask away from the center.\",\n        datatype=float,\n        rounding=1,\n        min_max=(-100.0, 100.0),\n        choices=[],\n        gui_radio=False,\n        group=\"settings\",\n        fixed=True,\n    ),\n)\n", "plugins/convert/mask/__init__.py": "", "tools/__init__.py": "", "tools/sort/sort_methods_aligned.py": "#!/usr/bin/env python3\n\"\"\" Sorting methods that use the properties of a :class:`lib.align.AlignedFace` object to obtain\ntheir sorting metrics.\n\"\"\"\nfrom __future__ import annotations\nimport logging\nimport operator\nimport sys\nimport typing as T\n\nimport numpy as np\nfrom tqdm import tqdm\n\nfrom lib.align import AlignedFace, LandmarkType\nfrom lib.utils import FaceswapError\nfrom .sort_methods import SortMethod\n\nif T.TYPE_CHECKING:\n    from argparse import Namespace\n    from lib.align.alignments import PNGHeaderAlignmentsDict\n\nlogger = logging.getLogger(__name__)\n\n\nclass SortAlignedMetric(SortMethod):\n    \"\"\" Sort by comparison of metrics stored in an Aligned Face objects. This is a parent class\n    for sort by aligned metrics methods. Individual methods should inherit from this class\n\n    Parameters\n    ----------\n    arguments: :class:`argparse.Namespace`\n        The command line arguments passed to the sort process\n    sort_reverse: bool, optional\n        ``True`` if the sorted results should be in reverse order. Default: ``True``\n    is_group: bool, optional\n        Set to ``True`` if this class is going to be called exclusively for binning.\n        Default: ``False``\n    \"\"\"\n\n    _logged_lm_count_once: bool = False\n\n    def _get_metric(self, aligned_face: AlignedFace) -> np.ndarray | float:\n        \"\"\" Obtain the correct metric for the given sort method\"\n\n        Parameters\n        ----------\n        aligned_face: :class:`lib.align.AlignedFace`\n            The aligned face to extract the metric from\n\n        Returns\n        -------\n        float or :class:`numpy.ndarray`\n            The metric for the current face based on chosen sort method\n        \"\"\"\n        raise NotImplementedError\n\n    def sort(self) -> None:\n        \"\"\" Sort by metric score. Order in reverse for distance sort. \"\"\"\n        logger.info(\"Sorting...\")\n        self._result = sorted(self._result, key=operator.itemgetter(1), reverse=True)\n\n    def score_image(self,\n                    filename: str,\n                    image: np.ndarray | None,\n                    alignments: PNGHeaderAlignmentsDict | None) -> None:\n        \"\"\" Score a single image for sort method: \"distance\", \"yaw\", \"pitch\" or \"size\" and add the\n        result to :attr:`_result`\n\n        Parameters\n        ----------\n        filename: str\n            The filename of the currently processing image\n        image: :class:`np.ndarray` or ``None``\n            A face image loaded from disk or ``None``\n        alignments: dict or ``None``\n            The alignments dictionary for the aligned face or ``None``\n        \"\"\"\n        if self._log_once:\n            msg = \"Grouping\" if self._is_group else \"Sorting\"\n            logger.info(\"%s by %s...\", msg, self._method)\n            self._log_once = False\n\n        if not alignments:\n            msg = (\"The images to be sorted do not contain alignment data. Images must have \"\n                   \"been generated by Faceswap's Extract process.\\nIf you are sorting an \"\n                   \"older faceset, then you should re-extract the faces from your source \"\n                   \"alignments file to generate this data.\")\n            raise FaceswapError(msg)\n\n        face = AlignedFace(np.array(alignments[\"landmarks_xy\"], dtype=\"float32\"))\n        if (not self._logged_lm_count_once\n                and face.landmark_type == LandmarkType.LM_2D_4\n                and self.__class__.__name__ != \"SortSize\"):\n            logger.warning(\"You have selected to sort by an aligned metric, but at least one face \"\n                           \"does not contain facial landmark data. This probably won't work\")\n            self._logged_lm_count_once = True\n        self._result.append((filename, self._get_metric(face)))\n\n\nclass SortDistance(SortAlignedMetric):\n    \"\"\" Sorting mechanism for sorting faces from small to large \"\"\"\n    def _get_metric(self, aligned_face: AlignedFace) -> float:\n        \"\"\" Obtain the distance from mean face metric for the given face\n\n        Parameters\n        ----------\n        aligned_face: :class:`lib.align.AlignedFace`\n            The aligned face to extract the metric from\n\n        Returns\n        -------\n        float\n            The distance metric for the current face\n        \"\"\"\n        return aligned_face.average_distance\n\n    def sort(self) -> None:\n        \"\"\" Override default sort to sort in ascending order. \"\"\"\n        logger.info(\"Sorting...\")\n        self._result = sorted(self._result, key=operator.itemgetter(1), reverse=False)\n\n    def binning(self) -> list[list[str]]:\n        \"\"\" Create bins to split linearly from the lowest to the highest sample value\n\n        Returns\n        -------\n        list\n            List of bins of filenames\n        \"\"\"\n        return self._binning_linear_threshold(multiplier=100)\n\n\nclass SortPitch(SortAlignedMetric):\n    \"\"\" Sorting mechansim for sorting a face by pitch (down to up) \"\"\"\n    def _get_metric(self, aligned_face: AlignedFace) -> float:\n        \"\"\" Obtain the pitch metric for the given face\n\n        Parameters\n        ----------\n        aligned_face: :class:`lib.align.AlignedFace`\n            The aligned face to extract the metric from\n\n        Returns\n        -------\n        float\n            The pitch metric for the current face\n        \"\"\"\n        return aligned_face.pose.pitch\n\n    def binning(self) -> list[list[str]]:\n        \"\"\" Create bins from 0 degrees to 180 degrees based on number of bins\n\n        Allocate item to bin when it is in range of one of the pre-allocated bins\n\n        Returns\n        -------\n        list\n            List of bins of filenames\n        \"\"\"\n        thresholds = np.linspace(90, -90, self._num_bins + 1)\n\n        # Start bin names from 0 for more intuitive experience\n        names = np.flip(thresholds.astype(\"int\")) + 90\n        self._bin_names = [f\"{self._method}_\"\n                           f\"{idx:03d}_{int(names[idx])}\"\n                           f\"degs_to_{int(names[idx + 1])}degs\"\n                           for idx in range(self._num_bins)]\n\n        bins: list[list[str]] = [[] for _ in range(self._num_bins)]\n        for filename, result in self._result:\n            result = np.clip(result, -90.0, 90.0)\n            bin_idx = next(bin_id for bin_id, thresh in enumerate(thresholds)\n                           if result >= thresh) - 1\n            bins[bin_idx].append(filename)\n        return bins\n\n\nclass SortYaw(SortPitch):\n    \"\"\" Sorting mechansim for sorting a face by yaw (left to right). Same logic as sort pitch, but\n    with different metric \"\"\"\n    def _get_metric(self, aligned_face: AlignedFace) -> float:\n        \"\"\" Obtain the yaw metric for the given face\n\n        Parameters\n        ----------\n        aligned_face: :class:`lib.align.AlignedFace`\n            The aligned face to extract the metric from\n\n        Returns\n        -------\n        float\n            The yaw metric for the current face\n        \"\"\"\n        return aligned_face.pose.yaw\n\n\nclass SortRoll(SortPitch):\n    \"\"\" Sorting mechansim for sorting a face by roll (rotation). Same logic as sort pitch, but\n    with different metric \"\"\"\n    def _get_metric(self, aligned_face: AlignedFace) -> float:\n        \"\"\" Obtain the roll metric for the given face\n\n        Parameters\n        ----------\n        aligned_face: :class:`lib.align.AlignedFace`\n            The aligned face to extract the metric from\n\n        Returns\n        -------\n        float\n            The yaw metric for the current face\n        \"\"\"\n        return aligned_face.pose.roll\n\n\nclass SortSize(SortAlignedMetric):\n    \"\"\" Sorting mechanism for sorting faces from small to large \"\"\"\n    def _get_metric(self, aligned_face: AlignedFace) -> float:\n        \"\"\" Obtain the size metric for the given face\n\n        Parameters\n        ----------\n        aligned_face: :class:`lib.align.AlignedFace`\n            The aligned face to extract the metric from\n\n        Returns\n        -------\n        float\n            The size metric for the current face\n        \"\"\"\n        roi = aligned_face.original_roi\n        size = ((roi[1][0] - roi[0][0]) ** 2 + (roi[1][1] - roi[0][1]) ** 2) ** 0.5\n        return size\n\n    def binning(self) -> list[list[str]]:\n        \"\"\" Create bins to split linearly from the lowest to the highest sample value\n\n        Allocate item to bin when it is in range of one of the pre-allocated bins\n\n        Returns\n        -------\n        list\n            List of bins of filenames\n        \"\"\"\n        return self._binning_linear_threshold(units=\"px\")\n\n\nclass SortFaceCNN(SortAlignedMetric):\n    \"\"\" Sort by landmark similarity or dissimilarity\n\n    Parameters\n    ----------\n    arguments: :class:`argparse.Namespace`\n        The command line arguments passed to the sort process\n    is_group: bool, optional\n        Set to ``True`` if this class is going to be called exclusively for binning.\n        Default: ``False``\n    \"\"\"\n    def __init__(self, arguments: Namespace, is_group: bool = False) -> None:\n        super().__init__(arguments, is_group=is_group)\n        self._is_dissim = self._method == \"face-cnn-dissim\"\n        self._threshold: float = 7.2 if arguments.threshold < 1.0 else arguments.threshold\n\n    def _get_metric(self, aligned_face: AlignedFace) -> np.ndarray:\n        \"\"\" Obtain the xy aligned landmarks for the face\"\n\n        Parameters\n        ----------\n        aligned_face: :class:`lib.align.AlignedFace`\n            The aligned face to extract the metric from\n\n        Returns\n        -------\n        float\n            The metric for the current face based on chosen sort method\n        \"\"\"\n        return aligned_face.landmarks\n\n    def sort(self) -> None:\n        \"\"\" Sort by landmarks. \"\"\"\n        logger.info(\"Comparing landmarks and sorting...\")\n        if self._is_dissim:\n            self._sort_landmarks_dissim()\n            return\n        self._sort_landmarks_ssim()\n\n    def _sort_landmarks_ssim(self) -> None:\n        \"\"\" Sort landmarks by similarity \"\"\"\n        img_list_len = len(self._result)\n        for i in tqdm(range(0, img_list_len - 1), desc=\"Comparing\", file=sys.stdout, leave=False):\n            min_score = float(\"inf\")\n            j_min_score = i + 1\n            for j in range(i + 1, img_list_len):\n                fl1 = self._result[i][1]\n                fl2 = self._result[j][1]\n                score = np.sum(np.absolute((fl2 - fl1).flatten()))\n                if score < min_score:\n                    min_score = score\n                    j_min_score = j\n            (self._result[i + 1], self._result[j_min_score]) = (self._result[j_min_score],\n                                                                self._result[i + 1])\n\n    def _sort_landmarks_dissim(self) -> None:\n        \"\"\" Sort landmarks by dissimilarity \"\"\"\n        logger.info(\"Comparing landmarks...\")\n        img_list_len = len(self._result)\n        for i in tqdm(range(0, img_list_len - 1), desc=\"Comparing\", file=sys.stdout, leave=False):\n            score_total = 0\n            for j in range(i + 1, img_list_len):\n                if i == j:\n                    continue\n                fl1 = self._result[i][1]\n                fl2 = self._result[j][1]\n                score_total += np.sum(np.absolute((fl2 - fl1).flatten()))\n            self._result[i][2] = score_total\n\n        logger.info(\"Sorting...\")\n        self._result = sorted(self._result, key=operator.itemgetter(2), reverse=True)\n\n    def binning(self) -> list[list[str]]:\n        \"\"\" Group into bins by CNN face similarity\n\n        Returns\n        -------\n        list\n            List of bins of filenames\n        \"\"\"\n        msg = \"dissimilarity\" if self._is_dissim else \"similarity\"\n        logger.info(\"Grouping by face-cnn %s...\", msg)\n\n        # Groups are of the form: group_num -> reference faces\n        reference_groups: dict[int, list[np.ndarray]] = {}\n\n        # Bins array, where index is the group number and value is\n        # an array containing the file paths to the images in that group.\n        bins: list[list[str]] = []\n\n        # Comparison threshold used to decide how similar\n        # faces have to be to be grouped together.\n        # It is multiplied by 1000 here to allow the cli option to use smaller\n        # numbers.\n        threshold = self._threshold * 1000\n        img_list_len = len(self._result)\n\n        for i in tqdm(range(0, img_list_len - 1),\n                      desc=\"Grouping\",\n                      file=sys.stdout,\n                      leave=False):\n            fl1 = self._result[i][1]\n\n            current_key = -1\n            current_score = float(\"inf\")\n\n            for key, references in reference_groups.items():\n                try:\n                    score = self._get_avg_score(fl1, references)\n                except TypeError:\n                    score = float(\"inf\")\n                except ZeroDivisionError:\n                    score = float(\"inf\")\n                if score < current_score:\n                    current_key, current_score = key, score\n\n            if current_score < threshold:\n                reference_groups[current_key].append(fl1[0])\n                bins[current_key].append(self._result[i][0])\n            else:\n                reference_groups[len(reference_groups)] = [self._result[i][1]]\n                bins.append([self._result[i][0]])\n\n        return bins\n\n    @classmethod\n    def _get_avg_score(cls, face: np.ndarray, references: list[np.ndarray]) -> float:\n        \"\"\" Return the average CNN similarity score between a face and reference images\n\n        Parameters\n        ----------\n        face: :class:`numpy.ndarray`\n            The face to check against reference images\n        references: list\n            List of reference arrays to compare the face against\n\n        Returns\n        -------\n        float\n            The average score between the face and the references\n        \"\"\"\n        scores = []\n        for ref in references:\n            score = np.sum(np.absolute((ref - face).flatten()))\n            scores.append(score)\n        return sum(scores) / len(scores)\n", "tools/sort/cli.py": "#!/usr/bin/env python3\n\"\"\" Command Line Arguments for tools \"\"\"\nimport argparse\nimport gettext\n\nfrom lib.cli.args import FaceSwapArgs\nfrom lib.cli.actions import DirFullPaths, SaveFileFullPaths, Radio, Slider\n\n\n# LOCALES\n_LANG = gettext.translation(\"tools.sort.cli\", localedir=\"locales\", fallback=True)\n_ = _LANG.gettext\n\n\n_HELPTEXT = _(\"This command lets you sort images using various methods.\")\n_SORT_METHODS = (\n    \"none\", \"blur\", \"blur-fft\", \"distance\", \"face\", \"face-cnn\", \"face-cnn-dissim\",\n    \"yaw\", \"pitch\", \"roll\", \"hist\", \"hist-dissim\", \"color-black\", \"color-gray\", \"color-luma\",\n    \"color-green\", \"color-orange\", \"size\")\n\n_GPTHRESHOLD = _(\" Adjust the '-t' ('--threshold') parameter to control the strength of grouping.\")\n_GPCOLOR = _(\" Adjust the '-b' ('--bins') parameter to control the number of bins for grouping. \"\n             \"Each image is allocated to a bin by the percentage of color pixels that appear in \"\n             \"the image.\")\n_GPDEGREES = _(\" Adjust the '-b' ('--bins') parameter to control the number of bins for grouping. \"\n               \"Each image is allocated to a bin by the number of degrees the face is orientated \"\n               \"from center.\")\n_GPLINEAR = _(\" Adjust the '-b' ('--bins') parameter to control the number of bins for grouping. \"\n              \"The minimum and maximum values are taken for the chosen sort metric. The bins \"\n              \"are then populated with the results from the group sorting.\")\n_METHOD_TEXT = {\n    \"blur\": _(\"faces by blurriness.\"),\n    \"blur-fft\": _(\"faces by fft filtered blurriness.\"),\n    \"distance\": _(\"faces by the estimated distance of the alignments from an 'average' face. This \"\n                  \"can be useful for eliminating misaligned faces. Sorts from most like an \"\n                  \"average face to least like an average face.\"),\n    \"face\": _(\"faces using VGG Face2 by face similarity. This uses a pairwise clustering \"\n              \"algorithm to check the distances between 512 features on every face in your set \"\n              \"and order them appropriately.\"),\n    \"face-cnn\": _(\"faces by their landmarks.\"),\n    \"face-cnn-dissim\": _(\"Like 'face-cnn' but sorts by dissimilarity.\"),\n    \"yaw\": _(\"faces by Yaw (rotation left to right).\"),\n    \"pitch\": _(\"faces by Pitch (rotation up and down).\"),\n    \"roll\": _(\"faces by Roll (rotation). Aligned faces should have a roll value close to zero. \"\n              \"The further the Roll value from zero the higher liklihood the face is misaligned.\"),\n    \"hist\": _(\"faces by their color histogram.\"),\n    \"hist-dissim\": _(\"Like 'hist' but sorts by dissimilarity.\"),\n    \"color-gray\": _(\"images by the average intensity of the converted grayscale color channel.\"),\n    \"color-black\": _(\"images by their number of black pixels. Useful when faces are near borders \"\n                     \"and a large part of the image is black.\"),\n    \"color-luma\": _(\"images by the average intensity of the converted Y color channel. Bright \"\n                    \"lighting and oversaturated images will be ranked first.\"),\n    \"color-green\": _(\"images by the average intensity of the converted Cg color channel. Green \"\n                     \"images will be ranked first and red images will be last.\"),\n    \"color-orange\": _(\"images by the average intensity of the converted Co color channel. Orange \"\n                      \"images will be ranked first and blue images will be last.\"),\n    \"size\": _(\"images by their size in the original frame. Faces further from the camera and from \"\n              \"lower resolution sources will be sorted first, whilst faces closer to the camera \"\n              \"and from higher resolution sources will be sorted last.\")}\n\n_BIN_TYPES = [\n    ((\"face\", \"face-cnn\", \"face-cnn-dissim\", \"hist\", \"hist-dissim\"), _GPTHRESHOLD),\n    ((\"color-black\", \"color-gray\", \"color-luma\", \"color-green\", \"color-orange\"), _GPCOLOR),\n    ((\"yaw\", \"pitch\", \"roll\"), _GPDEGREES),\n    ((\"blur\", \"blur-fft\", \"distance\", \"size\"), _GPLINEAR)]\n_SORT_HELP = \"\"\n_GROUP_HELP = \"\"\n\nfor method in sorted(_METHOD_TEXT):\n    _SORT_HELP += f\"\\nL|{method}: {_('Sort')} {_METHOD_TEXT[method]}\"\n    _GROUP_HELP += (f\"\\nL|{method}: {_('Group')} {_METHOD_TEXT[method]} \"\n                    f\"{next((x[1] for x in _BIN_TYPES if method in x[0]), '')}\")\n\n\nclass SortArgs(FaceSwapArgs):\n    \"\"\" Class to parse the command line arguments for sort tool \"\"\"\n\n    @staticmethod\n    def get_info():\n        \"\"\" Return command information \"\"\"\n        return _(\"Sort faces using a number of different techniques\")\n\n    @staticmethod\n    def get_argument_list():\n        \"\"\" Put the arguments in a list so that they are accessible from both argparse and gui \"\"\"\n        argument_list = []\n        argument_list.append({\n            \"opts\": ('-i', '--input'),\n            \"action\": DirFullPaths,\n            \"dest\": \"input_dir\",\n            \"group\": _(\"data\"),\n            \"help\": _(\"Input directory of aligned faces.\"),\n            \"required\": True})\n        argument_list.append({\n            \"opts\": ('-o', '--output'),\n            \"action\": DirFullPaths,\n            \"dest\": \"output_dir\",\n            \"group\": _(\"data\"),\n            \"help\": _(\n                \"Output directory for sorted aligned faces. If not provided and 'keep' is \"\n                \"selected then a new folder called 'sorted' will be created within the input \"\n                \"folder to house the output. If not provided and 'keep' is not selected then the \"\n                \"images will be sorted in-place, overwriting the original contents of the \"\n                \"'input_dir'\")})\n        argument_list.append({\n            \"opts\": (\"-B\", \"--batch-mode\"),\n            \"action\": \"store_true\",\n            \"dest\": \"batch_mode\",\n            \"default\": False,\n            \"group\": _(\"data\"),\n            \"help\": _(\n                \"R|If selected then the input_dir should be a parent folder containing multiple \"\n                \"folders of faces you wish to sort. The faces will be output to separate sub-\"\n                \"folders in the output_dir\")})\n        argument_list.append({\n            \"opts\": ('-s', '--sort-by'),\n            \"action\": Radio,\n            \"type\": str,\n            \"choices\": _SORT_METHODS,\n            \"dest\": 'sort_method',\n            \"group\": _(\"sort settings\"),\n            \"default\": \"face\",\n            \"help\": _(\n                \"R|Choose how images are sorted. Selecting a sort method gives the images a new \"\n                \"filename based on the order the image appears within the given method.\"\n                \"\\nL|'none': Don't sort the images. When a 'group-by' method is selected, \"\n                \"selecting 'none' means that the files will be moved/copied into their respective \"\n                \"bins, but the files will keep their original filenames. Selecting 'none' for \"\n                \"both 'sort-by' and 'group-by' will do nothing\" + _SORT_HELP + \"\\nDefault: face\")})\n        argument_list.append({\n            \"opts\": ('-g', '--group-by'),\n            \"action\": Radio,\n            \"type\": str,\n            \"choices\": _SORT_METHODS,\n            \"dest\": 'group_method',\n            \"group\": _(\"group settings\"),\n            \"default\": \"none\",\n            \"help\": _(\n                \"R|Selecting a group by method will move/copy files into numbered bins based on \"\n                \"the selected method.\"\n                \"\\nL|'none': Don't bin the images. Folders will be sorted by the selected 'sort-\"\n                \"by' but will not be binned, instead they will be sorted into a single folder. \"\n                \"Selecting 'none' for both 'sort-by' and 'group-by' will do nothing\" +\n                _GROUP_HELP + \"\\nDefault: none\")})\n        argument_list.append({\n            \"opts\": ('-k', '--keep'),\n            \"action\": 'store_true',\n            \"dest\": 'keep_original',\n            \"default\": False,\n            \"group\": _(\"data\"),\n            \"help\": _(\n                \"Whether to keep the original files in their original location. Choosing a 'sort-\"\n                \"by' method means that the files have to be renamed. Selecting 'keep' means that \"\n                \"the original files will be kept, and the renamed files will be created in the \"\n                \"specified output folder. Unselecting keep means that the original files will be \"\n                \"moved and renamed based on the selected sort/group criteria.\")})\n        argument_list.append({\n            \"opts\": ('-t', '--threshold'),\n            \"action\": Slider,\n            \"min_max\": (-1.0, 10.0),\n            \"rounding\": 2,\n            \"type\": float,\n            \"dest\": 'threshold',\n            \"group\": _(\"group settings\"),\n            \"default\": -1.0,\n            \"help\": _(\n                \"R|Float value. Minimum threshold to use for grouping comparison with 'face-cnn' \"\n                \"'hist' and 'face' methods.\"\n                \"\\nThe lower the value the more discriminating the grouping is. Leaving -1.0 will \"\n                \"allow Faceswap to choose the default value.\"\n                \"\\nL|For 'face-cnn' 7.2 should be enough, with 4 being very discriminating. \"\n                \"\\nL|For 'hist' 0.3 should be enough, with 0.2 being very discriminating. \"\n                \"\\nL|For 'face' between 0.1 (more bins) to 0.5 (fewer bins) should be about right.\"\n                \"\\nBe careful setting a value that's too extrene in a directory with many images, \"\n                \"as this could result in a lot of folders being created. Defaults: face-cnn 7.2, \"\n                \"hist 0.3, face 0.25\")})\n        argument_list.append({\n            \"opts\": ('-b', '--bins'),\n            \"action\": Slider,\n            \"min_max\": (1, 100),\n            \"rounding\": 1,\n            \"type\": int,\n            \"dest\": 'num_bins',\n            \"group\": _(\"group settings\"),\n            \"default\": 5,\n            \"help\": _(\n                \"R|Integer value. Used to control the number of bins created for grouping by: any \"\n                \"'blur' methods, 'color' methods or 'face metric' methods ('distance', 'size') \"\n                \"and 'orientation; methods ('yaw', 'pitch'). For any other grouping \"\n                \"methods see the '-t' ('--threshold') option.\"\n                \"\\nL|For 'face metric' methods the bins are filled, according the the \"\n                \"distribution of faces between the minimum and maximum chosen metric.\"\n                \"\\nL|For 'color' methods the number of bins represents the divider of the \"\n                \"percentage of colored pixels. Eg. For a bin number of '5': The first folder will \"\n                \"have the faces with 0%% to 20%% colored pixels, second 21%% to 40%%, etc. Any \"\n                \"empty bins will be deleted, so you may end up with fewer bins than selected.\"\n                \"\\nL|For 'blur' methods folder 0 will be the least blurry, while the last folder \"\n                \"will be the blurriest.\"\n                \"\\nL|For 'orientation' methods the number of bins is dictated by how much 180 \"\n                \"degrees is divided. Eg. If 18 is selected, then each folder will be a 10 degree \"\n                \"increment. Folder 0 will contain faces looking the most to the left/down whereas \"\n                \"the last folder will contain the faces looking the most to the right/up. NB: \"\n                \"Some bins may be empty if faces do not fit the criteria. \\nDefault value: 5\")})\n        argument_list.append({\n            \"opts\": ('-l', '--log-changes'),\n            \"action\": 'store_true',\n            \"group\": _(\"settings\"),\n            \"default\": False,\n            \"help\": _(\n                \"Logs file renaming changes if grouping by renaming, or it logs the file copying/\"\n                \"movement if grouping by folders. If no log file is specified  with '--log-file', \"\n                \"then a 'sort_log.json' file will be created in the input directory.\")})\n        argument_list.append({\n            \"opts\": ('-f', '--log-file'),\n            \"action\": SaveFileFullPaths,\n            \"filetypes\": \"alignments\",\n            \"group\": _(\"settings\"),\n            \"dest\": 'log_file_path',\n            \"default\": 'sort_log.json',\n            \"help\": _(\n                \"Specify a log file to use for saving the renaming or grouping information. If \"\n                \"specified extension isn't 'json' or 'yaml', then json will be used as the \"\n                \"serializer, with the supplied filename. Default: sort_log.json\")})\n        # Deprecated multi-character switches\n        argument_list.append({\n            \"opts\": (\"-lf\", ),\n            \"type\": str,\n            \"dest\": \"depr_log-file_lf_f\",\n            \"help\": argparse.SUPPRESS})\n        return argument_list\n", "tools/sort/sort.py": "#!/usr/bin/env python3\n\"\"\"\nA tool that allows for sorting and grouping images in different ways.\n\"\"\"\nfrom __future__ import annotations\nimport logging\nimport os\nimport sys\nimport typing as T\n\nfrom argparse import Namespace\nfrom shutil import copyfile, rmtree\n\nfrom tqdm import tqdm\n\n# faceswap imports\nfrom lib.serializer import Serializer, get_serializer_from_filename\nfrom lib.utils import handle_deprecated_cliopts\n\nfrom .sort_methods import SortBlur, SortColor, SortFace, SortHistogram, SortMultiMethod\nfrom .sort_methods_aligned import SortDistance, SortFaceCNN, SortPitch, SortSize, SortYaw, SortRoll\n\nif T.TYPE_CHECKING:\n    from .sort_methods import SortMethod\n\nlogger = logging.getLogger(__name__)\n\n\nclass Sort():\n    \"\"\" Sorts folders of faces based on input criteria\n\n    Wrapper for the sort process to run in either batch mode or single use mode\n\n    Parameters\n    ----------\n    arguments: :class:`argparse.Namespace`\n        The arguments to be passed to the extraction process as generated from Faceswap's command\n        line arguments\n    \"\"\"\n    def __init__(self, arguments: Namespace) -> None:\n        logger.debug(\"Initializing: %s (args: %s)\", self.__class__.__name__, arguments)\n        self._args = handle_deprecated_cliopts(arguments)\n        self._input_locations = self._get_input_locations()\n        logger.debug(\"Initialized: %s\", self.__class__.__name__)\n\n    def _get_input_locations(self) -> list[str]:\n        \"\"\" Obtain the full path to input locations. Will be a list of locations if batch mode is\n        selected, or a containing a single location if batch mode is not selected.\n\n        Returns\n        -------\n        list:\n            The list of input location paths\n        \"\"\"\n        if not self._args.batch_mode:\n            return [self._args.input_dir]\n\n        retval = [os.path.join(self._args.input_dir, fname)\n                  for fname in os.listdir(self._args.input_dir)\n                  if os.path.isdir(os.path.join(self._args.input_dir, fname))]\n        logger.debug(\"Input locations: %s\", retval)\n        return retval\n\n    def _output_for_input(self, input_location: str) -> str:\n        \"\"\" Obtain the path to an output folder for faces for a given input location.\n\n        If not running in batch mode, then the user supplied output location will be returned,\n        otherwise a sub-folder within the user supplied output location will be returned based on\n        the input filename\n\n        Parameters\n        ----------\n        input_location: str\n            The full path to an input video or folder of images\n        \"\"\"\n        if not self._args.batch_mode or self._args.output_dir is None:\n            return self._args.output_dir\n\n        retval = os.path.join(self._args.output_dir, os.path.basename(input_location))\n        logger.debug(\"Returning output: '%s' for input: '%s'\", retval, input_location)\n        return retval\n\n    def process(self) -> None:\n        \"\"\" The entry point for triggering the Sort Process.\n\n        Should only be called from  :class:`lib.cli.launcher.ScriptExecutor`\n        \"\"\"\n        logger.info('Starting, this may take a while...')\n        inputs = self._input_locations\n        if self._args.batch_mode:\n            logger.info(\"Batch mode selected processing: %s\", self._input_locations)\n        for job_no, location in enumerate(self._input_locations):\n            if self._args.batch_mode:\n                logger.info(\"Processing job %s of %s: '%s'\", job_no + 1, len(inputs), location)\n                arguments = Namespace(**self._args.__dict__)\n                arguments.input_dir = location\n                arguments.output_dir = self._output_for_input(location)\n            else:\n                arguments = self._args\n            sort = _Sort(arguments)\n            sort.process()\n\n\nclass _Sort():\n    \"\"\" Sorts folders of faces based on input criteria \"\"\"\n    def __init__(self, arguments: Namespace) -> None:\n        logger.debug(\"Initializing %s: arguments: %s\", self.__class__.__name__, arguments)\n        self._processes = {\"blur\": SortBlur,\n                           \"blur_fft\": SortBlur,\n                           \"distance\": SortDistance,\n                           \"yaw\": SortYaw,\n                           \"pitch\": SortPitch,\n                           \"roll\": SortRoll,\n                           \"size\": SortSize,\n                           \"face\": SortFace,\n                           \"face_cnn\": SortFaceCNN,\n                           \"face_cnn_dissim\": SortFaceCNN,\n                           \"hist\": SortHistogram,\n                           \"hist_dissim\": SortHistogram,\n                           \"color_black\": SortColor,\n                           \"color_gray\": SortColor,\n                           \"color_luma\": SortColor,\n                           \"color_green\": SortColor,\n                           \"color_orange\": SortColor}\n\n        self._args = self._parse_arguments(arguments)\n        self._changes: dict[str, str] = {}\n        self.serializer: Serializer | None = None\n\n        if arguments.log_changes:\n            self.serializer = get_serializer_from_filename(arguments.log_file_path)\n\n        self._sorter = self._get_sorter()\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    def _set_output_folder(self, arguments):\n        \"\"\" Set the output folder correctly if it has not been provided\n        Parameters\n        ----------\n        arguments: :class:`argparse.Namespace`\n            The command line arguments passed to the sort process\n\n        Returns\n        -------\n        :class:`argparse.Namespace`\n            The command line arguments with output folder correctly set\n        \"\"\"\n        logger.debug(\"setting output folder: %s\", arguments.output_dir)\n        input_dir = arguments.input_dir\n        output_dir = arguments.output_dir\n        sort_method = arguments.sort_method\n        group_method = arguments.group_method\n\n        needs_rename = sort_method != \"none\" and group_method == \"none\"\n\n        if needs_rename and arguments.keep_original and (not output_dir or\n                                                         output_dir == input_dir):\n            output_dir = os.path.join(input_dir, \"sorted\")\n            logger.warning(\"No output folder selected, but files need renaming. \"\n                           \"Outputting to: '%s'\", output_dir)\n        elif not output_dir:\n            output_dir = input_dir\n            logger.warning(\"No output folder selected, files will be sorted in place in: '%s'\",\n                           output_dir)\n\n        arguments.output_dir = output_dir\n        logger.debug(\"Set output folder: %s\", arguments.output_dir)\n        return arguments\n\n    def _parse_arguments(self, arguments):\n        \"\"\" Parse the arguments and update/format relevant choices\n\n        Parameters\n        ----------\n        arguments: :class:`argparse.Namespace`\n            The command line arguments passed to the sort process\n\n        Returns\n        -------\n        :class:`argparse.Namespace`\n            The formatted command line arguments\n        \"\"\"\n        logger.debug(\"Cleaning arguments: %s\", arguments)\n        if arguments.sort_method == \"none\" and arguments.group_method == \"none\":\n            logger.error(\"Both sort-by and group-by are 'None'. Nothing to do.\")\n            sys.exit(1)\n\n        # Prepare sort, group and final process method names\n        arguments.sort_method = arguments.sort_method.lower().replace(\"-\", \"_\")\n        arguments.group_method = arguments.group_method.lower().replace(\"-\", \"_\")\n\n        arguments = self._set_output_folder(arguments)\n\n        if arguments.log_changes and arguments.log_file_path == \"sort_log.json\":\n            # Assign default sort_log.json value if user didn't specify one\n            arguments.log_file_path = os.path.join(self._args.input_dir, 'sort_log.json')\n\n        logger.debug(\"Cleaned arguments: %s\", arguments)\n        return arguments\n\n    def _get_sorter(self) -> SortMethod:\n        \"\"\" Obtain a sorter/grouper combo for the selected sort/group by options\n\n        Returns\n        -------\n        :class:`SortMethod`\n            The sorter or combined sorter for sorting and grouping based on user selections\n        \"\"\"\n        sort_method = self._args.sort_method\n        group_method = self._args.group_method\n\n        sort_method = group_method if sort_method == \"none\" else sort_method\n        sorter = self._processes[sort_method](self._args,\n                                              is_group=self._args.sort_method == \"none\")\n\n        if sort_method != \"none\" and group_method != \"none\" and group_method != sort_method:\n            grouper = self._processes[group_method](self._args, is_group=True)\n            retval = SortMultiMethod(self._args, sorter, grouper)\n            logger.debug(\"Got sorter + grouper: %s (%s, %s)\", retval, sorter, grouper)\n\n        else:\n\n            retval = sorter\n\n        logger.debug(\"Final sorter: %s\", retval)\n        return retval\n\n    def _write_to_log(self, changes):\n        \"\"\" Write the changes to log file \"\"\"\n        logger.info(\"Writing sort log to: '%s'\", self._args.log_file_path)\n        self.serializer.save(self._args.log_file_path, changes)\n\n    def process(self) -> None:\n        \"\"\" Main processing function of the sort tool\n\n        This method dynamically assigns the functions that will be used to run\n        the core process of sorting, optionally grouping, renaming/moving into\n        folders. After the functions are assigned they are executed.\n        \"\"\"\n        if self._args.group_method != \"none\":\n            # Check if non-dissimilarity sort method and group method are not the same\n            self._output_groups()\n        else:\n            self._output_non_grouped()\n\n        if self._args.log_changes:\n            self._write_to_log(self._changes)\n\n        logger.info(\"Done.\")\n\n    def _sort_file(self, source: str, destination: str) -> None:\n        \"\"\" Copy or move a file based on whether 'keep original' has been selected and log changes\n        if required.\n\n        Parameters\n        ----------\n        source: str\n            The full path to the source file that is being sorted\n        destination: str\n            The full path to where the source file should be moved/renamed\n        \"\"\"\n        try:\n            if self._args.keep_original:\n                copyfile(source, destination)\n            else:\n                os.rename(source, destination)\n        except FileNotFoundError as err:\n            logger.error(\"Failed to sort '%s' to '%s'. Original error: %s\",\n                         source, destination, str(err))\n\n        if self._args.log_changes:\n            self._changes[source] = destination\n\n    def _output_groups(self) -> None:\n        \"\"\" Move the files to folders.\n\n        Obtains the bins and original filenames from :attr:`_sorter` and outputs into appropriate\n        bins in the output location\n        \"\"\"\n        is_rename = self._args.sort_method != \"none\"\n\n        logger.info(\"Creating %s group folders in '%s'.\",\n                    len(self._sorter.binned), self._args.output_dir)\n        bin_names = [f\"_{b}\" for b in self._sorter.bin_names]\n        if is_rename:\n            bin_names = [f\"{name}_by_{self._args.sort_method}\" for name in bin_names]\n        for name in bin_names:\n            folder = os.path.join(self._args.output_dir, name)\n            if os.path.exists(folder):\n                rmtree(folder)\n            os.makedirs(folder)\n\n        description = f\"{'Copying' if self._args.keep_original else 'Moving'} into groups\"\n        description += \" and renaming\" if is_rename else \"\"\n\n        pbar = tqdm(range(len(self._sorter.sorted_filelist)),\n                    desc=description,\n                    file=sys.stdout,\n                    leave=False)\n        idx = 0\n        for bin_id, bin_ in enumerate(self._sorter.binned):\n            pbar.set_description(f\"{description}: Bin {bin_id + 1} of {len(self._sorter.binned)}\")\n            output_path = os.path.join(self._args.output_dir, bin_names[bin_id])\n            if not bin_:\n                logger.debug(\"Removing empty bin: %s\", output_path)\n                os.rmdir(output_path)\n            for source in bin_:\n                basename = os.path.basename(source)\n                dst_name = f\"{idx:06d}_{basename}\" if is_rename else basename\n                dest = os.path.join(output_path, dst_name)\n                self._sort_file(source, dest)\n                idx += 1\n                pbar.update(1)\n\n    # Output methods\n    def _output_non_grouped(self) -> None:\n        \"\"\" Output non-grouped files.\n\n        These are files which are sorted but not binned, so just the filename gets updated\n        \"\"\"\n        output_dir = self._args.output_dir\n        os.makedirs(output_dir, exist_ok=True)\n\n        description = f\"{'Copying' if self._args.keep_original else 'Moving'} and renaming\"\n        for idx, source in enumerate(tqdm(self._sorter.sorted_filelist,\n                                          desc=description,\n                                          file=sys.stdout,\n                                          leave=False)):\n            dest = os.path.join(output_dir, f\"{idx:06d}_{os.path.basename(source)}\")\n\n            self._sort_file(source, dest)\n", "tools/sort/__init__.py": "", "tools/sort/sort_methods.py": "#!/usr/bin/env python3\n\"\"\" Sorting methods for the sorting tool.\n\nAll sorting methods inherit from :class:`SortMethod` and control functions for scorting one item,\nsorting a full list of scores and binning based on those sorted scores.\n\"\"\"\nfrom __future__ import annotations\nimport logging\nimport operator\nimport sys\nimport typing as T\n\nfrom collections.abc import Generator\n\nimport cv2\nimport numpy as np\nfrom tqdm import tqdm\n\nfrom lib.align import AlignedFace, DetectedFace, LandmarkType\nfrom lib.image import FacesLoader, ImagesLoader, read_image_meta_batch, update_existing_metadata\nfrom lib.utils import FaceswapError\nfrom plugins.extract.recognition.vgg_face2 import Cluster, Recognition as VGGFace\n\nif T.TYPE_CHECKING:\n    from argparse import Namespace\n    from lib.align.alignments import PNGHeaderAlignmentsDict, PNGHeaderSourceDict\n\nlogger = logging.getLogger(__name__)\n\n\nImgMetaType: T.TypeAlias = Generator[tuple[str,\n                                           np.ndarray | None,\n                                           T.Union[\"PNGHeaderAlignmentsDict\", None]], None, None]\n\n\nclass InfoLoader():\n    \"\"\" Loads aligned faces and/or face metadata\n\n    Parameters\n    ----------\n    input_dir: str\n        Full path to containing folder of faces to be supported\n    loader_type: [\"face\", \"meta\", \"all\"]\n        Dictates the type of iterator that will be used. \"face\" just loads the image with the\n        filename, \"meta\" just loads the image alignment data with the filename. \"all\" loads\n        the image and the alignment data with the filename\n    \"\"\"\n    def __init__(self,\n                 input_dir: str,\n                 info_type: T.Literal[\"face\", \"meta\", \"all\"]) -> None:\n        logger.debug(\"Initializing: %s (input_dir: %s, info_type: %s)\",\n                     self.__class__.__name__, input_dir, info_type)\n        self._info_type = info_type\n        self._iterator = None\n        self._description = \"Reading image statistics...\"\n        self._loader = ImagesLoader(input_dir) if info_type == \"face\" else FacesLoader(input_dir)\n        self._cached_source_data: dict[str, PNGHeaderSourceDict] = {}\n        if self._loader.count == 0:\n            logger.error(\"No images to process in location: '%s'\", input_dir)\n            sys.exit(1)\n\n        logger.debug(\"Initialized: %s\", self.__class__.__name__)\n\n    @property\n    def filelist_count(self) -> int:\n        \"\"\" int: The number of files to be processed \"\"\"\n        return len(self._loader.file_list)\n\n    def _get_iterator(self) -> ImgMetaType:\n        \"\"\" Obtain the iterator for the selected :attr:`info_type`.\n\n        Returns\n        -------\n        generator\n            The correct generator for the given info_type\n        \"\"\"\n        if self._info_type == \"all\":\n            return self._full_data_reader()\n        if self._info_type == \"meta\":\n            return self._metadata_reader()\n        return self._image_data_reader()\n\n    def __call__(self) -> ImgMetaType:\n        \"\"\" Return the selected iterator\n\n        The resulting generator:\n\n        Yields\n        ------\n        filename: str\n            The filename that has been read\n        image: :class:`numpy.ndarray or ``None``\n            The aligned face image loaded from disk for 'face' and 'all' info_types\n            otherwise ``None``\n        alignments: dict or ``None``\n            The alignments dict for 'all' and 'meta' infor_types otherwise ``None``\n        \"\"\"\n        iterator = self._get_iterator()\n        return iterator\n\n    def _get_alignments(self,\n                        filename: str,\n                        metadata: dict[str, T.Any]) -> PNGHeaderAlignmentsDict | None:\n        \"\"\" Obtain the alignments from a PNG Header.\n\n        The other image metadata is cached locally in case a sort method needs to write back to the\n        PNG header\n\n        Parameters\n        ----------\n        filename: str\n            Full path to the image PNG file\n        metadata: dict\n            The header data from a PNG file\n\n        Returns\n        -------\n        dict or ``None``\n            The alignments dictionary from the PNG header, if it exists, otherwise ``None``\n        \"\"\"\n        if not metadata or not metadata.get(\"alignments\") or not metadata.get(\"source\"):\n            return None\n        self._cached_source_data[filename] = metadata[\"source\"]\n        return metadata[\"alignments\"]\n\n    def _metadata_reader(self) -> ImgMetaType:\n        \"\"\" Load metadata from saved aligned faces\n\n        Yields\n        ------\n        filename: str\n            The filename that has been read\n        image: None\n            This will always be ``None`` with the metadata reader\n        alignments: dict or ``None``\n            The alignment data for the given face or ``None`` if no alignments found\n        \"\"\"\n        for filename, metadata in tqdm(read_image_meta_batch(self._loader.file_list),\n                                       total=self._loader.count,\n                                       desc=self._description,\n                                       leave=False):\n            alignments = self._get_alignments(filename, metadata.get(\"itxt\", {}))\n            yield filename, None, alignments\n\n    def _full_data_reader(self) -> ImgMetaType:\n        \"\"\" Load the image and metadata from a folder of aligned faces\n\n        Yields\n        ------\n        filename: str\n            The filename that has been read\n        image: :class:`numpy.ndarray\n            The aligned face image loaded from disk\n        alignments: dict or ``None``\n            The alignment data for the given face or ``None`` if no alignments found\n        \"\"\"\n        for filename, image, metadata in tqdm(self._loader.load(),\n                                              desc=self._description,\n                                              total=self._loader.count,\n                                              leave=False):\n            alignments = self._get_alignments(filename, metadata)\n            yield filename, image, alignments\n\n    def _image_data_reader(self) -> ImgMetaType:\n        \"\"\" Just loads the images with their filenames\n\n        Yields\n        ------\n        filename: str\n            The filename that has been read\n        image: :class:`numpy.ndarray\n            The aligned face image loaded from disk\n        alignments: ``None``\n            Alignments will always be ``None`` with the image data reader\n        \"\"\"\n        for filename, image in tqdm(self._loader.load(),\n                                    desc=self._description,\n                                    total=self._loader.count,\n                                    leave=False):\n            yield filename, image, None\n\n    def update_png_header(self, filename: str, alignments: PNGHeaderAlignmentsDict) -> None:\n        \"\"\" Update the PNG header of the given file with the given alignments.\n\n        NB: Header information can only be updated if the face is already on at least alignment\n        version 2.2. If below this version, then the header is not updated\n\n\n        Parameters\n        ----------\n        filename: str\n            Full path to the PNG file to update\n        alignments: dict\n            The alignments to update into the PNG header\n        \"\"\"\n        vers = self._cached_source_data[filename][\"alignments_version\"]\n        if vers < 2.2:\n            return\n\n        self._cached_source_data[filename][\"alignments_version\"] = 2.3 if vers == 2.2 else vers\n        header = {\"alignments\": alignments, \"source\": self._cached_source_data[filename]}\n        update_existing_metadata(filename, header)\n\n\nclass SortMethod():\n    \"\"\" Parent class for sort methods. All sort methods should inherit from this class\n\n    Parameters\n    ----------\n    arguments: :class:`argparse.Namespace`\n        The command line arguments passed to the sort process\n    loader_type: [\"face\", \"meta\", \"all\"]\n        The type of image loader to use. \"face\" just loads the image with the filename, \"meta\"\n        just loads the image alignment data with the filename. \"all\" loads the image and the\n        alignment data with the filename\n    is_group: bool, optional\n        Set to ``True`` if this class is going to be called exclusively for binning.\n        Default: ``False``\n    \"\"\"\n    _log_mask_once = False\n\n    def __init__(self,\n                 arguments: Namespace,\n                 loader_type: T.Literal[\"face\", \"meta\", \"all\"] = \"meta\",\n                 is_group: bool = False) -> None:\n        logger.debug(\"Initializing %s: loader_type: '%s' is_group: %s, arguments: %s\",\n                     self.__class__.__name__, loader_type, is_group, arguments)\n        self._is_group = is_group\n        self._log_once = True\n        self._method = arguments.group_method if self._is_group else arguments.sort_method\n\n        self._num_bins: int = arguments.num_bins\n        self._bin_names: list[str] = []\n\n        self._loader_type = loader_type\n        self._iterator = self._get_file_iterator(arguments.input_dir)\n\n        self._result: list[tuple[str, float | np.ndarray]] = []\n        self._binned: list[list[str]] = []\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    @property\n    def loader_type(self) -> T.Literal[\"face\", \"meta\", \"all\"]:\n        \"\"\"  [\"face\", \"meta\", \"all\"]: The loader that this sorter uses \"\"\"\n        return self._loader_type\n\n    @property\n    def binned(self) -> list[list[str]]:\n        \"\"\" list: List of bins (list) containing the filenames belonging to the bin. The binning\n        process is called when this property is first accessed\"\"\"\n        if not self._binned:\n            self._binned = self._binning()\n            logger.debug({f\"bin_{idx}\": len(bin_) for idx, bin_ in enumerate(self._binned)})\n        return self._binned\n\n    @property\n    def sorted_filelist(self) -> list[str]:\n        \"\"\" list: List of sorted filenames for given sorter in a single list. The sort process is\n        called when this property is first accessed \"\"\"\n        if not self._result:\n            self._sort_filelist()\n            retval = [item[0] for item in self._result]\n            logger.debug(retval)\n        else:\n            retval = [item[0] for item in self._result]\n        return retval\n\n    @property\n    def bin_names(self) -> list[str]:\n        \"\"\" list: The name of each created bin, if they exist, otherwise an empty list \"\"\"\n        return self._bin_names\n\n    def _get_file_iterator(self, input_dir: str) -> InfoLoader:\n        \"\"\" Override for method specific iterators.\n\n        Parameters\n        ----------\n        input_dir: str\n            Full path to containing folder of faces to be supported\n\n        Returns\n        -------\n        :class:`InfoLoader`\n            The correct InfoLoader iterator for the current sort method\n        \"\"\"\n        return InfoLoader(input_dir, self.loader_type)\n\n    def _sort_filelist(self) -> None:\n        \"\"\" Call the sort method's logic to populate the :attr:`_results` attribute.\n\n        Put logic for scoring an individual frame in in :attr:`score_image` of the child\n\n        Returns\n        -------\n        list\n            The sorted file. A list of tuples with the filename in the first position and score in\n            the second position\n        \"\"\"\n        for filename, image, alignments in self._iterator():\n            self.score_image(filename, image, alignments)\n\n        self.sort()\n        logger.debug(\"sorted list: %s\",\n                     [r[0] if isinstance(r, (tuple, list)) else r for r in self._result])\n\n    @classmethod\n    def _get_unique_labels(cls, numbers: np.ndarray) -> list[str]:\n        \"\"\" For a list of threshold values for displaying in the bin name, get the lowest number of\n        decimal figures (down to int) required to have a unique set of folder names and return the\n        formatted numbers.\n\n        Parameters\n        ----------\n        numbers: :class:`numpy.ndarray`\n            The list of floating point threshold numbers being used as boundary points\n\n        Returns\n        -------\n        list[str]\n            The string formatted numbers at the lowest precision possible to represent them\n            uniquely\n        \"\"\"\n        i = 0\n        while True:\n            rounded = [round(n, i) for n in numbers]\n            if len(set(rounded)) == len(numbers):\n                break\n            i += 1\n\n        if i == 0:\n            retval = [str(int(n)) for n in rounded]\n        else:\n            pre, post = zip(*[str(r).split(\".\") for r in rounded])\n            rpad = max(len(x) for x in post)\n            retval = [f\"{str(int(left))}.{str(int(right)).ljust(rpad, '0')}\"\n                      for left, right in zip(pre, post)]\n        logger.debug(\"rounded values: %s, formatted labels: %s\", rounded, retval)\n        return retval\n\n    def _binning_linear_threshold(self, units: str = \"\", multiplier: int = 1) -> list[list[str]]:\n        \"\"\" Standard linear binning method for binning by threshold.\n\n        The minimum and maximum result from :attr:`_result` are taken, A range is created between\n        these min and max values and is divided to get the number of bins to hold the data\n\n        Parameters\n        ----------\n        units, str, optional\n            The units to use for the bin name for displaying the threshold values. This this should\n            correspond the value in position 1 of :attr:`_result`.\n            Default: \"\" (no units)\n        multiplier: int, optional\n            The amount to multiply the contents in position 1 of :attr:`_results` for displaying in\n            the bin folder name\n\n        Returns\n        -------\n        list\n            List of bins of filenames\n        \"\"\"\n        sizes = np.array([i[1] for i in self._result])\n        thresholds = np.linspace(sizes.min(), sizes.max(), self._num_bins + 1)\n        labels = self._get_unique_labels(thresholds * multiplier)\n\n        self._bin_names = [f\"{self._method}_{idx:03d}_\"\n                           f\"{labels[idx]}{units}_to_{labels[idx + 1]}{units}\"\n                           for idx in range(self._num_bins)]\n\n        bins: list[list[str]] = [[] for _ in range(self._num_bins)]\n        for filename, result in self._result:\n            bin_idx = next(bin_id for bin_id, thresh in enumerate(thresholds)\n                           if result <= thresh) - 1\n            bins[bin_idx].append(filename)\n\n        return bins\n\n    def _binning(self) -> list[list[str]]:\n        \"\"\" Called when :attr:`binning` is first accessed. Checks if sorting has been done, if not\n        triggers it, then does binning\n\n        Returns\n        -------\n        list\n            List of bins of filenames\n        \"\"\"\n        if not self._result:\n            self._sort_filelist()\n        retval = self.binning()\n\n        if not self._bin_names:\n            self._bin_names = [f\"{self._method}_{i:03d}\" for i in range(len(retval))]\n\n        logger.debug({bin_name: len(bin_) for bin_name, bin_ in zip(self._bin_names, retval)})\n\n        return retval\n\n    def sort(self) -> None:\n        \"\"\" Override for method specific logic for sorting the loaded statistics\n\n        The scored list :attr:`_result` should be sorted in place\n        \"\"\"\n        raise NotImplementedError()\n\n    def score_image(self,\n                    filename: str,\n                    image: np.ndarray | None,\n                    alignments: PNGHeaderAlignmentsDict | None) -> None:\n        \"\"\" Override for sort method's specificic logic. This method should be executed to get a\n        single score from a single image  and add the result to :attr:`_result`\n\n        Parameters\n        ----------\n        filename: str\n            The filename of the currently processing image\n        image: :class:`np.ndarray` or ``None``\n            A face image loaded from disk or ``None``\n        alignments: dict or ``None``\n            The alignments dictionary for the aligned face or ``None``\n        \"\"\"\n        raise NotImplementedError()\n\n    def binning(self) -> list[list[str]]:\n        \"\"\" Group into bins by their sorted score. Override for method specific binning techniques.\n\n        Binning takes the results from :attr:`_result` compiled during :func:`_sort_filelist` and\n        organizes into bins for output.\n\n        Returns\n        -------\n        list\n            List of bins of filenames\n        \"\"\"\n        raise NotImplementedError()\n\n    @classmethod\n    def _mask_face(cls, image: np.ndarray, alignments: PNGHeaderAlignmentsDict) -> np.ndarray:\n        \"\"\" Function for applying the mask to an aligned face if both the face image and alignment\n        data are available.\n\n        Parameters\n        ----------\n        image: :class:`numpy.ndarray`\n            The aligned face image loaded from disk\n        alignments: Dict\n            The alignments data corresponding to the loaded image\n\n        Returns\n        -------\n        :class:`numpy.ndarray`\n            The original image with the mask applied\n        \"\"\"\n        det_face = DetectedFace()\n        det_face.from_png_meta(alignments)\n        aln_face = AlignedFace(np.array(alignments[\"landmarks_xy\"], dtype=\"float32\"),\n                               image=image,\n                               centering=\"legacy\",\n                               size=256,\n                               is_aligned=True)\n        assert aln_face.face is not None\n\n        mask = det_face.mask.get(\"components\",  det_face.mask.get(\"extended\", None))\n\n        if mask is None and not cls._log_mask_once:\n            logger.warning(\"No masks are available for masking the data. Results are likely to be \"\n                           \"sub-standard\")\n            cls._log_mask_once = True\n\n        if mask is None:\n            return aln_face.face\n\n        mask.set_sub_crop(aln_face.pose.offset[mask.stored_centering],\n                          aln_face.pose.offset[\"legacy\"],\n                          centering=\"legacy\")\n        nmask = cv2.resize(mask.mask, (256, 256), interpolation=cv2.INTER_CUBIC)[..., None]\n        return np.minimum(aln_face.face, nmask)\n\n\nclass SortMultiMethod(SortMethod):\n    \"\"\" A Parent sort method that runs 2 different underlying methods (one for sorting one for\n    binning) in instances where grouping has been requested, but the sort method is different from\n    the group method\n\n    Parameters\n    ----------\n    arguments: :class:`argparse.Namespace`\n        The command line arguments passed to the sort process\n    sort_method: :class:`SortMethod`\n        A sort method object for sorting the images\n    group_method: :class:`SortMethod`\n        A sort method object used for sorting and binning the images\n    \"\"\"\n    def __init__(self,\n                 arguments: Namespace,\n                 sort_method: SortMethod,\n                 group_method: SortMethod) -> None:\n        self._sorter = sort_method\n        self._grouper = group_method\n        self._is_built = False\n        super().__init__(arguments)\n\n    def _get_file_iterator(self, input_dir: str) -> InfoLoader:\n        \"\"\" Override to get a group specific iterator. If the sorter and grouper use the same kind\n        of iterator, use that. Otherwise return the 'all' iterator, as which ever way it is cut all\n        outputs will be required. Monkey patch the actual loader used into the children in case of\n        any callbacks.\n\n        Parameters\n        ----------\n        input_dir: str\n            Full path to containing folder of faces to be supported\n\n        Returns\n        -------\n        :class:`InfoLoader`\n            The correct InfoLoader iterator for the current sort method\n        \"\"\"\n        if self._sorter.loader_type == self._grouper.loader_type:\n            retval = InfoLoader(input_dir, self._sorter.loader_type)\n        else:\n            retval = InfoLoader(input_dir, \"all\")\n        self._sorter._iterator = retval  # pylint:disable=protected-access\n        self._grouper._iterator = retval  # pylint:disable=protected-access\n        return retval\n\n    def score_image(self,\n                    filename: str,\n                    image: np.ndarray | None,\n                    alignments: PNGHeaderAlignmentsDict | None) -> None:\n        \"\"\" Score a single image for sort method: \"distance\", \"yaw\" \"pitch\" or \"size\" and add the\n        result to :attr:`_result`\n\n        Parameters\n        ----------\n        filename: str\n            The filename of the currently processing image\n        image: :class:`np.ndarray` or ``None``\n            A face image loaded from disk or ``None``\n        alignments: dict or ``None``\n            The alignments dictionary for the aligned face or ``None``\n        \"\"\"\n        self._sorter.score_image(filename, image, alignments)\n        self._grouper.score_image(filename, image, alignments)\n\n    def sort(self) -> None:\n        \"\"\" Sort the sorter and grouper methods \"\"\"\n        logger.debug(\"Sorting\")\n        self._sorter.sort()\n        self._result = self._sorter.sorted_filelist  # type:ignore\n        self._grouper.sort()\n        self._binned = self._grouper.binned\n        self._bin_names = self._grouper.bin_names\n        logger.debug(\"Sorted\")\n\n    def binning(self) -> list[list[str]]:\n        \"\"\" Override standard binning, to bin by the group-by method and sort by the sorting\n        method.\n\n        Go through the grouped binned results, and reorder each bin contents based on the\n        sorted list\n\n        Returns\n        -------\n        list\n            List of bins of filenames\n        \"\"\"\n        sorted_ = self._result\n        output: list[list[str]] = []\n        for bin_ in tqdm(self._binned, desc=\"Binning and sorting\", file=sys.stdout, leave=False):\n            indices: dict[int, str] = {}\n            for filename in bin_:\n                indices[sorted_.index(filename)] = filename\n            output.append([indices[idx] for idx in sorted(indices)])\n        return output\n\n\nclass SortBlur(SortMethod):\n    \"\"\" Sort images by blur or blur-fft amount\n\n    Parameters\n    ----------\n    arguments: :class:`argparse.Namespace`\n        The command line arguments passed to the sort process\n    is_group: bool, optional\n        Set to ``True`` if this class is going to be called exclusively for binning.\n        Default: ``False``\n    \"\"\"\n    def __init__(self, arguments: Namespace, is_group: bool = False) -> None:\n        super().__init__(arguments, loader_type=\"all\", is_group=is_group)\n        method = arguments.group_method if self._is_group else arguments.sort_method\n        self._use_fft = method == \"blur_fft\"\n\n    def estimate_blur(self, image: np.ndarray, alignments=None) -> float:\n        \"\"\" Estimate the amount of blur an image has with the variance of the Laplacian.\n        Normalize by pixel number to offset the effect of image size on pixel gradients & variance.\n\n        Parameters\n        ----------\n        image: :class:`numpy.ndarray`\n            The face image to calculate blur for\n        alignments: dict, optional\n            The metadata for the face image or ``None`` if no metadata is available. If metadata is\n            provided the face will be masked by the \"components\" mask prior to calculating blur.\n            Default:``None``\n\n        Returns\n        -------\n        float\n            The estimated blur score for the face\n        \"\"\"\n        if alignments is not None:\n            image = self._mask_face(image, alignments)\n        if image.ndim == 3:\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n        blur_map = cv2.Laplacian(image, cv2.CV_32F)\n        score = np.var(blur_map) / np.sqrt(image.shape[0] * image.shape[1])\n        return score\n\n    def estimate_blur_fft(self,\n                          image: np.ndarray,\n                          alignments: PNGHeaderAlignmentsDict | None = None) -> float:\n        \"\"\" Estimate the amount of blur a fft filtered image has.\n\n        Parameters\n        ----------\n        image: :class:`numpy.ndarray`\n            Use Fourier Transform to analyze the frequency characteristics of the masked\n            face using 2D Discrete Fourier Transform (DFT) filter to find the frequency domain.\n            A mean value is assigned to the magnitude spectrum and returns a blur score.\n            Adapted from https://www.pyimagesearch.com/2020/06/15/\n            opencv-fast-fourier-transform-fft-for-blur-detection-in-images-and-video-streams/\n        alignments: dict, optional\n            The metadata for the face image or ``None`` if no metadata is available. If metadata is\n            provided the face will be masked by the \"components\" mask prior to calculating blur.\n            Default:``None``\n\n        Returns\n        -------\n        float\n            The estimated fft blur score for the face\n        \"\"\"\n        if alignments is not None:\n            image = self._mask_face(image, alignments)\n\n        if image.ndim == 3:\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n\n        height, width = image.shape\n        c_height, c_width = (int(height / 2.0), int(width / 2.0))\n        fft = np.fft.fft2(image)\n        fft_shift = np.fft.fftshift(fft)\n        fft_shift[c_height - 75:c_height + 75, c_width - 75:c_width + 75] = 0\n        ifft_shift = np.fft.ifftshift(fft_shift)\n        shift_back = np.fft.ifft2(ifft_shift)\n        magnitude = np.log(np.abs(shift_back))\n        score = np.mean(magnitude)\n\n        return score\n\n    def score_image(self,\n                    filename: str,\n                    image: np.ndarray | None,\n                    alignments: PNGHeaderAlignmentsDict | None) -> None:\n        \"\"\" Score a single image for blur or blur-fft and add the result to :attr:`_result`\n\n        Parameters\n        ----------\n        filename: str\n            The filename of the currently processing image\n        image: :class:`np.ndarray`\n            A face image loaded from disk\n        alignments: dict or ``None``\n            The alignments dictionary for the aligned face or ``None``\n        \"\"\"\n        assert image is not None\n        if self._log_once:\n            msg = \"Grouping\" if self._is_group else \"Sorting\"\n            inf = \"fft_filtered \" if self._use_fft else \" \"\n            logger.info(\"%s by estimated %simage blur...\", msg, inf)\n            self._log_once = False\n\n        estimator = self.estimate_blur_fft if self._use_fft else self.estimate_blur\n        self._result.append((filename, estimator(image, alignments)))\n\n    def sort(self) -> None:\n        \"\"\" Sort by metric score. Order in reverse for distance sort. \"\"\"\n        logger.info(\"Sorting...\")\n        self._result = sorted(self._result, key=operator.itemgetter(1), reverse=True)\n\n    def binning(self) -> list[list[str]]:\n        \"\"\" Create bins to split linearly from the lowest to the highest sample value\n\n        Returns\n        -------\n        list\n            List of bins of filenames\n        \"\"\"\n        return self._binning_linear_threshold(multiplier=100)\n\n\nclass SortColor(SortMethod):\n    \"\"\" Score by channel average intensity or black pixels.\n\n    Parameters\n    ----------\n    arguments: :class:`argparse.Namespace`\n        The command line arguments passed to the sort process\n    is_group: bool, optional\n        Set to ``True`` if this class is going to be called exclusively for binning.\n        Default: ``False``\n    \"\"\"\n    def __init__(self, arguments: Namespace, is_group: bool = False) -> None:\n        super().__init__(arguments, loader_type=\"face\", is_group=is_group)\n        self._desired_channel = {'gray': 0, 'luma': 0, 'orange': 1, 'green': 2}\n\n        method = arguments.group_method if self._is_group else arguments.sort_method\n        self._method = method.replace(\"color_\", \"\")\n\n    def _convert_color(self, image: np.ndarray) -> np.ndarray:\n        \"\"\" Helper function to convert color spaces\n\n        Parameters\n        ----------\n        image: :class:`numpy.ndarray`\n            The original image to convert color space for\n\n        Returns\n        -------\n        :class:`numpy.ndarray`\n            The color converted image\n        \"\"\"\n        if self._method == 'gray':\n            conversion = np.array([[0.0722], [0.7152], [0.2126]])\n        else:\n            conversion = np.array([[0.25, 0.5, 0.25], [-0.5, 0.0, 0.5], [-0.25, 0.5, -0.25]])\n\n        operation = 'ijk, kl -> ijl' if self._method == \"gray\" else 'ijl, kl -> ijk'\n        path = np.einsum_path(operation, image[..., :3], conversion, optimize='optimal')[0]\n        return np.einsum(operation, image[..., :3], conversion, optimize=path).astype('float32')\n\n    def _near_split(self, bin_range: int) -> list[int]:\n        \"\"\" Obtain the split for the given number of bins for the given range\n\n        Parameters\n        ----------\n        bin_range: int\n            The range of data to separate into bins\n\n        Returns\n        -------\n        list\n            The split dividers for the given number of bins for the given range\n        \"\"\"\n        quotient, remainder = divmod(bin_range, self._num_bins)\n        seps = [quotient + 1] * remainder + [quotient] * (self._num_bins - remainder)\n        uplimit = 0\n        bins = [0]\n        for sep in seps:\n            bins.append(uplimit + sep)\n            uplimit += sep\n        return bins\n\n    def binning(self) -> list[list[str]]:\n        \"\"\" Group into bins by percentage of black pixels \"\"\"\n        # TODO. Only grouped by black pixels. Check color\n\n        logger.info(\"Grouping by percentage of %s...\", self._method)\n\n        # Starting the binning process\n        bins: list[list[str]] = [[] for _ in range(self._num_bins)]\n        # Get edges of bins from 0 to 100\n        bins_edges = self._near_split(100)\n        # Get the proper bin number for each img order\n        img_bins = np.digitize([float(x[1]) for x in self._result], bins_edges, right=True)\n\n        # Place imgs in bins\n        for idx, _bin in enumerate(img_bins):\n            bins[_bin].append(self._result[idx][0])\n\n        retval = [b for b in bins if b]\n        return retval\n\n    def score_image(self,\n                    filename: str,\n                    image: np.ndarray | None,\n                    alignments: PNGHeaderAlignmentsDict | None) -> None:\n        \"\"\" Score a single image for color\n\n        Parameters\n        ----------\n        filename: str\n            The filename of the currently processing image\n        image: :class:`np.ndarray`\n            A face image loaded from disk\n        alignments: dict or ``None``\n            The alignments dictionary for the aligned face or ``None``\n        \"\"\"\n        if self._log_once:\n            msg = \"Grouping\" if self._is_group else \"Sorting\"\n            if self._method == \"black\":\n                logger.info(\"%s by percentage of black pixels...\", msg)\n            else:\n                logger.info(\"%s by channel average intensity...\", msg)\n            self._log_once = False\n\n        assert image is not None\n        if self._method == \"black\":\n            score = np.ndarray.all(image == [0, 0, 0], axis=2).sum()/image.size*100*3\n        else:\n            channel_to_sort = self._desired_channel[self._method]\n            score = np.average(self._convert_color(image), axis=(0, 1))[channel_to_sort]\n        self._result.append((filename, score))\n\n    def sort(self) -> None:\n        \"\"\" Sort by metric score. Order in reverse for distance sort. \"\"\"\n        if self._method == \"black\":\n            self._sort_black_pixels()\n            return\n        self._result = sorted(self._result, key=operator.itemgetter(1), reverse=True)\n\n    def _sort_black_pixels(self) -> None:\n        \"\"\" Sort by percentage of black pixels\n\n         Calculates the sum of black pixels, gets the percentage X 3 channels\n        \"\"\"\n        img_list_len = len(self._result)\n        for i in tqdm(range(0, img_list_len - 1),\n                      desc=\"Comparing black pixels\", file=sys.stdout,\n                      leave=False):\n            for j in range(0, img_list_len-i-1):\n                if self._result[j][1] > self._result[j+1][1]:\n                    temp = self._result[j]\n                    self._result[j] = self._result[j+1]\n                    self._result[j+1] = temp\n\n\nclass SortFace(SortMethod):\n    \"\"\" Sort by identity similarity using VGG Face 2\n\n    Parameters\n    ----------\n    arguments: :class:`argparse.Namespace`\n        The command line arguments passed to the sort process\n    is_group: bool, optional\n        Set to ``True`` if this class is going to be called exclusively for binning.\n        Default: ``False``\n    \"\"\"\n\n    _logged_lm_count_once = False\n    _warning = (\"Extracted faces do not contain facial landmark data. Results sorted by this \"\n                \"method are likely to be sub-standard.\")\n\n    def __init__(self, arguments: Namespace, is_group: bool = False) -> None:\n        super().__init__(arguments, loader_type=\"all\", is_group=is_group)\n        self._vgg_face = VGGFace(exclude_gpus=arguments.exclude_gpus)\n        self._vgg_face.init_model()\n        threshold = arguments.threshold\n        self._output_update_info = True\n        self._threshold: float | None = 0.25 if threshold < 0 else threshold\n\n    def score_image(self,\n                    filename: str,\n                    image: np.ndarray | None,\n                    alignments: PNGHeaderAlignmentsDict | None) -> None:\n        \"\"\" Processing logic for sort by face method.\n\n        Reads header information from the PNG file to look for VGGFace2 embedding. If it does not\n        exist, the embedding is obtained and added back into the PNG Header.\n\n        Parameters\n        ----------\n        filename: str\n            The filename of the currently processing image\n        image: :class:`np.ndarray`\n            A face image loaded from disk\n        alignments: dict or ``None``\n            The alignments dictionary for the aligned face or ``None``\n        \"\"\"\n        if not alignments:\n            msg = (\"The images to be sorted do not contain alignment data. Images must have \"\n                   \"been generated by Faceswap's Extract process.\\nIf you are sorting an \"\n                   \"older faceset, then you should re-extract the faces from your source \"\n                   \"alignments file to generate this data.\")\n            raise FaceswapError(msg)\n\n        if self._log_once:\n            msg = \"Grouping\" if self._is_group else \"Sorting\"\n            logger.info(\"%s by identity similarity...\", msg)\n            self._log_once = False\n\n        if alignments.get(\"identity\", {}).get(\"vggface2\"):\n            embedding = np.array(alignments[\"identity\"][\"vggface2\"], dtype=\"float32\")\n\n            if not self._logged_lm_count_once and len(alignments[\"landmarks_xy\"]) == 4:\n                logger.warning(self._warning)\n                self._logged_lm_count_once = True\n\n            self._result.append((filename, embedding))\n            return\n\n        if self._output_update_info:\n            logger.info(\"VGG Face2 Embeddings are being written to the image header. \"\n                        \"Sorting by this method will be quicker next time\")\n            self._output_update_info = False\n\n        a_face = AlignedFace(np.array(alignments[\"landmarks_xy\"], dtype=\"float32\"),\n                             image=image,\n                             centering=\"legacy\",\n                             size=self._vgg_face.input_size,\n                             is_aligned=True)\n\n        if a_face.landmark_type == LandmarkType.LM_2D_4 and not self._logged_lm_count_once:\n            logger.warning(self._warning)\n            self._logged_lm_count_once = True\n\n        face = a_face.face\n        assert face is not None\n        embedding = self._vgg_face.predict(face[None, ...])[0]\n        alignments.setdefault(\"identity\", {})[\"vggface2\"] = embedding.tolist()\n        self._iterator.update_png_header(filename, alignments)\n        self._result.append((filename, embedding))\n\n    def sort(self) -> None:\n        \"\"\" Sort by dendogram.\n\n        Parameters\n        ----------\n        matched_list: list\n            The list of tuples with filename in first position and face encoding in the 2nd\n\n        Returns\n        -------\n        list\n            The original list, sorted for this metric\n        \"\"\"\n        logger.info(\"Sorting by ward linkage. This may take some time...\")\n        preds = np.array([item[1] for item in self._result])\n        indices = Cluster(np.array(preds), \"ward\", threshold=self._threshold)()\n        self._result = [(self._result[idx][0], float(score)) for idx, score in indices]\n\n    def binning(self) -> list[list[str]]:\n        \"\"\" Group into bins by their sorted score\n\n        The bin ID has been output in the 2nd column of :attr:`_result` so use that for binnin\n\n        Returns\n        -------\n        list\n            List of bins of filenames\n        \"\"\"\n        num_bins = len(set(int(i[1]) for i in self._result))\n        logger.info(\"Grouping by %s...\", self.__class__.__name__.replace(\"Sort\", \"\"))\n        bins: list[list[str]] = [[] for _ in range(num_bins)]\n\n        for filename, bin_id in self._result:\n            bins[int(bin_id)].append(filename)\n\n        return bins\n\n\nclass SortHistogram(SortMethod):\n    \"\"\" Sort by image histogram similarity or dissimilarity\n\n    Parameters\n    ----------\n    arguments: :class:`argparse.Namespace`\n        The command line arguments passed to the sort process\n    is_group: bool, optional\n        Set to ``True`` if this class is going to be called exclusively for binning.\n        Default: ``False``\n    \"\"\"\n    def __init__(self, arguments: Namespace, is_group: bool = False) -> None:\n        super().__init__(arguments, loader_type=\"all\", is_group=is_group)\n        method = arguments.group_method if self._is_group else arguments.sort_method\n        self._is_dissim = method == \"hist-dissim\"\n        self._threshold: float = 0.3 if arguments.threshold < 0.0 else arguments.threshold\n\n    def _calc_histogram(self,\n                        image: np.ndarray,\n                        alignments: PNGHeaderAlignmentsDict | None) -> np.ndarray:\n        if alignments:\n            image = self._mask_face(image, alignments)\n        return cv2.calcHist([image], [0], None, [256], [0, 256])\n\n    def _sort_dissim(self) -> None:\n        \"\"\" Sort histograms by dissimilarity \"\"\"\n        img_list_len = len(self._result)\n        for i in tqdm(range(0, img_list_len),\n                      desc=\"Comparing histograms\",\n                      file=sys.stdout,\n                      leave=False):\n            score_total = 0\n            for j in range(0, img_list_len):\n                if i == j:\n                    continue\n                score_total += cv2.compareHist(self._result[i][1],\n                                               self._result[j][1],\n                                               cv2.HISTCMP_BHATTACHARYYA)\n            self._result[i][2] = score_total\n\n        self._result = sorted(self._result, key=operator.itemgetter(2), reverse=True)\n\n    def _sort_sim(self) -> None:\n        \"\"\" Sort histograms by similarity \"\"\"\n        img_list_len = len(self._result)\n        for i in tqdm(range(0, img_list_len - 1),\n                      desc=\"Comparing histograms\",\n                      file=sys.stdout,\n                      leave=False):\n            min_score = float(\"inf\")\n            j_min_score = i + 1\n            for j in range(i + 1, img_list_len):\n                score = cv2.compareHist(self._result[i][1],\n                                        self._result[j][1],\n                                        cv2.HISTCMP_BHATTACHARYYA)\n                if score < min_score:\n                    min_score = score\n                    j_min_score = j\n            (self._result[i + 1], self._result[j_min_score]) = (self._result[j_min_score],\n                                                                self._result[i + 1])\n\n    @classmethod\n    def _get_avg_score(cls, image: np.ndarray, references: list[np.ndarray]) -> float:\n        \"\"\" Return the average histogram score between a face and reference images\n\n        Parameters\n        ----------\n        image: :class:`numpy.ndarray`\n            The image to test\n        references: list\n            List of reference images to test the original image against\n\n        Returns\n        -------\n        float\n            The average score between the histograms\n        \"\"\"\n        scores = []\n        for img2 in references:\n            score = cv2.compareHist(image, img2, cv2.HISTCMP_BHATTACHARYYA)\n            scores.append(score)\n        return sum(scores) / len(scores)\n\n    def binning(self) -> list[list[str]]:\n        \"\"\" Group into bins by histogram \"\"\"\n        msg = \"dissimilarity\" if self._is_dissim else \"similarity\"\n        logger.info(\"Grouping by %s...\", msg)\n\n        # Groups are of the form: group_num -> reference histogram\n        reference_groups: dict[int, list[np.ndarray]] = {}\n\n        # Bins array, where index is the group number and value is\n        # an array containing the file paths to the images in that group\n        bins: list[list[str]] = []\n\n        threshold = self._threshold\n\n        img_list_len = len(self._result)\n        reference_groups[0] = [T.cast(np.ndarray, self._result[0][1])]\n        bins.append([self._result[0][0]])\n\n        for i in tqdm(range(1, img_list_len),\n                      desc=\"Grouping\",\n                      file=sys.stdout,\n                      leave=False):\n            current_key = -1\n            current_score = float(\"inf\")\n            for key, value in reference_groups.items():\n                score = self._get_avg_score(self._result[i][1], value)\n                if score < current_score:\n                    current_key, current_score = key, score\n\n            if current_score < threshold:\n                reference_groups[T.cast(int, current_key)].append(self._result[i][1])\n                bins[current_key].append(self._result[i][0])\n            else:\n                reference_groups[len(reference_groups)] = [self._result[i][1]]\n                bins.append([self._result[i][0]])\n\n        return bins\n\n    def score_image(self,\n                    filename: str,\n                    image: np.ndarray | None,\n                    alignments: PNGHeaderAlignmentsDict | None) -> None:\n        \"\"\" Collect the histogram for the given face\n\n        Parameters\n        ----------\n        filename: str\n            The filename of the currently processing image\n        image: :class:`np.ndarray`\n            A face image loaded from disk\n        alignments: dict or ``None``\n            The alignments dictionary for the aligned face or ``None``\n        \"\"\"\n        if self._log_once:\n            msg = \"Grouping\" if self._is_group else \"Sorting\"\n            logger.info(\"%s by histogram similarity...\", msg)\n            self._log_once = False\n\n        assert image is not None\n        self._result.append((filename, self._calc_histogram(image, alignments)))\n\n    def sort(self) -> None:\n        \"\"\" Sort by histogram. \"\"\"\n        logger.info(\"Comparing histograms and sorting...\")\n        if self._is_dissim:\n            self._sort_dissim()\n            return\n        self._sort_sim()\n", "tools/effmpeg/cli.py": "#!/usr/bin/env python3\n\"\"\" Command Line Arguments for tools \"\"\"\nimport argparse\nimport gettext\n\nfrom lib.cli.args import FaceSwapArgs\nfrom lib.cli.actions import ContextFullPaths, FileFullPaths, Radio\nfrom lib.utils import IMAGE_EXTENSIONS\n\n\n# LOCALES\n_LANG = gettext.translation(\"tools.effmpeg.cli\", localedir=\"locales\", fallback=True)\n_ = _LANG.gettext\n\n_HELPTEXT = _(\"This command allows you to easily execute common ffmpeg tasks.\")\n\n\ndef __parse_transpose(value: str) -> str:\n    \"\"\" Parse transpose option\n\n    Parameters\n    ----------\n    value: str\n        The value to parse\n\n    Returns\n    -------\n    str\n        The option item for the given value\n    \"\"\"\n    index = 0\n    opts = [\"(0, 90CounterClockwise&VerticalFlip)\",\n            \"(1, 90Clockwise)\",\n            \"(2, 90CounterClockwise)\",\n            \"(3, 90Clockwise&VerticalFlip)\"]\n    if len(value) == 1:\n        index = int(value)\n    else:\n        for i in range(5):\n            if value in opts[i]:\n                index = i\n                break\n    return opts[index]\n\n\nclass EffmpegArgs(FaceSwapArgs):\n    \"\"\" Class to parse the command line arguments for EFFMPEG tool \"\"\"\n\n    @staticmethod\n    def get_info():\n        \"\"\" Return command information \"\"\"\n        return _(\"A wrapper for ffmpeg for performing image <> video converting.\")\n\n    @staticmethod\n    def get_argument_list():\n        argument_list = []\n        argument_list.append({\n            \"opts\": ('-a', '--action'),\n            \"action\": Radio,\n            \"dest\": \"action\",\n            \"choices\": (\"extract\", \"gen-vid\", \"get-fps\", \"get-info\", \"mux-audio\", \"rescale\",\n                        \"rotate\", \"slice\"),\n            \"default\": \"extract\",\n            \"help\": _(\"R|Choose which action you want ffmpeg ffmpeg to do.\"\n                      \"\\nL|'extract': turns videos into images \"\n                      \"\\nL|'gen-vid': turns images into videos \"\n                      \"\\nL|'get-fps' returns the chosen video's fps.\"\n                      \"\\nL|'get-info' returns information about a video.\"\n                      \"\\nL|'mux-audio' add audio from one video to another.\"\n                      \"\\nL|'rescale' resize video.\"\n                      \"\\nL|'rotate' rotate video.\"\n                      \"\\nL|'slice' cuts a portion of the video into a separate video file.\")})\n        argument_list.append({\n            \"opts\": ('-i', '--input'),\n            \"action\": ContextFullPaths,\n            \"dest\": \"input\",\n            \"default\": \"input\",\n            \"help\": _(\"Input file.\"),\n            \"group\": _(\"data\"),\n            \"required\": True,\n            \"action_option\": \"-a\",\n            \"filetypes\": \"video\"})\n        argument_list.append({\n            \"opts\": ('-o', '--output'),\n            \"action\": ContextFullPaths,\n            \"group\": _(\"data\"),\n            \"default\": \"\",\n            \"dest\": \"output\",\n            \"help\": _(\"Output file. If no output is specified then: if the output is meant to be \"\n                      \"a video then a video called 'out.mkv' will be created in the input \"\n                      \"directory; if the output is meant to be a directory then a directory \"\n                      \"called 'out' will be created inside the input directory. Note: the chosen \"\n                      \"output file extension will determine the file encoding.\"),\n            \"action_option\": \"-a\",\n            \"filetypes\": \"video\"})\n        argument_list.append({\n            \"opts\": ('-r', '--reference-video'),\n            \"action\": FileFullPaths,\n            \"dest\": \"ref_vid\",\n            \"group\": _(\"data\"),\n            \"default\": None,\n            \"help\": _(\"Path to reference video if 'input' was not a video.\"),\n            \"filetypes\": \"video\"})\n        argument_list.append({\n            \"opts\": ('-R', '--fps'),\n            \"type\": str,\n            \"dest\": \"fps\",\n            \"group\": _(\"output\"),\n            \"default\": \"-1.0\",\n            \"help\": _(\"Provide video fps. Can be an integer, float or fraction. Negative values \"\n                      \"will will make the program try to get the fps from the input or reference \"\n                      \"videos.\")})\n        argument_list.append({\n            \"opts\": (\"-E\", \"--extract-filetype\"),\n            \"action\": Radio,\n            \"choices\": IMAGE_EXTENSIONS,\n            \"dest\": \"extract_ext\",\n            \"group\": _(\"output\"),\n            \"default\": \".png\",\n            \"help\": _(\"Image format that extracted images should be saved as. '.bmp' will offer \"\n                      \"the fastest extraction speed, but will take the most storage space. '.png' \"\n                      \"will be slower but will take less storage.\")})\n        argument_list.append({\n            \"opts\": ('-s', '--start'),\n            \"type\": str,\n            \"dest\": \"start\",\n            \"group\": _(\"clip\"),\n            \"default\": \"00:00:00\",\n            \"help\": _(\"Enter the start time from which an action is to be applied. Default: \"\n                      \"00:00:00, in HH:MM:SS format. You can also enter the time with or without \"\n                      \"the colons, e.g. 00:0000 or 026010.\")})\n        argument_list.append({\n            \"opts\": ('-e', '--end'),\n            \"type\": str,\n            \"dest\": \"end\",\n            \"group\": _(\"clip\"),\n            \"default\": \"00:00:00\",\n            \"help\": _(\"Enter the end time to which an action is to be applied. If both an end \"\n                      \"time and duration are set, then the end time will be used and the duration \"\n                      \"will be ignored. Default: 00:00:00, in HH:MM:SS.\")})\n        argument_list.append({\n            \"opts\": ('-d', '--duration'),\n            \"type\": str,\n            \"dest\": \"duration\",\n            \"group\": _(\"clip\"),\n            \"default\": \"00:00:00\",\n            \"help\": _(\"Enter the duration of the chosen action, for example if you enter 00:00:10 \"\n                      \"for slice, then the first 10 seconds after and including the start time \"\n                      \"will be cut out into a new video. Default: 00:00:00, in HH:MM:SS format. \"\n                      \"You can also enter the time with or without the colons, e.g. 00:0000 or \"\n                      \"026010.\")})\n        argument_list.append({\n            \"opts\": ('-m', '--mux-audio'),\n            \"action\": \"store_true\",\n            \"dest\": \"mux_audio\",\n            \"group\": _(\"output\"),\n            \"default\": False,\n            \"help\": _(\"Mux the audio from the reference video into the input video. This option \"\n                      \"is only used for the 'gen-vid' action. 'mux-audio' action has this turned \"\n                      \"on implicitly.\")})\n        argument_list.append({\n            \"opts\": ('-T', '--transpose'),\n            \"choices\": (\"(0, 90CounterClockwise&VerticalFlip)\",\n                        \"(1, 90Clockwise)\",\n                        \"(2, 90CounterClockwise)\",\n                        \"(3, 90Clockwise&VerticalFlip)\"),\n            \"type\": lambda v: __parse_transpose(v),  # pylint:disable=unnecessary-lambda\n            \"dest\": \"transpose\",\n            \"group\": _(\"rotate\"),\n            \"default\": None,\n            \"help\": _(\"Transpose the video. If transpose is set, then degrees will be ignored. \"\n                      \"For cli you can enter either the number or the long command name, e.g. to \"\n                      \"use (1, 90Clockwise) -tr 1 or -tr 90Clockwise\")})\n        argument_list.append({\n            \"opts\": ('-D', '--degrees'),\n            \"type\": str,\n            \"dest\": \"degrees\",\n            \"default\": None,\n            \"group\": _(\"rotate\"),\n            \"help\": _(\"Rotate the video clockwise by the given number of degrees.\")})\n        argument_list.append({\n            \"opts\": ('-S', '--scale'),\n            \"type\": str,\n            \"dest\": \"scale\",\n            \"group\": _(\"output\"),\n            \"default\": \"1920x1080\",\n            \"help\": _(\"Set the new resolution scale if the chosen action is 'rescale'.\")})\n        argument_list.append({\n            \"opts\": ('-q', '--quiet'),\n            \"action\": \"store_true\",\n            \"dest\": \"quiet\",\n            \"group\": _(\"settings\"),\n            \"default\": False,\n            \"help\": _(\"Reduces output verbosity so that only serious errors are printed. If both \"\n                      \"quiet and verbose are set, verbose will override quiet.\")})\n        argument_list.append({\n            \"opts\": ('-v', '--verbose'),\n            \"action\": \"store_true\",\n            \"dest\": \"verbose\",\n            \"group\": _(\"settings\"),\n            \"default\": False,\n            \"help\": _(\"Increases output verbosity. If both quiet and verbose are set, verbose \"\n                      \"will override quiet.\")})\n        # Deprecated multi-character switches\n        argument_list.append({\n            \"opts\": ('-fps', ),\n            \"type\": str,\n            \"dest\": \"depr_fps_fps_R\",\n            \"help\": argparse.SUPPRESS})\n        argument_list.append({\n            \"opts\": (\"-ef\", ),\n            \"type\": str,\n            \"choices\": IMAGE_EXTENSIONS,\n            \"dest\": \"depr_extract-filetype_et_E\",\n            \"help\": argparse.SUPPRESS})\n        argument_list.append({\n            \"opts\": ('-tr', ),\n            \"choices\": (\"(0, 90CounterClockwise&VerticalFlip)\",\n                        \"(1, 90Clockwise)\",\n                        \"(2, 90CounterClockwise)\",\n                        \"(3, 90Clockwise&VerticalFlip)\"),\n            \"type\": lambda v: __parse_transpose(v),  # pylint:disable=unnecessary-lambda\n            \"dest\": \"depr_transpose_tr_T\",\n            \"help\": argparse.SUPPRESS})\n        argument_list.append({\n            \"opts\": ('-de', ),\n            \"type\": str,\n            \"dest\": \"depr_degrees_de_D\",\n            \"help\": argparse.SUPPRESS})\n        argument_list.append({\n            \"opts\": ('-sc', ),\n            \"type\": str,\n            \"dest\": \"depr_scale_sc_S\",\n            \"help\": argparse.SUPPRESS})\n        return argument_list\n", "tools/effmpeg/effmpeg.py": "#!/usr/bin/env python3\n# vim: set fileencoding=utf-8 :\n\"\"\"\nCreated on 2018-03-16 15:14\n\n@author: Lev Velykoivanenko (velykoivanenko.lev@gmail.com)\n\"\"\"\nimport logging\nimport os\nimport subprocess\nimport sys\nimport datetime\nfrom collections import OrderedDict\n\nimport imageio\nimport imageio_ffmpeg as im_ffm\nfrom ffmpy import FFmpeg, FFRuntimeError\n\n# faceswap imports\nfrom lib.utils import handle_deprecated_cliopts, IMAGE_EXTENSIONS, VIDEO_EXTENSIONS\n\nlogger = logging.getLogger(__name__)\n\n\nclass DataItem():\n    \"\"\"\n    A simple class used for storing the media data items and directories that\n    Effmpeg uses for 'input', 'output' and 'ref_vid'.\n    \"\"\"\n    vid_ext = VIDEO_EXTENSIONS\n    # future option in effmpeg to use audio file for muxing\n    audio_ext = [\".aiff\", \".flac\", \".mp3\", \".wav\"]\n    img_ext = IMAGE_EXTENSIONS\n\n    def __init__(self, path=None, name=None, item_type=None, ext=None,\n                 fps=None):\n        logger.debug(\"Initializing %s: (path: '%s', name: '%s', item_type: '%s', ext: '%s')\",\n                     self.__class__.__name__, path, name, item_type, ext)\n        self.path = path\n        self.name = name\n        self.type = item_type\n        self.ext = ext\n        self.fps = fps\n        self.dirname = None\n        self.set_type_ext(path)\n        self.set_dirname(self.path)\n        self.set_name(name)\n        if self.is_type(\"vid\") and self.fps is None:\n            self.set_fps()\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    def set_name(self, name=None):\n        \"\"\" Set the name \"\"\"\n        if name is None and self.path is not None:\n            self.name = os.path.basename(self.path)\n        elif name is not None and self.path is None:\n            self.name = os.path.basename(name)\n        elif name is not None and self.path is not None:\n            self.name = os.path.basename(name)\n        else:\n            self.name = None\n        logger.debug(self.name)\n\n    def set_type_ext(self, path=None):\n        \"\"\" Set the extension \"\"\"\n        if path is not None:\n            self.path = path\n        if self.path is not None:\n            item_ext = os.path.splitext(self.path)[1].lower()\n            if item_ext in DataItem.vid_ext:\n                item_type = \"vid\"\n            elif item_ext in DataItem.audio_ext:\n                item_type = \"audio\"\n            else:\n                item_type = \"dir\"\n            self.type = item_type\n            self.ext = item_ext\n            logger.debug(\"path: '%s', type: '%s', ext: '%s'\", self.path, self.type, self.ext)\n        else:\n            return\n\n    def set_dirname(self, path=None):\n        \"\"\" Set the folder name \"\"\"\n        if path is None and self.path is not None:\n            self.dirname = os.path.dirname(self.path)\n        elif path is not None and self.path is None:\n            self.dirname = os.path.dirname(path)\n        elif path is not None and self.path is not None:\n            self.dirname = os.path.dirname(path)\n        else:\n            self.dirname = None\n        logger.debug(\"path: '%s', dirname: '%s'\", path, self.dirname)\n\n    def is_type(self, item_type=None):\n        \"\"\" Get the type \"\"\"\n        if item_type == \"media\":\n            chk_type = self.type in \"vid audio\"\n        elif item_type == \"dir\":\n            chk_type = self.type == \"dir\"\n        elif item_type == \"vid\":\n            chk_type = self.type == \"vid\"\n        elif item_type == \"audio\":\n            chk_type = self.type == \"audio\"\n        elif item_type.lower() == \"none\":\n            chk_type = self.type is None\n        else:\n            chk_type = False\n        logger.debug(\"item_type: '%s', chk_type: '%s'\", item_type, chk_type)\n        return chk_type\n\n    def set_fps(self):\n        \"\"\" Set the Frames Per Second \"\"\"\n        try:\n            self.fps = Effmpeg.get_fps(self.path)\n        except FFRuntimeError:\n            self.fps = None\n        logger.debug(self.fps)\n\n\nclass Effmpeg():\n    \"\"\"\n    Class that allows for \"easy\" ffmpeg use. It provides a nice cli interface\n    for common video operations.\n    \"\"\"\n\n    _actions_req_fps = [\"extract\", \"gen_vid\"]\n    _actions_req_ref_video = [\"mux_audio\"]\n    _actions_can_use_ref_video = [\"gen_vid\"]\n    _actions_have_dir_output = [\"extract\"]\n    _actions_have_vid_output = [\"gen_vid\", \"mux_audio\", \"rescale\", \"rotate\",\n                                \"slice\"]\n    _actions_have_print_output = [\"get_fps\", \"get_info\"]\n    _actions_have_dir_input = [\"gen_vid\"]\n    _actions_have_vid_input = [\"extract\", \"get_fps\", \"get_info\", \"rescale\",\n                               \"rotate\", \"slice\"]\n\n    # Class variable that stores the target executable (ffmpeg or ffplay)\n    _executable = im_ffm.get_ffmpeg_exe()\n\n    # Class variable that stores the common ffmpeg arguments based on verbosity\n    __common_ffmpeg_args_dict = {\"normal\": \"-hide_banner \",\n                                 \"quiet\": \"-loglevel panic -hide_banner \",\n                                 \"verbose\": \"\"}\n\n    # _common_ffmpeg_args is the class variable that will get used by various\n    # actions and it will be set by the process_arguments() method based on\n    # passed verbosity\n    _common_ffmpeg_args = \"\"\n\n    def __init__(self, arguments):\n        logger.debug(\"Initializing %s: (arguments: %s)\", self.__class__.__name__, arguments)\n        self.args = handle_deprecated_cliopts(arguments)\n        self.exe = im_ffm.get_ffmpeg_exe()\n        self.input = DataItem()\n        self.output = DataItem()\n        self.ref_vid = DataItem()\n        self.start = \"\"\n        self.end = \"\"\n        self.duration = \"\"\n        self.print_ = False\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    def _set_output(self) -> None:\n        \"\"\" Set :attr:`output` based on input arguments \"\"\"\n        if self.args.action in self._actions_have_dir_output:\n            self.output = DataItem(path=self.__get_default_output())\n        elif self.args.action in self._actions_have_vid_output:\n            if self.__check_have_fps(self.args.fps) > 0:\n                self.output = DataItem(path=self.__get_default_output(),\n                                       fps=self.args.fps)\n            else:\n                self.output = DataItem(path=self.__get_default_output())\n\n    def _set_ref_video(self) -> None:\n        \"\"\" Set :attr:`ref_vid` based on input arguments \"\"\"\n        if self.args.ref_vid is None or self.args.ref_vid == \"\":\n            self.args.ref_vid = None\n\n        self.ref_vid = DataItem(path=self.args.ref_vid)\n\n    def _check_inputs(self) -> None:\n        \"\"\" Validate provided arguments are valid\n\n        Raises\n        ------\n        ValueError\n            If provided arguments are not valid\n        \"\"\"\n\n        if self.args.action in self._actions_have_dir_input and not self.input.is_type(\"dir\"):\n            raise ValueError(\"The chosen action requires a directory as its input, but you \"\n                             f\"entered: {self.input.path}\")\n        if self.args.action in self._actions_have_vid_input and not self.input.is_type(\"vid\"):\n            raise ValueError(\"The chosen action requires a video as its input, but you entered: \"\n                             f\"{self.input.path}\")\n        if self.args.action in self._actions_have_dir_output and not self.output.is_type(\"dir\"):\n            raise ValueError(\"The chosen action requires a directory as its output, but you \"\n                             f\"entered: {self.output.path}\")\n        if self.args.action in self._actions_have_vid_output and not self.output.is_type(\"vid\"):\n            raise ValueError(\"The chosen action requires a video as its output, but you entered: \"\n                             f\"{self.output.path}\")\n\n        # Check that ref_vid is a video when it needs to be\n        if self.args.action in self._actions_req_ref_video:\n            if self.ref_vid.is_type(\"none\"):\n                raise ValueError(\"The file chosen as the reference video is not a video, either \"\n                                 f\"leave the field blank or type 'None': {self.ref_vid.path}\")\n        elif self.args.action in self._actions_can_use_ref_video:\n            if self.ref_vid.is_type(\"none\"):\n                logger.warning(\"Warning: no reference video was supplied, even though \"\n                               \"one may be used with the chosen action. If this is \"\n                               \"intentional then ignore this warning.\")\n\n    def _set_times(self) -> None:\n        \"\"\"Set start, end and duration attributes \"\"\"\n        self.start = self.parse_time(self.args.start)\n        self.end = self.parse_time(self.args.end)\n        if not self.__check_equals_time(self.args.end, \"00:00:00\"):\n            self.duration = self.__get_duration(self.start, self.end)\n        else:\n            self.duration = self.parse_time(str(self.args.duration))\n\n    def _set_fps(self) -> None:\n        \"\"\" Set :attr:`arguments.fps` based on input arguments\"\"\"\n        # If fps was left blank in gui, set it to default -1.0 value\n        if self.args.fps == \"\":\n            self.args.fps = str(-1.0)\n\n        # Try to set fps automatically if needed and not supplied by user\n        if self.args.action in self._actions_req_fps \\\n                and self.__convert_fps(self.args.fps) <= 0:\n            if self.__check_have_fps([\"r\", \"i\"]):\n                _error_str = \"No fps, input or reference video was supplied, \"\n                _error_str += \"hence it's not possible to \"\n                _error_str += f\"'{self.args.action}'.\"\n                raise ValueError(_error_str)\n            if self.output.fps is not None and self.__check_have_fps([\"r\", \"i\"]):\n                self.args.fps = self.output.fps\n            elif self.ref_vid.fps is not None and self.__check_have_fps([\"i\"]):\n                self.args.fps = self.ref_vid.fps\n            elif self.input.fps is not None and self.__check_have_fps([\"r\"]):\n                self.args.fps = self.input.fps\n\n    def process(self):\n        \"\"\" EFFMPEG Process \"\"\"\n        logger.debug(\"Running Effmpeg\")\n        # Format action to match the method name\n        self.args.action = self.args.action.replace(\"-\", \"_\")\n        logger.debug(\"action: '%s'\", self.args.action)\n\n        # Instantiate input DataItem object\n        self.input = DataItem(path=self.args.input)\n\n        # Instantiate output DataItem object\n        self._set_output()\n\n        # Instantiate ref_vid DataItem object\n        self._set_ref_video()\n\n        # Check that correct input and output arguments were provided\n        self._check_inputs()\n\n        # Process start and duration arguments\n        self._set_times()\n\n        # Set fps\n        self._set_fps()\n\n        # Processing transpose\n        if self.args.transpose is None or \\\n                self.args.transpose.lower() == \"none\":\n            self.args.transpose = None\n        else:\n            self.args.transpose = self.args.transpose[1]\n\n        # Processing degrees\n        if self.args.degrees is None \\\n                or self.args.degrees.lower() == \"none\" \\\n                or self.args.degrees == \"\":\n            self.args.degrees = None\n        elif self.args.transpose is None:\n            try:\n                int(self.args.degrees)\n            except ValueError:\n                logger.error(\"You have entered an invalid value for degrees: %s\",\n                             self.args.degrees)\n                sys.exit(1)\n\n        # Set verbosity of output\n        self.__set_verbosity(self.args.quiet, self.args.verbose)\n\n        # Set self.print_ to True if output needs to be printed to stdout\n        if self.args.action in self._actions_have_print_output:\n            self.print_ = True\n\n        self.effmpeg_process()\n        logger.debug(\"Finished Effmpeg process\")\n\n    def effmpeg_process(self):\n        \"\"\" The effmpeg process \"\"\"\n        kwargs = {\"input_\": self.input,\n                  \"output\": self.output,\n                  \"ref_vid\": self.ref_vid,\n                  \"fps\": self.args.fps,\n                  \"extract_ext\": self.args.extract_ext,\n                  \"start\": self.start,\n                  \"duration\": self.duration,\n                  \"mux_audio\": self.args.mux_audio,\n                  \"degrees\": self.args.degrees,\n                  \"transpose\": self.args.transpose,\n                  \"scale\": self.args.scale,\n                  \"print_\": self.print_,\n                  \"exe\": self.exe}\n        action = getattr(self, self.args.action)\n        action(**kwargs)\n\n    @staticmethod\n    def extract(input_=None, output=None, fps=None,  # pylint:disable=unused-argument\n                extract_ext=None, start=None, duration=None, **kwargs):\n        \"\"\" Extract video to image frames \"\"\"\n        logger.debug(\"input_: %s, output: %s, fps: %s, extract_ext: '%s', start: %s, duration: %s\",\n                     input_, output, fps, extract_ext, start, duration)\n        _input_opts = Effmpeg._common_ffmpeg_args[:]\n        if start is not None and duration is not None:\n            _input_opts += f\"-ss {start} -t {duration}\"\n        _input = {input_.path: _input_opts}\n        _output_opts = '-y -vf fps=\"' + str(fps) + '\" -q:v 1'\n        _output_path = output.path + \"/\" + input_.name + \"_%05d\" + extract_ext\n        _output = {_output_path: _output_opts}\n        os.makedirs(output.path, exist_ok=True)\n        logger.debug(\"_input: %s, _output: %s\", _input, _output)\n        Effmpeg.__run_ffmpeg(inputs=_input, outputs=_output)\n\n    @staticmethod\n    def gen_vid(input_=None, output=None, fps=None,  # pylint:disable=unused-argument\n                mux_audio=False, ref_vid=None, exe=None, **kwargs):\n        \"\"\" Generate Video \"\"\"\n        logger.debug(\"input: %s, output: %s, fps: %s, mux_audio: %s, ref_vid: '%s'exe: '%s'\",\n                     input, output, fps, mux_audio, ref_vid, exe)\n        filename = Effmpeg.__get_extracted_filename(input_.path)\n        _input_opts = Effmpeg._common_ffmpeg_args[:]\n        _input_path = os.path.join(input_.path, filename)\n        _fps_arg = \"-r \" + str(fps) + \" \"\n        _input_opts += _fps_arg + \"-f image2 \"\n        _output_opts = \"-y \" + _fps_arg + \" -c:v libx264\"\n        if mux_audio:\n            _ref_vid_opts = \"-c copy -map 0:0 -map 1:1\"\n            _output_opts = _ref_vid_opts + \" \" + _output_opts\n            _inputs = OrderedDict([(_input_path, _input_opts), (ref_vid.path, None)])\n        else:\n            _inputs = {_input_path: _input_opts}\n        _outputs = {output.path: _output_opts}\n        logger.debug(\"_inputs: %s, _outputs: %s\", _inputs, _outputs)\n        Effmpeg.__run_ffmpeg(exe=exe, inputs=_inputs, outputs=_outputs)\n\n    @staticmethod\n    def get_fps(input_=None, print_=False, **kwargs):\n        \"\"\" Get Frames per Second \"\"\"\n        logger.debug(\"input_: %s, print_: %s, kwargs: %s\", input_, print_, kwargs)\n        input_ = input_ if isinstance(input_, str) else input_.path\n        logger.debug(\"input: %s\", input_)\n        reader = imageio.get_reader(input_, \"ffmpeg\")\n        _fps = reader.get_meta_data()[\"fps\"]\n        logger.debug(_fps)\n        reader.close()\n        if print_:\n            logger.info(\"Video fps: %s\", _fps)\n        return _fps\n\n    @staticmethod\n    def get_info(input_=None, print_=False, **kwargs):\n        \"\"\" Get video Info \"\"\"\n        logger.debug(\"input_: %s, print_: %s, kwargs: %s\", input_, print_, kwargs)\n        input_ = input_ if isinstance(input_, str) else input_.path\n        logger.debug(\"input: %s\", input_)\n        reader = imageio.get_reader(input_, \"ffmpeg\")\n        out = reader.get_meta_data()\n        logger.debug(out)\n        reader.close()\n        if print_:\n            logger.info(\"======== Video Info ========\",)\n            logger.info(\"path: %s\", input_)\n            for key, val in out.items():\n                logger.info(\"%s: %s\", key, val)\n        return out\n\n    @staticmethod\n    def rescale(input_=None, output=None, scale=None,  # pylint:disable=unused-argument\n                exe=None, **kwargs):\n        \"\"\" Rescale Video \"\"\"\n        _input_opts = Effmpeg._common_ffmpeg_args[:]\n        _output_opts = '-y -vf scale=\"' + str(scale) + '\"'\n        _inputs = {input_.path: _input_opts}\n        _outputs = {output.path: _output_opts}\n        Effmpeg.__run_ffmpeg(exe=exe, inputs=_inputs, outputs=_outputs)\n\n    @staticmethod\n    def rotate(input_=None, output=None, degrees=None,  # pylint:disable=unused-argument\n               transpose=None, exe=None, **kwargs):\n        \"\"\" Rotate Video \"\"\"\n        if transpose is None and degrees is None:\n            raise ValueError(\"You have not supplied a valid transpose or degrees value:\\n\"\n                             f\"transpose: {transpose}\\ndegrees: {degrees}\")\n\n        _input_opts = Effmpeg._common_ffmpeg_args[:]\n        _output_opts = \"-y -c:a copy -vf \"\n        _bilinear = \"\"\n        if transpose is not None:\n            _output_opts += 'transpose=\"' + str(transpose) + '\"'\n        elif int(degrees) != 0:\n            if int(degrees) % 90 == 0 and int(degrees) != 0:\n                _bilinear = \":bilinear=0\"\n            _output_opts += 'rotate=\"' + str(degrees) + '*(PI/180)'\n            _output_opts += _bilinear + '\" '\n\n        _inputs = {input_.path: _input_opts}\n        _outputs = {output.path: _output_opts}\n        Effmpeg.__run_ffmpeg(exe=exe, inputs=_inputs, outputs=_outputs)\n\n    @staticmethod\n    def mux_audio(input_=None, output=None, ref_vid=None,  # pylint:disable=unused-argument\n                  exe=None, **kwargs):\n        \"\"\" Mux Audio \"\"\"\n        _input_opts = Effmpeg._common_ffmpeg_args[:]\n        _ref_vid_opts = None\n        _output_opts = \"-y -c copy -map 0:0 -map 1:1 -shortest\"\n        _inputs = OrderedDict([(input_.path, _input_opts), (ref_vid.path, _ref_vid_opts)])\n        _outputs = {output.path: _output_opts}\n        Effmpeg.__run_ffmpeg(exe=exe, inputs=_inputs, outputs=_outputs)\n\n    @staticmethod\n    def slice(input_=None, output=None, start=None,  # pylint:disable=unused-argument\n              duration=None, exe=None, **kwargs):\n        \"\"\" Slice Video \"\"\"\n        _input_opts = Effmpeg._common_ffmpeg_args[:]\n        _input_opts += \"-ss \" + start\n        _output_opts = \"-t \" + duration + \" \"\n        _inputs = {input_.path: _input_opts}\n        _output = {output.path: _output_opts}\n        Effmpeg.__run_ffmpeg(exe=exe, inputs=_inputs, outputs=_output)\n\n    # Various helper methods\n    @classmethod\n    def __set_verbosity(cls, quiet, verbose):\n        if verbose:\n            cls._common_ffmpeg_args = cls.__common_ffmpeg_args_dict[\"verbose\"]\n        elif quiet:\n            cls._common_ffmpeg_args = cls.__common_ffmpeg_args_dict[\"quiet\"]\n        else:\n            cls._common_ffmpeg_args = cls.__common_ffmpeg_args_dict[\"normal\"]\n\n    def __get_default_output(self):\n        \"\"\" Set output to the same directory as input\n            if the user didn't specify it. \"\"\"\n        if self.args.output == \"\":\n            if self.args.action in self._actions_have_dir_output:\n                retval = os.path.join(self.input.dirname, \"out\")\n            elif self.args.action in self._actions_have_vid_output:\n                if self.input.is_type(\"media\"):\n                    # Using the same extension as input leads to very poor\n                    # output quality, hence the default is mkv for now\n                    retval = os.path.join(self.input.dirname, \"out.mkv\")  # + self.input.ext)\n                else:  # case if input was a directory\n                    retval = os.path.join(self.input.dirname, \"out.mkv\")\n        else:\n            retval = self.args.output\n        logger.debug(retval)\n        return retval\n\n    def __check_have_fps(self, items):\n        items_to_check = []\n        for i in items:\n            if i == \"r\":\n                items_to_check.append(\"ref_vid\")\n            elif i == \"i\":\n                items_to_check.append(\"input\")\n            elif i == \"o\":\n                items_to_check.append(\"output\")\n\n        return all(getattr(self, i).fps is None for i in items_to_check)\n\n    @staticmethod\n    def __run_ffmpeg(exe=im_ffm.get_ffmpeg_exe(), inputs=None, outputs=None):\n        \"\"\" Run ffmpeg \"\"\"\n        logger.debug(\"Running ffmpeg: (exe: '%s', inputs: %s, outputs: %s\", exe, inputs, outputs)\n        ffm = FFmpeg(executable=exe, inputs=inputs, outputs=outputs)\n        try:\n            ffm.run(stderr=subprocess.STDOUT)\n        except FFRuntimeError as ffe:\n            # After receiving SIGINT ffmpeg has a 255 exit code\n            if ffe.exit_code == 255:\n                pass\n            else:\n                raise ValueError(f\"An unexpected FFRuntimeError occurred: {ffe}\") from ffe\n        except KeyboardInterrupt:\n            pass  # Do nothing if voluntary interruption\n        logger.debug(\"ffmpeg finished\")\n\n    @staticmethod\n    def __convert_fps(fps):\n        \"\"\" Convert to Frames per Second \"\"\"\n        if \"/\" in fps:\n            _fps = fps.split(\"/\")\n            retval = float(_fps[0]) / float(_fps[1])\n        else:\n            retval = float(fps)\n        logger.debug(retval)\n        return retval\n\n    @staticmethod\n    def __get_duration(start_time, end_time):\n        \"\"\" Get the duration \"\"\"\n        start = [int(i) for i in start_time.split(\":\")]\n        end = [int(i) for i in end_time.split(\":\")]\n        start = datetime.timedelta(hours=start[0], minutes=start[1], seconds=start[2])\n        end = datetime.timedelta(hours=end[0], minutes=end[1], seconds=end[2])\n        delta = end - start\n        secs = delta.total_seconds()\n        retval = f\"{int(secs // 3600):02}:{int(secs % 3600 // 60):02}:{int(secs % 60):02}\"\n        logger.debug(retval)\n        return retval\n\n    @staticmethod\n    def __get_extracted_filename(path):\n        \"\"\" Get the extracted filename \"\"\"\n        logger.debug(\"path: '%s'\", path)\n        filename = \"\"\n        for file in os.listdir(path):\n            if any(i in file for i in DataItem.img_ext):\n                filename = file\n                break\n        logger.debug(\"sample filename: '%s'\", filename)\n        filename, img_ext = os.path.splitext(filename)\n        zero_pad = Effmpeg.__get_zero_pad(filename)\n        name = filename[:-zero_pad]\n        retval = f\"{name}%{zero_pad}d{img_ext}\"\n        logger.debug(\"filename: %s, img_ext: '%s', zero_pad: %s, name: '%s'\",\n                     filename, img_ext, zero_pad, name)\n        logger.debug(retval)\n        return retval\n\n    @staticmethod\n    def __get_zero_pad(filename):\n        \"\"\" Return the starting position of zero padding from a filename \"\"\"\n        chkstring = filename[::-1]\n        logger.trace(\"filename: %s, chkstring: %s\", filename, chkstring)\n        pos = 0\n        for char in chkstring:\n            if not char.isdigit():\n                break\n        logger.debug(\"filename: '%s', pos: %s\", filename, pos)\n        return pos\n\n    @staticmethod\n    def __check_equals_time(value, time):\n        \"\"\" Check equals time \"\"\"\n        val = value.replace(\":\", \"\")\n        tme = time.replace(\":\", \"\")\n        retval = val.zfill(6) == tme.zfill(6)\n        logger.debug(\"value: '%s', time: %s, retval: %s\", value, time, retval)\n        return retval\n\n    @staticmethod\n    def parse_time(txt):\n        \"\"\" Parse Time \"\"\"\n        clean_txt = txt.replace(\":\", \"\")\n        hours = clean_txt[0:2]\n        minutes = clean_txt[2:4]\n        seconds = clean_txt[4:6]\n        retval = hours + \":\" + minutes + \":\" + seconds\n        logger.debug(\"txt: '%s', retval: %s\", txt, retval)\n        return retval\n", "tools/effmpeg/__init__.py": "", "tools/model/model.py": "#!/usr/bin/env python3\n\"\"\" Tool to restore models from backup \"\"\"\nfrom __future__ import annotations\nimport logging\nimport os\nimport sys\nimport typing as T\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\n\nfrom lib.model.backup_restore import Backup\n\n# Import the following libs for custom objects\nfrom lib.model import initializers, layers, normalization  # noqa # pylint:disable=unused-import\nfrom plugins.train.model._base.model import _Inference\n\n\nif T.TYPE_CHECKING:\n    import argparse\n\nlogger = logging.getLogger(__name__)\n\n\nclass Model():\n    \"\"\" Tool to perform actions on a model file.\n\n    Parameters\n    ----------\n    :class:`argparse.Namespace`\n        The command line arguments calling the model tool\n    \"\"\"\n    def __init__(self, arguments: argparse.Namespace) -> None:\n        logger.debug(\"Initializing %s: (arguments: '%s'\", self.__class__.__name__, arguments)\n        self._configure_tensorflow()\n        self._model_dir = self._check_folder(arguments.model_dir)\n        self._job = self._get_job(arguments)\n\n    @classmethod\n    def _configure_tensorflow(cls) -> None:\n        \"\"\" Disable eager execution and force Tensorflow into CPU mode. \"\"\"\n        tf.config.set_visible_devices([], device_type=\"GPU\")\n        tf.compat.v1.disable_eager_execution()\n\n    @classmethod\n    def _get_job(cls, arguments: argparse.Namespace) -> T.Any:\n        \"\"\" Get the correct object that holds the selected job.\n\n        Parameters\n        ----------\n        arguments: :class:`argparse.Namespace`\n            The command line arguments received for the Model tool which will be used to initiate\n            the selected job\n\n        Returns\n        -------\n        class\n            The object that will perform the selected job\n        \"\"\"\n        jobs = {\"inference\": Inference,\n                \"nan-scan\": NaNScan,\n                \"restore\": Restore}\n        return jobs[arguments.job](arguments)\n\n    @classmethod\n    def _check_folder(cls, model_dir: str) -> str:\n        \"\"\" Check that the passed in model folder exists and contains a valid model.\n\n        If the passed in value fails any checks, process exits.\n\n        Parameters\n        ----------\n        model_dir: str\n            The model folder to be checked\n\n        Returns\n        -------\n        str\n            The confirmed location of the model folder.\n        \"\"\"\n        if not os.path.exists(model_dir):\n            logger.error(\"Model folder does not exist: '%s'\", model_dir)\n            sys.exit(1)\n\n        chkfiles = [fname\n                    for fname in os.listdir(model_dir)\n                    if fname.endswith(\".h5\")\n                    and not os.path.splitext(fname)[0].endswith(\"_inference\")]\n\n        if not chkfiles:\n            logger.error(\"Could not find a model in the supplied folder: '%s'\", model_dir)\n            sys.exit(1)\n\n        if len(chkfiles) > 1:\n            logger.error(\"More than one model file found in the model folder: '%s'\", model_dir)\n            sys.exit(1)\n\n        model_name = os.path.splitext(chkfiles[0])[0].title()\n        logger.info(\"%s Model found\", model_name)\n        return model_dir\n\n    def process(self) -> None:\n        \"\"\" Call the selected model job.\"\"\"\n        self._job.process()\n\n\nclass Inference():\n    \"\"\" Save an inference model from a trained Faceswap model.\n\n    Parameters\n    ----------\n    :class:`argparse.Namespace`\n        The command line arguments calling the model tool\n    \"\"\"\n    def __init__(self, arguments: argparse.Namespace) -> None:\n        self._switch = arguments.swap_model\n        self._format = arguments.format\n        self._input_file, self._output_file = self._get_output_file(arguments.model_dir)\n\n    def _get_output_file(self, model_dir: str) -> tuple[str, str]:\n        \"\"\" Obtain the full path for the output model file/folder\n\n        Parameters\n        ----------\n        model_dir: str\n            The full path to the folder containing the Faceswap trained model .h5 file\n\n        Returns\n        -------\n        str\n            The full path to the source model file\n        str\n            The full path to the inference model save location\n         \"\"\"\n        model_name = next(fname for fname in os.listdir(model_dir) if fname.endswith(\".h5\"))\n        in_path = os.path.join(model_dir, model_name)\n        logger.debug(\"Model input path: '%s'\", in_path)\n\n        model_name = f\"{os.path.splitext(model_name)[0]}_inference\"\n        model_name = f\"{model_name}.h5\" if self._format == \"h5\" else model_name\n        out_path = os.path.join(model_dir, model_name)\n        logger.debug(\"Inference output path: '%s'\", out_path)\n        return in_path, out_path\n\n    def process(self) -> None:\n        \"\"\" Run the inference model creation process. \"\"\"\n        logger.info(\"Loading model '%s'\", self._input_file)\n        model = keras.models.load_model(self._input_file, compile=False)\n        logger.info(\"Creating inference model...\")\n        inference = _Inference(model, self._switch).model\n        logger.info(\"Saving to: '%s'\", self._output_file)\n        inference.save(self._output_file)\n\n\nclass NaNScan():\n    \"\"\" Tool to scan for NaN and Infs in model weights.\n\n    Parameters\n    ----------\n    :class:`argparse.Namespace`\n        The command line arguments calling the model tool\n    \"\"\"\n    def __init__(self, arguments: argparse.Namespace) -> None:\n        logger.debug(\"Initializing %s: (arguments: '%s'\", self.__class__.__name__, arguments)\n        self._model_file = self._get_model_filename(arguments.model_dir)\n\n    @classmethod\n    def _get_model_filename(cls, model_dir: str) -> str:\n        \"\"\" Obtain the full path the model's .h5 file.\n\n        Parameters\n        ----------\n        model_dir: str\n            The full path to the folder containing the model file\n\n        Returns\n        -------\n        str\n            The full path to the saved model file\n        \"\"\"\n        model_file = next(fname for fname in os.listdir(model_dir) if fname.endswith(\".h5\"))\n        return os.path.join(model_dir, model_file)\n\n    def _parse_weights(self,\n                       layer: keras.models.Model | keras.layers.Layer) -> dict:\n        \"\"\" Recursively pass through sub-models to scan layer weights\"\"\"\n        weights = layer.get_weights()\n        logger.debug(\"Processing weights for layer '%s', length: '%s'\",\n                     layer.name, len(weights))\n\n        if not weights:\n            logger.debug(\"Skipping layer with no weights: %s\", layer.name)\n            return {}\n\n        if hasattr(layer, \"layers\"):  # Must be a submodel\n            retval = {}\n            for lyr in layer.layers:\n                info = self._parse_weights(lyr)\n                if not info:\n                    continue\n                retval[lyr.name] = info\n            return retval\n\n        nans = sum(np.count_nonzero(np.isnan(w)) for w in weights)\n        infs = sum(np.count_nonzero(np.isinf(w)) for w in weights)\n\n        if nans + infs == 0:\n            return {}\n        return {\"nans\": nans, \"infs\": infs}\n\n    def _parse_output(self, errors: dict, indent: int = 0) -> None:\n        \"\"\" Parse the output of the errors dictionary and print a pretty summary.\n\n        Parameters\n        ----------\n        errors: dict\n            The nested dictionary of errors found when parsing the weights\n\n        indent: int, optional\n            How far should the current printed line be indented. Default: `0`\n        \"\"\"\n        for key, val in errors.items():\n            logline = f\"|{'--' * indent} \"\n            logline += key.ljust(50 - len(logline))\n            if isinstance(val, dict) and \"nans\" not in val:\n                logger.info(logline)\n                self._parse_output(val, indent + 1)\n            elif isinstance(val, dict) and \"nans\" in val:\n                logline += f\"nans: {val['nans']}, infs: {val['infs']}\"\n                logger.info(logline.ljust(30))\n\n    def process(self) -> None:\n        \"\"\" Scan the loaded model for NaNs and Infs and output summary. \"\"\"\n        logger.info(\"Loading model...\")\n        model = keras.models.load_model(self._model_file, compile=False)\n        logger.info(\"Parsing weights for invalid values...\")\n        errors = self._parse_weights(model)\n\n        if not errors:\n            logger.info(\"No invalid values found in model: '%s'\", self._model_file)\n            sys.exit(1)\n\n        logger.info(\"Invalid values found in model: %s\", self._model_file)\n        self._parse_output(errors)\n\n\nclass Restore():\n    \"\"\" Restore a model from backup.\n\n    Parameters\n    ----------\n    :class:`argparse.Namespace`\n        The command line arguments calling the model tool\n    \"\"\"\n    def __init__(self, arguments: argparse.Namespace) -> None:\n        logger.debug(\"Initializing %s: (arguments: '%s'\", self.__class__.__name__, arguments)\n        self._model_dir = arguments.model_dir\n        self._model_name = self._get_model_name()\n\n    def process(self) -> None:\n        \"\"\" Perform the Restore process \"\"\"\n        logger.info(\"Starting Model Restore...\")\n        backup = Backup(self._model_dir, self._model_name)\n        backup.restore()\n        logger.info(\"Completed Model Restore\")\n\n    def _get_model_name(self) -> str:\n        \"\"\" Additional checks to make sure that a backup exists in the model location. \"\"\"\n        bkfiles = [fname for fname in os.listdir(self._model_dir) if fname.endswith(\".bk\")]\n        if not bkfiles:\n            logger.error(\"Could not find any backup files in the supplied folder: '%s'\",\n                         self._model_dir)\n            sys.exit(1)\n        logger.verbose(\"Backup files: %s)\", bkfiles)  # type:ignore\n\n        model_name = next(fname for fname in bkfiles if fname.endswith(\".h5.bk\"))\n        return model_name[:-6]\n", "tools/model/cli.py": "#!/usr/bin/env python3\n\"\"\" Command Line Arguments for tools \"\"\"\nimport gettext\nimport typing as T\n\nfrom lib.cli.args import FaceSwapArgs\nfrom lib.cli.actions import DirFullPaths, Radio\n\n# LOCALES\n_LANG = gettext.translation(\"tools.restore.cli\", localedir=\"locales\", fallback=True)\n_ = _LANG.gettext\n\n_HELPTEXT = _(\"This tool lets you perform actions on saved Faceswap models.\")\n\n\nclass ModelArgs(FaceSwapArgs):\n    \"\"\" Class to perform actions on  model files \"\"\"\n\n    @staticmethod\n    def get_info() -> str:\n        \"\"\" Return command information \"\"\"\n        return _(\"A tool for performing actions on Faceswap trained model files\")\n\n    @staticmethod\n    def get_argument_list() -> list[dict[str, T.Any]]:\n        \"\"\" Put the arguments in a list so that they are accessible from both argparse and gui \"\"\"\n        argument_list = []\n        argument_list.append({\n            \"opts\": (\"-m\", \"--model-dir\"),\n            \"action\": DirFullPaths,\n            \"dest\": \"model_dir\",\n            \"required\": True,\n            \"help\": _(\n                \"Model directory. A directory containing the model you wish to perform an action \"\n                \"on.\")})\n        argument_list.append({\n            \"opts\": (\"-j\", \"--job\"),\n            \"action\": Radio,\n            \"type\": str,\n            \"choices\": (\"inference\", \"nan-scan\", \"restore\"),\n            \"required\": True,\n            \"help\": _(\n                \"R|Choose which action you want to perform.\"\n                \"\\nL|'inference' - Create an inference only copy of the model. Strips any layers \"\n                \"from the model which are only required for training. NB: This is for exporting \"\n                \"the model for use in external applications. Inference generated models cannot be \"\n                \"used within Faceswap. See the 'format' option for specifying the model output \"\n                \"format.\"\n                \"\\nL|'nan-scan' - Scan the model file for NaNs or Infs (invalid data).\"\n                \"\\nL|'restore' - Restore a model from backup.\")})\n        argument_list.append({\n            \"opts\": (\"-f\", \"--format\"),\n            \"action\": Radio,\n            \"type\": str,\n            \"choices\": (\"h5\", \"saved-model\"),\n            \"default\": \"h5\",\n            \"group\": _(\"inference\"),\n            \"help\": _(\n                \"R|The format to save the model as. Note: Only used for 'inference' job.\"\n                \"\\nL|'h5' - Standard Keras H5 format. Does not store any custom layer \"\n                \"information. Layers will need to be loaded from Faceswap to use.\"\n                \"\\nL|'saved-model' - Tensorflow's Saved Model format. Contains all information \"\n                \"required to load the model outside of Faceswap.\")})\n        argument_list.append({\n            \"opts\": (\"-s\", \"--swap-model\"),\n            \"action\": \"store_true\",\n            \"dest\": \"swap_model\",\n            \"default\": False,\n            \"group\": _(\"inference\"),\n            \"help\": _(\n                \"Only used for 'inference' job. Generate the inference model for B -> A  instead \"\n                \"of A -> B.\")})\n        return argument_list\n", "tools/model/__init__.py": "", "tools/alignments/jobs_frames.py": "#!/usr/bin/env python3\n\"\"\" Tools for manipulating the alignments using Frames as a source \"\"\"\nfrom __future__ import annotations\nimport logging\nimport os\nimport sys\nimport typing as T\n\nfrom datetime import datetime\n\nimport cv2\nimport numpy as np\nfrom tqdm import tqdm\n\nfrom lib.align import DetectedFace, EXTRACT_RATIOS, LANDMARK_PARTS, LandmarkType\nfrom lib.align.alignments import _VERSION, PNGHeaderDict\nfrom lib.image import encode_image, generate_thumbnail, ImagesSaver\nfrom plugins.extract import ExtractMedia, Extractor\nfrom .media import ExtractedFaces, Frames\n\nif T.TYPE_CHECKING:\n    from argparse import Namespace\n    from .media import AlignmentData\n\nlogger = logging.getLogger(__name__)\n\n\nclass Draw():\n    \"\"\" Draws annotations onto original frames and saves into a sub-folder next to the original\n    frames.\n\n    Parameters\n    ---------\n    alignments: :class:`tools.alignments.media.AlignmentsData`\n        The loaded alignments corresponding to the frames to be annotated\n    arguments: :class:`argparse.Namespace`\n        The command line arguments that have called this job\n    \"\"\"\n    def __init__(self, alignments: AlignmentData, arguments: Namespace) -> None:\n        logger.debug(\"Initializing %s: (arguments: %s)\", self.__class__.__name__, arguments)\n        self._alignments = alignments\n        self._frames = Frames(arguments.frames_dir)\n        self._output_folder = self._set_output()\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    def _set_output(self) -> str:\n        \"\"\" Set the output folder path.\n\n        If annotating a folder of frames, output will be placed in a sub folder within the frames\n        folder. If annotating a video, output will be a folder next to the original video.\n\n        Returns\n        -------\n        str\n            Full path to the output folder\n\n        \"\"\"\n        now = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        folder_name = f\"drawn_landmarks_{now}\"\n        if self._frames.is_video:\n            dest_folder = os.path.dirname(self._frames.folder)\n        else:\n            dest_folder = self._frames.folder\n        output_folder = os.path.join(dest_folder, folder_name)\n        logger.debug(\"Creating folder: '%s'\", output_folder)\n        os.makedirs(output_folder)\n        return output_folder\n\n    def process(self) -> None:\n        \"\"\" Runs the process to draw face annotations onto original source frames. \"\"\"\n        logger.info(\"[DRAW LANDMARKS]\")  # Tidy up cli output\n        frames_drawn = 0\n        for frame in tqdm(self._frames.file_list_sorted, desc=\"Drawing landmarks\", leave=False):\n            frame_name = frame[\"frame_fullname\"]\n\n            if not self._alignments.frame_exists(frame_name):\n                logger.verbose(\"Skipping '%s' - Alignments not found\", frame_name)  # type:ignore\n                continue\n\n            self._annotate_image(frame_name)\n            frames_drawn += 1\n        logger.info(\"%s Frame(s) output\", frames_drawn)\n\n    def _annotate_image(self, frame_name: str) -> None:\n        \"\"\" Annotate the frame with each face that appears in the alignments file.\n\n        Parameters\n        ----------\n        frame_name: str\n            The full path to the original frame\n        \"\"\"\n        logger.trace(\"Annotating frame: '%s'\", frame_name)  # type:ignore\n        image = self._frames.load_image(frame_name)\n\n        for idx, alignment in enumerate(self._alignments.get_faces_in_frame(frame_name)):\n            face = DetectedFace()\n            face.from_alignment(alignment, image=image)\n            # Bounding Box\n            assert face.left is not None\n            assert face.top is not None\n            cv2.rectangle(image, (face.left, face.top), (face.right, face.bottom), (255, 0, 0), 1)\n            self._annotate_landmarks(image, np.rint(face.landmarks_xy).astype(\"int32\"))\n            self._annotate_extract_boxes(image, face, idx)\n            self._annotate_pose(image, face)  # Pose (head is still loaded)\n\n        self._frames.save_image(self._output_folder, frame_name, image)\n\n    def _annotate_landmarks(self, image: np.ndarray, landmarks: np.ndarray) -> None:\n        \"\"\" Annotate the extract boxes onto the frame.\n\n        Parameters\n        ----------\n        image: :class:`numpy.ndarray`\n            The frame that extract boxes are to be annotated on to\n        landmarks: :class:`numpy.ndarray`\n            The facial landmarks that are to be annotated onto the frame\n        \"\"\"\n        # Mesh\n        for start, end, fill in LANDMARK_PARTS[LandmarkType.from_shape(landmarks.shape)].values():\n            cv2.polylines(image, [landmarks[start:end]], fill, (255, 255, 0), 1)\n        # Landmarks\n        for (pos_x, pos_y) in landmarks:\n            cv2.circle(image, (pos_x, pos_y), 1, (0, 255, 255), -1)\n\n    @classmethod\n    def _annotate_extract_boxes(cls, image: np.ndarray, face: DetectedFace, index: int) -> None:\n        \"\"\" Annotate the mesh and landmarks boxes onto the frame.\n\n        Parameters\n        ----------\n        image: :class:`numpy.ndarray`\n            The frame that mesh and landmarks are to be annotated on to\n        face: :class:`lib.align.DetectedFace`\n            The aligned face\n        index: int\n            The face index for the given face\n        \"\"\"\n        for area in T.get_args(T.Literal[\"face\", \"head\"]):\n            face.load_aligned(image, centering=area, force=True)\n            color = (0, 255, 0) if area == \"face\" else (0, 0, 255)\n            top_left = face.aligned.original_roi[0]\n            top_left = (top_left[0], top_left[1] - 10)\n            cv2.putText(image, str(index), top_left, cv2.FONT_HERSHEY_DUPLEX, 1.0, color, 1)\n            cv2.polylines(image, [face.aligned.original_roi], True, color, 1)\n\n    @classmethod\n    def _annotate_pose(cls, image: np.ndarray, face: DetectedFace) -> None:\n        \"\"\" Annotate the pose onto the frame.\n\n        Parameters\n        ----------\n        image: :class:`numpy.ndarray`\n            The frame that pose is to be annotated on to\n        face: :class:`lib.align.DetectedFace`\n            The aligned face loaded for head centering\n        \"\"\"\n        center = np.array((face.aligned.size / 2,\n                           face.aligned.size / 2)).astype(\"int32\").reshape(1, 2)\n        center = np.rint(face.aligned.transform_points(center, invert=True)).astype(\"int32\")\n        points = face.aligned.pose.xyz_2d * face.aligned.size\n        points = np.rint(face.aligned.transform_points(points, invert=True)).astype(\"int32\")\n        cv2.line(image, tuple(center), tuple(points[1]), (0, 255, 0), 2)\n        cv2.line(image, tuple(center), tuple(points[0]), (255, 0, 0), 2)\n        cv2.line(image, tuple(center), tuple(points[2]), (0, 0, 255), 2)\n\n\nclass Extract():\n    \"\"\" Re-extract faces from source frames based on Alignment data\n\n    Parameters\n    ----------\n    alignments: :class:`tools.lib_alignments.media.AlignmentData`\n        The alignments data loaded from an alignments file for this rename job\n    arguments: :class:`argparse.Namespace`\n        The :mod:`argparse` arguments as passed in from :mod:`tools.py`\n    \"\"\"\n    def __init__(self, alignments: AlignmentData, arguments: Namespace) -> None:\n        logger.debug(\"Initializing %s: (arguments: %s)\", self.__class__.__name__, arguments)\n        self._arguments = arguments\n        self._alignments = alignments\n        self._is_legacy = self._alignments.version == 1.0  # pylint:disable=protected-access\n        self._mask_pipeline: Extractor | None = None\n        self._faces_dir = arguments.faces_dir\n        self._min_size = self._get_min_size(arguments.size, arguments.min_size)\n\n        self._frames = Frames(arguments.frames_dir, self._get_count())\n        self._extracted_faces = ExtractedFaces(self._frames,\n                                               self._alignments,\n                                               size=arguments.size)\n        self._saver: ImagesSaver | None = None\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    @classmethod\n    def _get_min_size(cls, extract_size: int, min_size: int) -> int:\n        \"\"\" Obtain the minimum size that a face has been resized from to be included as a valid\n        extract.\n\n        Parameters\n        ----------\n        extract_size: int\n            The requested size of the extracted images\n        min_size: int\n            The percentage amount that has been supplied for valid faces (as a percentage of\n            extract size)\n\n        Returns\n        -------\n        int\n            The minimum size, in pixels, that a face is resized from to be considered valid\n        \"\"\"\n        retval = 0 if min_size == 0 else max(4, int(extract_size * (min_size / 100.)))\n        logger.debug(\"Extract size: %s, min percentage size: %s, min_size: %s\",\n                     extract_size, min_size, retval)\n        return retval\n\n    def _get_count(self) -> int | None:\n        \"\"\" If the alignments file has been run through the manual tool, then it will hold video\n        meta information, meaning that the count of frames in the alignment file can be relied\n        on to be accurate.\n\n        Returns\n        -------\n        int or ``None``\n        For video input which contain video meta-data in the alignments file then the count of\n        frames is returned. In all other cases ``None`` is returned\n        \"\"\"\n        meta = self._alignments.video_meta_data\n        has_meta = all(val is not None for val in meta.values())\n        if has_meta:\n            retval: int | None = len(T.cast(dict[str, list[int] | list[float]], meta[\"pts_time\"]))\n        else:\n            retval = None\n        logger.debug(\"Frame count from alignments file: (has_meta: %s, %s\", has_meta, retval)\n        return retval\n\n    def process(self) -> None:\n        \"\"\" Run the re-extraction from Alignments file process\"\"\"\n        logger.info(\"[EXTRACT FACES]\")  # Tidy up cli output\n        self._check_folder()\n        if self._is_legacy:\n            self._legacy_check()\n        self._saver = ImagesSaver(self._faces_dir, as_bytes=True)\n\n        if self._min_size > 0:\n            logger.info(\"Only selecting faces that have been resized from a minimum resolution \"\n                        \"of %spx\", self._min_size)\n\n        self._export_faces()\n\n    def _check_folder(self) -> None:\n        \"\"\" Check that the faces folder doesn't pre-exist and create. \"\"\"\n        err = None\n        if not self._faces_dir:\n            err = \"ERROR: Output faces folder not provided.\"\n        elif not os.path.isdir(self._faces_dir):\n            logger.debug(\"Creating folder: '%s'\", self._faces_dir)\n            os.makedirs(self._faces_dir)\n        elif os.listdir(self._faces_dir):\n            err = f\"ERROR: Output faces folder should be empty: '{self._faces_dir}'\"\n        if err:\n            logger.error(err)\n            sys.exit(0)\n        logger.verbose(\"Creating output folder at '%s'\", self._faces_dir)  # type:ignore\n\n    def _legacy_check(self) -> None:\n        \"\"\" Check whether the alignments file was created with the legacy extraction method.\n\n        If so, force user to re-extract all faces if any options have been specified, otherwise\n        raise the appropriate warnings and set the legacy options.\n        \"\"\"\n        if self._min_size > 0 or self._arguments.extract_every_n != 1:\n            logger.warning(\"This alignments file was generated with the legacy extraction method.\")\n            logger.warning(\"You should run this extraction job, but with 'min_size' set to 0 and \"\n                           \"'extract-every-n' set to 1 to update the alignments file.\")\n            logger.warning(\"You can then re-run this extraction job with your chosen options.\")\n            sys.exit(0)\n\n        maskers = [\"components\", \"extended\"]\n        nn_masks = [mask for mask in list(self._alignments.mask_summary) if mask not in maskers]\n        logtype = logger.warning if nn_masks else logger.info\n        logtype(\"This alignments file was created with the legacy extraction method and will be \"\n                \"updated.\")\n        logtype(\"Faces will be extracted using the new method and landmarks based masks will be \"\n                \"regenerated.\")\n        if nn_masks:\n            logtype(\"However, the NN based masks '%s' will be cropped to the legacy extraction \"\n                    \"method, so you may want to run the mask tool to regenerate these \"\n                    \"masks.\", \"', '\".join(nn_masks))\n        self._mask_pipeline = Extractor(None, None, maskers, multiprocess=True)\n        self._mask_pipeline.launch()\n        # Update alignments versioning\n        self._alignments._io._version = _VERSION  # pylint:disable=protected-access\n\n    def _export_faces(self) -> None:\n        \"\"\" Export the faces to the output folder. \"\"\"\n        extracted_faces = 0\n        skip_list = self._set_skip_list()\n        count = self._frames.count if skip_list is None else self._frames.count - len(skip_list)\n\n        for filename, image in tqdm(self._frames.stream(skip_list=skip_list),\n                                    total=count, desc=\"Saving extracted faces\",\n                                    leave=False):\n            frame_name = os.path.basename(filename)\n            if not self._alignments.frame_exists(frame_name):\n                logger.verbose(\"Skipping '%s' - Alignments not found\", frame_name)  # type:ignore\n                continue\n            extracted_faces += self._output_faces(frame_name, image)\n        if self._is_legacy and extracted_faces != 0 and self._min_size == 0:\n            self._alignments.save()\n        logger.info(\"%s face(s) extracted\", extracted_faces)\n\n    def _set_skip_list(self) -> list[int] | None:\n        \"\"\" Set the indices for frames that should be skipped based on the `extract_every_n`\n        command line option.\n\n        Returns\n        -------\n        list or ``None``\n            A list of indices to be skipped if extract_every_n is not `1` otherwise\n            returns ``None``\n        \"\"\"\n        skip_num = self._arguments.extract_every_n\n        if skip_num == 1:\n            logger.debug(\"Not skipping any frames\")\n            return None\n        skip_list = []\n        for idx, item in enumerate(T.cast(list[dict[str, str]], self._frames.file_list_sorted)):\n            if idx % skip_num != 0:\n                logger.trace(\"Adding image '%s' to skip list due to \"  # type:ignore\n                             \"extract_every_n = %s\", item[\"frame_fullname\"], skip_num)\n                skip_list.append(idx)\n        logger.debug(\"Adding skip list: %s\", skip_list)\n        return skip_list\n\n    def _output_faces(self, filename: str, image: np.ndarray) -> int:\n        \"\"\" For each frame save out the faces\n\n        Parameters\n        ----------\n        filename: str\n            The filename (without the full path) of the current frame\n        image: :class:`numpy.ndarray`\n            The full frame that faces are to be extracted from\n\n        Returns\n        -------\n        int\n            The total number of faces that have been extracted\n        \"\"\"\n        logger.trace(\"Outputting frame: %s\", filename)  # type:ignore\n        face_count = 0\n        frame_name = os.path.splitext(filename)[0]\n        faces = self._select_valid_faces(filename, image)\n        assert self._saver is not None\n        if not faces:\n            return face_count\n        if self._is_legacy:\n            faces = self._process_legacy(filename, image, faces)\n\n        for idx, face in enumerate(faces):\n            output = f\"{frame_name}_{idx}.png\"\n            meta: PNGHeaderDict = {\n                \"alignments\": face.to_png_meta(),\n                \"source\": {\"alignments_version\": self._alignments.version,\n                           \"original_filename\": output,\n                           \"face_index\": idx,\n                           \"source_filename\": filename,\n                           \"source_is_video\": self._frames.is_video,\n                           \"source_frame_dims\": T.cast(tuple[int, int], image.shape[:2])}}\n            assert face.aligned.face is not None\n            self._saver.save(output, encode_image(face.aligned.face, \".png\", metadata=meta))\n            if self._min_size == 0 and self._is_legacy:\n                face.thumbnail = generate_thumbnail(face.aligned.face, size=96, quality=60)\n                self._alignments.data[filename][\"faces\"][idx] = face.to_alignment()\n            face_count += 1\n        self._saver.close()\n        return face_count\n\n    def _select_valid_faces(self, frame: str, image: np.ndarray) -> list[DetectedFace]:\n        \"\"\" Return the aligned faces from a frame that meet the selection criteria,\n\n        Parameters\n        ----------\n        frame: str\n            The filename (without the full path) of the current frame\n        image: :class:`numpy.ndarray`\n            The full frame that faces are to be extracted from\n\n        Returns\n        -------\n        list:\n            List of valid :class:`lib,align.DetectedFace` objects\n        \"\"\"\n        faces = self._extracted_faces.get_faces_in_frame(frame, image=image)\n        if self._min_size == 0:\n            valid_faces = faces\n        else:\n            sizes = self._extracted_faces.get_roi_size_for_frame(frame)\n            valid_faces = [faces[idx] for idx, size in enumerate(sizes)\n                           if size >= self._min_size]\n        logger.trace(\"frame: '%s', total_faces: %s, valid_faces: %s\",  # type:ignore\n                     frame, len(faces), len(valid_faces))\n        return valid_faces\n\n    def _process_legacy(self,\n                        filename: str,\n                        image: np.ndarray,\n                        detected_faces: list[DetectedFace]) -> list[DetectedFace]:\n        \"\"\" Process legacy face extractions to new extraction method.\n\n        Updates stored masks to new extract size\n\n        Parameters\n        ----------\n        filename: str\n            The current frame filename\n        image: :class:`numpy.ndarray`\n            The current image the contains the faces\n        detected_faces: list\n            list of :class:`lib.align.DetectedFace` objects for the current frame\n\n        Returns\n        -------\n        list\n            The updated list of :class:`lib.align.DetectedFace` objects for the current frame\n        \"\"\"\n        # Update landmarks based masks for face centering\n        assert self._mask_pipeline is not None\n        mask_item = ExtractMedia(filename, image, detected_faces=detected_faces)\n        self._mask_pipeline.input_queue.put(mask_item)\n        faces = next(self._mask_pipeline.detected_faces()).detected_faces\n\n        # Pad and shift Neural Network based masks to face centering\n        for face in faces:\n            self._pad_legacy_masks(face)\n        return faces\n\n    @classmethod\n    def _pad_legacy_masks(cls, detected_face: DetectedFace) -> None:\n        \"\"\" Recenter legacy Neural Network based masks from legacy centering to face centering\n        and pad accordingly.\n\n        Update the masks back into the detected face objects.\n\n        Parameters\n        ----------\n        detected_face: :class:`lib.align.DetectedFace`\n            The detected face to update the masks for\n        \"\"\"\n        offset = detected_face.aligned.pose.offset[\"face\"]\n        for name, mask in detected_face.mask.items():  # Re-center mask and pad to face size\n            if name in (\"components\", \"extended\"):\n                continue\n            old_mask = mask.mask.astype(\"float32\") / 255.0\n            size = old_mask.shape[0]\n            new_size = int(size + (size * EXTRACT_RATIOS[\"face\"]) / 2)\n\n            shift = np.rint(offset * (size - (size * EXTRACT_RATIOS[\"face\"]))).astype(\"int32\")\n            pos = np.array([(new_size // 2 - size // 2) - shift[1],\n                            (new_size // 2) + (size // 2) - shift[1],\n                            (new_size // 2 - size // 2) - shift[0],\n                            (new_size // 2) + (size // 2) - shift[0]])\n            bounds = np.array([max(0, pos[0]), min(new_size, pos[1]),\n                               max(0, pos[2]), min(new_size, pos[3])])\n\n            slice_in = [slice(0 - (pos[0] - bounds[0]), size - (pos[1] - bounds[1])),\n                        slice(0 - (pos[2] - bounds[2]), size - (pos[3] - bounds[3]))]\n            slice_out = [slice(bounds[0], bounds[1]), slice(bounds[2], bounds[3])]\n\n            new_mask = np.zeros((new_size, new_size, 1), dtype=\"float32\")\n            new_mask[slice_out[0], slice_out[1], :] = old_mask[slice_in[0], slice_in[1], :]\n\n            mask.replace_mask(new_mask)\n            # Get the affine matrix from recently generated components mask\n            # pylint:disable=protected-access\n            mask._affine_matrix = detected_face.mask[\"components\"].affine_matrix\n", "tools/alignments/jobs.py": "#!/usr/bin/env python3\n\"\"\" Tools for manipulating the alignments serialized file \"\"\"\nfrom __future__ import annotations\nimport logging\nimport os\nimport sys\nimport typing as T\n\nfrom datetime import datetime\n\nimport numpy as np\nfrom scipy import signal\nfrom sklearn import decomposition\nfrom tqdm import tqdm\n\nfrom lib.logger import parse_class_init\nfrom lib.serializer import get_serializer\nfrom lib.utils import FaceswapError\n\nfrom .media import Faces, Frames\nfrom .jobs_faces import FaceToFile\n\nif T.TYPE_CHECKING:\n    from collections.abc import Generator\n    from argparse import Namespace\n    from lib.align.alignments import AlignmentFileDict, PNGHeaderDict\n    from .media import AlignmentData\n\nlogger = logging.getLogger(__name__)\n\n\nclass Check:\n    \"\"\" Frames and faces checking tasks.\n\n    Parameters\n    ---------\n    alignments: :class:`tools.alignments.media.AlignmentsData`\n        The loaded alignments corresponding to the frames to be annotated\n    arguments: :class:`argparse.Namespace`\n        The command line arguments that have called this job\n    \"\"\"\n    def __init__(self, alignments: AlignmentData, arguments: Namespace) -> None:\n        logger.debug(parse_class_init(locals()))\n        self._alignments = alignments\n        self._job = arguments.job\n        self._type: T.Literal[\"faces\", \"frames\"] | None = None\n        self._is_video = False  # Set when getting items\n        self._output = arguments.output\n        self._source_dir = self._get_source_dir(arguments)\n        self._validate()\n        self._items = self._get_items()\n\n        self.output_message = \"\"\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    def _get_source_dir(self, arguments: Namespace) -> str:\n        \"\"\" Set the correct source folder\n\n        Parameters\n        ----------\n        arguments: :class:`argparse.Namespace`\n            The command line arguments for the Alignments tool\n\n        Returns\n        -------\n        str\n            Full path to the source folder\n        \"\"\"\n        if (hasattr(arguments, \"faces_dir\") and arguments.faces_dir and\n                hasattr(arguments, \"frames_dir\") and arguments.frames_dir):\n            logger.error(\"Only select a source frames (-fr) or source faces (-fc) folder\")\n            sys.exit(1)\n        elif hasattr(arguments, \"faces_dir\") and arguments.faces_dir:\n            self._type = \"faces\"\n            source_dir = arguments.faces_dir\n        elif hasattr(arguments, \"frames_dir\") and arguments.frames_dir:\n            self._type = \"frames\"\n            source_dir = arguments.frames_dir\n        else:\n            logger.error(\"No source folder (-fr or -fc) was provided\")\n            sys.exit(1)\n        logger.debug(\"type: '%s', source_dir: '%s'\", self._type, source_dir)\n        return source_dir\n\n    def _get_items(self) -> list[dict[str, str]] | list[tuple[str, PNGHeaderDict]]:\n        \"\"\" Set the correct items to process\n\n        Returns\n        -------\n        list\n            Sorted list of dictionaries for either faces or frames. If faces the dictionaries\n            have the current filename as key, with the header source data as value. If frames\n            the dictionaries will contain the keys 'frame_fullname', 'frame_name', 'extension'.\n        \"\"\"\n        assert self._type is not None\n        items: Frames | Faces = globals()[self._type.title()](self._source_dir)\n        self._is_video = items.is_video\n        return T.cast(list[dict[str, str]] | list[tuple[str, \"PNGHeaderDict\"]],\n                      items.file_list_sorted)\n\n    def process(self) -> None:\n        \"\"\" Process the frames check against the alignments file \"\"\"\n        assert self._type is not None\n        logger.info(\"[CHECK %s]\", self._type.upper())\n        items_output = self._compile_output()\n\n        if self._type == \"faces\":\n            filelist = T.cast(list[tuple[str, \"PNGHeaderDict\"]], self._items)\n            check_update = FaceToFile(self._alignments, [val[1] for val in filelist])\n            if check_update():\n                self._alignments.save()\n\n        self._output_results(items_output)\n\n    def _validate(self) -> None:\n        \"\"\" Check that the selected type is valid for selected task and job \"\"\"\n        if self._job == \"missing-frames\" and self._output == \"move\":\n            logger.warning(\"Missing_frames was selected with move output, but there will \"\n                           \"be nothing to move. Defaulting to output: console\")\n            self._output = \"console\"\n        if self._type == \"faces\" and self._job != \"multi-faces\":\n            logger.error(\"The selected folder is not valid. Faces folder (-fc) is only \"\n                         \"supported for 'multi-faces'\")\n            sys.exit(1)\n\n    def _compile_output(self) -> list[str] | list[tuple[str, int]]:\n        \"\"\" Compile list of frames that meet criteria\n\n        Returns\n        -------\n        list\n            List of filenames or filenames and face indices for the selected criteria\n        \"\"\"\n        action = self._job.replace(\"-\", \"_\")\n        processor = getattr(self, f\"_get_{action}\")\n        logger.debug(\"Processor: %s\", processor)\n        return [item for item in processor()]  # pylint:disable=unnecessary-comprehension\n\n    def _get_no_faces(self) -> Generator[str, None, None]:\n        \"\"\" yield each frame that has no face match in alignments file\n\n        Yields\n        ------\n        str\n            The frame name of any frames which have no faces\n        \"\"\"\n        self.output_message = \"Frames with no faces\"\n        for frame in tqdm(T.cast(list[dict[str, str]], self._items),\n                          desc=self.output_message,\n                          leave=False):\n            logger.trace(frame)  # type:ignore\n            frame_name = frame[\"frame_fullname\"]\n            if not self._alignments.frame_has_faces(frame_name):\n                logger.debug(\"Returning: '%s'\", frame_name)\n                yield frame_name\n\n    def _get_multi_faces(self) -> (Generator[str, None, None] |\n                                   Generator[tuple[str, int], None, None]):\n        \"\"\" yield each frame or face that has multiple faces matched in alignments file\n\n        Yields\n        ------\n        str or tuple\n            The frame name of any frames which have multiple faces and potentially the face id\n        \"\"\"\n        process_type = getattr(self, f\"_get_multi_faces_{self._type}\")\n        for item in process_type():\n            yield item\n\n    def _get_multi_faces_frames(self) -> Generator[str, None, None]:\n        \"\"\" Return Frames that contain multiple faces\n\n        Yields\n        ------\n        str\n            The frame name of any frames which have multiple faces\n        \"\"\"\n        self.output_message = \"Frames with multiple faces\"\n        for item in tqdm(T.cast(list[dict[str, str]], self._items),\n                         desc=self.output_message,\n                         leave=False):\n            filename = item[\"frame_fullname\"]\n            if not self._alignments.frame_has_multiple_faces(filename):\n                continue\n            logger.trace(\"Returning: '%s'\", filename)  # type:ignore\n            yield filename\n\n    def _get_multi_faces_faces(self) -> Generator[tuple[str, int], None, None]:\n        \"\"\" Return Faces when there are multiple faces in a frame\n\n        Yields\n        ------\n        tuple\n            The frame name and the face id of any frames which have multiple faces\n        \"\"\"\n        self.output_message = \"Multiple faces in frame\"\n        for item in tqdm(T.cast(list[tuple[str, \"PNGHeaderDict\"]], self._items),\n                         desc=self.output_message,\n                         leave=False):\n            src = item[1][\"source\"]\n            if not self._alignments.frame_has_multiple_faces(src[\"source_filename\"]):\n                continue\n            retval = (item[0], src[\"face_index\"])\n            logger.trace(\"Returning: '%s'\", retval)  # type:ignore\n            yield retval\n\n    def _get_missing_alignments(self) -> Generator[str, None, None]:\n        \"\"\" yield each frame that does not exist in alignments file\n\n        Yields\n        ------\n        str\n            The frame name of any frames missing alignments\n        \"\"\"\n        self.output_message = \"Frames missing from alignments file\"\n        exclude_filetypes = set([\"yaml\", \"yml\", \"p\", \"json\", \"txt\"])\n        for frame in tqdm(T.cast(dict[str, str], self._items),\n                          desc=self.output_message,\n                          leave=False):\n            frame_name = frame[\"frame_fullname\"]\n            if (frame[\"frame_extension\"] not in exclude_filetypes\n                    and not self._alignments.frame_exists(frame_name)):\n                logger.debug(\"Returning: '%s'\", frame_name)\n                yield frame_name\n\n    def _get_missing_frames(self) -> Generator[str, None, None]:\n        \"\"\" yield each frame in alignments that does not have a matching file\n\n        Yields\n        ------\n        str\n            The frame name of any frames in alignments with no matching file\n        \"\"\"\n        self.output_message = \"Missing frames that are in alignments file\"\n        frames = set(item[\"frame_fullname\"] for item in T.cast(list[dict[str, str]], self._items))\n        for frame in tqdm(self._alignments.data.keys(), desc=self.output_message, leave=False):\n            if frame not in frames:\n                logger.debug(\"Returning: '%s'\", frame)\n                yield frame\n\n    def _output_results(self, items_output: list[str] | list[tuple[str, int]]) -> None:\n        \"\"\" Output the results in the requested format\n\n        Parameters\n        ----------\n        items_output\n            The list of frame names, and potentially face ids, of any items which met the\n            selection criteria\n        \"\"\"\n        logger.trace(\"items_output: %s\", items_output)  # type:ignore\n        if self._output == \"move\" and self._is_video and self._type == \"frames\":\n            logger.warning(\"Move was selected with an input video. This is not possible so \"\n                           \"falling back to console output\")\n            self._output = \"console\"\n        if not items_output:\n            logger.info(\"No %s were found meeting the criteria\", self._type)\n            return\n        if self._output == \"move\":\n            self._move_file(items_output)\n            return\n        if self._job == \"multi-faces\" and self._type == \"faces\":\n            # Strip the index for printed/file output\n            final_output = [item[0] for item in items_output]\n        else:\n            final_output = T.cast(list[str], items_output)\n        output_message = \"-----------------------------------------------\\r\\n\"\n        output_message += f\" {self.output_message} ({len(final_output)})\\r\\n\"\n        output_message += \"-----------------------------------------------\\r\\n\"\n        output_message += \"\\r\\n\".join(final_output)\n        if self._output == \"console\":\n            for line in output_message.splitlines():\n                logger.info(line)\n        if self._output == \"file\":\n            self.output_file(output_message, len(final_output))\n\n    def _get_output_folder(self) -> str:\n        \"\"\" Return output folder. Needs to be in the root if input is a video and processing\n        frames\n\n        Returns\n        -------\n        str\n            Full path to the output folder\n        \"\"\"\n        if self._is_video and self._type == \"frames\":\n            return os.path.dirname(self._source_dir)\n        return self._source_dir\n\n    def _get_filename_prefix(self) -> str:\n        \"\"\" Video name needs to be prefixed to filename if input is a video and processing frames\n\n        Returns\n        -------\n        str\n            The common filename prefix to use\n        \"\"\"\n        if self._is_video and self._type == \"frames\":\n            return f\"{os.path.basename(self._source_dir)}_\"\n        return \"\"\n\n    def output_file(self, output_message: str, items_discovered: int) -> None:\n        \"\"\" Save the output to a text file in the frames directory\n\n        Parameters\n        ----------\n        output_message: str\n            The message to write out to file\n        items_discovered: int\n            The number of items which matched the criteria\n        \"\"\"\n        now = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        dst_dir = self._get_output_folder()\n        filename = (f\"{self._get_filename_prefix()}{self.output_message.replace(' ', '_').lower()}\"\n                    f\"_{now}.txt\")\n        output_file = os.path.join(dst_dir, filename)\n        logger.info(\"Saving %s result(s) to '%s'\", items_discovered, output_file)\n        with open(output_file, \"w\", encoding=\"utf8\") as f_output:\n            f_output.write(output_message)\n\n    def _move_file(self, items_output: list[str] | list[tuple[str, int]]) -> None:\n        \"\"\" Move the identified frames to a new sub folder\n\n        Parameters\n        ----------\n        items_output: list\n            List of items to move\n        \"\"\"\n        now = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        folder_name = (f\"{self._get_filename_prefix()}\"\n                       f\"{self.output_message.replace(' ','_').lower()}_{now}\")\n        dst_dir = self._get_output_folder()\n        output_folder = os.path.join(dst_dir, folder_name)\n        logger.debug(\"Creating folder: '%s'\", output_folder)\n        os.makedirs(output_folder)\n        move = getattr(self, f\"_move_{self._type}\")\n        logger.debug(\"Move function: %s\", move)\n        move(output_folder, items_output)\n\n    def _move_frames(self, output_folder: str, items_output: list[str]) -> None:\n        \"\"\" Move frames into single sub folder\n\n        Parameters\n        ----------\n        output_folder: str\n            The folder to move the output to\n        items_output: list\n            List of items to move\n        \"\"\"\n        logger.info(\"Moving %s frame(s) to '%s'\", len(items_output), output_folder)\n        for frame in items_output:\n            src = os.path.join(self._source_dir, frame)\n            dst = os.path.join(output_folder, frame)\n            logger.debug(\"Moving: '%s' to '%s'\", src, dst)\n            os.rename(src, dst)\n\n    def _move_faces(self, output_folder: str, items_output: list[tuple[str, int]]) -> None:\n        \"\"\" Make additional sub folders for each face that appears Enables easier manual sorting\n\n        Parameters\n        ----------\n        output_folder: str\n            The folder to move the output to\n        items_output: list\n            List of items and face indices to move\n        \"\"\"\n        logger.info(\"Moving %s faces(s) to '%s'\", len(items_output), output_folder)\n        for frame, idx in items_output:\n            src = os.path.join(self._source_dir, frame)\n            dst_folder = os.path.join(output_folder, str(idx)) if idx != -1 else output_folder\n            if not os.path.isdir(dst_folder):\n                logger.debug(\"Creating folder: '%s'\", dst_folder)\n                os.makedirs(dst_folder)\n            dst = os.path.join(dst_folder, frame)\n            logger.debug(\"Moving: '%s' to '%s'\", src, dst)\n            os.rename(src, dst)\n\n\nclass Export:\n    \"\"\" Export alignments from a Faceswap .fsa file to a json formatted file.\n\n        Parameters\n    ----------\n    alignments: :class:`tools.lib_alignments.media.AlignmentData`\n        The alignments data loaded from an alignments file for this rename job\n    arguments: :class:`argparse.Namespace`\n        The :mod:`argparse` arguments as passed in from :mod:`tools.py`. Unused\n    \"\"\"\n    def __init__(self,\n                 alignments: AlignmentData,\n                 arguments: Namespace) -> None:  # pylint:disable=unused-argument\n        logger.debug(parse_class_init(locals()))\n        self._alignments = alignments\n        self._serializer = get_serializer(\"json\")\n        self._output_file = self._get_output_file()\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    def _get_output_file(self) -> str:\n        \"\"\" Obtain the name of an output file. If a file of the request name exists, then append a\n        digit to the end until a unique filename is found\n\n        Returns\n        -------\n        str\n            Full path to an output json file\n        \"\"\"\n        in_file = self._alignments.file\n        base_filename = f\"{os.path.splitext(in_file)[0]}_export\"\n        out_file = f\"{base_filename}.json\"\n        idx = 1\n        while True:\n            if not os.path.exists(out_file):\n                break\n            logger.debug(\"Output file exists: '%s'\", out_file)\n            out_file = f\"{base_filename}_{idx}.json\"\n            idx += 1\n        logger.debug(\"Setting output file to '%s'\", out_file)\n        return out_file\n\n    @classmethod\n    def _format_face(cls, face: AlignmentFileDict) -> dict[str, list[int] | list[list[float]]]:\n        \"\"\" Format the relevant keys from an alignment file's face into the correct format for\n        export/import\n\n        Parameters\n        ----------\n        face: :class:`~lib.align.alignments.AlignmentFileDict`\n            The alignment dictionary for a face to process\n\n        Returns\n        -------\n        dict[str, list[int] | list[list[float]]]\n            The face formatted for exporting to a json file\n        \"\"\"\n        lms = face[\"landmarks_xy\"]\n        assert isinstance(lms, np.ndarray)\n        retval = {\"detected\": [int(round(face[\"x\"], 0)),\n                               int(round(face[\"y\"], 0)),\n                               int(round(face[\"x\"] + face[\"w\"], 0)),\n                               int(round(face[\"y\"] + face[\"h\"], 0))],\n                  \"landmarks_2d\": lms.tolist()}\n        return retval\n\n    def process(self) -> None:\n        \"\"\" Parse the imported alignments file and output relevant information to a json file \"\"\"\n        logger.info(\"[EXPORTING ALIGNMENTS]\")  # Tidy up cli output\n        formatted = {key: [self._format_face(face) for face in val[\"faces\"]]\n                     for key, val in self._alignments.data.items()}\n        logger.info(\"Saving export alignments to '%s'...\", self._output_file)\n        self._serializer.save(self._output_file, formatted)\n\n\nclass Sort:\n    \"\"\" Sort alignments' index by the order they appear in an image in left to right order.\n\n    Parameters\n    ----------\n    alignments: :class:`tools.lib_alignments.media.AlignmentData`\n        The alignments data loaded from an alignments file for this rename job\n    arguments: :class:`argparse.Namespace`\n        The :mod:`argparse` arguments as passed in from :mod:`tools.py`. Unused\n    \"\"\"\n    def __init__(self,\n                 alignments: AlignmentData,\n                 arguments: Namespace) -> None:  # pylint:disable=unused-argument\n        logger.debug(parse_class_init(locals()))\n        self._alignments = alignments\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    def process(self) -> None:\n        \"\"\" Execute the sort process \"\"\"\n        logger.info(\"[SORT INDEXES]\")  # Tidy up cli output\n        reindexed = self.reindex_faces()\n        if reindexed:\n            self._alignments.save()\n            logger.warning(\"If you have a face-set corresponding to the alignment file you \"\n                           \"processed then you should run the 'Extract' job to regenerate it.\")\n\n    def reindex_faces(self) -> int:\n        \"\"\" Re-Index the faces \"\"\"\n        reindexed = 0\n        for alignment in tqdm(self._alignments.yield_faces(),\n                              desc=\"Sort alignment indexes\",\n                              total=self._alignments.frames_count,\n                              leave=False):\n            frame, alignments, count, key = alignment\n            if count <= 1:\n                logger.trace(\"0 or 1 face in frame. Not sorting: '%s'\", frame)  # type:ignore\n                continue\n            sorted_alignments = sorted(alignments, key=lambda x: (x[\"x\"]))\n            if sorted_alignments == alignments:\n                logger.trace(\"Alignments already in correct order. Not \"  # type:ignore\n                             \"sorting: '%s'\", frame)\n                continue\n            logger.trace(\"Sorting alignments for frame: '%s'\", frame)  # type:ignore\n            self._alignments.data[key][\"faces\"] = sorted_alignments\n            reindexed += 1\n        logger.info(\"%s Frames had their faces reindexed\", reindexed)\n        return reindexed\n\n\nclass Spatial:\n    \"\"\" Apply spatial temporal filtering to landmarks\n\n    Parameters\n    ----------\n    alignments: :class:`tools.lib_alignments.media.AlignmentData`\n        The alignments data loaded from an alignments file for this rename job\n    arguments: :class:`argparse.Namespace`\n        The :mod:`argparse` arguments as passed in from :mod:`tools.py`\n\n    Reference\n    ---------\n    https://www.kaggle.com/selfishgene/animating-and-smoothing-3d-facial-keypoints/notebook\n    \"\"\"\n    def __init__(self, alignments: AlignmentData, arguments: Namespace) -> None:\n        logger.debug(parse_class_init(locals()))\n        self.arguments = arguments\n        self._alignments = alignments\n        self._mappings: dict[int, str] = {}\n        self._normalized: dict[str, np.ndarray] = {}\n        self._shapes_model: decomposition.PCA | None = None\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    def process(self) -> None:\n        \"\"\" Perform spatial filtering \"\"\"\n        logger.info(\"[SPATIO-TEMPORAL FILTERING]\")  # Tidy up cli output\n        logger.info(\"NB: The process only processes the alignments for the first \"\n                    \"face it finds for any given frame. For best results only run this when \"\n                    \"there is only a single face in the alignments file and all false positives \"\n                    \"have been removed\")\n\n        self._normalize()\n        self._shape_model()\n        landmarks = self._spatially_filter()\n        landmarks = self._temporally_smooth(landmarks)\n        self._update_alignments(landmarks)\n        self._alignments.save()\n        logger.warning(\"If you have a face-set corresponding to the alignment file you \"\n                       \"processed then you should run the 'Extract' job to regenerate it.\")\n\n    # Define shape normalization utility functions\n    @staticmethod\n    def _normalize_shapes(shapes_im_coords: np.ndarray\n                          ) -> tuple[np.ndarray, np.ndarray, np.ndarray]:\n        \"\"\" Normalize a 2D or 3D shape\n\n        Parameters\n        ----------\n        shaped_im_coords: :class:`numpy.ndarray`\n            The facial landmarks\n\n        Returns\n        -------\n        shapes_normalized: :class:`numpy.ndarray`\n            The normalized shapes\n        scale_factors: :class:`numpy.ndarray`\n            The scale factors\n        mean_coords: :class:`numpy.ndarray`\n            The mean coordinates\n        \"\"\"\n        logger.debug(\"Normalize shapes\")\n        (num_pts, num_dims, _) = shapes_im_coords.shape\n\n        # Calculate mean coordinates and subtract from shapes\n        mean_coords = shapes_im_coords.mean(axis=0)\n        shapes_centered = np.zeros(shapes_im_coords.shape)\n        shapes_centered = shapes_im_coords - np.tile(mean_coords, [num_pts, 1, 1])\n\n        # Calculate scale factors and divide shapes\n        scale_factors = np.sqrt((shapes_centered**2).sum(axis=1)).mean(axis=0)\n        shapes_normalized = np.zeros(shapes_centered.shape)\n        shapes_normalized = shapes_centered / np.tile(scale_factors, [num_pts, num_dims, 1])\n\n        logger.debug(\"Normalized shapes: (shapes_normalized: %s, scale_factors: %s, mean_coords: \"\n                     \"%s\", shapes_normalized, scale_factors, mean_coords)\n        return shapes_normalized, scale_factors, mean_coords\n\n    @staticmethod\n    def _normalized_to_original(shapes_normalized: np.ndarray,\n                                scale_factors: np.ndarray,\n                                mean_coords: np.ndarray) -> np.ndarray:\n        \"\"\" Transform a normalized shape back to original image coordinates\n\n        Parameters\n        ----------\n        shapes_normalized: :class:`numpy.ndarray`\n            The normalized shapes\n        scale_factors: :class:`numpy.ndarray`\n            The scale factors\n        mean_coords: :class:`numpy.ndarray`\n            The mean coordinates\n\n        Returns\n        -------\n        :class:`numpy.ndarray`\n            The normalized shape transformed back to original coordinates\n        \"\"\"\n        logger.debug(\"Normalize to original\")\n        (num_pts, num_dims, _) = shapes_normalized.shape\n\n        # move back to the correct scale\n        shapes_centered = shapes_normalized * np.tile(scale_factors, [num_pts, num_dims, 1])\n        # move back to the correct location\n        shapes_im_coords = shapes_centered + np.tile(mean_coords, [num_pts, 1, 1])\n\n        logger.debug(\"Normalized to original: %s\", shapes_im_coords)\n        return shapes_im_coords\n\n    def _normalize(self) -> None:\n        \"\"\" Compile all original and normalized alignments \"\"\"\n        logger.debug(\"Normalize\")\n        count = sum(1 for val in self._alignments.data.values() if val[\"faces\"])\n\n        sample_lm = next((val[\"faces\"][0][\"landmarks_xy\"]\n                          for val in self._alignments.data.values() if val[\"faces\"]), 68)\n        assert isinstance(sample_lm, np.ndarray)\n        lm_count = sample_lm.shape[0]\n        if lm_count != 68:\n            raise FaceswapError(\"Spatial smoothing only supports 68 point facial landmarks\")\n\n        landmarks_all = np.zeros((lm_count, 2, int(count)))\n\n        end = 0\n        for key in tqdm(sorted(self._alignments.data.keys()), desc=\"Compiling\", leave=False):\n            val = self._alignments.data[key][\"faces\"]\n            if not val:\n                continue\n            # We should only be normalizing a single face, so just take\n            # the first landmarks found\n            landmarks = np.array(val[0][\"landmarks_xy\"]).reshape((lm_count, 2, 1))\n            start = end\n            end = start + landmarks.shape[2]\n            # Store in one big array\n            landmarks_all[:, :, start:end] = landmarks\n            # Make sure we keep track of the mapping to the original frame\n            self._mappings[start] = key\n\n        # Normalize shapes\n        normalized_shape = self._normalize_shapes(landmarks_all)\n        self._normalized[\"landmarks\"] = normalized_shape[0]\n        self._normalized[\"scale_factors\"] = normalized_shape[1]\n        self._normalized[\"mean_coords\"] = normalized_shape[2]\n        logger.debug(\"Normalized: %s\", self._normalized)\n\n    def _shape_model(self) -> None:\n        \"\"\" build 2D shape model \"\"\"\n        logger.debug(\"Shape model\")\n        landmarks_norm = self._normalized[\"landmarks\"]\n        num_components = 20\n        normalized_shapes_tbl = np.reshape(landmarks_norm, [68*2, landmarks_norm.shape[2]]).T\n        self._shapes_model = decomposition.PCA(n_components=num_components,\n                                               whiten=True,\n                                               random_state=1).fit(normalized_shapes_tbl)\n        explained = self._shapes_model.explained_variance_ratio_.sum()\n        logger.info(\"Total explained percent by PCA model with %s components is %s%%\",\n                    num_components, round(100 * explained, 1))\n        logger.debug(\"Shaped model\")\n\n    def _spatially_filter(self) -> np.ndarray:\n        \"\"\" interpret the shapes using our shape model (project and reconstruct)\n\n        Returns\n        -------\n        :class:`numpy.ndarray`\n            The filtered landmarks in original coordinate space\n        \"\"\"\n        logger.debug(\"Spatially Filter\")\n        assert self._shapes_model is not None\n        landmarks_norm = self._normalized[\"landmarks\"]\n        # Convert to matrix form\n        landmarks_norm_table = np.reshape(landmarks_norm, [68 * 2, landmarks_norm.shape[2]]).T\n        # Project onto shapes model and reconstruct\n        landmarks_norm_table_rec = self._shapes_model.inverse_transform(\n            self._shapes_model.transform(landmarks_norm_table))\n        # Convert back to shapes (numKeypoint, num_dims, numFrames)\n        landmarks_norm_rec = np.reshape(landmarks_norm_table_rec.T,\n                                        [68, 2, landmarks_norm.shape[2]])\n        # Transform back to image co-ordinates\n        retval = self._normalized_to_original(landmarks_norm_rec,\n                                              self._normalized[\"scale_factors\"],\n                                              self._normalized[\"mean_coords\"])\n\n        logger.debug(\"Spatially Filtered: %s\", retval)\n        return retval\n\n    @staticmethod\n    def _temporally_smooth(landmarks: np.ndarray) -> np.ndarray:\n        \"\"\" apply temporal filtering on the 2D points\n\n        Parameters\n        ----------\n        landmarks: :class:`numpy.ndarray`\n            68 point landmarks to be temporally smoothed\n\n        Returns\n        -------\n        :class: `numpy.ndarray`\n            The temporally smoothed landmarks\n        \"\"\"\n        logger.debug(\"Temporally Smooth\")\n        filter_half_length = 2\n        temporal_filter = np.ones((1, 1, 2 * filter_half_length + 1))\n        temporal_filter = temporal_filter / temporal_filter.sum()\n\n        start_tileblock = np.tile(landmarks[:, :, 0][:, :, np.newaxis], [1, 1, filter_half_length])\n        end_tileblock = np.tile(landmarks[:, :, -1][:, :, np.newaxis], [1, 1, filter_half_length])\n        landmarks_padded = np.dstack((start_tileblock, landmarks, end_tileblock))\n\n        retval = signal.convolve(landmarks_padded, temporal_filter, mode='valid', method='fft')\n        logger.debug(\"Temporally Smoothed: %s\", retval)\n        return retval\n\n    def _update_alignments(self, landmarks: np.ndarray) -> None:\n        \"\"\" Update smoothed landmarks back to alignments\n\n        Parameters\n        ----------\n        landmarks: :class:`numpy.ndarray`\n            The smoothed landmarks\n        \"\"\"\n        logger.debug(\"Update alignments\")\n        for idx, frame in tqdm(self._mappings.items(), desc=\"Updating\", leave=False):\n            logger.trace(\"Updating: (frame: %s)\", frame)  # type:ignore\n            landmarks_update = landmarks[:, :, idx]\n            landmarks_xy = landmarks_update.reshape(68, 2).tolist()\n            self._alignments.data[frame][\"faces\"][0][\"landmarks_xy\"] = landmarks_xy\n            logger.trace(\"Updated: (frame: '%s', landmarks: %s)\",  # type:ignore\n                         frame, landmarks_xy)\n        logger.debug(\"Updated alignments\")\n", "tools/alignments/jobs_faces.py": "#!/usr/bin/env python3\n\"\"\" Tools for manipulating the alignments using extracted Faces as a source \"\"\"\nfrom __future__ import annotations\nimport logging\nimport os\nimport typing as T\n\nfrom argparse import Namespace\nfrom operator import itemgetter\n\nimport numpy as np\nfrom tqdm import tqdm\n\nfrom lib.align import DetectedFace\nfrom lib.image import update_existing_metadata  # TODO remove\nfrom scripts.fsmedia import Alignments\n\nfrom .media import Faces\n\nif T.TYPE_CHECKING:\n    from .media import AlignmentData\n    from lib.align.alignments import (AlignmentDict, AlignmentFileDict,\n                                      PNGHeaderDict, PNGHeaderAlignmentsDict)\n\nlogger = logging.getLogger(__name__)\n\n\nclass FromFaces():\n    \"\"\" Scan a folder of Faceswap Extracted Faces and re-create the associated alignments file(s)\n\n    Parameters\n    ----------\n    alignments: NoneType\n        Parameter included for standard job naming convention, but not used for this process.\n    arguments: :class:`argparse.Namespace`\n        The :mod:`argparse` arguments as passed in from :mod:`tools.py`\n    \"\"\"\n    def __init__(self, alignments: None, arguments: Namespace) -> None:\n        logger.debug(\"Initializing %s: (alignments: %s, arguments: %s)\",\n                     self.__class__.__name__, alignments, arguments)\n        self._faces_dir = arguments.faces_dir\n        self._faces = Faces(arguments.faces_dir)\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    def process(self) -> None:\n        \"\"\" Run the job to read faces from a folder to create alignments file(s). \"\"\"\n        logger.info(\"[CREATE ALIGNMENTS FROM FACES]\")  # Tidy up cli output\n\n        all_versions: dict[str, list[float]] = {}\n        d_align: dict[str, dict[str, list[tuple[int, AlignmentFileDict, str, dict]]]] = {}\n        filelist = T.cast(list[tuple[str, \"PNGHeaderDict\"]], self._faces.file_list_sorted)\n        for filename, meta in tqdm(filelist,\n                                   desc=\"Generating Alignments\",\n                                   total=len(filelist),\n                                   leave=False):\n\n            align_fname = self._get_alignments_filename(meta[\"source\"])\n            source_name, f_idx, alignment = self._extract_alignment(meta)\n            full_info = (f_idx, alignment, filename, meta[\"source\"])\n\n            d_align.setdefault(align_fname, {}).setdefault(source_name, []).append(full_info)\n            all_versions.setdefault(align_fname, []).append(meta[\"source\"][\"alignments_version\"])\n\n        versions = {k: min(v) for k, v in all_versions.items()}\n        alignments = self._sort_alignments(d_align)\n        self._save_alignments(alignments, versions)\n\n    @classmethod\n    def _get_alignments_filename(cls, source_data: dict) -> str:\n        \"\"\" Obtain the name of the alignments file from the source information contained within the\n        PNG metadata.\n\n        Parameters\n        ----------\n        source_data: dict\n            The source information contained within a Faceswap extracted PNG\n\n        Returns\n        -------\n        str:\n            If the face was generated from a video file, the filename will be\n            `'<video_name>_alignments.fsa'`. If it was extracted from an image file it will be\n            `'alignments.fsa'`\n        \"\"\"\n        is_video = source_data[\"source_is_video\"]\n        src_name = source_data[\"source_filename\"]\n        prefix = f\"{src_name.rpartition('_')[0]}_\" if is_video else \"\"\n        retval = f\"{prefix}alignments.fsa\"\n        logger.trace(\"Extracted alignments file filename: '%s'\", retval)  # type:ignore\n        return retval\n\n    def _extract_alignment(self, metadata: dict) -> tuple[str, int, AlignmentFileDict]:\n        \"\"\" Extract alignment data from a PNG image's itxt header.\n\n        Formats the landmarks into a numpy array and adds in mask centering information if it is\n        from an older extract.\n\n        Parameters\n        ----------\n        metadata: dict\n            An extracted faces PNG Header data\n\n        Returns\n        -------\n        tuple\n            The alignment's source frame name in position 0. The index of the face within the\n            alignment file in position 1. The alignment data correctly formatted for writing to an\n            alignments file in positin 2\n        \"\"\"\n        alignment = metadata[\"alignments\"]\n        alignment[\"landmarks_xy\"] = np.array(alignment[\"landmarks_xy\"], dtype=\"float32\")\n\n        src = metadata[\"source\"]\n        frame_name = src[\"source_filename\"]\n        face_index = int(src[\"face_index\"])\n\n        logger.trace(\"Extracted alignment for frame: '%s', face index: %s\",  # type:ignore\n                     frame_name, face_index)\n        return frame_name, face_index, alignment\n\n    def _sort_alignments(self,\n                         alignments: dict[str, dict[str, list[tuple[int,\n                                                                    AlignmentFileDict,\n                                                                    str,\n                                                                    dict]]]]\n                         ) -> dict[str, dict[str, AlignmentDict]]:\n        \"\"\" Sort the faces into face index order as they appeared in the original alignments file.\n\n        If the face index stored in the png header does not match it's position in the alignments\n        file (i.e. A face has been removed from a frame) then update the header of the\n        corresponding png to the correct index as exists in the newly created alignments file.\n\n        Parameters\n        ----------\n        alignments: dict\n            The unsorted alignments file(s) as generated from the face PNG headers, including the\n            face index of the face within it's respective frame, the original face filename and\n            the orignal face header source information\n\n        Returns\n        -------\n        dict\n            The alignments file dictionaries sorted into the correct face order, ready for saving\n        \"\"\"\n        logger.info(\"Sorting and checking faces...\")\n        aln_sorted: dict[str, dict[str, AlignmentDict]] = {}\n        for fname, frames in alignments.items():\n            this_file: dict[str, AlignmentDict] = {}\n            for frame in tqdm(sorted(frames), desc=f\"Sorting {fname}\", leave=False):\n                this_file[frame] = {\"video_meta\": {}, \"faces\": []}\n                for real_idx, (f_id, almt, f_path, f_src) in enumerate(sorted(frames[frame],\n                                                                              key=itemgetter(0))):\n                    if real_idx != f_id:\n                        full_path = os.path.join(self._faces_dir, f_path)\n                        self._update_png_header(full_path, real_idx, almt, f_src)\n                    this_file[frame][\"faces\"].append(almt)\n            aln_sorted[fname] = this_file\n        return aln_sorted\n\n    @classmethod\n    def _update_png_header(cls,\n                           face_path: str,\n                           new_index: int,\n                           alignment: AlignmentFileDict,\n                           source_info: dict) -> None:\n        \"\"\" Update the PNG header for faces where the stored index does not correspond with the\n        alignments file. This can occur when frames with multiple faces have had some faces deleted\n        from the faces folder.\n\n        Updates the original filename and index in the png header.\n\n        Parameters\n        ----------\n        face_path: str\n            Full path to the saved face image that requires updating\n        new_index: int\n            The new index as it appears in the newly generated alignments file\n        alignment: dict\n            The alignment information to store in the png header\n        source_info: dict\n            The face source information as extracted from the original face png file\n        \"\"\"\n        face = DetectedFace()\n        face.from_alignment(alignment)\n        new_filename = f\"{os.path.splitext(source_info['source_filename'])[0]}_{new_index}.png\"\n\n        logger.trace(\"Updating png header for '%s': (face index from %s to %s, \"  # type:ignore\n                     \"original filename from '%s' to '%s'\", face_path, source_info[\"face_index\"],\n                     new_index, source_info[\"original_filename\"], new_filename)\n\n        source_info[\"face_index\"] = new_index\n        source_info[\"original_filename\"] = new_filename\n        meta = {\"alignments\": face.to_png_meta(), \"source\": source_info}\n        update_existing_metadata(face_path, meta)\n\n    def _save_alignments(self,\n                         all_alignments: dict[str, dict[str, AlignmentDict]],\n                         versions: dict[str, float]) -> None:\n        \"\"\" Save the newely generated alignments file(s).\n\n        If an alignments file already exists in the source faces folder, back it up rather than\n        overwriting\n\n        Parameters\n        ----------\n        all_alignments: dict\n            The alignment(s) dictionaries found in the faces folder. Alignment filename as key,\n            corresponding alignments as value.\n        versions: dict\n            The minimum version number that exists in a face set for each alignments file to be\n            generated\n        \"\"\"\n        for fname, alignments in all_alignments.items():\n            version = versions[fname]\n            alignments_path = os.path.join(self._faces_dir, fname)\n            dummy_args = Namespace(alignments_path=alignments_path)\n            aln = Alignments(dummy_args, is_extract=True)\n            aln.update_from_dict(alignments)\n            aln._io._version = version  # pylint:disable=protected-access\n            aln._io.update_legacy()  # pylint:disable=protected-access\n            aln.backup()\n            aln.save()\n\n\nclass Rename():\n    \"\"\" Rename faces in a folder to match their filename as stored in an alignments file.\n\n    Parameters\n    ----------\n    alignments: :class:`tools.lib_alignments.media.AlignmentData`\n        The alignments data loaded from an alignments file for this rename job\n    arguments: :class:`argparse.Namespace`\n        The :mod:`argparse` arguments as passed in from :mod:`tools.py`\n    faces: :class:`tools.lib_alignments.media.Faces`, Optional\n        An optional faces object, if the rename task is being called by another job.\n        Default: ``None``\n    \"\"\"\n    def __init__(self,\n                 alignments: AlignmentData,\n                 arguments: Namespace | None,\n                 faces: Faces | None = None) -> None:\n        logger.debug(\"Initializing %s: (arguments: %s, faces: %s)\",\n                     self.__class__.__name__, arguments, faces)\n        self._alignments = alignments\n\n        kwargs = {}\n        if alignments.version < 2.1:\n            # Update headers of faces generated with hash based alignments\n            kwargs[\"alignments\"] = alignments\n        if faces:\n            self._faces = faces\n        else:\n            assert arguments is not None\n            self._faces = Faces(arguments.faces_dir, **kwargs)  # type:ignore  # needs TypedDict :/\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    def process(self) -> None:\n        \"\"\" Process the face renaming \"\"\"\n        logger.info(\"[RENAME FACES]\")  # Tidy up cli output\n        filelist = T.cast(list[tuple[str, \"PNGHeaderDict\"]], self._faces.file_list_sorted)\n        rename_mappings = sorted([(face[0], face[1][\"source\"][\"original_filename\"])\n                                  for face in filelist\n                                  if face[0] != face[1][\"source\"][\"original_filename\"]],\n                                 key=lambda x: x[1])\n        rename_count = self._rename_faces(rename_mappings)\n        logger.info(\"%s faces renamed\", rename_count)\n\n        filelist = T.cast(list[tuple[str, \"PNGHeaderDict\"]], self._faces.file_list_sorted)\n        copyback = FaceToFile(self._alignments, [val[1] for val in filelist])\n        if copyback():\n            self._alignments.save()\n\n    def _rename_faces(self, filename_mappings: list[tuple[str, str]]) -> int:\n        \"\"\" Rename faces back to their original name as exists in the alignments file.\n\n        If the source and destination filename are the same then skip that file.\n\n        Parameters\n        ----------\n        filename_mappings: list\n            List of tuples of (`source filename`, `destination filename`) ordered by destination\n            filename\n\n        Returns\n        -------\n        int\n            The number of faces that have been renamed\n        \"\"\"\n        if not filename_mappings:\n            return 0\n\n        rename_count = 0\n        conflicts = []\n        for src, dst in tqdm(filename_mappings, desc=\"Renaming Faces\", leave=False):\n            old = os.path.join(self._faces.folder, src)\n            new = os.path.join(self._faces.folder, dst)\n\n            if os.path.exists(new):\n                # Interim add .tmp extension to files that will cause a rename conflict, to\n                # process afterwards\n                logger.debug(\"interim renaming file to avoid conflict: (src: '%s', dst: '%s')\",\n                             src, dst)\n                new = new + \".tmp\"\n                conflicts.append(new)\n\n            logger.verbose(\"Renaming '%s' to '%s'\", old, new)  # type:ignore\n            os.rename(old, new)\n            rename_count += 1\n        if conflicts:\n            for old in tqdm(conflicts, desc=\"Renaming Faces\", leave=False):\n                new = old[:-4]  # Remove .tmp extension\n                if os.path.exists(new):\n                    # This should only be running on faces. If there is still a conflict\n                    # then the user has done something stupid, so we will delete the file and\n                    # replace. They can always re-extract :/\n                    os.remove(new)\n                logger.verbose(\"Renaming '%s' to '%s'\", old, new)  # type:ignore\n                os.rename(old, new)\n        return rename_count\n\n\nclass RemoveFaces():\n    \"\"\" Remove items from alignments file.\n\n    Parameters\n    ---------\n    alignments: :class:`tools.alignments.media.AlignmentsData`\n        The loaded alignments containing faces to be removed\n    arguments: :class:`argparse.Namespace`\n        The command line arguments that have called this job\n    \"\"\"\n    def __init__(self, alignments: AlignmentData, arguments: Namespace) -> None:\n        logger.debug(\"Initializing %s: (arguments: %s)\", self.__class__.__name__, arguments)\n        self._alignments = alignments\n\n        self._items = Faces(arguments.faces_dir, alignments=alignments)\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    def process(self) -> None:\n        \"\"\" Run the job to remove faces from an alignments file that do not exist within a faces\n        folder. \"\"\"\n        logger.info(\"[REMOVE FACES FROM ALIGNMENTS]\")  # Tidy up cli output\n\n        if not self._items.items:\n            logger.error(\"No matching faces found in your faces folder. This would remove all \"\n                         \"faces from your alignments file. Process aborted.\")\n            return\n\n        items = T.cast(dict[str, list[int]], self._items.items)\n        pre_face_count = self._alignments.faces_count\n        self._alignments.filter_faces(items, filter_out=False)\n        del_count = pre_face_count - self._alignments.faces_count\n        if del_count == 0:\n            logger.info(\"No changes made to alignments file. Exiting\")\n            return\n\n        logger.info(\"%s alignment(s) were removed from alignments file\", del_count)\n\n        self._update_png_headers()\n        self._alignments.save()\n\n        rename = Rename(self._alignments, None, self._items)\n        rename.process()\n\n    def _update_png_headers(self) -> None:\n        \"\"\" Update the EXIF iTXt field of any face PNGs that have had their face index changed.\n\n        Notes\n        -----\n        This could be quicker if parellizing in threads, however, Windows (at least) does not seem\n        to like this and has a tendency to throw permission errors, so this remains single threaded\n        for now.\n        \"\"\"\n        items = T.cast(dict[str, list[int]], self._items.items)\n        srcs = [(x[0], x[1][\"source\"])\n                for x in T.cast(list[tuple[str, \"PNGHeaderDict\"]], self._items.file_list_sorted)]\n        to_update = [  # Items whose face index has changed\n            x for x in srcs\n            if x[1][\"face_index\"] != items[x[1][\"source_filename\"]].index(x[1][\"face_index\"])]\n\n        for item in tqdm(to_update, desc=\"Updating PNG Headers\", leave=False):\n            filename, file_info = item\n            frame = file_info[\"source_filename\"]\n            face_index = file_info[\"face_index\"]\n            new_index = items[frame].index(face_index)\n\n            fullpath = os.path.join(self._items.folder, filename)\n            logger.debug(\"Updating png header for '%s': face index from %s to %s\",\n                         fullpath, face_index, new_index)\n\n            # Update file_list_sorted for rename task\n            orig_filename = f\"{os.path.splitext(frame)[0]}_{new_index}.png\"\n            file_info[\"face_index\"] = new_index\n            file_info[\"original_filename\"] = orig_filename\n\n            face = DetectedFace()\n            face.from_alignment(self._alignments.get_faces_in_frame(frame)[new_index])\n            meta = {\"alignments\": face.to_png_meta(),\n                    \"source\": {\"alignments_version\": file_info[\"alignments_version\"],\n                               \"original_filename\": orig_filename,\n                               \"face_index\": new_index,\n                               \"source_filename\": frame,\n                               \"source_is_video\": file_info[\"source_is_video\"],\n                               \"source_frame_dims\": file_info.get(\"source_frame_dims\")}}\n            update_existing_metadata(fullpath, meta)\n\n        logger.info(\"%s Extracted face(s) had their header information updated\", len(to_update))\n\n\nclass FaceToFile():\n    \"\"\" Updates any optional/missing keys in the alignments file with any data that has been\n    populated in a PNGHeader. Includes masks and identity fields.\n\n    Parameters\n    ---------\n    alignments: :class:`tools.alignments.media.AlignmentsData`\n        The loaded alignments containing faces to be removed\n    face_data: list\n        List of :class:`PNGHeaderDict` objects\n    \"\"\"\n    def __init__(self, alignments: AlignmentData, face_data: list[PNGHeaderDict]) -> None:\n        logger.debug(\"Initializing %s: alignments: %s, face_data: %s\",\n                     self.__class__.__name__, alignments, len(face_data))\n        self._alignments = alignments\n        self._face_alignments = face_data\n        self._updatable_keys: list[T.Literal[\"identity\", \"mask\"]] = [\"identity\", \"mask\"]\n        self._counts: dict[str, int] = {}\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    def _check_and_update(self,\n                          alignment: PNGHeaderAlignmentsDict,\n                          face: AlignmentFileDict) -> None:\n        \"\"\" Check whether the key requires updating and update it.\n\n        alignment: dict\n            The alignment dictionary from the PNG Header\n        face: dict\n            The alignment dictionary for the face from the alignments file\n        \"\"\"\n        for key in self._updatable_keys:\n            if key == \"mask\":\n                exist_masks = face[\"mask\"]\n                for mask_name, mask_data in alignment[\"mask\"].items():\n                    if mask_name in exist_masks:\n                        continue\n                    exist_masks[mask_name] = mask_data\n                    count_key = f\"mask_{mask_name}\"\n                    self._counts[count_key] = self._counts.get(count_key, 0) + 1\n                continue\n\n            if not face.get(key, {}) and alignment.get(key):\n                face[key] = alignment[key]\n                self._counts[key] = self._counts.get(key, 0) + 1\n\n    def __call__(self) -> bool:\n        \"\"\" Parse through the face data updating any entries in the alignments file.\n\n        Returns\n        -------\n        bool\n            ``True`` if any alignment information was updated otherwise ``False``\n        \"\"\"\n        for meta in tqdm(self._face_alignments,\n                         desc=\"Updating Alignments File from PNG Header\",\n                         leave=False):\n            src = meta[\"source\"]\n            alignment = meta[\"alignments\"]\n            if not any(alignment.get(key, {}) for key in self._updatable_keys):\n                continue\n\n            faces = self._alignments.get_faces_in_frame(src[\"source_filename\"])\n            if len(faces) < src[\"face_index\"] + 1:  # list index out of range\n                logger.debug(\"Skipped face '%s'. Index does not exist in alignments file\",\n                             src[\"original_filename\"])\n                continue\n\n            face = faces[src[\"face_index\"]]\n            self._check_and_update(alignment, face)\n\n        retval = False\n        if self._counts:\n            retval = True\n            logger.info(\"Updated alignments file from PNG Data: %s\", self._counts)\n        return retval\n", "tools/alignments/cli.py": "#!/usr/bin/env python3\n\"\"\" Command Line Arguments for tools \"\"\"\nimport argparse\nimport sys\nimport gettext\nimport typing as T\n\nfrom lib.cli.args import FaceSwapArgs\nfrom lib.cli.actions import DirOrFileFullPaths, DirFullPaths, FileFullPaths, Radio, Slider\n\n# LOCALES\n_LANG = gettext.translation(\"tools.alignments.cli\", localedir=\"locales\", fallback=True)\n_ = _LANG.gettext\n\n\n_HELPTEXT = _(\"This command lets you perform various tasks pertaining to an alignments file.\")\n\n\nclass AlignmentsArgs(FaceSwapArgs):\n    \"\"\" Class to parse the command line arguments for Alignments tool \"\"\"\n\n    @staticmethod\n    def get_info() -> str:\n        \"\"\" Obtain command information.\n\n        Returns\n        -------\n        str\n            The help text for displaying in argparses help output\n         \"\"\"\n        return _(\"Alignments tool\\nThis tool allows you to perform numerous actions on or using \"\n                 \"an alignments file against its corresponding faceset/frame source.\")\n\n    @staticmethod\n    def get_argument_list() -> list[dict[str, T.Any]]:\n        \"\"\" Collect the argparse argument options.\n\n        Returns\n        -------\n        dict\n            The argparse command line options for processing by argparse\n        \"\"\"\n        frames_dir = _(\" Must Pass in a frames folder/source video file (-r).\")\n        faces_dir = _(\" Must Pass in a faces folder (-c).\")\n        frames_or_faces_dir = _(\" Must Pass in either a frames folder/source video file OR a \"\n                                \"faces folder (-r or -c).\")\n        frames_and_faces_dir = _(\" Must Pass in a frames folder/source video file AND a faces \"\n                                 \"folder (-r and -c).\")\n        output_opts = _(\" Use the output option (-o) to process results.\")\n        argument_list = []\n        argument_list.append({\n            \"opts\": (\"-j\", \"--job\"),\n            \"action\": Radio,\n            \"type\": str,\n            \"choices\": (\"draw\", \"extract\", \"export\", \"from-faces\", \"missing-alignments\",\n                        \"missing-frames\", \"multi-faces\", \"no-faces\", \"remove-faces\", \"rename\",\n                        \"sort\", \"spatial\"),\n            \"group\": _(\"processing\"),\n            \"required\": True,\n            \"help\": _(\n                \"R|Choose which action you want to perform. NB: All actions require an \"\n                \"alignments file (-a) to be passed in.\"\n                \"\\nL|'draw': Draw landmarks on frames in the selected folder/video. A \"\n                \"subfolder will be created within the frames folder to hold the output.{0}\"\n                \"\\nL|'export': Export the contents of an alignments file to a json file. Can be \"\n                \"used for editing alignment information in external tools and then re-importing \"\n                \"by using Faceswap's Extract 'Import' plugins. Note: masks and identity vectors \"\n                \"will not be included in the exported file, so will be re-generated when the json \"\n                \"file is imported back into Faceswap. All data is exported with the origin (0, 0) \"\n                \"at the top left of the canvas.\"\n                \"\\nL|'extract': Re-extract faces from the source frames/video based on \"\n                \"alignment data. This is a lot quicker than re-detecting faces. Can pass in \"\n                \"the '-een' (--extract-every-n) parameter to only extract every nth frame.{1}\"\n                \"\\nL|'from-faces': Generate alignment file(s) from a folder of extracted \"\n                \"faces. if the folder of faces comes from multiple sources, then multiple \"\n                \"alignments files will be created. NB: for faces which have been extracted \"\n                \"from folders of source images, rather than a video, a single alignments file \"\n                \"will be created as there is no way for the process to know how many folders \"\n                \"of images were originally used. You do not need to provide an alignments file \"\n                \"path to run this job. {3}\"\n                \"\\nL|'missing-alignments': Identify frames that do not exist in the alignments \"\n                \"file.{2}{0}\"\n                \"\\nL|'missing-frames': Identify frames in the alignments file that do not \"\n                \"appear within the frames folder/video.{2}{0}\"\n                \"\\nL|'multi-faces': Identify where multiple faces exist within the alignments \"\n                \"file.{2}{4}\"\n                \"\\nL|'no-faces': Identify frames that exist within the alignment file but no \"\n                \"faces were detected.{2}{0}\"\n                \"\\nL|'remove-faces': Remove deleted faces from an alignments file. The \"\n                \"original alignments file will be backed up.{3}\"\n                \"\\nL|'rename' - Rename faces to correspond with their parent frame and \"\n                \"position index in the alignments file (i.e. how they are named after running \"\n                \"extract).{3}\"\n                \"\\nL|'sort': Re-index the alignments from left to right. For alignments with \"\n                \"multiple faces this will ensure that the left-most face is at index 0.\"\n                \"\\nL|'spatial': Perform spatial and temporal filtering to smooth alignments \"\n                \"(EXPERIMENTAL!)\").format(frames_dir, frames_and_faces_dir, output_opts,\n                                          faces_dir, frames_or_faces_dir)})\n        argument_list.append({\n            \"opts\": (\"-o\", \"--output\"),\n            \"action\": Radio,\n            \"type\": str,\n            \"choices\": (\"console\", \"file\", \"move\"),\n            \"group\": _(\"processing\"),\n            \"default\": \"console\",\n            \"help\": _(\n                \"R|How to output discovered items ('faces' and 'frames' only):\"\n                \"\\nL|'console': Print the list of frames to the screen. (DEFAULT)\"\n                \"\\nL|'file': Output the list of frames to a text file (stored within the \"\n                \"source directory).\"\n                \"\\nL|'move': Move the discovered items to a sub-folder within the source \"\n                \"directory.\")})\n        argument_list.append({\n            \"opts\": (\"-a\", \"--alignments_file\"),\n            \"action\": FileFullPaths,\n            \"dest\": \"alignments_file\",\n            \"type\": str,\n            \"group\": _(\"data\"),\n            # hacky solution to not require alignments file if creating alignments from faces:\n            \"required\": not any(val in sys.argv for val in [\"from-faces\",\n                                                            \"-r\",\n                                                            \"-frames_folder\"]),\n            \"filetypes\": \"alignments\",\n            \"help\": _(\n                \"Full path to the alignments file to be processed. If you have input a \"\n                \"'frames_dir' and don't provide this option, the process will try to find the \"\n                \"alignments file at the default location. All jobs require an alignments file \"\n                \"with the exception of 'from-faces' when the alignments file will be generated \"\n                \"in the specified faces folder.\")})\n        argument_list.append({\n            \"opts\": (\"-c\", \"-faces_folder\"),\n            \"action\": DirFullPaths,\n            \"dest\": \"faces_dir\",\n            \"group\": (\"data\"),\n            \"help\": (\"Directory containing extracted faces.\")})\n        argument_list.append({\n            \"opts\": (\"-r\", \"-frames_folder\"),\n            \"action\": DirOrFileFullPaths,\n            \"dest\": \"frames_dir\",\n            \"filetypes\": \"video\",\n            \"group\": _(\"data\"),\n            \"help\": _(\"Directory containing source frames that faces were extracted from.\")})\n        argument_list.append({\n            \"opts\": (\"-B\", \"--batch-mode\"),\n            \"action\": \"store_true\",\n            \"dest\": \"batch_mode\",\n            \"default\": False,\n            \"group\": _(\"data\"),\n            \"help\": _(\n                \"R|Run the aligmnents tool on multiple sources. The following jobs support \"\n                \"batch mode:\"\n                \"\\nL|draw, extract, from-faces, missing-alignments, missing-frames, no-faces, \"\n                \"sort, spatial.\"\n                \"\\nIf batch mode is selected then the other options should be set as follows:\"\n                \"\\nL|alignments_file: For 'sort' and 'spatial' this should point to the parent \"\n                \"folder containing the alignments files to be processed. For all other jobs \"\n                \"this option is ignored, and the alignments files must exist at their default \"\n                \"location relative to the original frames folder/video.\"\n                \"\\nL|faces_dir: For 'from-faces' this should be a parent folder, containing \"\n                \"sub-folders of extracted faces from which to generate alignments files. For \"\n                \"'extract' this should be a parent folder where sub-folders will be created \"\n                \"for each extraction to be run. For all other jobs this option is ignored.\"\n                \"\\nL|frames_dir: For 'draw', 'extract', 'missing-alignments', 'missing-frames' \"\n                \"and 'no-faces' this should be a parent folder containing video files or sub-\"\n                \"folders of images to perform the alignments job on. The alignments file \"\n                \"should exist at the default location. For all other jobs this option is \"\n                \"ignored.\")})\n        argument_list.append({\n            \"opts\": (\"-N\", \"--extract-every-n\"),\n            \"type\": int,\n            \"action\": Slider,\n            \"dest\": \"extract_every_n\",\n            \"min_max\": (1, 100),\n            \"default\": 1,\n            \"rounding\": 1,\n            \"group\": _(\"extract\"),\n            \"help\": _(\n                \"[Extract only] Extract every 'nth' frame. This option will skip frames when \"\n                \"extracting faces. For example a value of 1 will extract faces from every frame, \"\n                \"a value of 10 will extract faces from every 10th frame.\")})\n        argument_list.append({\n            \"opts\": (\"-z\", \"--size\"),\n            \"type\": int,\n            \"action\": Slider,\n            \"min_max\": (256, 1024),\n            \"rounding\": 64,\n            \"default\": 512,\n            \"group\": _(\"extract\"),\n            \"help\": _(\"[Extract only] The output size of extracted faces.\")})\n        argument_list.append({\n            \"opts\": (\"-m\", \"--min-size\"),\n            \"type\": int,\n            \"action\": Slider,\n            \"min_max\": (0, 200),\n            \"rounding\": 1,\n            \"default\": 0,\n            \"dest\": \"min_size\",\n            \"group\": _(\"extract\"),\n            \"help\": _(\n                \"[Extract only] Only extract faces that have been resized by this percent or \"\n                \"more to meet the specified extract size (`-sz`, `--size`). Useful for \"\n                \"excluding low-res images from a training set. Set to 0 to extract all faces. \"\n                \"Eg: For an extract size of 512px, A setting of 50 will only include faces \"\n                \"that have been resized from 256px or above. Setting to 100 will only extract \"\n                \"faces that have been resized from 512px or above. A setting of 200 will only \"\n                \"extract faces that have been downscaled from 1024px or above.\")})\n        # Deprecated multi-character switches\n        argument_list.append({\n            \"opts\": (\"-fc\", ),\n            \"type\": str,\n            \"dest\": \"depr_faces_folder_fc_c\",\n            \"help\": argparse.SUPPRESS})\n        argument_list.append({\n            \"opts\": (\"-fr\", ),\n            \"type\": str,\n            \"dest\": \"depr_extract-every-n_een_N\",\n            \"help\": argparse.SUPPRESS})\n        argument_list.append({\n            \"opts\": (\"-een\", ),\n            \"type\": int,\n            \"dest\": \"depr_faces_folder_fr_r\",\n            \"help\": argparse.SUPPRESS})\n        argument_list.append({\n            \"opts\": (\"-sz\", ),\n            \"type\": int,\n            \"dest\": \"depr_size_sz_z\",\n            \"help\": argparse.SUPPRESS})\n        return argument_list\n", "tools/alignments/alignments.py": "#!/usr/bin/env python3\n\"\"\" Tools for manipulating the alignments serialized file \"\"\"\nimport logging\nimport os\nimport sys\nimport typing as T\n\nfrom argparse import Namespace\nfrom multiprocessing import Process\n\nfrom lib.utils import FaceswapError, handle_deprecated_cliopts, VIDEO_EXTENSIONS\nfrom .media import AlignmentData\nfrom .jobs import Check, Export, Sort, Spatial  # noqa pylint:disable=unused-import\nfrom .jobs_faces import FromFaces, RemoveFaces, Rename  # noqa pylint:disable=unused-import\nfrom .jobs_frames import Draw, Extract  # noqa pylint:disable=unused-import\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass Alignments():\n    \"\"\" The main entry point for Faceswap's Alignments Tool. This tool is part of the Faceswap\n    Tools suite and should be called from the ``python tools.py alignments`` command.\n\n    The tool allows for manipulation, and working with Faceswap alignments files.\n\n    This parent class handles creating the individual job arguments when running in batch-mode or\n    triggers the job when not running in batch mode\n\n    Parameters\n    ----------\n    arguments: :class:`argparse.Namespace`\n        The :mod:`argparse` arguments as passed in from :mod:`tools.py`\n    \"\"\"\n    def __init__(self, arguments: Namespace) -> None:\n        logger.debug(\"Initializing %s: (arguments: %s)\", self.__class__.__name__, arguments)\n        self._requires_alignments = [\"export\", \"sort\", \"spatial\"]\n        self._requires_faces = [\"extract\", \"from-faces\"]\n        self._requires_frames = [\"draw\",\n                                 \"extract\",\n                                 \"missing-alignments\",\n                                 \"missing-frames\",\n                                 \"no-faces\"]\n\n        self._args = handle_deprecated_cliopts(arguments)\n        self._batch_mode = self._validate_batch_mode()\n        self._locations = self._get_locations()\n\n    def _validate_batch_mode(self) -> bool:\n        \"\"\" Validate that the selected job supports batch processing\n\n        Returns\n        -------\n        bool\n            ``True`` if batch mode has been selected otherwise ``False``\n        \"\"\"\n        batch_mode: bool = self._args.batch_mode\n        if not batch_mode:\n            logger.debug(\"Running in standard mode\")\n            return batch_mode\n        valid = self._requires_alignments + self._requires_faces + self._requires_frames\n        if self._args.job not in valid:\n            logger.error(\"Job '%s' does not support batch mode. Please select a job from %s or \"\n                         \"disable batch mode\", self._args.job, valid)\n            sys.exit(1)\n        logger.debug(\"Running in batch mode\")\n        return batch_mode\n\n    def _get_alignments_locations(self) -> dict[str, list[str | None]]:\n        \"\"\" Obtain the full path to alignments files in a parent (batch) location\n\n        These are jobs that only require an alignments file as input, so frames and face locations\n        are returned as a list of ``None`` values corresponding to the number of alignments files\n        detected\n\n        Returns\n        -------\n        dict[str, list[Optional[str]]]:\n            The list of alignments location paths and None lists for frames and faces locations\n        \"\"\"\n        if not self._args.alignments_file:\n            logger.error(\"Please provide an 'alignments_file' location for '%s' job\",\n                         self._args.job)\n            sys.exit(1)\n\n        alignments = [os.path.join(self._args.alignments_file, fname)\n                      for fname in os.listdir(self._args.alignments_file)\n                      if os.path.splitext(fname)[-1].lower() == \".fsa\"\n                      and os.path.splitext(fname)[0].endswith(\"alignments\")]\n        if not alignments:\n            logger.error(\"No alignment files found in '%s'\", self._args.alignments_file)\n            sys.exit(1)\n\n        logger.info(\"Batch mode selected. Processing alignments: %s\", alignments)\n        retval = {\"alignments_file\": alignments,\n                  \"faces_dir\": [None for _ in range(len(alignments))],\n                  \"frames_dir\": [None for _ in range(len(alignments))]}\n        return retval\n\n    def _get_frames_locations(self) -> dict[str, list[str | None]]:\n        \"\"\" Obtain the full path to frame locations along with corresponding alignments file\n        locations contained within the parent (batch) location\n\n        Returns\n        -------\n        dict[str, list[Optional[str]]]:\n            list of frames and alignments location paths. If the job requires an output faces\n            location then the faces folders are also returned, otherwise the faces will be a list\n            of ``Nones`` corresponding to the number of jobs to run\n        \"\"\"\n        if not self._args.frames_dir:\n            logger.error(\"Please provide a 'frames_dir' location for '%s' job\", self._args.job)\n            sys.exit(1)\n\n        frames: list[str] = []\n        alignments: list[str] = []\n        candidates = [os.path.join(self._args.frames_dir, fname)\n                      for fname in os.listdir(self._args.frames_dir)\n                      if os.path.isdir(os.path.join(self._args.frames_dir, fname))\n                      or os.path.splitext(fname)[-1].lower() in VIDEO_EXTENSIONS]\n        logger.debug(\"Frame candidates: %s\", candidates)\n\n        for candidate in candidates:\n            fname = os.path.join(candidate, \"alignments.fsa\")\n            if os.path.isdir(candidate) and os.path.exists(fname):\n                frames.append(candidate)\n                alignments.append(fname)\n                continue\n            fname = f\"{os.path.splitext(candidate)[0]}_alignments.fsa\"\n            if os.path.isfile(candidate) and os.path.exists(fname):\n                frames.append(candidate)\n                alignments.append(fname)\n                continue\n            logger.warning(\"Can't locate alignments file for '%s'. Skipping.\", candidate)\n\n        if not frames:\n            logger.error(\"No valid videos or frames folders found in '%s'\", self._args.frames_dir)\n            sys.exit(1)\n\n        if self._args.job not in self._requires_faces:  # faces not required for frames input\n            faces: list[str | None] = [None for _ in range(len(frames))]\n        else:\n            if not self._args.faces_dir:\n                logger.error(\"Please provide a 'faces_dir' location for '%s' job\", self._args.job)\n                sys.exit(1)\n            faces = [os.path.join(self._args.faces_dir, os.path.basename(os.path.splitext(frm)[0]))\n                     for frm in frames]\n\n        logger.info(\"Batch mode selected. Processing frames: %s\",\n                    [os.path.basename(frame) for frame in frames])\n\n        return {\"alignments_file\": T.cast(list[str | None], alignments),\n                \"frames_dir\": T.cast(list[str | None], frames),\n                \"faces_dir\": faces}\n\n    def _get_locations(self) -> dict[str, list[str | None]]:\n        \"\"\" Obtain the full path to any frame, face and alignments input locations for the\n        selected job when running in batch mode. If not running in batch mode, then the original\n        passed in values are returned in lists\n\n        Returns\n        -------\n        dict[str, list[Optional[str]]]\n            A dictionary corresponding to the alignments, frames_dir and faces_dir arguments\n            with a list of full paths for each job\n        \"\"\"\n        job: str = self._args.job\n        if not self._batch_mode:  # handle with given arguments\n            retval = {\"alignments_file\": [self._args.alignments_file],\n                      \"faces_dir\": [self._args.faces_dir],\n                      \"frames_dir\": [self._args.frames_dir]}\n\n        elif job in self._requires_alignments:  # Jobs only requiring an alignments file location\n            retval = self._get_alignments_locations()\n\n        elif job in self._requires_frames:  # Jobs that require a frames folder\n            retval = self._get_frames_locations()\n\n        elif job in self._requires_faces and job not in self._requires_frames:\n            # Jobs that require faces as input\n            faces = [os.path.join(self._args.faces_dir, folder)\n                     for folder in os.listdir(self._args.faces_dir)\n                     if os.path.isdir(os.path.join(self._args.faces_dir, folder))]\n            if not faces:\n                logger.error(\"No folders found in '%s'\", self._args.faces_dir)\n                sys.exit(1)\n\n            retval = {\"faces_dir\": faces,\n                      \"frames_dir\": [None for _ in range(len(faces))],\n                      \"alignments_file\": [None for _ in range(len(faces))]}\n            logger.info(\"Batch mode selected. Processing faces: %s\",\n                        [os.path.basename(folder) for folder in faces])\n        else:\n            raise FaceswapError(f\"Unhandled job: {self._args.job}. This is a bug. Please report \"\n                                \"to the developers\")\n\n        logger.debug(\"File locations: %s\", retval)\n        return retval\n\n    @staticmethod\n    def _run_process(arguments) -> None:\n        \"\"\" The alignements tool process to be run in a spawned process.\n\n        In some instances, batch-mode memory leaks. Launching each job in a separate process\n        prevents this leak.\n\n        Parameters\n        ----------\n        arguments: :class:`argparse.Namespace`\n            The :mod:`argparse` arguments to be used for the given job\n        \"\"\"\n        logger.debug(\"Starting process: (arguments: %s)\", arguments)\n        tool = _Alignments(arguments)\n        tool.process()\n        logger.debug(\"Finished process: (arguments: %s)\", arguments)\n\n    def process(self):\n        \"\"\" The entry point for the Alignments tool from :mod:`lib.tools.alignments.cli`.\n\n        Launches the selected alignments job.\n        \"\"\"\n        num_jobs = len(self._locations[\"frames_dir\"])\n        for idx, (frames, faces, alignments) in enumerate(zip(self._locations[\"frames_dir\"],\n                                                              self._locations[\"faces_dir\"],\n                                                              self._locations[\"alignments_file\"])):\n            if num_jobs > 1:\n                logger.info(\"Processing job %s of %s\", idx + 1, num_jobs)\n\n            args = Namespace(**self._args.__dict__)\n            args.frames_dir = frames\n            args.faces_dir = faces\n            args.alignments_file = alignments\n\n            if num_jobs > 1:\n                proc = Process(target=self._run_process, args=(args, ))\n                proc.start()\n                proc.join()\n            else:\n                self._run_process(args)\n\n\nclass _Alignments():\n    \"\"\" The main entry point for Faceswap's Alignments Tool. This tool is part of the Faceswap\n    Tools suite and should be called from the ``python tools.py alignments`` command.\n\n    The tool allows for manipulation, and working with Faceswap alignments files.\n\n    Parameters\n    ----------\n    arguments: :class:`argparse.Namespace`\n        The :mod:`argparse` arguments as passed in from :mod:`tools.py`\n    \"\"\"\n    def __init__(self, arguments: Namespace) -> None:\n        logger.debug(\"Initializing %s: (arguments: '%s'\", self.__class__.__name__, arguments)\n        self._args = arguments\n        job = self._args.job\n\n        if job == \"from-faces\":\n            self.alignments = None\n        else:\n            self.alignments = AlignmentData(self._find_alignments())\n\n        if (self.alignments is not None and\n                arguments.frames_dir and\n                os.path.isfile(arguments.frames_dir)):\n            self.alignments.update_legacy_has_source(os.path.basename(arguments.frames_dir))\n\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    def _find_alignments(self) -> str:\n        \"\"\" If an alignments folder is required and hasn't been provided, scan for a file based on\n        the video folder.\n\n        Exits if an alignments file cannot be located\n\n        Returns\n        -------\n        str\n            The full path to an alignments file\n        \"\"\"\n        fname = self._args.alignments_file\n        frames = self._args.frames_dir\n        if fname and os.path.isfile(fname) and os.path.splitext(fname)[-1].lower() == \".fsa\":\n            return fname\n        if fname:\n            logger.error(\"Not a valid alignments file: '%s'\", fname)\n            sys.exit(1)\n\n        if not frames or not os.path.exists(frames):\n            logger.error(\"Not a valid frames folder: '%s'. Can't scan for alignments.\", frames)\n            sys.exit(1)\n\n        fname = \"alignments.fsa\"\n        if os.path.isdir(frames) and os.path.exists(os.path.join(frames, fname)):\n            return fname\n\n        if os.path.isdir(frames) or os.path.splitext(frames)[-1] not in VIDEO_EXTENSIONS:\n            logger.error(\"Can't find a valid alignments file in location: %s\", frames)\n            sys.exit(1)\n\n        fname = f\"{os.path.splitext(frames)[0]}_{fname}\"\n        if not os.path.exists(fname):\n            logger.error(\"Can't find a valid alignments file for video: %s\", frames)\n            sys.exit(1)\n\n        return fname\n\n    def process(self) -> None:\n        \"\"\" The entry point for the Alignments tool from :mod:`lib.tools.alignments.cli`.\n\n        Launches the selected alignments job.\n        \"\"\"\n        if self._args.job in (\"missing-alignments\", \"missing-frames\", \"multi-faces\", \"no-faces\"):\n            job: T.Any = Check\n        else:\n            job = globals()[self._args.job.title().replace(\"-\", \"\")]\n        job = job(self.alignments, self._args)\n        logger.debug(job)\n        job.process()\n", "tools/alignments/media.py": "#!/usr/bin/env python3\n\"\"\" Media items (Alignments, Faces, Frames)\n    for alignments tool \"\"\"\nfrom __future__ import annotations\nimport logging\nfrom operator import itemgetter\nimport os\nimport sys\nimport typing as T\n\nimport cv2\nfrom tqdm import tqdm\n\n# TODO imageio single frame seek seems slow. Look into this\n# import imageio\n\nfrom lib.align import Alignments, DetectedFace, update_legacy_png_header\nfrom lib.image import (count_frames, generate_thumbnail, ImagesLoader,\n                       png_write_meta, read_image, read_image_meta_batch)\nfrom lib.utils import IMAGE_EXTENSIONS, VIDEO_EXTENSIONS, FaceswapError\n\nif T.TYPE_CHECKING:\n    from collections.abc import Generator\n    import numpy as np\n    from lib.align.alignments import AlignmentFileDict, PNGHeaderDict\n\nlogger = logging.getLogger(__name__)\n\n\nclass AlignmentData(Alignments):\n    \"\"\" Class to hold the alignment data\n\n    Parameters\n    ----------\n    alignments_file: str\n        Full path to an alignments file\n    \"\"\"\n    def __init__(self, alignments_file: str) -> None:\n        logger.debug(\"Initializing %s: (alignments file: '%s')\",\n                     self.__class__.__name__, alignments_file)\n        logger.info(\"[ALIGNMENT DATA]\")  # Tidy up cli output\n        folder, filename = self.check_file_exists(alignments_file)\n        super().__init__(folder, filename=filename)\n        logger.verbose(\"%s items loaded\", self.frames_count)  # type: ignore\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    @staticmethod\n    def check_file_exists(alignments_file: str) -> tuple[str, str]:\n        \"\"\"  Check if the alignments file exists, and returns a tuple of the folder and filename.\n\n        Parameters\n        ----------\n        alignments_file: str\n            Full path to an alignments file\n\n        Returns\n        -------\n        folder: str\n            The full path to the folder containing the alignments file\n        filename: str\n            The filename of the alignments file\n        \"\"\"\n        folder, filename = os.path.split(alignments_file)\n        if not os.path.isfile(alignments_file):\n            logger.error(\"ERROR: alignments file not found at: '%s'\", alignments_file)\n            sys.exit(0)\n        if folder:\n            logger.verbose(\"Alignments file exists at '%s'\", alignments_file)  # type: ignore\n        return folder, filename\n\n    def save(self) -> None:\n        \"\"\" Backup copy of old alignments and save new alignments \"\"\"\n        self.backup()\n        super().save()\n\n\nclass MediaLoader():\n    \"\"\" Class to load images.\n\n    Parameters\n    ----------\n    folder: str\n        The folder of images or video file to load images from\n    count: int or ``None``, optional\n        If the total frame count is known it can be passed in here which will skip\n        analyzing a video file. If the count is not passed in, it will be calculated.\n        Default: ``None``\n    \"\"\"\n    def __init__(self, folder: str, count: int | None = None):\n        logger.debug(\"Initializing %s: (folder: '%s')\", self.__class__.__name__, folder)\n        logger.info(\"[%s DATA]\", self.__class__.__name__.upper())\n        self._count = count\n        self.folder = folder\n        self._vid_reader = self.check_input_folder()\n        self.file_list_sorted = self.sorted_items()\n        self.items = self.load_items()\n        logger.verbose(\"%s items loaded\", self.count)  # type: ignore\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    @property\n    def is_video(self) -> bool:\n        \"\"\" bool: Return whether source is a video or not \"\"\"\n        return self._vid_reader is not None\n\n    @property\n    def count(self) -> int:\n        \"\"\" int: Number of faces or frames \"\"\"\n        if self._count is not None:\n            return self._count\n        if self.is_video:\n            self._count = int(count_frames(self.folder))\n        else:\n            self._count = len(self.file_list_sorted)\n        return self._count\n\n    def check_input_folder(self) -> cv2.VideoCapture | None:\n        \"\"\" Ensure that the frames or faces folder exists and is valid.\n            If frames folder contains a video file return imageio reader object\n\n        Returns\n        -------\n        :class:`cv2.VideoCapture`\n            Object for reading a video stream\n        \"\"\"\n        err = None\n        loadtype = self.__class__.__name__\n        if not self.folder:\n            err = f\"ERROR: A {loadtype} folder must be specified\"\n        elif not os.path.exists(self.folder):\n            err = f\"ERROR: The {loadtype} location {self.folder} could not be found\"\n        if err:\n            logger.error(err)\n            sys.exit(0)\n\n        if (loadtype == \"Frames\" and\n                os.path.isfile(self.folder) and\n                os.path.splitext(self.folder)[1].lower() in VIDEO_EXTENSIONS):\n            logger.verbose(\"Video exists at: '%s'\", self.folder)  # type: ignore\n            retval = cv2.VideoCapture(self.folder)  # pylint:disable=no-member\n            # TODO ImageIO single frame seek seems slow. Look into this\n            # retval = imageio.get_reader(self.folder, \"ffmpeg\")\n        else:\n            logger.verbose(\"Folder exists at '%s'\", self.folder)  # type: ignore\n            retval = None\n        return retval\n\n    @staticmethod\n    def valid_extension(filename) -> bool:\n        \"\"\" bool: Check whether passed in file has a valid extension \"\"\"\n        extension = os.path.splitext(filename)[1]\n        retval = extension.lower() in IMAGE_EXTENSIONS\n        logger.trace(\"Filename has valid extension: '%s': %s\", filename, retval)  # type: ignore\n        return retval\n\n    def sorted_items(self) -> list[dict[str, str]] | list[tuple[str, PNGHeaderDict]]:\n        \"\"\" Override for specific folder processing \"\"\"\n        raise NotImplementedError()\n\n    def process_folder(self) -> (Generator[dict[str, str], None, None] |\n                                 Generator[tuple[str, PNGHeaderDict], None, None]):\n        \"\"\" Override for specific folder processing \"\"\"\n        raise NotImplementedError()\n\n    def load_items(self) -> dict[str, list[int]] | dict[str, tuple[str, str]]:\n        \"\"\" Override for specific item loading \"\"\"\n        raise NotImplementedError()\n\n    def load_image(self, filename: str) -> np.ndarray:\n        \"\"\" Load an image\n\n        Parameters\n        ----------\n        filename: str\n            The filename of the image to load\n\n        Returns\n        -------\n        :class:`numpy.ndarray`\n            The loaded image\n        \"\"\"\n        if self.is_video:\n            image = self.load_video_frame(filename)\n        else:\n            src = os.path.join(self.folder, filename)\n            logger.trace(\"Loading image: '%s'\", src)  # type: ignore\n            image = read_image(src, raise_error=True)\n        return image\n\n    def load_video_frame(self, filename: str) -> np.ndarray:\n        \"\"\" Load a requested frame from video\n\n        Parameters\n        ----------\n        filename: str\n            The frame name to load\n\n        Returns\n        -------\n        :class:`numpy.ndarray`\n            The loaded image\n        \"\"\"\n        assert self._vid_reader is not None\n        frame = os.path.splitext(filename)[0]\n        logger.trace(\"Loading video frame: '%s'\", frame)  # type: ignore\n        frame_no = int(frame[frame.rfind(\"_\") + 1:]) - 1\n        self._vid_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_no)  # pylint:disable=no-member\n\n        _, image = self._vid_reader.read()\n        # TODO imageio single frame seek seems slow. Look into this\n        # self._vid_reader.set_image_index(frame_no)\n        # image = self._vid_reader.get_next_data()[:, :, ::-1]\n        return image\n\n    def stream(self, skip_list: list[int] | None = None\n               ) -> Generator[tuple[str, np.ndarray], None, None]:\n        \"\"\" Load the images in :attr:`folder` in the order they are received from\n        :class:`lib.image.ImagesLoader` in a background thread.\n\n        Parameters\n        ----------\n        skip_list: list, optional\n            A list of frame indices that should not be loaded. Pass ``None`` if all images should\n            be loaded. Default: ``None``\n\n        Yields\n        ------\n        str\n            The filename of the image that is being returned\n        numpy.ndarray\n            The image that has been loaded from disk\n        \"\"\"\n        loader = ImagesLoader(self.folder, queue_size=32, count=self._count)\n        if skip_list is not None:\n            loader.add_skip_list(skip_list)\n        for filename, image in loader.load():\n            yield filename, image\n\n    @staticmethod\n    def save_image(output_folder: str,\n                   filename: str,\n                   image: np.ndarray,\n                   metadata: PNGHeaderDict | None = None) -> None:\n        \"\"\" Save an image \"\"\"\n        output_file = os.path.join(output_folder, filename)\n        output_file = os.path.splitext(output_file)[0] + \".png\"\n        logger.trace(\"Saving image: '%s'\", output_file)  # type: ignore\n        if metadata:\n            encoded = cv2.imencode(\".png\", image)[1]\n            encoded_image = png_write_meta(encoded.tobytes(), metadata)\n            with open(output_file, \"wb\") as out_file:\n                out_file.write(encoded_image)\n        else:\n            cv2.imwrite(output_file, image)  # pylint:disable=no-member\n\n\nclass Faces(MediaLoader):\n    \"\"\" Object to load Extracted Faces from a folder.\n\n    Parameters\n    ----------\n    folder: str\n        The folder to load faces from\n    alignments: :class:`lib.align.Alignments`, optional\n        The alignments object that contains the faces. This can be used for 2 purposes:\n        - To update legacy hash based faces for <v2.1 alignments to png header based version.\n        - When the remove-faces job is being run, when the process will only load faces that exist\n        in the alignments file. Default: ``None``\n    \"\"\"\n    def __init__(self, folder: str, alignments: Alignments | None = None) -> None:\n        self._alignments = alignments\n        super().__init__(folder)\n\n    def _handle_legacy(self, fullpath: str, log: bool = False) -> PNGHeaderDict:\n        \"\"\"Handle facesets that are legacy (i.e. do not contain alignment information in the\n        header data)\n\n        Parameters\n        ----------\n        fullpath : str\n            The full path to the extracted face image\n        log : bool, optional\n            Whether to log a message that legacy updating is occurring\n\n        Returns\n        -------\n        :class:`~lib.align.alignments.PNGHeaderDict`\n            The Alignments information from the face in PNG Header dict format\n\n        Raises\n        ------\n        FaceswapError\n            If legacy faces can't be updated because the alignments file does not exist or some of\n            the faces do not appear in the provided alignments file\n        \"\"\"\n        if self._alignments is None:  # Can't update legacy\n            raise FaceswapError(f\"The folder '{self.folder}' contains images that do not include \"\n                                \"Faceswap metadata.\\nAll images in the provided folder should \"\n                                \"contain faces generated from Faceswap's extraction process.\\n\"\n                                \"Please double check the source and try again.\")\n        if log:\n            logger.warning(\"Legacy faces discovered. These faces will be updated\")\n\n        data = update_legacy_png_header(fullpath, self._alignments)\n        if not data:\n            raise FaceswapError(\n                f\"Some of the faces being passed in from '{self.folder}' could not be \"\n                f\"matched to the alignments file '{self._alignments.file}'\\nPlease double \"\n                \"check your sources and try again.\")\n        return data\n\n    def _handle_duplicate(self,\n                          fullpath: str,\n                          header_dict: PNGHeaderDict,\n                          seen: dict[str, list[int]]) -> bool:\n        \"\"\" Check whether the given face has already been seen for the source frame and face index\n        from an existing face. Can happen when filenames have changed due to sorting etc. and users\n        have done multiple extractions/copies and placed all of the faces in the same folder\n\n        Parameters\n        ----------\n        fullpath : str\n            The full path to the face image that is being checked\n        header_dict : class:`~lib.align.alignments.PNGHeaderDict`\n            The PNG header dictionary for the given face\n        seen : dict[str, list[int]]\n            Dictionary of original source filename and face indices that have already been seen and\n            will be updated with the face processing now\n\n        Returns\n        -------\n        bool\n            ``True`` if the face was a duplicate and has been removed, otherwise ``False``\n        \"\"\"\n        src_filename = header_dict[\"source\"][\"source_filename\"]\n        face_index = header_dict[\"source\"][\"face_index\"]\n\n        if src_filename in seen and face_index in seen[src_filename]:\n            dupe_dir = os.path.join(self.folder, \"_duplicates\")\n            os.makedirs(dupe_dir, exist_ok=True)\n            filename = os.path.basename(fullpath)\n            logger.trace(\"Moving duplicate: %s\", filename)  # type:ignore\n            os.rename(fullpath, os.path.join(dupe_dir, filename))\n            return True\n\n        seen.setdefault(src_filename, []).append(face_index)\n        return False\n\n    def process_folder(self) -> Generator[tuple[str, PNGHeaderDict], None, None]:\n        \"\"\" Iterate through the faces folder pulling out various information for each face.\n\n        Yields\n        ------\n        dict\n            A dictionary for each face found containing the keys returned from\n            :class:`lib.image.read_image_meta_batch`\n        \"\"\"\n        logger.info(\"Loading file list from %s\", self.folder)\n        filter_count = 0\n        dupe_count = 0\n        seen: dict[str, list[int]] = {}\n\n        if self._alignments is not None and self._alignments.version < 2.1:  # Legacy updating\n            filelist = [os.path.join(self.folder, face)\n                        for face in os.listdir(self.folder)\n                        if self.valid_extension(face)]\n        else:\n            filelist = [os.path.join(self.folder, face)\n                        for face in os.listdir(self.folder)\n                        if os.path.splitext(face)[-1] == \".png\"]\n\n        log_once = False\n        for fullpath, metadata in tqdm(read_image_meta_batch(filelist),\n                                       total=len(filelist),\n                                       desc=\"Reading Face Data\"):\n\n            if \"itxt\" not in metadata or \"source\" not in metadata[\"itxt\"]:\n                sub_dict = self._handle_legacy(fullpath, not log_once)\n                log_once = True\n            else:\n                sub_dict = T.cast(\"PNGHeaderDict\", metadata[\"itxt\"])\n\n            if self._handle_duplicate(fullpath, sub_dict, seen):\n                dupe_count += 1\n                continue\n\n            if (self._alignments is not None and  # filter existing\n                    not self._alignments.frame_exists(sub_dict[\"source\"][\"source_filename\"])):\n                filter_count += 1\n                continue\n\n            retval = (os.path.basename(fullpath), sub_dict)\n            yield retval\n\n        if self._alignments is not None:\n            logger.debug(\"Faces filtered out that did not exist in alignments file: %s\",\n                         filter_count)\n\n        if dupe_count > 0:\n            logger.warning(\"%s Duplicate face images were found. These files have been moved to \"\n                           \"'%s' from where they can be safely deleted\",\n                           dupe_count, os.path.join(self.folder, \"_duplicates\"))\n\n    def load_items(self) -> dict[str, list[int]]:\n        \"\"\" Load the face names into dictionary.\n\n        Returns\n        -------\n        dict\n            The source filename as key with list of face indices for the frame as value\n        \"\"\"\n        faces: dict[str, list[int]] = {}\n        for face in T.cast(list[tuple[str, \"PNGHeaderDict\"]], self.file_list_sorted):\n            src = face[1][\"source\"]\n            faces.setdefault(src[\"source_filename\"], []).append(src[\"face_index\"])\n        logger.trace(faces)  # type: ignore\n        return faces\n\n    def sorted_items(self) -> list[tuple[str, PNGHeaderDict]]:\n        \"\"\" Return the items sorted by the saved file name.\n\n        Returns\n        --------\n        list\n            List of `dict` objects for each face found, sorted by the face's current filename\n        \"\"\"\n        items = sorted(self.process_folder(), key=itemgetter(0))\n        logger.trace(items)  # type: ignore\n        return items\n\n\nclass Frames(MediaLoader):\n    \"\"\" Object to hold the frames that are to be checked against \"\"\"\n\n    def process_folder(self) -> Generator[dict[str, str], None, None]:\n        \"\"\" Iterate through the frames folder pulling the base filename\n\n        Yields\n        ------\n        dict\n            The full framename, the filename and the file extension of the frame\n        \"\"\"\n        iterator = self.process_video if self.is_video else self.process_frames\n        for item in iterator():\n            yield item\n\n    def process_frames(self) -> Generator[dict[str, str], None, None]:\n        \"\"\" Process exported Frames\n\n        Yields\n        ------\n        dict\n            The full framename, the filename and the file extension of the frame\n        \"\"\"\n        logger.info(\"Loading file list from %s\", self.folder)\n        for frame in os.listdir(self.folder):\n            if not self.valid_extension(frame):\n                continue\n            filename = os.path.splitext(frame)[0]\n            file_extension = os.path.splitext(frame)[1]\n\n            retval = {\"frame_fullname\": frame,\n                      \"frame_name\": filename,\n                      \"frame_extension\": file_extension}\n            logger.trace(retval)  # type: ignore\n            yield retval\n\n    def process_video(self) -> Generator[dict[str, str], None, None]:\n        \"\"\"Dummy in frames for video\n\n        Yields\n        ------\n        dict\n            The full framename, the filename and the file extension of the frame\n        \"\"\"\n        logger.info(\"Loading video frames from %s\", self.folder)\n        vidname, ext = os.path.splitext(os.path.basename(self.folder))\n        for i in range(self.count):\n            idx = i + 1\n            # Keep filename format for outputted face\n            filename = f\"{vidname}_{idx:06d}\"\n            retval = {\"frame_fullname\": f\"{filename}{ext}\",\n                      \"frame_name\": filename,\n                      \"frame_extension\": ext}\n            logger.trace(retval)  # type: ignore\n            yield retval\n\n    def load_items(self) -> dict[str, tuple[str, str]]:\n        \"\"\" Load the frame info into dictionary\n\n        Returns\n        -------\n        dict\n            Fullname as key, tuple of frame name and extension as value\n        \"\"\"\n        frames: dict[str, tuple[str, str]] = {}\n        for frame in T.cast(list[dict[str, str]], self.file_list_sorted):\n            frames[frame[\"frame_fullname\"]] = (frame[\"frame_name\"],\n                                               frame[\"frame_extension\"])\n        logger.trace(frames)  # type: ignore\n        return frames\n\n    def sorted_items(self) -> list[dict[str, str]]:\n        \"\"\" Return the items sorted by filename\n\n        Returns\n        -------\n        list\n            The sorted list of frame information\n        \"\"\"\n        items = sorted(self.process_folder(), key=lambda x: (x[\"frame_name\"]))\n        logger.trace(items)  # type: ignore\n        return items\n\n\nclass ExtractedFaces():\n    \"\"\" Holds the extracted faces and matrix for alignments\n\n    Parameters\n    ----------\n    frames: :class:`Frames`\n        The frames object to extract faces from\n    alignments: :class:`AlignmentData`\n        The alignment data corresponding to the frames\n    size: int, optional\n        The extract face size. Default: 512\n    \"\"\"\n    def __init__(self, frames: Frames, alignments: AlignmentData, size: int = 512) -> None:\n        logger.trace(\"Initializing %s: size: %s\",  # type: ignore\n                     self.__class__.__name__, size)\n        self.size = size\n        self.padding = int(size * 0.1875)\n        self.alignments = alignments\n        self.frames = frames\n        self.current_frame: str | None = None\n        self.faces: list[DetectedFace] = []\n        logger.trace(\"Initialized %s\", self.__class__.__name__)  # type: ignore\n\n    def get_faces(self, frame: str, image: np.ndarray | None = None) -> None:\n        \"\"\" Obtain faces and transformed landmarks for each face in a given frame with its\n        alignments\n\n        Parameters\n        ----------\n        frame: str\n            The frame name to obtain faces for\n        image: :class:`numpy.ndarray`, optional\n            The image to extract the face from, if we already have it, otherwise ``None`` to\n            load the image. Default: ``None``\n        \"\"\"\n        logger.trace(\"Getting faces for frame: '%s'\", frame)  # type: ignore\n        self.current_frame = None\n        alignments = self.alignments.get_faces_in_frame(frame)\n        logger.trace(\"Alignments for frame: (frame: '%s', alignments: %s)\",  # type: ignore\n                     frame, alignments)\n        if not alignments:\n            self.faces = []\n            return\n        image = self.frames.load_image(frame) if image is None else image\n        self.faces = [self.extract_one_face(alignment, image) for alignment in alignments]\n        self.current_frame = frame\n\n    def extract_one_face(self,\n                         alignment: AlignmentFileDict,\n                         image: np.ndarray) -> DetectedFace:\n        \"\"\" Extract one face from image\n\n        Parameters\n        ----------\n        alignment: dict\n            The alignment for a single face\n        image: :class:`numpy.ndarray`\n            The image to extract the face from\n\n        Returns\n        -------\n        :class:`~lib.align.DetectedFace`\n            The detected face object for the given alignment with the aligned face loaded\n        \"\"\"\n        logger.trace(\"Extracting one face: (frame: '%s', alignment: %s)\",  # type: ignore\n                     self.current_frame, alignment)\n        face = DetectedFace()\n        face.from_alignment(alignment, image=image)\n        face.load_aligned(image, size=self.size, centering=\"head\")\n        face.thumbnail = generate_thumbnail(face.aligned.face, size=80, quality=60)\n        return face\n\n    def get_faces_in_frame(self,\n                           frame: str,\n                           update: bool = False,\n                           image: np.ndarray | None = None) -> list[DetectedFace]:\n        \"\"\" Return the faces for the selected frame\n\n        Parameters\n        ----------\n        frame: str\n            The frame name to get the faces for\n        update: bool, optional\n            ``True`` if the faces should be refreshed regardless of current frame. ``False`` to not\n            force a refresh. Default ``False``\n        image: :class:`numpy.ndarray`, optional\n            Image to load faces from if it exists, otherwise ``None`` to load the image.\n            Default: ``None``\n\n        Returns\n        -------\n        list\n            List of :class:`~lib.align.DetectedFace` objects for the frame, with the aligned face\n            loaded\n        \"\"\"\n        logger.trace(\"frame: '%s', update: %s\", frame, update)  # type: ignore\n        if self.current_frame != frame or update:\n            self.get_faces(frame, image=image)\n        return self.faces\n\n    def get_roi_size_for_frame(self, frame: str) -> list[int]:\n        \"\"\" Return the size of the original extract box for the selected frame.\n\n        Parameters\n        ----------\n        frame: str\n            The frame to obtain the original sized bounding boxes for\n\n        Returns\n        -------\n        list\n            List of original pixel sizes of faces held within the frame\n        \"\"\"\n        logger.trace(\"frame: '%s'\", frame)  # type: ignore\n        if self.current_frame != frame:\n            self.get_faces(frame)\n        sizes = []\n        for face in self.faces:\n            roi = face.aligned.original_roi.squeeze()\n            top_left, top_right = roi[0], roi[3]\n            len_x = top_right[0] - top_left[0]\n            len_y = top_right[1] - top_left[1]\n            if top_left[1] == top_right[1]:\n                length = len_y\n            else:\n                length = int(((len_x ** 2) + (len_y ** 2)) ** 0.5)\n            sizes.append(length)\n        logger.trace(\"sizes: '%s'\", sizes)  # type: ignore\n        return sizes\n", "tools/alignments/__init__.py": "", "tools/mask/mask_generate.py": "#!/usr/bin/env python3\n\"\"\" Handles the generation of masks from faceswap for upating into an alignments file \"\"\"\nfrom __future__ import annotations\n\nimport logging\nimport os\nimport typing as T\n\nfrom lib.image import encode_image, ImagesSaver\nfrom lib.multithreading import MultiThread\nfrom plugins.extract import Extractor\n\nif T.TYPE_CHECKING:\n    from lib.align import Alignments, DetectedFace\n    from lib.align.alignments import PNGHeaderDict\n    from lib.queue_manager import EventQueue\n    from plugins.extract import ExtractMedia\n    from .loader import Loader\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass MaskGenerator:\n    \"\"\" Uses faceswap's extract pipeline to generate masks and update them into the alignments file\n    and/or extracted face PNG Headers\n\n    Parameters\n    ----------\n    mask_type: str\n        The mask type to generate\n    update_all: bool\n        ``True`` to update all faces, ``False`` to only update faces missing masks\n    input_is_faces: bool\n        ``True`` if the input are faceswap extracted faces otherwise ``False``\n    exclude_gpus: list[int]\n        List of any GPU IDs that should be excluded\n    loader: :class:`tools.mask.loader.Loader`\n        The loader for loading source images/video from disk\n    \"\"\"\n    def __init__(self,\n                 mask_type: str,\n                 update_all: bool,\n                 input_is_faces: bool,\n                 loader: Loader,\n                 alignments: Alignments | None,\n                 input_location: str,\n                 exclude_gpus: list[int]) -> None:\n        logger.debug(\"Initializing %s (mask_type: %s, update_all: %s, input_is_faces: %s, \"\n                     \"loader: %s, alignments: %s, input_location: %s, exclude_gpus: %s)\",\n                     self.__class__.__name__, mask_type, update_all, input_is_faces, loader,\n                     alignments, input_location, exclude_gpus)\n\n        self._update_all = update_all\n        self._is_faces = input_is_faces\n        self._alignments = alignments\n\n        self._extractor = self._get_extractor(mask_type, exclude_gpus)\n        self._mask_type = self._set_correct_mask_type(mask_type)\n        self._input_thread = self._set_loader_thread(loader)\n        self._saver = ImagesSaver(input_location, as_bytes=True) if input_is_faces else None\n\n        self._counts: dict[T.Literal[\"face\", \"update\"], int] = {\"face\": 0, \"update\": 0}\n\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    def _get_extractor(self, mask_type, exclude_gpus: list[int]) -> Extractor:\n        \"\"\" Obtain a Mask extractor plugin and launch it\n\n        Parameters\n        ----------\n        mask_type: str\n            The mask type to generate\n        exclude_gpus: list or ``None``\n            A list of indices correlating to connected GPUs that Tensorflow should not use. Pass\n            ``None`` to not exclude any GPUs.\n\n        Returns\n        -------\n        :class:`plugins.extract.pipeline.Extractor`:\n            The launched Extractor\n        \"\"\"\n        logger.debug(\"masker: %s\", mask_type)\n        extractor = Extractor(None, None, mask_type, exclude_gpus=exclude_gpus)\n        extractor.launch()\n        logger.debug(extractor)\n        return extractor\n\n    def _set_correct_mask_type(self, mask_type: str) -> str:\n        \"\"\" Some masks have multiple variants that they can be saved as depending on config options\n\n        Parameters\n        ----------\n        mask_type: str\n            The mask type to generate\n\n        Returns\n        -------\n        str\n            The actual mask variant to update\n        \"\"\"\n        if mask_type != \"bisenet-fp\":\n            return mask_type\n\n        # Hacky look up into masker to get the type of mask\n        mask_plugin = self._extractor._mask[0]  # pylint:disable=protected-access\n        assert mask_plugin is not None\n        mtype = \"head\" if mask_plugin.config.get(\"include_hair\", False) else \"face\"\n        new_type = f\"{mask_type}_{mtype}\"\n        logger.debug(\"Updating '%s' to '%s'\", mask_type, new_type)\n        return new_type\n\n    def _needs_update(self, frame: str, idx: int, face: DetectedFace) -> bool:\n        \"\"\" Check if the mask for the current alignment needs updating for the requested mask_type\n\n        Parameters\n        ----------\n        frame: str\n            The frame name in the alignments file\n        idx: int\n            The index of the face for this frame in the alignments file\n        face: :class:`~lib.align.DetectedFace`\n            The dected face object to check\n\n        Returns\n        -------\n        bool:\n            ``True`` if the mask needs to be updated otherwise ``False``\n        \"\"\"\n        if self._update_all:\n            return True\n\n        retval = not face.mask or face.mask.get(self._mask_type, None) is None\n\n        logger.trace(\"Needs updating: %s, '%s' - %s\",  # type:ignore[attr-defined]\n                     retval, frame, idx)\n        return retval\n\n    def _feed_extractor(self, loader: Loader, extract_queue: EventQueue) -> None:\n        \"\"\" Process to feed the extractor from inside a thread\n\n        Parameters\n        ----------\n        loader: class:`tools.mask.loader.Loader`\n            The loader for loading source images/video from disk\n        extract_queue: :class:`lib.queue_manager.EventQueue`\n            The input queue to the extraction pipeline\n        \"\"\"\n        for media in loader.load():\n            self._counts[\"face\"] += len(media.detected_faces)\n\n            if self._is_faces:\n                assert len(media.detected_faces) == 1\n                needs_update = self._needs_update(media.frame_metadata[\"source_filename\"],\n                                                  media.frame_metadata[\"face_index\"],\n                                                  media.detected_faces[0])\n            else:\n                # To keep face indexes correct/cover off where only one face in an image is missing\n                # a mask where there are multiple faces we process all faces again for any frames\n                # which have missing masks.\n                needs_update = any(self._needs_update(media.filename, idx, detected_face)\n                                   for idx, detected_face in enumerate(media.detected_faces))\n\n            if not needs_update:\n                logger.trace(\"No masks need updating in '%s'\",  # type:ignore[attr-defined]\n                             media.filename)\n                continue\n\n            logger.trace(\"Passing to extractor: '%s'\", media.filename)  # type:ignore[attr-defined]\n            extract_queue.put(media)\n\n        logger.debug(\"Terminating loader thread\")\n        extract_queue.put(\"EOF\")\n\n    def _set_loader_thread(self, loader: Loader) -> MultiThread:\n        \"\"\" Set the iterator to load ExtractMedia objects into the mask extraction pipeline\n        so we can just iterate through the output masks\n\n        Parameters\n        ----------\n        loader: class:`tools.mask.loader.Loader`\n            The loader for loading source images/video from disk\n        \"\"\"\n        in_queue = self._extractor.input_queue\n        logger.debug(\"Starting load thread: (loader: %s, queue: %s)\", loader, in_queue)\n        in_thread = MultiThread(self._feed_extractor, loader, in_queue, thread_count=1)\n        in_thread.start()\n        logger.debug(\"Started load thread: %s\", in_thread)\n        return in_thread\n\n    def _update_from_face(self, media: ExtractMedia) -> None:\n        \"\"\" Update the alignments file and/or the extracted face\n\n        Parameters\n        ----------\n        media: :class:`~lib.extract.pipeline.ExtractMedia`\n            The ExtractMedia object with updated masks\n        \"\"\"\n        assert len(media.detected_faces) == 1\n        assert self._saver is not None\n\n        fname = media.frame_metadata[\"source_filename\"]\n        idx = media.frame_metadata[\"face_index\"]\n        face = media.detected_faces[0]\n\n        if self._alignments is not None:\n            logger.trace(\"Updating face %s in frame '%s'\", idx, fname)  # type:ignore[attr-defined]\n            self._alignments.update_face(fname, idx, face.to_alignment())\n\n        logger.trace(\"Updating extracted face: '%s'\", media.filename)  # type:ignore[attr-defined]\n        meta: PNGHeaderDict = {\"alignments\": face.to_png_meta(), \"source\": media.frame_metadata}\n        self._saver.save(media.filename, encode_image(media.image, \".png\", metadata=meta))\n\n    def _update_from_frame(self, media: ExtractMedia) -> None:\n        \"\"\" Update the alignments file\n\n        Parameters\n        ----------\n        media: :class:`~lib.extract.pipeline.ExtractMedia`\n            The ExtractMedia object with updated masks\n        \"\"\"\n        assert self._alignments is not None\n        fname = os.path.basename(media.filename)\n        logger.trace(\"Updating %s faces in frame '%s'\",  # type:ignore[attr-defined]\n                     len(media.detected_faces), fname)\n        for idx, face in enumerate(media.detected_faces):\n            self._alignments.update_face(fname, idx, face.to_alignment())\n\n    def _finalize(self) -> None:\n        \"\"\" Close thread and save alignments on completion \"\"\"\n        logger.debug(\"Finalizing MaskGenerator\")\n        self._input_thread.join()\n\n        if self._counts[\"update\"] > 0 and self._alignments is not None:\n            logger.debug(\"Saving alignments\")\n            self._alignments.backup()\n            self._alignments.save()\n\n        if self._saver is not None:\n            logger.debug(\"Closing face saver\")\n            self._saver.close()\n\n        if self._counts[\"update\"] == 0:\n            logger.warning(\"No masks were updated of the %s faces seen\", self._counts[\"face\"])\n        else:\n            logger.info(\"Updated masks for %s faces of %s\",\n                        self._counts[\"update\"], self._counts[\"face\"])\n\n    def process(self) -> T.Generator[ExtractMedia, None, None]:\n        \"\"\" Process the output from the extractor pipeline\n\n        Yields\n        ------\n        :class:`~lib.extract.pipeline.ExtractMedia`\n            The ExtractMedia object with updated masks\n        \"\"\"\n        for media in self._extractor.detected_faces():\n            self._input_thread.check_and_raise_error()\n            self._counts[\"update\"] += len(media.detected_faces)\n\n            if self._is_faces:\n                self._update_from_face(media)\n            else:\n                self._update_from_frame(media)\n\n            yield media\n\n        self._finalize()\n        logger.debug(\"Completed MaskGenerator process\")\n", "tools/mask/cli.py": "#!/usr/bin/env python3\n\"\"\" Command Line Arguments for tools \"\"\"\nimport argparse\nimport gettext\n\nfrom lib.cli.args import FaceSwapArgs\nfrom lib.cli.actions import (DirOrFileFullPaths, DirFullPaths, FileFullPaths, Radio, Slider)\nfrom plugins.plugin_loader import PluginLoader\n\n\n# LOCALES\n_LANG = gettext.translation(\"tools.mask.cli\", localedir=\"locales\", fallback=True)\n_ = _LANG.gettext\n\n_HELPTEXT = _(\"This tool allows you to generate, import, export or preview masks for existing \"\n              \"alignments.\")\n\n\nclass MaskArgs(FaceSwapArgs):\n    \"\"\" Class to parse the command line arguments for Mask tool \"\"\"\n\n    @staticmethod\n    def get_info():\n        \"\"\" Return command information \"\"\"\n        return _(\"Mask tool\\nGenerate, import, export or preview masks for existing alignments \"\n                 \"files.\")\n\n    @staticmethod\n    def get_argument_list():\n        argument_list = []\n        argument_list.append({\n            \"opts\": (\"-a\", \"--alignments\"),\n            \"action\": FileFullPaths,\n            \"type\": str,\n            \"group\": _(\"data\"),\n            \"required\": False,\n            \"filetypes\": \"alignments\",\n            \"help\": _(\n                \"Full path to the alignments file that contains the masks if not at the \"\n                \"default location. NB: If the input-type is faces and you wish to update the \"\n                \"corresponding alignments file, then you must provide a value here as the \"\n                \"location cannot be automatically detected.\")})\n        argument_list.append({\n            \"opts\": (\"-i\", \"--input\"),\n            \"action\": DirOrFileFullPaths,\n            \"type\": str,\n            \"group\": _(\"data\"),\n            \"filetypes\": \"video\",\n            \"required\": True,\n            \"help\": _(\n                \"Directory containing extracted faces, source frames, or a video file.\")})\n        argument_list.append({\n            \"opts\": (\"-I\", \"--input-type\"),\n            \"action\": Radio,\n            \"type\": str.lower,\n            \"choices\": (\"faces\", \"frames\"),\n            \"dest\": \"input_type\",\n            \"group\": _(\"data\"),\n            \"default\": \"frames\",\n            \"help\": _(\n                \"R|Whether the `input` is a folder of faces or a folder frames/video\"\n                \"\\nL|faces: The input is a folder containing extracted faces.\"\n                \"\\nL|frames: The input is a folder containing frames or is a video\")})\n        argument_list.append({\n            \"opts\": (\"-B\", \"--batch-mode\"),\n            \"action\": \"store_true\",\n            \"dest\": \"batch_mode\",\n            \"default\": False,\n            \"group\": _(\"data\"),\n            \"help\": _(\n                \"R|Run the mask tool on multiple sources. If selected then the other options \"\n                \"should be set as follows:\"\n                \"\\nL|input: A parent folder containing either all of the video files to be \"\n                \"processed, or containing sub-folders of frames/faces.\"\n                \"\\nL|output-folder: If provided, then sub-folders will be created within the \"\n                \"given location to hold the previews for each input.\"\n                \"\\nL|alignments: Alignments field will be ignored for batch processing. The \"\n                \"alignments files must exist at the default location (for frames). For batch \"\n                \"processing of masks with 'faces' as the input type, then only the PNG header \"\n                \"within the extracted faces will be updated.\")})\n        argument_list.append({\n            \"opts\": (\"-M\", \"--masker\"),\n            \"action\": Radio,\n            \"type\": str.lower,\n            \"choices\": PluginLoader.get_available_extractors(\"mask\"),\n            \"default\": \"extended\",\n            \"group\": _(\"process\"),\n            \"help\": _(\n                \"R|Masker to use.\"\n                \"\\nL|bisenet-fp: Relatively lightweight NN based mask that provides more \"\n                \"refined control over the area to be masked including full head masking \"\n                \"(configurable in mask settings).\"\n                \"\\nL|components: Mask designed to provide facial segmentation based on the \"\n                \"positioning of landmark locations. A convex hull is constructed around the \"\n                \"exterior of the landmarks to create a mask.\"\n                \"\\nL|custom: A dummy mask that fills the mask area with all 1s or 0s \"\n                \"(configurable in settings). This is only required if you intend to manually \"\n                \"edit the custom masks yourself in the manual tool. This mask does not use the \"\n                \"GPU.\"\n                \"\\nL|extended: Mask designed to provide facial segmentation based on the \"\n                \"positioning of landmark locations. A convex hull is constructed around the \"\n                \"exterior of the landmarks and the mask is extended upwards onto the forehead.\"\n                \"\\nL|vgg-clear: Mask designed to provide smart segmentation of mostly frontal \"\n                \"faces clear of obstructions. Profile faces and obstructions may result in \"\n                \"sub-par performance.\"\n                \"\\nL|vgg-obstructed: Mask designed to provide smart segmentation of mostly \"\n                \"frontal faces. The mask model has been specifically trained to recognize \"\n                \"some facial obstructions (hands and eyeglasses). Profile faces may result in \"\n                \"sub-par performance.\"\n                \"\\nL|unet-dfl: Mask designed to provide smart segmentation of mostly frontal \"\n                \"faces. The mask model has been trained by community members. Profile faces \"\n                \"may result in sub-par performance.\")})\n        argument_list.append({\n            \"opts\": (\"-p\", \"--processing\"),\n            \"action\": Radio,\n            \"type\": str.lower,\n            \"choices\": (\"all\", \"missing\", \"output\", \"import\"),\n            \"default\": \"all\",\n            \"group\": _(\"process\"),\n            \"help\": _(\n                \"R|The Mask tool process to perform.\"\n                \"\\nL|all: Update the mask for all faces in the alignments file for the selected \"\n                \"'masker'.\"\n                \"\\nL|missing: Create a mask for all faces in the alignments file where a mask \"\n                \"does not previously exist for the selected 'masker'.\"\n                \"\\nL|output: Don't update the masks, just output the selected 'masker' for \"\n                \"review/editing in external tools to the given output folder.\"\n                \"\\nL|import: Import masks that have been edited outside of faceswap into the \"\n                \"alignments file. Note: 'custom' must be the selected 'masker' and the masks must \"\n                \"be in the same format as the 'input-type' (frames or faces)\")})\n        argument_list.append({\n            \"opts\": (\"-m\", \"--mask-path\"),\n            \"action\": DirFullPaths,\n            \"type\": str,\n            \"group\": _(\"import\"),\n            \"help\": _(\n                \"R|Import only. The path to the folder that contains masks to be imported.\"\n                \"\\nL|How the masks are provided is not important, but they will be stored, \"\n                \"internally, as 8-bit grayscale images.\"\n                \"\\nL|If the input are images, then the masks must be named exactly the same as \"\n                \"input frames/faces (excluding the file extension).\"\n                \"\\nL|If the input is a video file, then the filename of the masks is not \"\n                \"important but should contain the frame number at the end of the filename (but \"\n                \"before the file extension). The frame number can be separated from the rest of \"\n                \"the filename by any non-numeric character and can be padded by any number of \"\n                \"zeros. The frame number must correspond correctly to the frame number in the \"\n                \"original video (starting from frame 1).\")})\n        argument_list.append({\n            \"opts\": (\"-c\", \"--centering\"),\n            \"action\": Radio,\n            \"type\": str.lower,\n            \"choices\": (\"face\", \"head\", \"legacy\"),\n            \"default\": \"face\",\n            \"group\": _(\"import\"),\n            \"help\": _(\n                \"R|Import only. The centering to use when importing masks. Note: For any job \"\n                \"other than 'import' this option is ignored as mask centering is handled \"\n                \"internally.\"\n                \"\\nL|face: Centers the mask on the center of the face, adjusting for \"\n                \"pitch and yaw. Outside of requirements for full head masking/training, this \"\n                \"is likely to be the best choice.\"\n                \"\\nL|head: Centers the mask on the center of the head, adjusting for \"\n                \"pitch and yaw. Note: You should only select head centering if you intend to \"\n                \"include the full head (including hair) within the mask and are looking to \"\n                \"train a full head model.\"\n                \"\\nL|legacy: The 'original' extraction technique. Centers the mask near the \"\n                \" of the nose with and crops closely to the face. Can result in the edges of \"\n                \"the mask appearing outside of the training area.\")})\n        argument_list.append({\n            \"opts\": (\"-s\", \"--storage-size\"),\n            \"dest\": \"storage_size\",\n            \"action\": Slider,\n            \"type\": int,\n            \"group\": _(\"import\"),\n            \"min_max\": (64, 1024),\n            \"default\": 128,\n            \"rounding\": 64,\n            \"help\": _(\n                \"Import only. The size, in pixels to internally store the mask at.\\nThe default \"\n                \"is 128 which is fine for nearly all usecases. Larger sizes will result in larger \"\n                \"alignments files and longer processing.\")})\n        argument_list.append({\n            \"opts\": (\"-o\", \"--output-folder\"),\n            \"action\": DirFullPaths,\n            \"dest\": \"output\",\n            \"type\": str,\n            \"group\": _(\"output\"),\n            \"help\": _(\n                \"Optional output location. If provided, a preview of the masks created will \"\n                \"be output in the given folder.\")})\n        argument_list.append({\n            \"opts\": (\"-b\", \"--blur_kernel\"),\n            \"action\": Slider,\n            \"type\": int,\n            \"group\": _(\"output\"),\n            \"min_max\": (0, 9),\n            \"default\": 0,\n            \"rounding\": 1,\n            \"help\": _(\n                \"Apply gaussian blur to the mask output. Has the effect of smoothing the \"\n                \"edges of the mask giving less of a hard edge. the size is in pixels. This \"\n                \"value should be odd, if an even number is passed in then it will be rounded \"\n                \"to the next odd number. NB: Only effects the output preview. Set to 0 for \"\n                \"off\")})\n        argument_list.append({\n            \"opts\": (\"-t\", \"--threshold\"),\n            \"action\": Slider,\n            \"type\": int,\n            \"group\": _(\"output\"),\n            \"min_max\": (0, 50),\n            \"default\": 0,\n            \"rounding\": 1,\n            \"help\": _(\n                \"Helps reduce 'blotchiness' on some masks by making light shades white \"\n                \"and dark shades black. Higher values will impact more of the mask. NB: \"\n                \"Only effects the output preview. Set to 0 for off\")})\n        argument_list.append({\n            \"opts\": (\"-O\", \"--output-type\"),\n            \"action\": Radio,\n            \"type\": str.lower,\n            \"choices\": (\"combined\", \"masked\", \"mask\"),\n            \"default\": \"combined\",\n            \"group\": _(\"output\"),\n            \"help\": _(\n                \"R|How to format the output when processing is set to 'output'.\"\n                \"\\nL|combined: The image contains the face/frame, face mask and masked face.\"\n                \"\\nL|masked: Output the face/frame as rgba image with the face masked.\"\n                \"\\nL|mask: Only output the mask as a single channel image.\")})\n        argument_list.append({\n            \"opts\": (\"-f\", \"--full-frame\"),\n            \"action\": \"store_true\",\n            \"default\": False,\n            \"group\": _(\"output\"),\n            \"help\": _(\n                \"R|Whether to output the whole frame or only the face box when using \"\n                \"output processing. Only has an effect when using frames as input.\")})\n        # Deprecated multi-character switches\n        argument_list.append({\n            \"opts\": (\"-it\", ),\n            \"type\": str,\n            \"dest\": \"depr_input-type_it_I\",\n            \"help\": argparse.SUPPRESS})\n        return argument_list\n", "tools/mask/mask_import.py": "#!/usr/bin/env python3\n\"\"\" Import mask processing for faceswap's mask tool \"\"\"\nfrom __future__ import annotations\n\nimport logging\nimport os\nimport re\nimport sys\nimport typing as T\n\nimport cv2\nfrom tqdm import tqdm\n\nfrom lib.align import AlignedFace\nfrom lib.image import encode_image, ImagesSaver\nfrom lib.utils import get_image_paths\n\nif T.TYPE_CHECKING:\n    import numpy as np\n    from .loader import Loader\n    from plugins.extract import ExtractMedia\n    from lib.align import Alignments, DetectedFace\n    from lib.align.alignments import PNGHeaderDict\n    from lib.align.aligned_face import CenteringType\n\nlogger = logging.getLogger(__name__)\n\n\nclass Import:\n    \"\"\" Import masks from disk into an Alignments file\n\n    Parameters\n    ----------\n    import_path: str\n        The path to the input images\n    centering: Literal[\"face\", \"head\", \"legacy\"]\n        The centering to store the mask at\n    storage_size: int\n        The size to store the mask at\n    input_is_faces: bool\n        ``True`` if the input is aligned faces otherwise ``False``\n    loader: :class:`~tools.mask.loader.Loader`\n        The source file loader object\n    alignments: :class:`~lib.align.alignments.Alignments` | None\n        The alignments file object for the faces, if provided\n    mask_type: str\n        The mask type to update to\n    \"\"\"\n    def __init__(self,\n                 import_path: str,\n                 centering: CenteringType,\n                 storage_size: int,\n                 input_is_faces: bool,\n                 loader: Loader,\n                 alignments: Alignments | None,\n                 input_location: str,\n                 mask_type: str) -> None:\n        logger.debug(\"Initializing %s (import_path: %s, centering: %s, storage_size: %s, \"\n                     \"input_is_faces: %s, loader: %s, alignments: %s, input_location: %s, \"\n                     \"mask_type: %s)\", self.__class__.__name__, import_path, centering,\n                     storage_size, input_is_faces, loader, alignments, input_location, mask_type)\n\n        self._validate_mask_type(mask_type)\n\n        self._centering = centering\n        self._size = storage_size\n        self._is_faces = input_is_faces\n        self._alignments = alignments\n        self._re_frame_num = re.compile(r\"\\d+$\")\n        self._mapping = self._generate_mapping(import_path, loader)\n\n        self._saver = ImagesSaver(input_location, as_bytes=True) if input_is_faces else None\n        self._counts: dict[T.Literal[\"skip\", \"update\"], int] = {\"skip\": 0, \"update\": 0}\n\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    @property\n    def skip_count(self) -> int:\n        \"\"\" int: Number of masks that were skipped as they do not exist for given faces \"\"\"\n        return self._counts[\"skip\"]\n\n    @property\n    def update_count(self) -> int:\n        \"\"\" int: Number of masks that were skipped as they do not exist for given faces \"\"\"\n        return self._counts[\"update\"]\n\n    @classmethod\n    def _validate_mask_type(cls, mask_type: str) -> None:\n        \"\"\" Validate that the mask type is 'custom' to ensure user does not accidentally overwrite\n        existing masks they may have editted\n\n        Parameters\n        ----------\n        mask_type: str\n            The mask type that has been selected\n        \"\"\"\n        if mask_type == \"custom\":\n            return\n\n        logger.error(\"Masker 'custom' must be selected for importing masks\")\n        sys.exit(1)\n\n    @classmethod\n    def _get_file_list(cls, path: str) -> list[str]:\n        \"\"\" Check the nask folder exists and obtain the list of images\n\n        Parameters\n        ----------\n        path: str\n            Full path to the location of mask images to be imported\n\n        Returns\n        -------\n        list[str]\n            list of full paths to all of the images in the mask folder\n        \"\"\"\n        if not os.path.isdir(path):\n            logger.error(\"Mask path: '%s' is not a folder\", path)\n            sys.exit(1)\n        paths = get_image_paths(path)\n        if not paths:\n            logger.error(\"Mask path '%s' contains no images\", path)\n            sys.exit(1)\n        return paths\n\n    def _warn_extra_masks(self, file_list: list[str]) -> None:\n        \"\"\" Generate a warning for each mask that exists that does not correspond to a match in the\n        source input\n\n        Parameters\n        ----------\n        file_list: list[str]\n            List of mask files that could not be mapped to a source image\n        \"\"\"\n        if not file_list:\n            logger.debug(\"All masks exist in the source data\")\n            return\n\n        for fname in file_list:\n            logger.warning(\"Extra mask file found: '%s'\", os.path.basename(fname))\n\n        logger.warning(\"%s mask file(s) do not exist in the source data so will not be imported \"\n                       \"(see above)\", len(file_list))\n\n    def _file_list_to_frame_number(self, file_list: list[str]) -> dict[int, str]:\n        \"\"\" Extract frame numbers from mask file names and return as a dictionary\n\n        Parameters\n        ----------\n        file_list: list[str]\n            List of full paths to masks to extract frame number from\n\n        Returns\n        -------\n        dict[int, str]\n            Dictionary of frame numbers to filenames\n        \"\"\"\n        retval: dict[int, str] = {}\n        for filename in file_list:\n            frame_num = self._re_frame_num.findall(os.path.splitext(os.path.basename(filename))[0])\n\n            if not frame_num or len(frame_num) > 1:\n                logger.error(\"Could not detect frame number from mask file '%s'. \"\n                             \"Check your filenames\", os.path.basename(filename))\n                sys.exit(1)\n\n            fnum = int(frame_num[0])\n\n            if fnum in retval:\n                logger.error(\"Frame number %s for mask file '%s' already exists from file: '%s'. \"\n                             \"Check your filenames\",\n                             fnum, os.path.basename(filename), os.path.basename(retval[fnum]))\n                sys.exit(1)\n\n            retval[fnum] = filename\n\n        logger.debug(\"Files: %s, frame_numbers: %s\", len(file_list), len(retval))\n\n        return retval\n\n    def _map_video(self, file_list: list[str], source_files: list[str]) -> dict[str, str]:\n        \"\"\" Generate the mapping between the source data and the masks to be imported for\n        video sources\n\n        Parameters\n        ----------\n        file_list: list[str]\n            List of full paths to masks to be imported\n        source_files: list[str]\n            list of filenames withing the source file\n\n        Returns\n        -------\n        dict[str, str]\n            Source filenames mapped to full path location of mask to be imported\n        \"\"\"\n        retval = {}\n        unmapped = []\n        mask_frames = self._file_list_to_frame_number(file_list)\n        for filename in tqdm(source_files, desc=\"Mapping masks to input\", leave=False):\n            src_idx = int(os.path.splitext(filename)[0].rsplit(\"_\", maxsplit=1)[-1])\n            mapped = mask_frames.pop(src_idx, \"\")\n            if not mapped:\n                unmapped.append(filename)\n                continue\n            retval[os.path.basename(filename)] = mapped\n\n        if len(unmapped) == len(source_files):\n            logger.error(\"No masks map between the source data and the mask folder. \"\n                         \"Check your filenames\")\n            sys.exit(1)\n\n        self._warn_extra_masks(list(mask_frames.values()))\n        logger.debug(\"Source: %s, Mask: %s, Mapped: %s\",\n                     len(source_files), len(file_list), len(retval))\n        return retval\n\n    def _map_images(self, file_list: list[str], source_files: list[str]) -> dict[str, str]:\n        \"\"\" Generate the mapping between the source data and the masks to be imported for\n        folder of image sources\n\n        Parameters\n        ----------\n        file_list: list[str]\n            List of full paths to masks to be imported\n        source_files: list[str]\n            list of filenames withing the source file\n\n        Returns\n        -------\n        dict[str, str]\n            Source filenames mapped to full path location of mask to be imported\n        \"\"\"\n        mask_count = len(file_list)\n        retval = {}\n        unmapped = []\n        for filename in tqdm(source_files, desc=\"Mapping masks to input\", leave=False):\n            fname = os.path.splitext(os.path.basename(filename))[0]\n            mapped = next((f for f in file_list\n                           if os.path.splitext(os.path.basename(f))[0] == fname), \"\")\n            if not mapped:\n                unmapped.append(filename)\n                continue\n            retval[os.path.basename(filename)] = file_list.pop(file_list.index(mapped))\n\n        if len(unmapped) == len(source_files):\n            logger.error(\"No masks map between the source data and the mask folder. \"\n                         \"Check your filenames\")\n            sys.exit(1)\n\n        self._warn_extra_masks(file_list)\n\n        logger.debug(\"Source: %s, Mask: %s, Mapped: %s\",\n                     len(source_files), mask_count, len(retval))\n        return retval\n\n    def _generate_mapping(self, import_path: str, loader: Loader) -> dict[str, str]:\n        \"\"\" Generate the mapping between the source data and the masks to be imported\n\n        Parameters\n        ----------\n        import_path: str\n            The path to the input images\n        loader: :class:`~tools.mask.loader.Loader`\n            The source file loader object\n\n        Returns\n        -------\n        dict[str, str]\n            Source filenames mapped to full path location of mask to be imported\n        \"\"\"\n        file_list = self._get_file_list(import_path)\n        if loader.is_video:\n            retval = self._map_video(file_list, loader.file_list)\n        else:\n            retval = self._map_images(file_list, loader.file_list)\n\n        return retval\n\n    def _store_mask(self, face: DetectedFace, mask: np.ndarray) -> None:\n        \"\"\" Store the mask to the given DetectedFace object\n\n        Parameters\n        ----------\n        face: :class:`~lib.align.detected_face.DetectedFace`\n            The detected face object to store the mask to\n        mask: :class:`numpy.ndarray`\n            The mask to store\n        \"\"\"\n        aligned = AlignedFace(face.landmarks_xy,\n                              mask[..., None] if self._is_faces else mask,\n                              centering=self._centering,\n                              size=self._size,\n                              is_aligned=self._is_faces,\n                              dtype=\"float32\")\n        assert aligned.face is not None\n        face.add_mask(f\"custom_{self._centering}\",\n                      aligned.face / 255.,\n                      aligned.adjusted_matrix,\n                      aligned.interpolators[1],\n                      storage_size=self._size,\n                      storage_centering=self._centering)\n\n    def _store_mask_face(self, media: ExtractMedia, mask: np.ndarray) -> None:\n        \"\"\" Store the mask when the input is aligned faceswap faces\n\n        Parameters\n        ----------\n        media: :class:`~plugins.extract.extract_media.ExtractMedia`\n            The extract media object containing the face(s) to import the mask for\n\n        mask: :class:`numpy.ndarray`\n            The mask loaded from disk\n        \"\"\"\n        assert self._saver is not None\n        assert len(media.detected_faces) == 1\n\n        logger.trace(\"Adding mask for '%s'\", media.filename)  # type:ignore[attr-defined]\n\n        face = media.detected_faces[0]\n        self._store_mask(face, mask)\n\n        if self._alignments is not None:\n            idx = media.frame_metadata[\"source_filename\"]\n            fname = media.frame_metadata[\"face_index\"]\n            logger.trace(\"Updating face %s in frame '%s'\", idx, fname)  # type:ignore[attr-defined]\n            self._alignments.update_face(idx,\n                                         fname,\n                                         face.to_alignment())\n\n        logger.trace(\"Updating extracted face: '%s'\", media.filename)  # type:ignore[attr-defined]\n        meta: PNGHeaderDict = {\"alignments\": face.to_png_meta(), \"source\": media.frame_metadata}\n        self._saver.save(media.filename, encode_image(media.image, \".png\", metadata=meta))\n\n    @classmethod\n    def _resize_mask(cls, mask: np.ndarray, dims: tuple[int, int]) -> np.ndarray:\n        \"\"\" Resize a mask to the given dimensions\n\n        Parameters\n        ----------\n        mask: :class:`numpy.ndarray`\n            The mask to resize\n        dims: tuple[int, int]\n            The (height, width) target size\n\n        Returns\n        -------\n        :class:`numpy.ndarray`\n            The resized mask, or the original mask if no resizing required\n        \"\"\"\n        if mask.shape[:2] == dims:\n            return mask\n        logger.trace(\"Resizing mask from %s to %s\", mask.shape, dims)  # type:ignore[attr-defined]\n        interp = cv2.INTER_AREA if mask.shape[0] > dims[0] else cv2.INTER_CUBIC\n\n        mask = cv2.resize(mask, tuple(reversed(dims)), interpolation=interp)\n        return mask\n\n    def _store_mask_frame(self, media: ExtractMedia, mask: np.ndarray) -> None:\n        \"\"\" Store the mask when the input is frames\n\n        Parameters\n        ----------\n        media: :class:`~plugins.extract.extract_media.ExtractMedia`\n            The extract media object containing the face(s) to import the mask for\n\n        mask: :class:`numpy.ndarray`\n            The mask loaded from disk\n        \"\"\"\n        assert self._alignments is not None\n        logger.trace(\"Adding %s mask(s) for '%s'\",  # type:ignore[attr-defined]\n                     len(media.detected_faces), media.filename)\n\n        mask = self._resize_mask(mask, media.image_size)\n\n        for idx, face in enumerate(media.detected_faces):\n            self._store_mask(face, mask)\n            self._alignments.update_face(os.path.basename(media.filename),\n                                         idx,\n                                         face.to_alignment())\n\n    def import_mask(self, media: ExtractMedia) -> None:\n        \"\"\" Import the mask for the given Extract Media object\n\n        Parameters\n        ----------\n        media: :class:`~plugins.extract.extract_media.ExtractMedia`\n            The extract media object containing the face(s) to import the mask for\n        \"\"\"\n        mask_file = self._mapping.get(os.path.basename(media.filename))\n        if not mask_file:\n            self._counts[\"skip\"] += 1\n            logger.warning(\"No mask file found for: '%s'\", os.path.basename(media.filename))\n            return\n\n        mask = cv2.imread(mask_file, cv2.IMREAD_GRAYSCALE)\n\n        logger.trace(\"Loaded mask for frame '%s': %s\",  # type:ignore[attr-defined]\n                     os.path.basename(mask_file), mask.shape)\n\n        self._counts[\"update\"] += len(media.detected_faces)\n\n        if self._is_faces:\n            self._store_mask_face(media, mask)\n        else:\n            self._store_mask_frame(media, mask)\n", "tools/mask/mask.py": "#!/usr/bin/env python3\n\"\"\" Tool to generate masks and previews of masks for existing alignments file \"\"\"\nfrom __future__ import annotations\nimport logging\nimport os\nimport sys\n\nfrom argparse import Namespace\nfrom multiprocessing import Process\n\nfrom lib.align import Alignments\n\nfrom lib.utils import handle_deprecated_cliopts, VIDEO_EXTENSIONS\nfrom plugins.extract import ExtractMedia\n\nfrom .loader import Loader\nfrom .mask_import import Import\nfrom .mask_generate import MaskGenerator\nfrom .mask_output import Output\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass Mask:\n    \"\"\" This tool is part of the Faceswap Tools suite and should be called from\n    ``python tools.py mask`` command.\n\n    Faceswap Masks tool. Generate masks from existing alignments files, and output masks\n    for preview.\n\n    Wrapper for the mask process to run in either batch mode or single use mode\n\n    Parameters\n    ----------\n    arguments: :class:`argparse.Namespace`\n        The :mod:`argparse` arguments as passed in from :mod:`tools.py`\n    \"\"\"\n    def __init__(self, arguments: Namespace) -> None:\n        logger.debug(\"Initializing %s: (arguments: %s\", self.__class__.__name__, arguments)\n        if arguments.batch_mode and arguments.processing == \"import\":\n            logger.error(\"Batch mode is not supported for 'import' processing\")\n            sys.exit(0)\n\n        self._args = arguments\n        self._input_locations = self._get_input_locations()\n\n    def _get_input_locations(self) -> list[str]:\n        \"\"\" Obtain the full path to input locations. Will be a list of locations if batch mode is\n        selected, or containing a single location if batch mode is not selected.\n\n        Returns\n        -------\n        list:\n            The list of input location paths\n        \"\"\"\n        if not self._args.batch_mode:\n            return [self._args.input]\n\n        if not os.path.isdir(self._args.input):\n            logger.error(\"Batch mode is selected but input '%s' is not a folder\", self._args.input)\n            sys.exit(1)\n\n        retval = [os.path.join(self._args.input, fname)\n                  for fname in os.listdir(self._args.input)\n                  if os.path.isdir(os.path.join(self._args.input, fname))\n                  or os.path.splitext(fname)[-1].lower() in VIDEO_EXTENSIONS]\n        logger.info(\"Batch mode selected. Processing locations: %s\", retval)\n        return retval\n\n    def _get_output_location(self, input_location: str) -> str:\n        \"\"\" Obtain the path to an output folder for faces for a given input location.\n\n        A sub-folder within the user supplied output location will be returned based on\n        the input filename\n\n        Parameters\n        ----------\n        input_location: str\n            The full path to an input video or folder of images\n        \"\"\"\n        retval = os.path.join(self._args.output,\n                              os.path.splitext(os.path.basename(input_location))[0])\n        logger.debug(\"Returning output: '%s' for input: '%s'\", retval, input_location)\n        return retval\n\n    @staticmethod\n    def _run_mask_process(arguments: Namespace) -> None:\n        \"\"\" The mask process to be run in a spawned process.\n\n        In some instances, batch-mode memory leaks. Launching each job in a separate process\n        prevents this leak.\n\n        Parameters\n        ----------\n        arguments: :class:`argparse.Namespace`\n            The :mod:`argparse` arguments to be used for the given job\n        \"\"\"\n        logger.debug(\"Starting process: (arguments: %s)\", arguments)\n        mask = _Mask(arguments)\n        mask.process()\n        logger.debug(\"Finished process: (arguments: %s)\", arguments)\n\n    def process(self) -> None:\n        \"\"\" The entry point for triggering the Extraction Process.\n\n        Should only be called from  :class:`lib.cli.launcher.ScriptExecutor`\n        \"\"\"\n        for idx, location in enumerate(self._input_locations):\n            if self._args.batch_mode:\n                logger.info(\"Processing job %s of %s: %s\",\n                            idx + 1, len(self._input_locations), location)\n                arguments = Namespace(**self._args.__dict__)\n                arguments.input = location\n                # Due to differences in how alignments are handled for frames/faces, only default\n                # locations allowed\n                arguments.alignments = None\n                if self._args.output:\n                    arguments.output = self._get_output_location(location)\n            else:\n                arguments = self._args\n\n            if len(self._input_locations) > 1:\n                proc = Process(target=self._run_mask_process, args=(arguments, ))\n                proc.start()\n                proc.join()\n            else:\n                self._run_mask_process(arguments)\n\n\nclass _Mask:\n    \"\"\" This tool is part of the Faceswap Tools suite and should be called from\n    ``python tools.py mask`` command.\n\n    Faceswap Masks tool. Generate masks from existing alignments files, and output masks\n    for preview.\n\n    Parameters\n    ----------\n    arguments: :class:`argparse.Namespace`\n        The :mod:`argparse` arguments as passed in from :mod:`tools.py`\n    \"\"\"\n    def __init__(self, arguments: Namespace) -> None:\n        logger.debug(\"Initializing %s: (arguments: %s)\", self.__class__.__name__, arguments)\n        arguments = handle_deprecated_cliopts(arguments)\n        self._update_type = arguments.processing\n        self._input_is_faces = arguments.input_type == \"faces\"\n        self._check_input(arguments.input)\n\n        self._loader = Loader(arguments.input, self._input_is_faces)\n        self._alignments = self._get_alignments(arguments.alignments, arguments.input)\n\n        if self._loader.is_video and self._alignments is not None:\n            self._alignments.update_legacy_has_source(os.path.basename(self._loader.location))\n\n        self._loader.add_alignments(self._alignments)\n\n        self._output = Output(arguments, self._alignments, self._loader.file_list)\n\n        self._import = None\n        if self._update_type == \"import\":\n            self._import = Import(arguments.mask_path,\n                                  arguments.centering,\n                                  arguments.storage_size,\n                                  self._input_is_faces,\n                                  self._loader,\n                                  self._alignments,\n                                  arguments.input,\n                                  arguments.masker)\n\n        self._mask_gen: MaskGenerator | None = None\n        if self._update_type in (\"all\", \"missing\"):\n            self._mask_gen = MaskGenerator(arguments.masker,\n                                           self._update_type == \"all\",\n                                           self._input_is_faces,\n                                           self._loader,\n                                           self._alignments,\n                                           arguments.input,\n                                           arguments.exclude_gpus)\n\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    def _check_input(self, mask_input: str) -> None:\n        \"\"\" Check the input is valid. If it isn't exit with a logged error\n\n        Parameters\n        ----------\n        mask_input: str\n            Path to the input folder/video\n        \"\"\"\n        if not os.path.exists(mask_input):\n            logger.error(\"Location cannot be found: '%s'\", mask_input)\n            sys.exit(0)\n        if os.path.isfile(mask_input) and self._input_is_faces:\n            logger.error(\"Input type 'faces' was selected but input is not a folder: '%s'\",\n                         mask_input)\n            sys.exit(0)\n        logger.debug(\"input '%s' is valid\", mask_input)\n\n    def _get_alignments(self, alignments: str | None, input_location: str) -> Alignments | None:\n        \"\"\" Obtain the alignments from either the given alignments location or the default\n        location.\n\n        Parameters\n        ----------\n        alignments: str | None\n            Full path to the alignemnts file if provided or ``None`` if not\n        input_location: str\n            Full path to the source files to be used by the mask tool\n\n        Returns\n        -------\n        ``None`` or :class:`~lib.align.alignments.Alignments`:\n            If output is requested, returns a :class:`~lib.align.alignments.Alignments` otherwise\n            returns ``None``\n        \"\"\"\n        if alignments:\n            logger.debug(\"Alignments location provided: %s\", alignments)\n            return Alignments(os.path.dirname(alignments),\n                              filename=os.path.basename(alignments))\n        if self._input_is_faces and self._update_type == \"output\":\n            logger.debug(\"No alignments file provided for faces. Using PNG Header for output\")\n            return None\n        if self._input_is_faces:\n            logger.warning(\"Faces input selected without an alignments file. Masks wil only \"\n                           \"be updated in the faces' PNG Header\")\n            return None\n\n        folder = input_location\n        if self._loader.is_video:\n            logger.debug(\"Alignments from Video File: '%s'\", folder)\n            folder, filename = os.path.split(folder)\n            filename = f\"{os.path.splitext(filename)[0]}_alignments.fsa\"\n        else:\n            logger.debug(\"Alignments from Input Folder: '%s'\", folder)\n            filename = \"alignments\"\n\n        retval = Alignments(folder, filename=filename)\n        return retval\n\n    def _save_output(self, media: ExtractMedia) -> None:\n        \"\"\" Output masks to disk\n\n        Parameters\n        ----------\n        media: :class:`~plugins.extract.extract_media.ExtractMedia`\n            The extract media holding the faces to output\n        \"\"\"\n        filename = os.path.basename(media.frame_metadata[\"source_filename\"]\n                                    if self._input_is_faces else media.filename)\n        dims = media.frame_metadata[\"source_frame_dims\"] if self._input_is_faces else None\n        for idx, face in enumerate(media.detected_faces):\n            face_idx = media.frame_metadata[\"face_index\"] if self._input_is_faces else idx\n            face.image = media.image\n            self._output.save(filename, face_idx, face, frame_dims=dims)\n\n    def _generate_masks(self) -> None:\n        \"\"\" Generate masks from a mask plugin \"\"\"\n        assert self._mask_gen is not None\n\n        logger.info(\"Generating masks\")\n\n        for media in self._mask_gen.process():\n            if self._output.should_save:\n                self._save_output(media)\n\n    def _import_masks(self) -> None:\n        \"\"\" Import masks that have been generated outside of faceswap \"\"\"\n        assert self._import is not None\n        logger.info(\"Importing masks\")\n\n        for media in self._loader.load():\n            self._import.import_mask(media)\n            if self._output.should_save:\n                self._save_output(media)\n\n        if self._alignments is not None and self._import.update_count > 0:\n            self._alignments.backup()\n            self._alignments.save()\n\n        if self._import.skip_count > 0:\n            logger.warning(\"No masks were found for %s item(s), so these have not been imported\",\n                           self._import.skip_count)\n\n        logger.info(\"Imported masks for %s faces of %s\",\n                    self._import.update_count, self._import.update_count + self._import.skip_count)\n\n    def _output_masks(self) -> None:\n        \"\"\" Output masks to selected output folder \"\"\"\n        for media in self._loader.load():\n            self._save_output(media)\n\n    def process(self) -> None:\n        \"\"\" The entry point for the Mask tool from :file:`lib.tools.cli`. Runs the Mask process \"\"\"\n        logger.debug(\"Starting masker process\")\n\n        if self._update_type in (\"all\", \"missing\"):\n            self._generate_masks()\n\n        if self._update_type == \"import\":\n            self._import_masks()\n\n        if self._update_type == \"output\":\n            self._output_masks()\n\n        self._output.close()\n        logger.debug(\"Completed masker process\")\n", "tools/mask/loader.py": "#!/usr/bin/env python3\n\"\"\" Handles loading of faces/frames from source locations and pairing with alignments\ninformation \"\"\"\nfrom __future__ import annotations\n\nimport logging\nimport os\nimport typing as T\n\nimport numpy as np\nfrom tqdm import tqdm\n\nfrom lib.align import DetectedFace, update_legacy_png_header\nfrom lib.align.alignments import AlignmentFileDict\nfrom lib.image import FacesLoader, ImagesLoader\nfrom plugins.extract import ExtractMedia\n\nif T.TYPE_CHECKING:\n    from lib.align import Alignments\n    from lib.align.alignments import PNGHeaderDict\nlogger = logging.getLogger(__name__)\n\n\nclass Loader:\n    \"\"\" Loader for reading source data from disk, and yielding the output paired with alignment\n    information\n\n    Parameters\n    ----------\n    location: str\n        Full path to the source files location\n    is_faces: bool\n        ``True`` if the source is a folder of faceswap extracted faces\n    \"\"\"\n    def __init__(self, location: str, is_faces: bool) -> None:\n        logger.debug(\"Initializing %s (location: %s, is_faces: %s)\",\n                     self.__class__.__name__, location, is_faces)\n\n        self._is_faces = is_faces\n        self._loader = FacesLoader(location) if is_faces else ImagesLoader(location)\n        self._alignments: Alignments | None = None\n        self._skip_count = 0\n\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    @property\n    def file_list(self) -> list[str]:\n        \"\"\"list[str]: Full file list of source files to be loaded \"\"\"\n        return self._loader.file_list\n\n    @property\n    def is_video(self) -> bool:\n        \"\"\"bool: ``True`` if the source is a video file otherwise ``False`` \"\"\"\n        return self._loader.is_video\n\n    @property\n    def location(self) -> str:\n        \"\"\"str: Full path to the source folder/video file location \"\"\"\n        return self._loader.location\n\n    @property\n    def skip_count(self) -> int:\n        \"\"\"int: The number of faces/frames that have been skipped due to no match in alignments\n        file \"\"\"\n        return self._skip_count\n\n    def add_alignments(self, alignments: Alignments | None) -> None:\n        \"\"\" Add the loaded alignments to :attr:`_alignments` for content matching\n\n        Parameters\n        ----------\n        alignments: :class:`~lib.align.Alignments` | None\n            The alignments file object or ``None`` if not provided\n        \"\"\"\n        logger.debug(\"Adding alignments to loader: %s\", alignments)\n        self._alignments = alignments\n\n    @classmethod\n    def _get_detected_face(cls, alignment: AlignmentFileDict) -> DetectedFace:\n        \"\"\" Convert an alignment dict item to a detected_face object\n\n        Parameters\n        ----------\n        alignment: :class:`lib.align.alignments.AlignmentFileDict`\n            The alignment dict for a face\n\n        Returns\n        -------\n        :class:`~lib.align.detected_face.DetectedFace`:\n            The corresponding detected_face object for the alignment\n        \"\"\"\n        detected_face = DetectedFace()\n        detected_face.from_alignment(alignment)\n        return detected_face\n\n    def _process_face(self,\n                      filename: str,\n                      image: np.ndarray,\n                      metadata: PNGHeaderDict) -> ExtractMedia | None:\n        \"\"\" Process a single face when masking from face images\n\n        Parameters\n        ----------\n        filename: str\n            the filename currently being processed\n        image: :class:`numpy.ndarray`\n            The current face being processed\n        metadata: dict\n            The source frame metadata from the PNG header\n\n        Returns\n        -------\n        :class:`plugins.pipeline.ExtractMedia` | None\n            the extract media object for the processed face or ``None`` if alignment information\n            could not be found\n        \"\"\"\n        frame_name = metadata[\"source\"][\"source_filename\"]\n        face_index = metadata[\"source\"][\"face_index\"]\n\n        if self._alignments is None:  # mask from PNG header\n            lookup_index = 0\n            alignments = [T.cast(AlignmentFileDict, metadata[\"alignments\"])]\n        else:  # mask from Alignments file\n            lookup_index = face_index\n            alignments = self._alignments.get_faces_in_frame(frame_name)\n            if not alignments or face_index > len(alignments) - 1:\n                self._skip_count += 1\n                logger.warning(\"Skipping Face not found in alignments file: '%s'\", filename)\n                return None\n\n        alignment = alignments[lookup_index]\n        detected_face = self._get_detected_face(alignment)\n\n        retval = ExtractMedia(filename, image, detected_faces=[detected_face], is_aligned=True)\n        retval.add_frame_metadata(metadata[\"source\"])\n        return retval\n\n    def _from_faces(self) -> T.Generator[ExtractMedia, None, None]:\n        \"\"\" Load content from pre-aligned faces and pair with corresponding metadata\n\n        Yields\n        ------\n        :class:`plugins.pipeline.ExtractMedia`\n            the extract media object for the processed face\n        \"\"\"\n        log_once = False\n        for filename, image, metadata in tqdm(self._loader.load(), total=self._loader.count):\n            if not metadata:  # Legacy faces. Update the headers\n                if self._alignments is None:\n                    logger.error(\"Legacy faces have been discovered, but no alignments file \"\n                                 \"provided. You must provide an alignments file for this face set\")\n                    break\n\n                if not log_once:\n                    logger.warning(\"Legacy faces discovered. These faces will be updated\")\n                    log_once = True\n\n                metadata = update_legacy_png_header(filename, self._alignments)\n                if not metadata:  # Face not found\n                    self._skip_count += 1\n                    logger.warning(\"Legacy face not found in alignments file. This face has not \"\n                                   \"been updated: '%s'\", filename)\n                    continue\n\n            if \"source_frame_dims\" not in metadata.get(\"source\", {}):\n                logger.error(\"The faces need to be re-extracted as at least some of them do not \"\n                             \"contain information required to correctly generate masks.\")\n                logger.error(\"You can re-extract the face-set by using the Alignments Tool's \"\n                             \"Extract job.\")\n                break\n\n            retval = self._process_face(filename, image, metadata)\n            if retval is None:\n                continue\n\n            yield retval\n\n    def _from_frames(self) -> T.Generator[ExtractMedia, None, None]:\n        \"\"\" Load content from frames and and pair with corresponding metadata\n\n        Yields\n        ------\n        :class:`plugins.pipeline.ExtractMedia`\n            the extract media object for the processed face\n        \"\"\"\n        assert self._alignments is not None\n        for filename, image in tqdm(self._loader.load(), total=self._loader.count):\n            frame = os.path.basename(filename)\n\n            if not self._alignments.frame_exists(frame):\n                self._skip_count += 1\n                logger.warning(\"Skipping frame not in alignments file: '%s'\", frame)\n                continue\n\n            if not self._alignments.frame_has_faces(frame):\n                logger.debug(\"Skipping frame with no faces: '%s'\", frame)\n                continue\n\n            faces_in_frame = self._alignments.get_faces_in_frame(frame)\n            detected_faces = [self._get_detected_face(alignment) for alignment in faces_in_frame]\n            retval = ExtractMedia(filename, image, detected_faces=detected_faces)\n            yield retval\n\n    def load(self) -> T.Generator[ExtractMedia, None, None]:\n        \"\"\" Load content from source and pair with corresponding alignment data\n\n        Yields\n        ------\n        :class:`plugins.pipeline.ExtractMedia`\n            the extract media object for the processed face\n        \"\"\"\n        if self._is_faces:\n            iterator = self._from_faces\n        else:\n            iterator = self._from_frames\n\n        for media in iterator():\n            yield media\n\n        if self._skip_count > 0:\n            logger.warning(\"%s face(s) skipped due to not existing in the alignments file\",\n                           self._skip_count)\n", "tools/mask/__init__.py": "", "tools/mask/mask_output.py": "#!/usr/bin/env python3\n\"\"\" Output processing for faceswap's mask tool \"\"\"\nfrom __future__ import annotations\n\nimport logging\nimport os\nimport sys\nimport typing as T\nfrom argparse import Namespace\n\nimport cv2\nimport numpy as np\nfrom tqdm import tqdm\n\nfrom lib.align import AlignedFace\nfrom lib.align.alignments import AlignmentDict\n\nfrom lib.image import ImagesSaver, read_image_meta_batch\nfrom lib.utils import get_folder\nfrom scripts.fsmedia import Alignments as ExtractAlignments\n\nif T.TYPE_CHECKING:\n    from lib.align import Alignments, DetectedFace\n    from lib.align.aligned_face import CenteringType\n\nlogger = logging.getLogger(__name__)\n\n\nclass Output:\n    \"\"\" Handles outputting of masks for preview/editting to disk\n\n    Parameters\n    ----------\n    arguments: :class:`argparse.Namespace`\n        The command line arguments that the mask tool was called with\n    alignments: :class:~`lib.align.alignments.Alignments` | None\n        The alignments file object (or ``None`` if not provided and input is faces)\n    file_list: list[str]\n        Full file list for the loader. Used for extracting alignments from faces\n    \"\"\"\n    def __init__(self, arguments: Namespace,\n                 alignments: Alignments | None,\n                 file_list: list[str]) -> None:\n        logger.debug(\"Initializing %s (arguments: %s, alignments: %s, file_list: %s)\",\n                     self.__class__.__name__, arguments, alignments, len(file_list))\n\n        self._blur_kernel: int = arguments.blur_kernel\n        self._threshold: int = arguments.threshold\n        self._type: T.Literal[\"combined\", \"masked\", \"mask\"] = arguments.output_type\n        self._full_frame: bool = arguments.full_frame\n        self._mask_type = arguments.masker\n\n        self._input_is_faces = arguments.input_type == \"faces\"\n        self._saver = self._set_saver(arguments.output, arguments.processing)\n        self._alignments = self._get_alignments(alignments, file_list)\n\n        self._full_frame_cache: dict[str, list[tuple[int, DetectedFace]]] = {}\n\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    @property\n    def should_save(self) -> bool:\n        \"\"\"bool: ``True`` if mask images should be output otherwise ``False`` \"\"\"\n        return self._saver is not None\n\n    def _get_subfolder(self, output: str) -> str:\n        \"\"\" Obtain a subfolder within the output folder to save the output based on selected\n        output options.\n\n        Parameters\n        ----------\n        output: str\n            Full path to the root output folder\n\n        Returns\n        -------\n        str:\n            The full path to where masks should be saved\n        \"\"\"\n        out_type = \"frame\" if self._full_frame else \"face\"\n        retval = os.path.join(output,\n                              f\"{self._mask_type}_{out_type}_{self._type}\")\n        logger.info(\"Saving masks to '%s'\", retval)\n        return retval\n\n    def _set_saver(self, output: str | None, processing: str) -> ImagesSaver | None:\n        \"\"\" set the saver in a background thread\n\n        Parameters\n        ----------\n        output: str\n            Full path to the root output folder if provided\n        processing: str\n            The processing that has been selected\n\n        Returns\n        -------\n        ``None`` or :class:`lib.image.ImagesSaver`:\n            If output is requested, returns a :class:`lib.image.ImagesSaver` otherwise\n            returns ``None``\n        \"\"\"\n        if output is None or not output:\n            if processing == \"output\":\n                logger.error(\"Processing set as 'output' but no output folder provided.\")\n                sys.exit(0)\n            logger.debug(\"No output provided. Not creating saver\")\n            return None\n        output_dir = get_folder(self._get_subfolder(output), make_folder=True)\n        retval = ImagesSaver(output_dir)\n        logger.debug(retval)\n        return retval\n\n    def _get_alignments(self,\n                        alignments: Alignments | None,\n                        file_list: list[str]) -> Alignments | None:\n        \"\"\" Obtain the alignments file. If input is faces and full frame output is requested then\n        the file needs to be generated from the input faces, if not provided\n\n        Parameters\n        ----------\n        alignments: :class:~`lib.align.alignments.Alignments` | None\n            The alignments file object (or ``None`` if not provided and input is faces)\n        file_list: list[str]\n            Full paths to ihe mask tool input files\n\n        Returns\n        -------\n        :class:~`lib.align.alignments.Alignments` | None\n            The alignments file if provided and/or is required otherwise ``None``\n        \"\"\"\n        if alignments is not None or not self._full_frame:\n            return alignments\n        logger.debug(\"Generating alignments from faces\")\n\n        data = T.cast(dict[str, AlignmentDict], {})\n        for _, meta in tqdm(read_image_meta_batch(file_list),\n                            desc=\"Reading alignments from faces\",\n                            total=len(file_list),\n                            leave=False):\n            fname = meta[\"itxt\"][\"source\"][\"source_filename\"]\n            aln = meta[\"itxt\"][\"alignments\"]\n            data.setdefault(fname, {}).setdefault(\"faces\",  # type:ignore[typeddict-item]\n                                                  []).append(aln)\n\n        dummy_args = Namespace(alignments_path=\"/dummy/alignments.fsa\")\n        retval = ExtractAlignments(dummy_args, is_extract=True)\n        retval.update_from_dict(data)\n        return retval\n\n    def _get_background_frame(self, detected_faces: list[DetectedFace], frame_dims: tuple[int, int]\n                              ) -> np.ndarray:\n        \"\"\" Obtain the background image when final output is in full frame format. There will only\n        ever be one background, even when there are multiple faces\n\n        The output image will depend on the requested output type and whether the input is faces\n        or frames\n\n        Parameters\n        ----------\n        detected_faces: list[:class:`~lib.align.detected_face.DetectedFace`]\n            Detected face objects for the output image\n        frame_dims: tuple[int, int]\n            The size of the original frame\n\n        Returns\n        -------\n        :class:`numpy.ndarray`\n            The full frame background image for applying masks to\n        \"\"\"\n        if self._type == \"mask\":\n            return np.zeros(frame_dims, dtype=\"uint8\")\n\n        if not self._input_is_faces:  # Frame is in the detected faces object\n            assert detected_faces[0].image is not None\n            return np.ascontiguousarray(detected_faces[0].image)\n\n        # Outputting to frames, but input is faces. Apply the face patches to an empty canvas\n        retval = np.zeros((*frame_dims, 3), dtype=\"uint8\")\n        for detected_face in detected_faces:\n            assert detected_face.image is not None\n            face = AlignedFace(detected_face.landmarks_xy,\n                               image=detected_face.image,\n                               centering=\"head\",\n                               size=detected_face.image.shape[0],\n                               is_aligned=True)\n            border = cv2.BORDER_TRANSPARENT if len(detected_faces) > 1 else cv2.BORDER_CONSTANT\n            assert face.face is not None\n            cv2.warpAffine(face.face,\n                           face.adjusted_matrix,\n                           tuple(reversed(frame_dims)),\n                           retval,\n                           flags=cv2.WARP_INVERSE_MAP | face.interpolators[1],\n                           borderMode=border)\n        return retval\n\n    def _get_background_face(self,\n                             detected_face: DetectedFace,\n                             mask_centering: CenteringType,\n                             mask_size: int) -> np.ndarray:\n        \"\"\" Obtain the background images when the output is faces\n\n        The output image will depend on the requested output type and whether the input is faces\n        or frames\n\n        Parameters\n        ----------\n        detected_face: :class:`~lib.align.detected_face.DetectedFace`\n            Detected face object for the output image\n        mask_centering: Literal[\"face\", \"head\", \"legacy\"]\n            The centering of the stored mask\n        mask_size: int\n            The pixel size of the stored mask\n\n        Returns\n        -------\n        list[]:class:`numpy.ndarray`]\n            The face background image for applying masks to for each detected face object\n        \"\"\"\n        if self._type == \"mask\":\n            return np.zeros((mask_size, mask_size), dtype=\"uint8\")\n\n        assert detected_face.image is not None\n\n        if self._input_is_faces:\n            retval = AlignedFace(detected_face.landmarks_xy,\n                                 image=detected_face.image,\n                                 centering=mask_centering,\n                                 size=mask_size,\n                                 is_aligned=True).face\n        else:\n            centering: CenteringType = (\"legacy\" if self._alignments is not None and\n                                        self._alignments.version == 1.0\n                                        else mask_centering)\n            detected_face.load_aligned(detected_face.image,\n                                       size=mask_size,\n                                       centering=centering,\n                                       force=True)\n            retval = detected_face.aligned.face\n\n        assert retval is not None\n        return retval\n\n    def _get_background(self,\n                        detected_faces: list[DetectedFace],\n                        frame_dims: tuple[int, int],\n                        mask_centering: CenteringType,\n                        mask_size: int) -> np.ndarray:\n        \"\"\" Obtain the background image that the final outut will be placed on\n\n        Parameters\n        ----------\n        detected_faces: list[:class:`~lib.align.detected_face.DetectedFace`]\n            Detected face objects for the output image\n        frame_dims: tuple[int, int]\n            The size of the original frame\n        mask_centering: Literal[\"face\", \"head\", \"legacy\"]\n            The centering of the stored mask\n        mask_size: int\n            The pixel size of the stored mask\n\n        Returns\n        -------\n        :class:`numpy.ndarray`\n            The background image for the mask output\n        \"\"\"\n        if self._full_frame:\n            retval = self._get_background_frame(detected_faces, frame_dims)\n        else:\n            assert len(detected_faces) == 1  # If outputting faces, we should only receive 1 face\n            retval = self._get_background_face(detected_faces[0], mask_centering, mask_size)\n\n        logger.trace(\"Background image (size: %s, dtype: %s)\",  # type:ignore[attr-defined]\n                     retval.shape, retval.dtype)\n        return retval\n\n    def _get_mask(self,\n                  detected_faces: list[DetectedFace],\n                  mask_type: str,\n                  mask_dims: tuple[int, int]) -> np.ndarray:\n        \"\"\" Generate the mask to be applied to the final output frame\n\n        Parameters\n        ----------\n        detected_faces: list[:class:`~lib.align.detected_face.DetectedFace`]\n            Detected face objects to generate the masks from\n        mask_type: str\n            The mask-type to use\n        mask_dims : tuple[int, int]\n            The size of the mask to output\n\n        Returns\n        -------\n        :class:`numpy.ndarray`\n            The final mask to apply to the output image\n        \"\"\"\n        retval = np.zeros(mask_dims, dtype=\"uint8\")\n        for face in detected_faces:\n            mask_object = face.mask[mask_type]\n            mask_object.set_blur_and_threshold(blur_kernel=self._blur_kernel,\n                                               threshold=self._threshold)\n            if self._full_frame:\n                mask = mask_object.get_full_frame_mask(*reversed(mask_dims))\n            else:\n                mask = mask_object.mask[..., 0]\n            np.maximum(retval, mask, out=retval)\n        logger.trace(\"Final mask (shape: %s, dtype: %s)\",  # type:ignore[attr-defined]\n                     retval.shape, retval.dtype)\n        return retval\n\n    def _build_output_image(self, background: np.ndarray, mask: np.ndarray) -> np.ndarray:\n        \"\"\" Collate the mask and images for the final output image, depending on selected output\n        type\n\n        Parameters\n        ----------\n        background: :class:`numpy.ndarray`\n            The image that the mask will be applied to\n        mask: :class:`numpy.ndarray`\n            The mask to output\n\n        Returns\n        -------\n        :class:`numpy.ndarray`\n            The final output image\n        \"\"\"\n        if self._type == \"mask\":\n            return mask\n\n        mask = mask[..., None]\n        if self._type == \"masked\":\n            return np.concatenate([background, mask], axis=-1)\n\n        height, width = background.shape[:2]\n        masked = (background.astype(\"float32\") * mask.astype(\"float32\") / 255.).astype(\"uint8\")\n        mask = np.tile(mask, 3)\n        for img in (background, masked, mask):\n            cv2.rectangle(img, (0, 0), (width - 1, height - 1), (255, 255, 255), 1)\n        axis = 0 if background.shape[0] < background.shape[1] else 1\n        retval = np.concatenate((background, masked, mask), axis=axis)\n\n        return retval\n\n    def _create_image(self,\n                      detected_faces: list[DetectedFace],\n                      mask_type: str,\n                      frame_dims: tuple[int, int] | None) -> np.ndarray:\n        \"\"\" Create a mask preview image for saving out to disk\n\n        Parameters\n        ----------\n        detected_faces: list[:class:`~lib.align.detected_face.DetectedFace`]\n            Detected face objects for the output image\n        mask_type: str\n            The mask_type to process\n        frame_dims: tuple[int, int] | None\n            The size of the original frame, if input is faces otherwise ``None``\n\n        Returns\n        -------\n        :class:`numpy.ndarray`:\n            A preview image depending on the output type in one of the following forms:\n              - Containing 3 sub images: The original face, the masked face and the mask\n              - The mask only\n              - The masked face\n        \"\"\"\n        assert detected_faces[0].image is not None\n        dims = T.cast(tuple[int, int],\n                      frame_dims if self._input_is_faces else detected_faces[0].image.shape[:2])\n        assert dims is not None and len(dims) == 2\n\n        mask_centering = detected_faces[0].mask[mask_type].stored_centering\n        mask_size = detected_faces[0].mask[mask_type].stored_size\n\n        background = self._get_background(detected_faces, dims, mask_centering, mask_size)\n        mask = self._get_mask(detected_faces,\n                              mask_type,\n                              dims if self._full_frame else (mask_size, mask_size))\n        retval = self._build_output_image(background, mask)\n\n        logger.trace(\"Output image (shape: %s, dtype: %s)\",  # type:ignore[attr-defined]\n                     retval.shape, retval.dtype)\n        return retval\n\n    def _handle_cache(self,\n                      frame: str,\n                      idx: int,\n                      detected_face: DetectedFace) -> list[tuple[int, DetectedFace]]:\n        \"\"\" For full frame output, cache any faces until all detected faces have been seen. For\n        face output, just return the detected_face object inside a list\n\n        Parameters\n        ----------\n        frame: str\n            The frame name in the alignments file\n        idx: int\n            The index of the face for this frame in the alignments file\n        detected_face: :class:`~lib.align.detected_face.DetectedFace`\n            A detected_face object for a face\n\n        Returns\n        -------\n        list[tuple[int, :class:`~lib.align.detected_face.DetectedFace`]]\n            Face index and detected face objects to be processed for this output, if any\n        \"\"\"\n        if not self._full_frame:\n            return [(idx, detected_face)]\n\n        assert self._alignments is not None\n        faces_in_frame = self._alignments.count_faces_in_frame(frame)\n        if faces_in_frame == 1:\n            return [(idx, detected_face)]\n\n        self._full_frame_cache.setdefault(frame, []).append((idx, detected_face))\n\n        if len(self._full_frame_cache[frame]) != faces_in_frame:\n            logger.trace(\"Caching face for frame '%s'\", frame)  # type:ignore[attr-defined]\n            return []\n\n        retval = self._full_frame_cache.pop(frame)\n        logger.trace(\"Processing '%s' from cache: %s\", frame, retval)  # type:ignore[attr-defined]\n        return retval\n\n    def _get_mask_types(self,\n                        frame: str,\n                        detected_faces: list[tuple[int, DetectedFace]]) -> list[str]:\n        \"\"\" Get the mask type names for the select mask type. Remove any detected faces where\n        the selected mask does not exist\n\n        Parameters\n        ----------\n        frame: str\n            The frame name in the alignments file\n        idx: int\n            The index of the face for this frame in the alignments file\n        detected_face: list[tuple[int, :class:`~lib.align.detected_face.DetectedFace`]\n            The face index and detected_face object for output\n\n        Returns\n        -------\n        list[str]\n            List of mask type names to be processed\n        \"\"\"\n        if self._mask_type == \"bisenet-fp\":\n            mask_types = [f\"{self._mask_type}_{area}\" for area in (\"face\", \"head\")]\n        else:\n            mask_types = [self._mask_type]\n\n        final_masks = set()\n        for idx in reversed(range(len(detected_faces))):\n            face_idx, detected_face = detected_faces[idx]\n            if detected_face.mask is None or not any(mask in detected_face.mask\n                                                     for mask in mask_types):\n                logger.warning(\"Mask type '%s' does not exist for frame '%s' index %s. Skipping\",\n                               self._mask_type, frame, face_idx)\n                del detected_faces[idx]\n                continue\n            final_masks.update([m for m in detected_face.mask if m in mask_types])\n\n        retval = list(final_masks)\n        logger.trace(\"Handling mask types: %s\", retval)  # type:ignore[attr-defined]\n        return retval\n\n    def save(self,\n             frame: str,\n             idx: int,\n             detected_face: DetectedFace,\n             frame_dims: tuple[int, int] | None = None) -> None:\n        \"\"\" Build the mask preview image and save\n\n        Parameters\n        ----------\n        frame: str\n            The frame name in the alignments file\n        idx: int\n            The index of the face for this frame in the alignments file\n        detected_face: :class:`~lib.align.detected_face.DetectedFace`\n            A detected_face object for a face\n        frame_dims: tuple[int, int] | None, optional\n            The size of the original frame, if input is faces otherwise ``None``. Default: ``None``\n        \"\"\"\n        assert self._saver is not None\n\n        faces = self._handle_cache(frame, idx, detected_face)\n        if not faces:\n            return\n\n        mask_types = self._get_mask_types(frame, faces)\n        if not faces or not mask_types:\n            logger.debug(\"No valid faces/masks to process for '%s'\", frame)\n            return\n\n        for mask_type in mask_types:\n            detected_faces = [f[1] for f in faces if mask_type in f[1].mask]\n            if not detected_face:\n                logger.warning(\"No '%s' masks to output for '%s'\", mask_type, frame)\n                continue\n            if len(detected_faces) != len(faces):\n                logger.warning(\"Some '%s' masks are missing for '%s'\", mask_type, frame)\n\n            image = self._create_image(detected_faces, mask_type, frame_dims)\n            filename = os.path.splitext(frame)[0]\n            if len(mask_types) > 1:\n                filename += f\"_{mask_type}\"\n            if not self._full_frame:\n                filename += f\"_{idx}\"\n            filename = os.path.join(self._saver.location, f\"{filename}.png\")\n            logger.trace(\"filename: '%s', image_shape: %s\", filename, image.shape)  # type: ignore\n            self._saver.save(filename, image)\n\n    def close(self) -> None:\n        \"\"\" Shut down the image saver if it is open \"\"\"\n        if self._saver is None:\n            return\n        logger.debug(\"Shutting down saver\")\n        self._saver.close()\n", "tools/manual/manual.py": "#!/usr/bin/env python3\n\"\"\" The Manual Tool is a tkinter driven GUI app for editing alignments files with visual tools.\nThis module is the main entry point into the Manual Tool. \"\"\"\nfrom __future__ import annotations\n\nimport logging\nimport os\nimport sys\nimport typing as T\nimport tkinter as tk\nfrom tkinter import ttk\nfrom time import sleep\n\nimport cv2\nimport numpy as np\n\nfrom lib.gui.control_helper import ControlPanel\nfrom lib.gui.utils import get_images, get_config, initialize_config, initialize_images\nfrom lib.image import SingleFrameLoader, read_image_meta\nfrom lib.multithreading import MultiThread\nfrom lib.utils import handle_deprecated_cliopts, VIDEO_EXTENSIONS\nfrom plugins.extract import ExtractMedia, Extractor\n\nfrom .detected_faces import DetectedFaces\nfrom .faceviewer.frame import FacesFrame\nfrom .frameviewer.frame import DisplayFrame\nfrom .thumbnails import ThumbsCreator\n\nif T.TYPE_CHECKING:\n    from lib.align import DetectedFace, Mask\n    from lib.queue_manager import EventQueue\n\nlogger = logging.getLogger(__name__)\n\nTypeManualExtractor = T.Literal[\"FAN\", \"cv2-dnn\", \"mask\"]\n\n\nclass Manual(tk.Tk):\n    \"\"\" The main entry point for Faceswap's Manual Editor Tool. This tool is part of the Faceswap\n    Tools suite and should be called from ``python tools.py manual`` command.\n\n    Allows for visual interaction with frames, faces and alignments file to perform various\n    adjustments to the alignments file.\n\n    Parameters\n    ----------\n    arguments: :class:`argparse.Namespace`\n        The :mod:`argparse` arguments as passed in from :mod:`tools.py`\n    \"\"\"\n\n    def __init__(self, arguments):\n        logger.debug(\"Initializing %s: (arguments: '%s')\", self.__class__.__name__, arguments)\n        super().__init__()\n        arguments = handle_deprecated_cliopts(arguments)\n        self._validate_non_faces(arguments.frames)\n\n        self._initialize_tkinter()\n        self._globals = TkGlobals(arguments.frames)\n\n        extractor = Aligner(self._globals, arguments.exclude_gpus)\n        self._detected_faces = DetectedFaces(self._globals,\n                                             arguments.alignments_path,\n                                             arguments.frames,\n                                             extractor)\n\n        video_meta_data = self._detected_faces.video_meta_data\n        valid_meta = all(val is not None for val in video_meta_data.values())\n\n        loader = FrameLoader(self._globals, arguments.frames, video_meta_data)\n        if valid_meta:  # Load the faces whilst other threads complete if we have valid meta data\n            self._detected_faces.load_faces()\n\n        self._containers = self._create_containers()\n        self._wait_for_threads(extractor, loader, valid_meta)\n        if not valid_meta:\n            # Load the faces after other threads complete if meta data required updating\n            self._detected_faces.load_faces()\n\n        self._generate_thumbs(arguments.frames, arguments.thumb_regen, arguments.single_process)\n\n        self._display = DisplayFrame(self._containers[\"top\"],\n                                     self._globals,\n                                     self._detected_faces)\n        _Options(self._containers[\"top\"], self._globals, self._display)\n\n        self._faces_frame = FacesFrame(self._containers[\"bottom\"],\n                                       self._globals,\n                                       self._detected_faces,\n                                       self._display)\n        self._display.tk_selected_action.set(\"View\")\n\n        self.bind(\"<Key>\", self._handle_key_press)\n        self._set_initial_layout()\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    @classmethod\n    def _validate_non_faces(cls, frames_folder):\n        \"\"\" Quick check on the input to make sure that a folder of extracted faces is not being\n        passed in. \"\"\"\n        if not os.path.isdir(frames_folder):\n            logger.debug(\"Input '%s' is not a folder\", frames_folder)\n            return\n        test_file = next((fname\n                          for fname in os.listdir(frames_folder)\n                          if os.path.splitext(fname)[-1].lower() == \".png\"),\n                         None)\n        if not test_file:\n            logger.debug(\"Input '%s' does not contain any .pngs\", frames_folder)\n            return\n        test_file = os.path.join(frames_folder, test_file)\n        meta = read_image_meta(test_file)\n        logger.debug(\"Test file: (filename: %s, metadata: %s)\", test_file, meta)\n        if \"itxt\" in meta and \"alignments\" in meta[\"itxt\"]:\n            logger.error(\"The input folder '%s' contains extracted faces.\", frames_folder)\n            logger.error(\"The Manual Tool works with source frames or a video file, not extracted \"\n                         \"faces. Please update your input.\")\n            sys.exit(1)\n        logger.debug(\"Test input file '%s' does not contain Faceswap header data\", test_file)\n\n    def _wait_for_threads(self, extractor, loader, valid_meta):\n        \"\"\" The :class:`Aligner` and :class:`FramesLoader` are launched in background threads.\n        Wait for them to be initialized prior to proceeding.\n\n        Parameters\n        ----------\n        extractor: :class:`Aligner`\n            The extraction pipeline for the Manual Tool\n        loader: :class:`FramesLoader`\n            The frames loader for the Manual Tool\n        valid_meta: bool\n            Whether the input video had valid meta-data on import, or if it had to be created.\n            ``True`` if valid meta data existed previously, ``False`` if it needed to be created\n\n        Notes\n        -----\n        Because some of the initialize checks perform extra work once their threads are complete,\n        they should only return ``True`` once, and should not be queried again.\n        \"\"\"\n        extractor_init = False\n        frames_init = False\n        while True:\n            extractor_init = extractor_init if extractor_init else extractor.is_initialized\n            frames_init = frames_init if frames_init else loader.is_initialized\n            if extractor_init and frames_init:\n                logger.debug(\"Threads inialized\")\n                break\n            logger.debug(\"Threads not initialized. Waiting...\")\n            sleep(1)\n\n        extractor.link_faces(self._detected_faces)\n        if not valid_meta:\n            logger.debug(\"Saving video meta data to alignments file\")\n            self._detected_faces.save_video_meta_data(**loader.video_meta_data)\n\n    def _generate_thumbs(self, input_location, force, single_process):\n        \"\"\" Check whether thumbnails are stored in the alignments file and if not generate them.\n\n        Parameters\n        ----------\n        input_location: str\n            The input video or folder of images\n        force: bool\n            ``True`` if the thumbnails should be regenerated even if they exist, otherwise\n            ``False``\n        single_process: bool\n            ``True`` will extract thumbs from a video in a single process, ``False`` will run\n            parallel threads\n        \"\"\"\n        thumbs = ThumbsCreator(self._detected_faces, input_location, single_process)\n        if thumbs.has_thumbs and not force:\n            return\n        logger.debug(\"Generating thumbnails cache\")\n        thumbs.generate_cache()\n        logger.debug(\"Generated thumbnails cache\")\n\n    def _initialize_tkinter(self):\n        \"\"\" Initialize a standalone tkinter instance. \"\"\"\n        logger.debug(\"Initializing tkinter\")\n        for widget in (\"TButton\", \"TCheckbutton\", \"TRadiobutton\"):\n            self.unbind_class(widget, \"<Key-space>\")\n        initialize_config(self, None, None)\n        initialize_images()\n        get_config().set_geometry(940, 600, fullscreen=True)\n        self.title(\"Faceswap.py - Visual Alignments\")\n        logger.debug(\"Initialized tkinter\")\n\n    def _create_containers(self):\n        \"\"\" Create the paned window containers for various GUI elements\n\n        Returns\n        -------\n        dict:\n            The main containers of the manual tool.\n        \"\"\"\n        logger.debug(\"Creating containers\")\n        main = ttk.PanedWindow(self,\n                               orient=tk.VERTICAL,\n                               name=\"pw_main\")\n        main.pack(fill=tk.BOTH, expand=True)\n\n        top = ttk.Frame(main, name=\"frame_top\")\n        main.add(top)\n\n        bottom = ttk.Frame(main, name=\"frame_bottom\")\n        main.add(bottom)\n        retval = {\"main\": main, \"top\": top, \"bottom\": bottom}\n        logger.debug(\"Created containers: %s\", retval)\n        return retval\n\n    def _handle_key_press(self, event):\n        \"\"\" Keyboard shortcuts\n\n        Parameters\n        ----------\n        event: :class:`tkinter.Event()`\n            The tkinter key press event\n\n        Notes\n        -----\n        The following keys are reserved for the :mod:`tools.lib_manual.editor` classes\n            * Delete - Used for deleting faces\n            * [] - decrease / increase brush size\n            * B, D, E, M - Optional Actions (Brush, Drag, Erase, Zoom)\n        \"\"\"\n        # Alt modifier appears to be broken in Windows so don't use it.\n        modifiers = {0x0001: 'shift',\n                     0x0004: 'ctrl'}\n\n        tk_pos = self._globals.tk_frame_index\n        bindings = {\n            \"z\": self._display.navigation.decrement_frame,\n            \"x\": self._display.navigation.increment_frame,\n            \"space\": self._display.navigation.handle_play_button,\n            \"home\": self._display.navigation.goto_first_frame,\n            \"end\": self._display.navigation.goto_last_frame,\n            \"down\": lambda d=\"down\": self._faces_frame.canvas_scroll(d),\n            \"up\": lambda d=\"up\": self._faces_frame.canvas_scroll(d),\n            \"next\": lambda d=\"page-down\": self._faces_frame.canvas_scroll(d),\n            \"prior\": lambda d=\"page-up\": self._faces_frame.canvas_scroll(d),\n            \"f\": self._display.cycle_filter_mode,\n            \"f1\": lambda k=event.keysym: self._display.set_action(k),\n            \"f2\": lambda k=event.keysym: self._display.set_action(k),\n            \"f3\": lambda k=event.keysym: self._display.set_action(k),\n            \"f4\": lambda k=event.keysym: self._display.set_action(k),\n            \"f5\": lambda k=event.keysym: self._display.set_action(k),\n            \"f9\": lambda k=event.keysym: self._faces_frame.set_annotation_display(k),\n            \"f10\": lambda k=event.keysym: self._faces_frame.set_annotation_display(k),\n            \"c\": lambda f=tk_pos.get(), d=\"prev\": self._detected_faces.update.copy(f, d),\n            \"v\": lambda f=tk_pos.get(), d=\"next\": self._detected_faces.update.copy(f, d),\n            \"ctrl_s\": self._detected_faces.save,\n            \"r\": lambda f=tk_pos.get(): self._detected_faces.revert_to_saved(f)}\n\n        # Allow keypad keys to be used for numbers\n        press = event.keysym.replace(\"KP_\", \"\") if event.keysym.startswith(\"KP_\") else event.keysym\n        modifier = \"_\".join(val for key, val in modifiers.items() if event.state & key != 0)\n        key_press = \"_\".join([modifier, press]) if modifier else press\n        if key_press.lower() in bindings:\n            logger.trace(\"key press: %s, action: %s\", key_press, bindings[key_press.lower()])\n            self.focus_set()\n            bindings[key_press.lower()]()\n\n    def _set_initial_layout(self):\n        \"\"\" Set the favicon and the bottom frame position to correct location to display full\n        frame window.\n\n        Notes\n        -----\n        The favicon pops the tkinter GUI (without loaded elements) as soon as it is called, so\n        this is set last.\n        \"\"\"\n        logger.debug(\"Setting initial layout\")\n        self.tk.call(\"wm\",\n                     \"iconphoto\",\n                     self._w, get_images().icons[\"favicon\"])  # pylint:disable=protected-access\n        location = int(self.winfo_screenheight() // 1.5)\n        self._containers[\"main\"].sashpos(0, location)\n        self.update_idletasks()\n\n    def process(self):\n        \"\"\" The entry point for the Visual Alignments tool from :mod:`lib.tools.manual.cli`.\n\n        Launch the tkinter Visual Alignments Window and run main loop.\n        \"\"\"\n        logger.debug(\"Launching mainloop\")\n        self.mainloop()\n\n\nclass _Options(ttk.Frame):  # pylint:disable=too-many-ancestors\n    \"\"\" Control panel options for currently displayed Editor. This is the right hand panel of the\n    GUI that holds editor specific settings and annotation display settings.\n\n    parent: :class:`tkinter.ttk.Frame`\n        The parent frame for the control panel options\n    tk_globals: :class:`~tools.manual.manual.TkGlobals`\n        The tkinter variables that apply to the whole of the GUI\n    display_frame: :class:`DisplayFrame`\n        The frame that holds the editors\n    \"\"\"\n    def __init__(self, parent, tk_globals, display_frame):\n        logger.debug(\"Initializing %s: (parent: %s, tk_globals: %s, display_frame: %s)\",\n                     self.__class__.__name__, parent, tk_globals, display_frame)\n        super().__init__(parent)\n\n        self._globals = tk_globals\n        self._display_frame = display_frame\n        self._control_panels = self._initialize()\n        self._set_tk_callbacks()\n        self._update_options()\n        self.pack(side=tk.RIGHT, fill=tk.Y)\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    def _initialize(self):\n        \"\"\" Initialize all of the control panels, then display the default panel.\n\n        Adds the control panel to :attr:`_control_panels` and sets the traceback to update\n        display when a panel option has been changed.\n\n        Notes\n        -----\n        All panels must be initialized at the beginning so that the global format options are not\n        reset to default when the editor is first selected.\n\n        The Traceback must be set after the panel has first been packed as otherwise it interferes\n        with the loading of the faces pane.\n        \"\"\"\n        self._initialize_face_options()\n        frame = ttk.Frame(self)\n        frame.pack(side=tk.TOP, fill=tk.BOTH, expand=True)\n        panels = {}\n        for name, editor in self._display_frame.editors.items():\n            logger.debug(\"Initializing control panel for '%s' editor\", name)\n            controls = editor.controls\n            panel = ControlPanel(frame, controls[\"controls\"],\n                                 option_columns=2,\n                                 columns=1,\n                                 max_columns=1,\n                                 header_text=controls[\"header\"],\n                                 blank_nones=False,\n                                 label_width=12,\n                                 style=\"CPanel\",\n                                 scrollbar=False)\n            panel.pack_forget()\n            panels[name] = panel\n        return panels\n\n    def _initialize_face_options(self):\n        \"\"\" Set the Face Viewer options panel, beneath the standard control options. \"\"\"\n        frame = ttk.Frame(self)\n        frame.pack(side=tk.BOTTOM, fill=tk.X, padx=5, pady=5)\n        size_frame = ttk.Frame(frame)\n        size_frame.pack(side=tk.RIGHT)\n        lbl = ttk.Label(size_frame, text=\"Face Size:\")\n        lbl.pack(side=tk.LEFT)\n        cmb = ttk.Combobox(size_frame,\n                           value=[\"Tiny\", \"Small\", \"Medium\", \"Large\", \"Extra Large\"],\n                           state=\"readonly\",\n                           textvariable=self._globals.tk_faces_size)\n        self._globals.tk_faces_size.set(\"Medium\")\n        cmb.pack(side=tk.RIGHT, padx=5)\n\n    def _set_tk_callbacks(self):\n        \"\"\" Sets the callback to change to the relevant control panel options when the selected\n        editor is changed, and the display update on panel option change.\"\"\"\n        self._display_frame.tk_selected_action.trace(\"w\", self._update_options)\n        seen_controls = set()\n        for name, editor in self._display_frame.editors.items():\n            for ctl in editor.controls[\"controls\"]:\n                if ctl in seen_controls:\n                    # Some controls are re-used (annotation format), so skip if trace has already\n                    # been set\n                    continue\n                logger.debug(\"Adding control update callback: (editor: %s, control: %s)\",\n                             name, ctl.title)\n                seen_controls.add(ctl)\n                ctl.tk_var.trace(\"w\", lambda *e: self._globals.tk_update.set(True))\n\n    def _update_options(self, *args):  # pylint:disable=unused-argument\n        \"\"\" Update the control panel display for the current editor.\n\n        If the options have not already been set, then adds the control panel to\n        :attr:`_control_panels`. Displays the current editor's control panel\n\n        Parameters\n        ----------\n        args: tuple\n            Unused but required for tkinter variable callback\n        \"\"\"\n        self._clear_options_frame()\n        editor = self._display_frame.tk_selected_action.get()\n        logger.debug(\"Displaying control panel for editor: '%s'\", editor)\n        self._control_panels[editor].pack(expand=True, fill=tk.BOTH)\n\n    def _clear_options_frame(self):\n        \"\"\" Hides the currently displayed control panel \"\"\"\n        for editor, panel in self._control_panels.items():\n            if panel.winfo_ismapped():\n                logger.debug(\"Hiding control panel for: %s\", editor)\n                panel.pack_forget()\n\n\nclass TkGlobals():\n    \"\"\" Holds Tkinter Variables and other frame information that need to be accessible from all\n    areas of the GUI.\n\n    Parameters\n    ----------\n    input_location: str\n        The location of the input folder of frames or video file\n    \"\"\"\n    def __init__(self, input_location):\n        logger.debug(\"Initializing %s: (input_location: %s)\",\n                     self.__class__.__name__, input_location)\n        self._tk_vars = self._get_tk_vars()\n\n        self._is_video = self._check_input(input_location)\n        self._frame_count = 0  # set by FrameLoader\n        self._frame_display_dims = (int(round(896 * get_config().scaling_factor)),\n                                    int(round(504 * get_config().scaling_factor)))\n        self._current_frame = {\"image\": None,\n                               \"scale\": None,\n                               \"interpolation\": None,\n                               \"display_dims\": None,\n                               \"filename\": None}\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    @classmethod\n    def _get_tk_vars(cls):\n        \"\"\" Create and initialize the tkinter variables.\n\n        Returns\n        -------\n        dict\n            The variable name as key, the variable as value\n        \"\"\"\n        retval = {}\n        for name in (\"frame_index\", \"transport_index\", \"face_index\", \"filter_distance\"):\n            var = tk.IntVar()\n            var.set(10 if name == \"filter_distance\" else 0)\n            retval[name] = var\n        for name in (\"update\", \"update_active_viewport\", \"is_zoomed\"):\n            var = tk.BooleanVar()\n            var.set(False)\n            retval[name] = var\n        for name in (\"filter_mode\", \"faces_size\"):\n            retval[name] = tk.StringVar()\n        return retval\n\n    @property\n    def current_frame(self):\n        \"\"\" dict: The currently displayed frame in the frame viewer with it's meta information. Key\n        and Values are as follows:\n\n            **image** (:class:`numpy.ndarry`): The currently displayed frame in original dimensions\n\n            **scale** (`float`): The scaling factor to use to resize the image to the display\n            window\n\n            **interpolation** (`int`): The opencv interpolator ID to use for resizing the image to\n            the display window\n\n            **display_dims** (`tuple`): The size of the currently displayed frame, sized for the\n            display window\n\n            **filename** (`str`): The filename of the currently displayed frame\n        \"\"\"\n        return self._current_frame\n\n    @property\n    def frame_count(self):\n        \"\"\" int: The total number of frames for the input location \"\"\"\n        return self._frame_count\n\n    @property\n    def tk_face_index(self):\n        \"\"\" :class:`tkinter.IntVar`: The variable that holds the face index of the selected face\n        within the current frame when in zoomed mode. \"\"\"\n        return self._tk_vars[\"face_index\"]\n\n    @property\n    def tk_update_active_viewport(self):\n        \"\"\" :class:`tkinter.BooleanVar`: Boolean Variable that is traced by the viewport's active\n        frame to update.. \"\"\"\n        return self._tk_vars[\"update_active_viewport\"]\n\n    @property\n    def face_index(self):\n        \"\"\" int: The currently displayed face index when in zoomed mode. \"\"\"\n        return self._tk_vars[\"face_index\"].get()\n\n    @property\n    def frame_display_dims(self):\n        \"\"\" tuple: The (`width`, `height`) of the video display frame in pixels. \"\"\"\n        return self._frame_display_dims\n\n    @property\n    def frame_index(self):\n        \"\"\" int: The currently displayed frame index. NB This returns -1 if there are no frames\n        that meet the currently selected filter criteria. \"\"\"\n        return self._tk_vars[\"frame_index\"].get()\n\n    @property\n    def tk_frame_index(self):\n        \"\"\" :class:`tkinter.IntVar`: The variable holding the current frame index. \"\"\"\n        return self._tk_vars[\"frame_index\"]\n\n    @property\n    def filter_mode(self):\n        \"\"\" str: The currently selected navigation mode. \"\"\"\n        return self._tk_vars[\"filter_mode\"].get()\n\n    @property\n    def tk_filter_mode(self):\n        \"\"\" :class:`tkinter.StringVar`: The variable holding the currently selected navigation\n        filter mode. \"\"\"\n        return self._tk_vars[\"filter_mode\"]\n\n    @property\n    def tk_filter_distance(self):\n        \"\"\" :class:`tkinter.DoubleVar`: The variable holding the currently selected threshold\n        distance for misaligned filter mode. \"\"\"\n        return self._tk_vars[\"filter_distance\"]\n\n    @property\n    def tk_faces_size(self):\n        \"\"\" :class:`tkinter.StringVar`: The variable holding the currently selected Faces Viewer\n        thumbnail size. \"\"\"\n        return self._tk_vars[\"faces_size\"]\n\n    @property\n    def is_video(self):\n        \"\"\" bool: ``True`` if the input is a video file, ``False`` if it is a folder of images. \"\"\"\n        return self._is_video\n\n    @property\n    def tk_is_zoomed(self):\n        \"\"\" :class:`tkinter.BooleanVar`: The variable holding the value indicating whether the\n        frame viewer is zoomed into a face or zoomed out to the full frame. \"\"\"\n        return self._tk_vars[\"is_zoomed\"]\n\n    @property\n    def is_zoomed(self):\n        \"\"\" bool: ``True`` if the frame viewer is zoomed into a face, ``False`` if the frame viewer\n        is displaying a full frame. \"\"\"\n        return self._tk_vars[\"is_zoomed\"].get()\n\n    @property\n    def tk_transport_index(self):\n        \"\"\" :class:`tkinter.IntVar`: The current index of the display frame's transport slider. \"\"\"\n        return self._tk_vars[\"transport_index\"]\n\n    @property\n    def tk_update(self):\n        \"\"\" :class:`tkinter.BooleanVar`: The variable holding the trigger that indicates that a\n        full update needs to occur. \"\"\"\n        return self._tk_vars[\"update\"]\n\n    @staticmethod\n    def _check_input(frames_location):\n        \"\"\" Check whether the input is a video\n\n        Parameters\n        ----------\n        frames_location: str\n            The input location for video or images\n\n        Returns\n        -------\n        bool: 'True' if input is a video 'False' if it is a folder.\n        \"\"\"\n        if os.path.isdir(frames_location):\n            retval = False\n        elif os.path.splitext(frames_location)[1].lower() in VIDEO_EXTENSIONS:\n            retval = True\n        else:\n            logger.error(\"The input location '%s' is not valid\", frames_location)\n            sys.exit(1)\n        logger.debug(\"Input '%s' is_video: %s\", frames_location, retval)\n        return retval\n\n    def set_frame_count(self, count):\n        \"\"\" Set the count of total number of frames to :attr:`frame_count` when the\n        :class:`FramesLoader` has completed loading.\n\n        Parameters\n        ----------\n        count: int\n            The number of frames that exist for this session\n        \"\"\"\n        logger.debug(\"Setting frame_count to : %s\", count)\n        self._frame_count = count\n\n    def set_current_frame(self, image, filename):\n        \"\"\" Set the frame and meta information for the currently displayed frame. Populates the\n        attribute :attr:`current_frame`\n\n        Parameters\n        ----------\n        image: :class:`numpy.ndarray`\n            The image used to display in the Frame Viewer\n        filename: str\n            The filename of the current frame\n        \"\"\"\n        scale = min(self.frame_display_dims[0] / image.shape[1],\n                    self.frame_display_dims[1] / image.shape[0])\n        self._current_frame[\"image\"] = image\n        self._current_frame[\"filename\"] = filename\n        self._current_frame[\"scale\"] = scale\n        self._current_frame[\"interpolation\"] = cv2.INTER_CUBIC if scale > 1.0 else cv2.INTER_AREA\n        self._current_frame[\"display_dims\"] = (int(round(image.shape[1] * scale)),\n                                               int(round(image.shape[0] * scale)))\n        logger.trace({k: v.shape if isinstance(v, np.ndarray) else v\n                      for k, v in self._current_frame.items()})\n\n    def set_frame_display_dims(self, width, height):\n        \"\"\" Set the size, in pixels, of the video frame display window and resize the displayed\n        frame.\n\n        Used on a frame resize callback, sets the :attr:frame_display_dims`.\n\n        Parameters\n        ----------\n        width: int\n            The width of the frame holding the video canvas in pixels\n        height: int\n            The height of the frame holding the video canvas in pixels\n        \"\"\"\n        self._frame_display_dims = (int(width), int(height))\n        image = self._current_frame[\"image\"]\n        scale = min(self.frame_display_dims[0] / image.shape[1],\n                    self.frame_display_dims[1] / image.shape[0])\n        self._current_frame[\"scale\"] = scale\n        self._current_frame[\"interpolation\"] = cv2.INTER_CUBIC if scale > 1.0 else cv2.INTER_AREA\n        self._current_frame[\"display_dims\"] = (int(round(image.shape[1] * scale)),\n                                               int(round(image.shape[0] * scale)))\n        logger.trace({k: v.shape if isinstance(v, np.ndarray) else v\n                      for k, v in self._current_frame.items()})\n\n\nclass Aligner():\n    \"\"\" The :class:`Aligner` class sets up an extraction pipeline for each of the current Faceswap\n    Aligners, along with the Landmarks based Maskers. When new landmarks are required, the bounding\n    boxes from the GUI are passed to this class for pushing through the pipeline. The resulting\n    Landmarks and Masks are then returned.\n\n    Parameters\n    ----------\n    tk_globals: :class:`~tools.manual.manual.TkGlobals`\n        The tkinter variables that apply to the whole of the GUI\n    exclude_gpus: list or ``None``\n        A list of indices correlating to connected GPUs that Tensorflow should not use. Pass\n        ``None`` to not exclude any GPUs.\n    \"\"\"\n    def __init__(self, tk_globals: TkGlobals, exclude_gpus: list[int] | None) -> None:\n        logger.debug(\"Initializing: %s (tk_globals: %s, exclude_gpus: %s)\",\n                     self.__class__.__name__, tk_globals, exclude_gpus)\n        self._globals = tk_globals\n        self._exclude_gpus = exclude_gpus\n\n        self._detected_faces: DetectedFaces | None = None\n        self._frame_index: int | None = None\n        self._face_index: int | None = None\n\n        self._aligners: dict[TypeManualExtractor, Extractor | None] = {\"cv2-dnn\": None,\n                                                                       \"FAN\": None,\n                                                                       \"mask\": None}\n        self._aligner: TypeManualExtractor = \"FAN\"\n\n        self._init_thread = self._background_init_aligner()\n        logger.debug(\"Initialized: %s\", self.__class__.__name__)\n\n    @property\n    def _in_queue(self) -> EventQueue:\n        \"\"\" :class:`queue.Queue` - The input queue to the extraction pipeline. \"\"\"\n        aligner = self._aligners[self._aligner]\n        assert aligner is not None\n        return aligner.input_queue\n\n    @property\n    def _feed_face(self) -> ExtractMedia:\n        \"\"\" :class:`~plugins.extract.extract_media.ExtractMedia`: The current face for feeding into\n        the aligner, formatted for the pipeline \"\"\"\n        assert self._frame_index is not None\n        assert self._face_index is not None\n        assert self._detected_faces is not None\n        face = self._detected_faces.current_faces[self._frame_index][self._face_index]\n        return ExtractMedia(\n            self._globals.current_frame[\"filename\"],\n            self._globals.current_frame[\"image\"],\n            detected_faces=[face])\n\n    @property\n    def is_initialized(self) -> bool:\n        \"\"\" bool: The Aligners are initialized in a background thread so that other tasks can be\n        performed whilst we wait for initialization. ``True`` is returned if the aligner has\n        completed initialization otherwise ``False``.\"\"\"\n        thread_is_alive = self._init_thread.is_alive()\n        if thread_is_alive:\n            logger.trace(\"Aligner not yet initialized\")  # type:ignore[attr-defined]\n            self._init_thread.check_and_raise_error()\n        else:\n            logger.trace(\"Aligner initialized\")  # type:ignore[attr-defined]\n            self._init_thread.join()\n        return not thread_is_alive\n\n    def _background_init_aligner(self) -> MultiThread:\n        \"\"\" Launch the aligner in a background thread so we can run other tasks whilst\n        waiting for initialization\n\n        Returns\n        -------\n        :class:`lib.multithreading.MultiThread\n            The background aligner loader thread\n        \"\"\"\n        logger.debug(\"Launching aligner initialization thread\")\n        thread = MultiThread(self._init_aligner,\n                             thread_count=1,\n                             name=f\"{self.__class__.__name__}.init_aligner\")\n        thread.start()\n        logger.debug(\"Launched aligner initialization thread\")\n        return thread\n\n    def _init_aligner(self) -> None:\n        \"\"\" Initialize Aligner in a background thread, and set it to :attr:`_aligner`. \"\"\"\n        logger.debug(\"Initialize Aligner\")\n        # Make sure non-GPU aligner is allocated first\n        for model in T.get_args(TypeManualExtractor):\n            logger.debug(\"Initializing aligner: %s\", model)\n            plugin = None if model == \"mask\" else model\n            exclude_gpus = self._exclude_gpus if model == \"FAN\" else None\n            aligner = Extractor(None,\n                                plugin,\n                                [\"components\", \"extended\"],\n                                exclude_gpus=exclude_gpus,\n                                multiprocess=True,\n                                normalize_method=\"hist\",\n                                disable_filter=True)\n            if plugin:\n                aligner.set_batchsize(\"align\", 1)  # Set the batchsize to 1\n            aligner.launch()\n            logger.debug(\"Initialized %s Extractor\", model)\n            self._aligners[model] = aligner\n\n    def link_faces(self, detected_faces: DetectedFaces) -> None:\n        \"\"\" As the Aligner has the potential to take the longest to initialize, it is kicked off\n        as early as possible. At this time :class:`~tools.manual.detected_faces.DetectedFaces` is\n        not yet available.\n\n        Once the Aligner has initialized, this function is called to add the\n        :class:`~tools.manual.detected_faces.DetectedFaces` class as a property of the Aligner.\n\n        Parameters\n        ----------\n        detected_faces: :class:`~tools.manual.detected_faces.DetectedFaces`\n            The class that holds the :class:`~lib.align.DetectedFace` objects for the\n            current Manual session\n        \"\"\"\n        logger.debug(\"Linking detected_faces: %s\", detected_faces)\n        self._detected_faces = detected_faces\n\n    def get_landmarks(self, frame_index: int, face_index: int, aligner: TypeManualExtractor\n                      ) -> np.ndarray:\n        \"\"\" Feed the detected face into the alignment pipeline and retrieve the landmarks.\n\n        The face to feed into the aligner is generated from the given frame and face indices.\n\n        Parameters\n        ----------\n        frame_index: int\n            The frame index to extract the aligned face for\n        face_index: int\n            The face index within the current frame to extract the face for\n        aligner: Literal[\"FAN\", \"cv2-dnn\"]\n            The aligner to use to extract the face\n\n        Returns\n        -------\n        :class:`numpy.ndarray`\n            The 68 point landmark alignments\n        \"\"\"\n        logger.trace(\"frame_index: %s, face_index: %s, aligner: %s\",  # type:ignore[attr-defined]\n                     frame_index, face_index, aligner)\n        self._frame_index = frame_index\n        self._face_index = face_index\n        self._aligner = aligner\n        self._in_queue.put(self._feed_face)\n        extractor = self._aligners[aligner]\n        assert extractor is not None\n        detected_face = next(extractor.detected_faces()).detected_faces[0]\n        logger.trace(\"landmarks: %s\", detected_face.landmarks_xy)  # type:ignore[attr-defined]\n        return detected_face.landmarks_xy\n\n    def _remove_nn_masks(self, detected_face: DetectedFace) -> None:\n        \"\"\" Remove any non-landmarks based masks on a landmark edit\n\n        Parameters\n        ----------\n        detected_face:\n            The detected face object to remove masks from\n        \"\"\"\n        del_masks = {m for m in detected_face.mask if m not in (\"components\", \"extended\")}\n        logger.debug(\"Removing masks after landmark update: %s\", del_masks)\n        for mask in del_masks:\n            del detected_face.mask[mask]\n\n    def get_masks(self, frame_index: int, face_index: int) -> dict[str, Mask]:\n        \"\"\" Feed the aligned face into the mask pipeline and retrieve the updated masks.\n\n        The face to feed into the aligner is generated from the given frame and face indices.\n        This is to be called when a manual update is done on the landmarks, and new masks need\n        generating.\n\n        Parameters\n        ----------\n        frame_index: int\n            The frame index to extract the aligned face for\n        face_index: int\n            The face index within the current frame to extract the face for\n\n        Returns\n        -------\n        dict[str, :class:`~lib.align.aligned_mask.Mask`]\n            The updated masks\n        \"\"\"\n        logger.trace(\"frame_index: %s, face_index: %s\",  # type:ignore[attr-defined]\n                     frame_index, face_index)\n        self._frame_index = frame_index\n        self._face_index = face_index\n        self._aligner = \"mask\"\n        self._in_queue.put(self._feed_face)\n        assert self._aligners[\"mask\"] is not None\n        detected_face = next(self._aligners[\"mask\"].detected_faces()).detected_faces[0]\n        self._remove_nn_masks(detected_face)\n        logger.debug(\"mask: %s\", detected_face.mask)\n        return detected_face.mask\n\n    def set_normalization_method(self, method: T.Literal[\"none\", \"clahe\", \"hist\", \"mean\"]) -> None:\n        \"\"\" Change the normalization method for faces fed into the aligner.\n        The normalization method is user adjustable from the GUI. When this method is triggered\n        the method is updated for all aligner pipelines.\n\n        Parameters\n        ----------\n        method: Literal[\"none\", \"clahe\", \"hist\", \"mean\"]\n            The normalization method to use\n        \"\"\"\n        logger.debug(\"Setting normalization method to: '%s'\", method)\n        for plugin, aligner in self._aligners.items():\n            assert aligner is not None\n            if plugin == \"mask\":\n                continue\n            logger.debug(\"Setting to: '%s'\", method)\n            aligner.aligner.set_normalize_method(method)\n\n\nclass FrameLoader():\n    \"\"\" Loads the frames, sets the frame count to :attr:`TkGlobals.frame_count` and handles the\n    return of the correct frame for the GUI.\n\n    Parameters\n    ----------\n    tk_globals: :class:`~tools.manual.manual.TkGlobals`\n        The tkinter variables that apply to the whole of the GUI\n    frames_location: str\n        The path to the input frames\n    video_meta_data: dict\n        The meta data held within the alignments file, if it exists and the input is a video\n    \"\"\"\n    def __init__(self, tk_globals, frames_location, video_meta_data):\n        logger.debug(\"Initializing %s: (tk_globals: %s, frames_location: '%s', \"\n                     \"video_meta_data: %s)\", self.__class__.__name__, tk_globals, frames_location,\n                     video_meta_data)\n        self._globals = tk_globals\n        self._loader = None\n        self._current_idx = 0\n        self._init_thread = self._background_init_frames(frames_location, video_meta_data)\n        self._globals.tk_frame_index.trace(\"w\", self._set_frame)\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    @property\n    def is_initialized(self):\n        \"\"\" bool: ``True`` if the Frame Loader has completed initialization otherwise\n        ``False``. \"\"\"\n        thread_is_alive = self._init_thread.is_alive()\n        if thread_is_alive:\n            self._init_thread.check_and_raise_error()\n        else:\n            self._init_thread.join()\n            # Setting the initial frame cannot be done in the thread, so set when queried from main\n            self._set_frame(initialize=True)\n        return not thread_is_alive\n\n    @property\n    def video_meta_data(self):\n        \"\"\" dict: The pts_time and key frames for the loader. \"\"\"\n        return self._loader.video_meta_data\n\n    def _background_init_frames(self, frames_location, video_meta_data):\n        \"\"\" Launch the images loader in a background thread so we can run other tasks whilst\n        waiting for initialization. \"\"\"\n        thread = MultiThread(self._load_images,\n                             frames_location,\n                             video_meta_data,\n                             thread_count=1,\n                             name=f\"{self.__class__.__name__}.init_frames\")\n        thread.start()\n        return thread\n\n    def _load_images(self, frames_location, video_meta_data):\n        \"\"\" Load the images in a background thread. \"\"\"\n        self._loader = SingleFrameLoader(frames_location, video_meta_data=video_meta_data)\n        self._globals.set_frame_count(self._loader.count)\n\n    def _set_frame(self, *args, initialize=False):  # pylint:disable=unused-argument\n        \"\"\" Set the currently loaded frame to :attr:`_current_frame` and trigger a full GUI update.\n\n        If the loader has not been initialized, or the navigation position is the same as the\n        current position and the face is not zoomed in, then this returns having done nothing.\n\n        Parameters\n        ----------\n        args: tuple\n            :class:`tkinter.Event` arguments. Required but not used.\n        initialize: bool, optional\n            ``True`` if initializing for the first frame to be displayed otherwise ``False``.\n            Default: ``False``\n        \"\"\"\n        position = self._globals.frame_index\n        if not initialize and (position == self._current_idx and not self._globals.is_zoomed):\n            logger.trace(\"Update criteria not met. Not updating: (initialize: %s, position: %s, \"\n                         \"current_idx: %s, is_zoomed: %s)\", initialize, position,\n                         self._current_idx, self._globals.is_zoomed)\n            return\n        if position == -1:\n            filename = \"No Frame\"\n            frame = np.ones(self._globals.frame_display_dims + (3, ), dtype=\"uint8\")\n        else:\n            filename, frame = self._loader.image_from_index(position)\n        logger.trace(\"filename: %s, frame: %s, position: %s\", filename, frame.shape, position)\n        self._globals.set_current_frame(frame, filename)\n        self._current_idx = position\n        self._globals.tk_update.set(True)\n        self._globals.tk_update_active_viewport.set(True)\n", "tools/manual/cli.py": "#!/usr/bin/env python3\n\"\"\" The Command Line Arguments for the Manual Editor tool. \"\"\"\nimport argparse\nimport gettext\n\nfrom lib.cli.args import FaceSwapArgs\nfrom lib.cli.actions import DirOrFileFullPaths, FileFullPaths\n\n# LOCALES\n_LANG = gettext.translation(\"tools.manual\", localedir=\"locales\", fallback=True)\n_ = _LANG.gettext\n\n_HELPTEXT = _(\"This command lets you perform various actions on frames, \"\n              \"faces and alignments files using visual tools.\")\n\n\nclass ManualArgs(FaceSwapArgs):\n    \"\"\" Generate the command line options for the Manual Editor Tool.\"\"\"\n\n    @staticmethod\n    def get_info():\n        \"\"\" Obtain the information about what the Manual Tool does. \"\"\"\n        return _(\"A tool to perform various actions on frames, faces and alignments files using \"\n                 \"visual tools\")\n\n    @staticmethod\n    def get_argument_list():\n        \"\"\" Generate the command line argument list for the Manual Tool. \"\"\"\n        argument_list = []\n        argument_list.append({\n            \"opts\": (\"-a\", \"--alignments\"),\n            \"action\": FileFullPaths,\n            \"filetypes\": \"alignments\",\n            \"type\": str,\n            \"group\": _(\"data\"),\n            \"dest\": \"alignments_path\",\n            \"help\": _(\n                \"Path to the alignments file for the input, if not at the default location\")})\n        argument_list.append({\n            \"opts\": (\"-f\", \"--frames\"),\n            \"action\": DirOrFileFullPaths,\n            \"filetypes\": \"video\",\n            \"required\": True,\n            \"group\": _(\"data\"),\n            \"help\": _(\n                \"Video file or directory containing source frames that faces were extracted \"\n                \"from.\")})\n        argument_list.append({\n            \"opts\": (\"-t\", \"--thumb-regen\"),\n            \"action\": \"store_true\",\n            \"dest\": \"thumb_regen\",\n            \"default\": False,\n            \"group\": _(\"options\"),\n            \"help\": _(\n                \"Force regeneration of the low resolution jpg thumbnails in the alignments \"\n                \"file.\")})\n        argument_list.append({\n            \"opts\": (\"-s\", \"--single-process\"),\n            \"action\": \"store_true\",\n            \"dest\": \"single_process\",\n            \"default\": False,\n            \"group\": _(\"options\"),\n            \"help\": _(\n                \"The process attempts to speed up generation of thumbnails by extracting from the \"\n                \"video in parallel threads. For some videos, this causes the caching process to \"\n                \"hang. If this happens, then set this option to generate the thumbnails in a \"\n                \"slower, but more stable single thread.\")})\n        # Deprecated multi-character switches\n        argument_list.append({\n            \"opts\": (\"-al\", ),\n            \"type\": str,\n            \"dest\": \"depr_alignments_al_a\",\n            \"help\": argparse.SUPPRESS})\n        argument_list.append({\n            \"opts\": (\"-fr\", ),\n            \"type\": str,\n            \"dest\": \"depr_frames_fr_f\",\n            \"help\": argparse.SUPPRESS})\n        return argument_list\n", "tools/manual/detected_faces.py": "#!/usr/bin/env python3\n\"\"\" Alignments handling for Faceswap's Manual Adjustments tool. Handles the conversion of\nalignments data to :class:`~lib.align.DetectedFace` objects, and the update of these faces\nwhen edits are made in the GUI. \"\"\"\nfrom __future__ import annotations\nimport logging\nimport os\nimport sys\nimport tkinter as tk\nimport typing as T\nfrom copy import deepcopy\nfrom queue import Queue, Empty\n\nimport cv2\nimport numpy as np\n\nfrom lib.align import Alignments, AlignedFace, DetectedFace\nfrom lib.gui.custom_widgets import PopupProgress\nfrom lib.gui.utils import FileHandler\nfrom lib.image import ImagesLoader, ImagesSaver, encode_image, generate_thumbnail\nfrom lib.multithreading import MultiThread\nfrom lib.utils import get_folder\n\nif T.TYPE_CHECKING:\n    from . import manual\n    from lib.align.alignments import AlignmentFileDict, PNGHeaderDict\n\nlogger = logging.getLogger(__name__)\n\n\nclass DetectedFaces():\n    \"\"\" Handles the manipulation of :class:`~lib.align.DetectedFace` objects stored\n    in the alignments file. Acts as a parent class for the IO operations (saving and loading from\n    an alignments file), the face update operations (when changes are made to alignments in the\n    GUI) and the face filters (when a user changes the filter navigation mode.)\n\n    Parameters\n    ----------\n    tk_globals: :class:`~tools.manual.manual.TkGlobals`\n        The tkinter variables that apply to the whole of the GUI\n    alignments_path: str\n        The full path to the alignments file\n    input_location: str\n        The location of the input folder of frames or video file\n    extractor: :class:`~tools.manual.manual.Aligner`\n        The pipeline for passing faces through the aligner and retrieving results\n    \"\"\"\n    def __init__(self,\n                 tk_globals: manual.TkGlobals,\n                 alignments_path: str,\n                 input_location: str,\n                 extractor: manual.Aligner) -> None:\n        logger.debug(\"Initializing %s: (tk_globals: %s. alignments_path: %s, input_location: %s \"\n                     \"extractor: %s)\", self.__class__.__name__, tk_globals, alignments_path,\n                     input_location, extractor)\n        self._globals = tk_globals\n        self._frame_faces: list[list[DetectedFace]] = []\n        self._updated_frame_indices: set[int] = set()\n\n        self._alignments: Alignments = self._get_alignments(alignments_path, input_location)\n        self._alignments.update_legacy_has_source(os.path.basename(input_location))\n\n        self._extractor = extractor\n        self._tk_vars = self._set_tk_vars()\n\n        self._io = _DiskIO(self, input_location)\n        self._update = FaceUpdate(self)\n        self._filter = Filter(self)\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    # <<<< PUBLIC PROPERTIES >>>> #\n    # << SUBCLASSES >> #\n    @property\n    def extractor(self) -> manual.Aligner:\n        \"\"\" :class:`~tools.manual.manual.Aligner`: The pipeline for passing faces through the\n        aligner and retrieving results. \"\"\"\n        return self._extractor\n\n    @property\n    def filter(self) -> Filter:\n        \"\"\" :class:`Filter`: Handles returning of faces and stats based on the current user set\n        navigation mode filter. \"\"\"\n        return self._filter\n\n    @property\n    def update(self) -> FaceUpdate:\n        \"\"\" :class:`FaceUpdate`: Handles the adding, removing and updating of\n        :class:`~lib.align.DetectedFace` stored within the alignments file. \"\"\"\n        return self._update\n\n    # << TKINTER VARIABLES >> #\n    @property\n    def tk_unsaved(self) -> tk.BooleanVar:\n        \"\"\" :class:`tkinter.BooleanVar`: The variable indicating whether the alignments have been\n        updated since the last save. \"\"\"\n        return self._tk_vars[\"unsaved\"]\n\n    @property\n    def tk_edited(self) -> tk.BooleanVar:\n        \"\"\" :class:`tkinter.BooleanVar`: The variable indicating whether an edit has occurred\n        meaning a GUI redraw needs to be triggered. \"\"\"\n        return self._tk_vars[\"edited\"]\n\n    @property\n    def tk_face_count_changed(self) -> tk.BooleanVar:\n        \"\"\" :class:`tkinter.BooleanVar`: The variable indicating whether a face has been added or\n        removed meaning the :class:`FaceViewer` grid redraw needs to be triggered. \"\"\"\n        return self._tk_vars[\"face_count_changed\"]\n\n    # << STATISTICS >> #\n    @property\n    def available_masks(self) -> dict[str, int]:\n        \"\"\" dict[str, int]: The mask type names stored in the alignments; type as key with the\n        number of faces which possess the mask type as value. \"\"\"\n        return self._alignments.mask_summary\n\n    @property\n    def current_faces(self) -> list[list[DetectedFace]]:\n        \"\"\" list[list[:class:`~lib.align.DetectedFace`]]: The most up to date full list of detected\n        face objects. \"\"\"\n        return self._frame_faces\n\n    @property\n    def video_meta_data(self) -> dict[str, list[int] | list[float] | None]:\n        \"\"\" dict[str, list[int] | list[float] | None]: The frame meta data stored in the alignments\n        file. If data does not exist in the alignments file then ``None`` is returned for each\n        Key \"\"\"\n        return self._alignments.video_meta_data\n\n    @property\n    def face_count_per_index(self) -> list[int]:\n        \"\"\" list[int]: Count of faces for each frame. List is in frame index order.\n\n        The list needs to be calculated on the fly as the number of faces in a frame\n        can change based on user actions. \"\"\"\n        return [len(faces) for faces in self._frame_faces]\n\n    # <<<< PUBLIC METHODS >>>> #\n    def is_frame_updated(self, frame_index: int) -> bool:\n        \"\"\" Check whether the given frame index has been updated\n\n        Parameters\n        ----------\n        frame_index: int\n            The frame index to check\n\n        Returns\n        -------\n        bool:\n            ``True`` if the given frame index has updated faces within it otherwise ``False``\n        \"\"\"\n        return frame_index in self._updated_frame_indices\n\n    def load_faces(self) -> None:\n        \"\"\" Load the faces as :class:`~lib.align.DetectedFace` objects from the alignments\n        file. \"\"\"\n        self._io.load()\n\n    def save(self) -> None:\n        \"\"\" Save the alignments file with the latest edits. \"\"\"\n        self._io.save()\n\n    def revert_to_saved(self, frame_index):\n        \"\"\" Revert the frame's alignments to their saved version for the given frame index.\n\n        Parameters\n        ----------\n        frame_index: int\n            The frame that should have their faces reverted to their saved version\n        \"\"\"\n        self._io.revert_to_saved(frame_index)\n\n    def extract(self) -> None:\n        \"\"\" Extract the faces in the current video to a user supplied folder. \"\"\"\n        self._io.extract()\n\n    def save_video_meta_data(self, pts_time: list[float], keyframes: list[int]) -> None:\n        \"\"\" Save video meta data to the alignments file. This is executed if the video meta data\n        does not already exist in the alignments file, so the video does not need to be scanned\n        on every use of the Manual Tool.\n\n        Parameters\n        ----------\n        pts_time: list[float]\n            A list of presentation timestamps in frame index order for every frame in the input\n            video\n        keyframes: list[int]\n            A list of frame indices corresponding to the key frames in the input video.\n        \"\"\"\n        if self._globals.is_video:\n            self._alignments.save_video_meta_data(pts_time, keyframes)\n\n    # <<<< PRIVATE METHODS >>> #\n    # << INIT >> #\n    @staticmethod\n    def _set_tk_vars() -> dict[T.Literal[\"unsaved\", \"edited\", \"face_count_changed\"],\n                               tk.BooleanVar]:\n        \"\"\" Set the required tkinter variables.\n\n        The alignments specific `unsaved` and `edited` are set here.\n        The global variables are added into the dictionary with `None` as value, so the\n        objects exist. Their actual variables are populated during :func:`load_faces`.\n\n        Returns\n        -------\n        dict\n            The internal variable name as key with the tkinter variable as value\n        \"\"\"\n        retval = {}\n        for name in T.get_args(T.Literal[\"unsaved\", \"edited\", \"face_count_changed\"]):\n            var = tk.BooleanVar()\n            var.set(False)\n            retval[name] = var\n        logger.debug(retval)\n        return retval\n\n    def _get_alignments(self, alignments_path: str, input_location: str) -> Alignments:\n        \"\"\" Get the :class:`~lib.align.Alignments` object for the given location.\n\n        Parameters\n        ----------\n        alignments_path: str\n            Full path to the alignments file. If empty string is passed then location is calculated\n            from the source folder\n        input_location: str\n            The location of the input folder of frames or video file\n\n        Returns\n        -------\n        :class:`~lib.align.Alignments`\n            The alignments object for the given input location\n        \"\"\"\n        logger.debug(\"alignments_path: %s, input_location: %s\", alignments_path, input_location)\n        if alignments_path:\n            folder, filename = os.path.split(alignments_path)\n        else:\n            filename = \"alignments.fsa\"\n            if self._globals.is_video:\n                folder, vid = os.path.split(os.path.splitext(input_location)[0])\n                filename = f\"{vid}_{filename}\"\n            else:\n                folder = input_location\n        retval = Alignments(folder, filename)\n        if retval.version == 1.0:\n            logger.error(\"The Manual Tool is not compatible with legacy Alignments files.\")\n            logger.info(\"You can update legacy Alignments files by using the Extract job in the \"\n                        \"Alignments tool to re-extract the faces in full-head format.\")\n            sys.exit(0)\n        logger.debug(\"folder: %s, filename: %s, alignments: %s\", folder, filename, retval)\n        return retval\n\n\nclass _DiskIO():\n    \"\"\" Handles the loading of :class:`~lib.align.DetectedFaces` from the alignments file\n    into :class:`DetectedFaces` and the saving of this data (in the opposite direction) to an\n    alignments file.\n\n    Parameters\n    ----------\n    detected_faces: :class:`DetectedFaces`\n        The parent :class:`DetectedFaces` object\n    input_location: str\n        The location of the input folder of frames or video file\n    \"\"\"\n    def __init__(self, detected_faces: DetectedFaces, input_location: str) -> None:\n        logger.debug(\"Initializing %s: (detected_faces: %s, input_location: %s)\",\n                     self.__class__.__name__, detected_faces, input_location)\n        self._input_location = input_location\n        self._alignments = detected_faces._alignments\n        self._frame_faces = detected_faces._frame_faces\n        self._updated_frame_indices = detected_faces._updated_frame_indices\n        self._tk_unsaved = detected_faces.tk_unsaved\n        self._tk_edited = detected_faces.tk_edited\n        self._tk_face_count_changed = detected_faces.tk_face_count_changed\n        self._globals = detected_faces._globals\n\n        # Must be populated after loading faces as video_meta_data may have increased frame count\n        self._sorted_frame_names: list[str] = []\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    def load(self) -> None:\n        \"\"\" Load the faces from the alignments file, convert to\n        :class:`~lib.align.DetectedFace`. objects and add to :attr:`_frame_faces`. \"\"\"\n        for key in sorted(self._alignments.data):\n            this_frame_faces: list[DetectedFace] = []\n            for item in self._alignments.data[key][\"faces\"]:\n                face = DetectedFace()\n                face.from_alignment(item, with_thumb=True)\n                face.load_aligned(None)\n                _ = face.aligned.average_distance  # cache the distances\n                this_frame_faces.append(face)\n            self._frame_faces.append(this_frame_faces)\n        self._sorted_frame_names = sorted(self._alignments.data)\n\n    def save(self) -> None:\n        \"\"\" Convert updated :class:`~lib.align.DetectedFace` objects to alignments format\n        and save the alignments file. \"\"\"\n        if not self._tk_unsaved.get():\n            logger.debug(\"Alignments not updated. Returning\")\n            return\n        frames = list(self._updated_frame_indices)\n        logger.verbose(\"Saving alignments for %s updated frames\",  # type:ignore[attr-defined]\n                       len(frames))\n\n        for idx, faces in zip(frames,\n                              np.array(self._frame_faces, dtype=\"object\")[np.array(frames)]):\n            frame = self._sorted_frame_names[idx]\n            self._alignments.data[frame][\"faces\"] = [face.to_alignment() for face in faces]\n\n        self._alignments.backup()\n        self._alignments.save()\n        self._updated_frame_indices.clear()\n        self._tk_unsaved.set(False)\n\n    def revert_to_saved(self, frame_index: int) -> None:\n        \"\"\" Revert the frame's alignments to their saved version for the given frame index.\n\n        Parameters\n        ----------\n        frame_index: int\n            The frame that should have their faces reverted to their saved version\n        \"\"\"\n        if frame_index not in self._updated_frame_indices:\n            logger.debug(\"Alignments not amended. Returning\")\n            return\n        logger.verbose(\"Reverting alignments for frame_index %s\",  # type:ignore[attr-defined]\n                       frame_index)\n        alignments = self._alignments.data[self._sorted_frame_names[frame_index]][\"faces\"]\n        faces = self._frame_faces[frame_index]\n\n        reset_grid = self._add_remove_faces(alignments, faces)\n\n        for detected_face, face in zip(faces, alignments):\n            detected_face.from_alignment(face, with_thumb=True)\n            detected_face.load_aligned(None, force=True)\n            _ = detected_face.aligned.average_distance  # cache the distances\n\n        self._updated_frame_indices.remove(frame_index)\n        if not self._updated_frame_indices:\n            self._tk_unsaved.set(False)\n\n        if reset_grid:\n            self._tk_face_count_changed.set(True)\n        else:\n            self._tk_edited.set(True)\n        self._globals.tk_update.set(True)\n\n    @classmethod\n    def _add_remove_faces(cls,\n                          alignments: list[AlignmentFileDict],\n                          faces: list[DetectedFace]) -> bool:\n        \"\"\" On a revert, ensure that the alignments and detected face object counts for each frame\n        are in sync.\n\n        Parameters\n        ----------\n        alignments: list[:class:`~lib.align.alignments.AlignmentFileDict`]\n            Alignments stored for a frame\n\n        faces: list[:class:`~lib.align.DetectedFace`]\n            List of detected faces for a frame\n\n        Returns\n        -------\n        bool\n            ``True`` if a face was added or removed otherwise ``False``\n        \"\"\"\n        num_alignments = len(alignments)\n        num_faces = len(faces)\n        if num_alignments == num_faces:\n            retval = False\n        elif num_alignments > num_faces:\n            faces.extend([DetectedFace() for _ in range(num_faces, num_alignments)])\n            retval = True\n        else:\n            del faces[num_alignments:]\n            retval = True\n        return retval\n\n    def extract(self) -> None:\n        \"\"\" Extract the current faces to a folder.\n\n        To stop the GUI becoming completely unresponsive (particularly in Windows) the extract is\n        done in a background thread, with the process count passed back in a queue to the main\n        thread to update the progress bar.\n        \"\"\"\n        dirname = FileHandler(\"dir\", None,\n                              initial_folder=os.path.dirname(self._input_location),\n                              title=\"Select output folder...\").return_file\n        if not dirname:\n            return\n        logger.debug(dirname)\n\n        queue: Queue = Queue()\n        pbar = PopupProgress(\"Extracting Faces...\", self._alignments.frames_count + 1)\n        thread = MultiThread(self._background_extract, dirname, queue)\n        thread.start()\n        self._monitor_extract(thread, queue, pbar)\n\n    def _monitor_extract(self,\n                         thread: MultiThread,\n                         queue: Queue,\n                         progress_bar: PopupProgress) -> None:\n        \"\"\" Monitor the extraction thread, and update the progress bar.\n\n        On completion, save alignments and clear progress bar.\n\n        Parameters\n        ----------\n        thread: :class:`~lib.multithreading.MultiThread`\n            The thread that is performing the extraction task\n        queue: :class:`queue.Queue`\n            The queue that the worker thread is putting it's incremental counts to\n        progress_bar: :class:`~lib.gui.custom_widget.PopupProgress`\n            The popped up progress bar\n        \"\"\"\n        thread.check_and_raise_error()\n        if not thread.is_alive():\n            thread.join()\n            progress_bar.stop()\n            return\n\n        while True:\n            try:\n                progress_bar.step(queue.get(False, 0))\n            except Empty:\n                break\n        progress_bar.after(100, self._monitor_extract, thread, queue, progress_bar)\n\n    def _background_extract(self, output_folder: str, progress_queue: Queue) -> None:\n        \"\"\" Perform the background extraction in a thread so GUI doesn't become unresponsive.\n\n        Parameters\n        ----------\n        output_folder: str\n            The location to save the output faces to\n        progress_queue: :class:`queue.Queue`\n            The queue to place incremental counts to for updating the GUI's progress bar\n        \"\"\"\n        saver = ImagesSaver(get_folder(output_folder), as_bytes=True)\n        loader = ImagesLoader(self._input_location, count=self._alignments.frames_count)\n        for frame_idx, (filename, image) in enumerate(loader.load()):\n            logger.trace(\"Outputting frame: %s: %s\",  # type:ignore[attr-defined]\n                         frame_idx, filename)\n            src_filename = os.path.basename(filename)\n            progress_queue.put(1)\n\n            for face_idx, face in enumerate(self._frame_faces[frame_idx]):\n                output = f\"{os.path.splitext(src_filename)[0]}_{face_idx}.png\"\n                aligned = AlignedFace(face.landmarks_xy,\n                                      image=image,\n                                      centering=\"head\",\n                                      size=512)  # TODO user selectable size\n                meta: PNGHeaderDict = {\"alignments\": face.to_png_meta(),\n                                       \"source\": {\"alignments_version\": self._alignments.version,\n                                                  \"original_filename\": output,\n                                                  \"face_index\": face_idx,\n                                                  \"source_filename\": src_filename,\n                                                  \"source_is_video\": self._globals.is_video,\n                                                  \"source_frame_dims\": image.shape[:2]}}\n\n                assert aligned.face is not None\n                b_image = encode_image(aligned.face, \".png\", metadata=meta)\n                saver.save(output, b_image)\n        saver.close()\n\n\nclass Filter():\n    \"\"\" Returns stats and frames for filtered frames based on the user selected navigation mode\n    filter.\n\n    Parameters\n    ----------\n    detected_faces: :class:`DetectedFaces`\n        The parent :class:`DetectedFaces` object\n    \"\"\"\n    def __init__(self, detected_faces: DetectedFaces) -> None:\n        logger.debug(\"Initializing %s: (detected_faces: %s)\",\n                     self.__class__.__name__, detected_faces)\n        self._globals = detected_faces._globals\n        self._detected_faces = detected_faces\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    @property\n    def frame_meets_criteria(self) -> bool:\n        \"\"\" bool: ``True`` if the current frame meets the selected filter criteria otherwise\n        ``False`` \"\"\"\n        filter_mode = self._globals.filter_mode\n        frame_faces = self._detected_faces.current_faces[self._globals.frame_index]\n        distance = self._filter_distance\n\n        retval = (\n            filter_mode == \"All Frames\" or\n            (filter_mode == \"No Faces\" and not frame_faces) or\n            (filter_mode == \"Has Face(s)\" and len(frame_faces) > 0) or\n            (filter_mode == \"Multiple Faces\" and len(frame_faces) > 1) or\n            (filter_mode == \"Misaligned Faces\" and any(face.aligned.average_distance > distance\n                                                       for face in frame_faces)))\n        assert isinstance(retval, bool)\n        logger.trace(\"filter_mode: %s, frame meets criteria: %s\",  # type:ignore[attr-defined]\n                     filter_mode, retval)\n        return retval\n\n    @property\n    def _filter_distance(self) -> float:\n        \"\"\" float: The currently selected distance when Misaligned Faces filter is selected. \"\"\"\n        try:\n            retval = self._globals.tk_filter_distance.get()\n        except tk.TclError:\n            # Suppress error when distance box is empty\n            retval = 0\n        return retval / 100.\n\n    @property\n    def count(self) -> int:\n        \"\"\" int: The number of frames that meet the filter criteria returned by\n        :attr:`~tools.manual.manual.TkGlobals.filter_mode`. \"\"\"\n        face_count_per_index = self._detected_faces.face_count_per_index\n        if self._globals.filter_mode == \"No Faces\":\n            retval = sum(1 for fcount in face_count_per_index if fcount == 0)\n        elif self._globals.filter_mode == \"Has Face(s)\":\n            retval = sum(1 for fcount in face_count_per_index if fcount != 0)\n        elif self._globals.filter_mode == \"Multiple Faces\":\n            retval = sum(1 for fcount in face_count_per_index if fcount > 1)\n        elif self._globals.filter_mode == \"Misaligned Faces\":\n            distance = self._filter_distance\n            retval = sum(1 for frame in self._detected_faces.current_faces\n                         if any(face.aligned.average_distance > distance for face in frame))\n        else:\n            retval = len(face_count_per_index)\n        logger.trace(\"filter mode: %s, frame count: %s\",  # type:ignore[attr-defined]\n                     self._globals.filter_mode, retval)\n        return retval\n\n    @property\n    def raw_indices(self) -> dict[T.Literal[\"frame\", \"face\"], list[int]]:\n        \"\"\" dict[str, int]: The frame and face indices that meet the current filter criteria for\n        each displayed face. \"\"\"\n        frame_indices: list[int] = []\n        face_indices: list[int] = []\n        face_counts = self._detected_faces.face_count_per_index  # Copy to avoid recalculations\n\n        for frame_idx in self.frames_list:\n            for face_idx in range(face_counts[frame_idx]):\n                frame_indices.append(frame_idx)\n                face_indices.append(face_idx)\n\n        retval: dict[T.Literal[\"frame\", \"face\"], list[int]] = {\"frame\": frame_indices,\n                                                               \"face\": face_indices}\n        logger.trace(\"frame_indices: %s, face_indices: %s\",  # type:ignore[attr-defined]\n                     frame_indices, face_indices)\n        return retval\n\n    @property\n    def frames_list(self) -> list[int]:\n        \"\"\" list[int]: The list of frame indices that meet the filter criteria returned by\n        :attr:`~tools.manual.manual.TkGlobals.filter_mode`. \"\"\"\n        face_count_per_index = self._detected_faces.face_count_per_index\n        if self._globals.filter_mode == \"No Faces\":\n            retval = [idx for idx, count in enumerate(face_count_per_index) if count == 0]\n        elif self._globals.filter_mode == \"Multiple Faces\":\n            retval = [idx for idx, count in enumerate(face_count_per_index) if count > 1]\n        elif self._globals.filter_mode == \"Has Face(s)\":\n            retval = [idx for idx, count in enumerate(face_count_per_index) if count != 0]\n        elif self._globals.filter_mode == \"Misaligned Faces\":\n            distance = self._filter_distance\n            retval = [idx for idx, frame in enumerate(self._detected_faces.current_faces)\n                      if any(face.aligned.average_distance > distance for face in frame)]\n        else:\n            retval = list(range(len(face_count_per_index)))\n        logger.trace(\"filter mode: %s, number_frames: %s\",  # type:ignore[attr-defined]\n                     self._globals.filter_mode, len(retval))\n        return retval\n\n\nclass FaceUpdate():\n    \"\"\" Perform updates on :class:`~lib.align.DetectedFace` objects stored in\n    :class:`DetectedFaces` when changes are made within the GUI.\n\n    Parameters\n    ----------\n    detected_faces: :class:`DetectedFaces`\n        The parent :class:`DetectedFaces` object\n    \"\"\"\n    def __init__(self, detected_faces: DetectedFaces) -> None:\n        logger.debug(\"Initializing %s: (detected_faces: %s)\",\n                     self.__class__.__name__, detected_faces)\n        self._detected_faces = detected_faces\n        self._globals = detected_faces._globals\n        self._frame_faces = detected_faces._frame_faces\n        self._updated_frame_indices = detected_faces._updated_frame_indices\n        self._tk_unsaved = detected_faces.tk_unsaved\n        self._extractor = detected_faces.extractor\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    @property\n    def _tk_edited(self) -> tk.BooleanVar:\n        \"\"\" :class:`tkinter.BooleanVar`: The variable indicating whether an edit has occurred\n        meaning a GUI redraw needs to be triggered.\n\n        Notes\n        -----\n        The variable is still a ``None`` when this class is initialized, so referenced explicitly.\n        \"\"\"\n        return self._detected_faces.tk_edited\n\n    @property\n    def _tk_face_count_changed(self) -> tk.BooleanVar:\n        \"\"\" :class:`tkinter.BooleanVar`: The variable indicating whether an edit has occurred\n        meaning a GUI redraw needs to be triggered.\n\n        Notes\n        -----\n        The variable is still a ``None`` when this class is initialized, so referenced explicitly.\n        \"\"\"\n        return self._detected_faces.tk_face_count_changed\n\n    def _faces_at_frame_index(self, frame_index: int) -> list[DetectedFace]:\n        \"\"\" Checks whether the frame has already been added to :attr:`_updated_frame_indices` and\n        adds it. Triggers the unsaved variable if this is the first edited frame. Returns the\n        detected face objects for the given frame.\n\n        Parameters\n        ----------\n        frame_index: int\n            The frame index to check whether there are updated alignments available\n\n        Returns\n        -------\n        list\n            The :class:`~lib.align.DetectedFace` objects for the requested frame\n        \"\"\"\n        if not self._updated_frame_indices and not self._tk_unsaved.get():\n            self._tk_unsaved.set(True)\n        self._updated_frame_indices.add(frame_index)\n        retval = self._frame_faces[frame_index]\n        return retval\n\n    def add(self, frame_index: int, pnt_x: int, width: int, pnt_y: int, height: int) -> None:\n        \"\"\" Add a :class:`~lib.align.DetectedFace` object to the current frame with the\n        given dimensions.\n\n        Parameters\n        ----------\n        frame_index: int\n            The frame that the face is being set for\n        pnt_x: int\n            The left point of the bounding box\n        width: int\n            The width of the bounding box\n        pnt_y: int\n            The top point of the bounding box\n        height: int\n            The height of the bounding box\n        \"\"\"\n        face = DetectedFace()\n        faces = self._faces_at_frame_index(frame_index)\n        faces.append(face)\n        face_index = len(faces) - 1\n\n        self.bounding_box(frame_index, face_index, pnt_x, width, pnt_y, height, aligner=\"cv2-dnn\")\n        face.load_aligned(None)\n        self._tk_face_count_changed.set(True)\n\n    def delete(self, frame_index: int, face_index: int) -> None:\n        \"\"\" Delete the :class:`~lib.align.DetectedFace` object for the given frame and face\n        indices.\n\n        Parameters\n        ----------\n        frame_index: int\n            The frame that the face is being set for\n        face_index: int\n            The face index within the frame\n        \"\"\"\n        logger.debug(\"Deleting face at frame index: %s face index: %s\", frame_index, face_index)\n        faces = self._faces_at_frame_index(frame_index)\n        del faces[face_index]\n        self._tk_face_count_changed.set(True)\n        self._globals.tk_update.set(True)\n\n    def bounding_box(self,\n                     frame_index: int,\n                     face_index: int,\n                     pnt_x: int,\n                     width: int,\n                     pnt_y: int,\n                     height: int,\n                     aligner: manual.TypeManualExtractor = \"FAN\") -> None:\n        \"\"\" Update the bounding box for the :class:`~lib.align.DetectedFace` object at the\n        given frame and face indices, with the given dimensions and update the 68 point landmarks\n        from the :class:`~tools.manual.manual.Aligner` for the updated bounding box.\n\n        Parameters\n        ----------\n        frame_index: int\n            The frame that the face is being set for\n        face_index: int\n            The face index within the frame\n        pnt_x: int\n            The left point of the bounding box\n        width: int\n            The width of the bounding box\n        pnt_y: int\n            The top point of the bounding box\n        height: int\n            The height of the bounding box\n        aligner: [\"cv2-dnn\", \"FAN\"], optional\n            The aligner to use to generate the landmarks. Default: \"FAN\"\n        \"\"\"\n        logger.trace(\"frame_index: %s, face_index %s, pnt_x %s, \"  # type:ignore[attr-defined]\n                     \"width %s, pnt_y %s, height %s, aligner: %s\",\n                     frame_index, face_index, pnt_x, width, pnt_y, height, aligner)\n        face = self._faces_at_frame_index(frame_index)[face_index]\n        face.left = pnt_x\n        face.width = width\n        face.top = pnt_y\n        face.height = height\n        face.add_landmarks_xy(self._extractor.get_landmarks(frame_index, face_index, aligner))\n        self._globals.tk_update.set(True)\n\n    def landmark(self,\n                 frame_index: int, face_index: int,\n                 landmark_index: int,\n                 shift_x: int,\n                 shift_y: int,\n                 is_zoomed: bool) -> None:\n        \"\"\" Shift a single landmark point for the :class:`~lib.align.DetectedFace` object\n        at the given frame and face indices by the given x and y values.\n\n        Parameters\n        ----------\n        frame_index: int\n            The frame that the face is being set for\n        face_index: int\n            The face index within the frame\n        landmark_index: int or list\n            The landmark index to shift. If a list is provided, this should be a list of landmark\n            indices to be shifted\n        shift_x: int\n            The amount to shift the landmark by along the x axis\n        shift_y: int\n            The amount to shift the landmark by along the y axis\n        is_zoomed: bool\n            ``True`` if landmarks are being adjusted on a zoomed image otherwise ``False``\n        \"\"\"\n        face = self._faces_at_frame_index(frame_index)[face_index]\n        if is_zoomed:\n            aligned = AlignedFace(face.landmarks_xy,\n                                  centering=\"face\",\n                                  size=min(self._globals.frame_display_dims))\n            landmark = aligned.landmarks[landmark_index]\n            landmark += (shift_x, shift_y)\n            matrix = aligned.adjusted_matrix\n            matrix = cv2.invertAffineTransform(matrix)\n            if landmark.ndim == 1:\n                landmark = np.reshape(landmark, (1, 1, 2))\n                landmark = cv2.transform(landmark, matrix, landmark.shape).squeeze()\n                face.landmarks_xy[landmark_index] = landmark\n            else:\n                for lmk, idx in zip(landmark, landmark_index):  # type:ignore[call-overload]\n                    lmk = np.reshape(lmk, (1, 1, 2))\n                    lmk = cv2.transform(lmk, matrix, lmk.shape).squeeze()\n                    face.landmarks_xy[idx] = lmk\n        else:\n            face.landmarks_xy[landmark_index] += (shift_x, shift_y)\n        self._globals.tk_update.set(True)\n\n    def landmarks(self, frame_index: int, face_index: int, shift_x: int, shift_y: int) -> None:\n        \"\"\" Shift all of the landmarks and bounding box for the\n        :class:`~lib.align.DetectedFace` object at the given frame and face indices by the\n        given x and y values and update the masks.\n\n        Parameters\n        ----------\n        frame_index: int\n            The frame that the face is being set for\n        face_index: int\n            The face index within the frame\n        shift_x: int\n            The amount to shift the landmarks by along the x axis\n        shift_y: int\n            The amount to shift the landmarks by along the y axis\n\n        Notes\n        -----\n        Whilst the bounding box does not need to be shifted, it is anyway, to ensure that it is\n        aligned with the newly adjusted landmarks.\n        \"\"\"\n        face = self._faces_at_frame_index(frame_index)[face_index]\n        assert face.left is not None and face.top is not None\n        face.left += shift_x\n        face.top += shift_y\n        face.add_landmarks_xy(face.landmarks_xy + (shift_x, shift_y))\n        self._globals.tk_update.set(True)\n\n    def landmarks_rotate(self,\n                         frame_index: int,\n                         face_index: int,\n                         angle: float,\n                         center: np.ndarray) -> None:\n        \"\"\" Rotate the landmarks on an Extract Box rotate for the\n        :class:`~lib.align.DetectedFace` object at the given frame and face indices for the\n        given angle from the given center point.\n\n        Parameters\n        ----------\n        frame_index: int\n            The frame that the face is being set for\n        face_index: int\n            The face index within the frame\n        angle: float\n            The angle, in radians to rotate the points by\n        center: :class:`numpy.ndarray`\n            The center point of the Landmark's Extract Box\n        \"\"\"\n        face = self._faces_at_frame_index(frame_index)[face_index]\n        rot_mat = cv2.getRotationMatrix2D(tuple(center.astype(\"float32\")), angle, 1.)\n        face.add_landmarks_xy(cv2.transform(np.expand_dims(face.landmarks_xy, axis=0),\n                                            rot_mat).squeeze())\n        self._globals.tk_update.set(True)\n\n    def landmarks_scale(self,\n                        frame_index: int,\n                        face_index: int,\n                        scale: np.ndarray,\n                        center: np.ndarray) -> None:\n        \"\"\" Scale the landmarks on an Extract Box resize for the\n        :class:`~lib.align.DetectedFace` object at the given frame and face indices from the\n        given center point.\n\n        Parameters\n        ----------\n        frame_index: int\n            The frame that the face is being set for\n        face_index: int\n            The face index within the frame\n        scale: float\n            The amount to scale the landmarks by\n        center: :class:`numpy.ndarray`\n            The center point of the Landmark's Extract Box\n        \"\"\"\n        face = self._faces_at_frame_index(frame_index)[face_index]\n        face.add_landmarks_xy(((face.landmarks_xy - center) * scale) + center)\n        self._globals.tk_update.set(True)\n\n    def mask(self, frame_index: int, face_index: int, mask: np.ndarray, mask_type: str) -> None:\n        \"\"\" Update the mask on an edit for the :class:`~lib.align.DetectedFace` object at\n        the given frame and face indices, for the given mask and mask type.\n\n        Parameters\n        ----------\n        frame_index: int\n            The frame that the face is being set for\n        face_index: int\n            The face index within the frame\n        mask: class:`numpy.ndarray`:\n            The mask to replace\n        mask_type: str\n            The name of the mask that is to be replaced\n        \"\"\"\n        face = self._faces_at_frame_index(frame_index)[face_index]\n        face.mask[mask_type].replace_mask(mask)\n        self._tk_edited.set(True)\n        self._globals.tk_update.set(True)\n\n    def copy(self, frame_index: int, direction: T.Literal[\"prev\", \"next\"]) -> None:\n        \"\"\" Copy the alignments from the previous or next frame that has alignments\n        to the current frame.\n\n        Parameters\n        ----------\n        frame_index: int\n            The frame that the needs to have alignments copied to it\n        direction: [\"prev\", \"next\"]\n            Whether to copy alignments from the previous frame with alignments, or the next\n            frame with alignments\n        \"\"\"\n        logger.debug(\"frame: %s, direction: %s\", frame_index, direction)\n        faces = self._faces_at_frame_index(frame_index)\n        frames_with_faces = [idx for idx, faces in enumerate(self._detected_faces.current_faces)\n                             if len(faces) > 0]\n        if direction == \"prev\":\n            idx = next((idx for idx in reversed(frames_with_faces)\n                        if idx < frame_index), None)\n        else:\n            idx = next((idx for idx in frames_with_faces\n                        if idx > frame_index), None)\n        if idx is None:\n            # No previous/next frame available\n            return\n        logger.debug(\"Copying alignments from frame %s to frame: %s\", idx, frame_index)\n\n        # aligned_face cannot be deep copied, so remove and recreate\n        to_copy = self._faces_at_frame_index(idx)\n        for face in to_copy:\n            face._aligned = None  # pylint:disable=protected-access\n        copied = deepcopy(to_copy)\n\n        for old_face, new_face in zip(to_copy, copied):\n            old_face.load_aligned(None)\n            new_face.load_aligned(None)\n\n        faces.extend(copied)\n        self._tk_face_count_changed.set(True)\n        self._globals.tk_update.set(True)\n\n    def post_edit_trigger(self, frame_index: int, face_index: int) -> None:\n        \"\"\" Update the jpg thumbnail, the viewport thumbnail, the landmark masks and the aligned\n        face on a face edit.\n\n        Parameters\n        ----------\n        frame_index: int\n            The frame that the face is being set for\n        face_index: int\n            The face index within the frame\n        \"\"\"\n        face = self._frame_faces[frame_index][face_index]\n        face.load_aligned(None, force=True)  # Update average distance\n        face.mask = self._extractor.get_masks(frame_index, face_index)\n        face.clear_all_identities()\n\n        aligned = AlignedFace(face.landmarks_xy,\n                              image=self._globals.current_frame[\"image\"],\n                              centering=\"head\",\n                              size=96)\n        assert aligned.face is not None\n        face.thumbnail = generate_thumbnail(aligned.face, size=96)\n        if self._globals.filter_mode == \"Misaligned Faces\":\n            self._detected_faces.tk_face_count_changed.set(True)\n        self._tk_edited.set(True)\n", "tools/manual/thumbnails.py": "#!/usr/bin/env python3\n\"\"\" Thumbnail generator for the manual tool \"\"\"\nfrom __future__ import annotations\nimport logging\nimport typing as T\nimport os\n\nfrom dataclasses import dataclass\nfrom time import sleep\nfrom threading import Lock\n\nimport imageio\nimport numpy as np\n\nfrom tqdm import tqdm\nfrom lib.align import AlignedFace\nfrom lib.image import SingleFrameLoader, generate_thumbnail\nfrom lib.multithreading import MultiThread\n\nif T.TYPE_CHECKING:\n    from .detected_faces import DetectedFaces\n\nlogger = logging.getLogger(__name__)\n\n\n@dataclass\nclass ProgressBar:\n    \"\"\" Thread-safe progress bar for tracking thumbnail generation progress \"\"\"\n    pbar: tqdm | None = None\n    lock = Lock()\n\n\n@dataclass\nclass VideoMeta:\n    \"\"\" Holds meta information about a video file\n\n    Parameters\n    ----------\n    key_frames: list[int]\n        List of key frame indices for the video\n    pts_times: list[float]\n        List of presentation timestams for the video\n    \"\"\"\n    key_frames: list[int] | None = None\n    pts_times: list[float] | None = None\n\n\nclass ThumbsCreator():\n    \"\"\" Background loader to generate thumbnails for the alignments file. Generates low resolution\n    thumbnails in parallel threads for faster processing.\n\n    Parameters\n    ----------\n    detected_faces: :class:`~tool.manual.faces.DetectedFaces`\n        The :class:`~lib.align.DetectedFace` objects for this video\n    input_location: str\n        The location of the input folder of frames or video file\n    single_process: bool\n        ``True`` to generated thumbs in a single process otherwise ``False``\n    \"\"\"\n    def __init__(self,\n                 detected_faces: DetectedFaces,\n                 input_location: str,\n                 single_process: bool) -> None:\n        logger.debug(\"Initializing %s: (detected_faces: %s, input_location: %s, \"\n                     \"single_process: %s)\", self.__class__.__name__, detected_faces,\n                     input_location, single_process)\n        self._size = 80\n        self._pbar = ProgressBar()\n        self._meta = VideoMeta(\n            key_frames=T.cast(list[int] | None,\n                              detected_faces.video_meta_data.get(\"keyframes\", None)),\n            pts_times=T.cast(list[float] | None,\n                             detected_faces.video_meta_data.get(\"pts_time\", None)))\n        self._location = input_location\n        self._alignments = detected_faces._alignments\n        self._frame_faces = detected_faces._frame_faces\n\n        self._is_video = self._meta.pts_times is not None and self._meta.key_frames is not None\n\n        cpu_count = os.cpu_count()\n        self._num_threads = 1 if cpu_count is None or cpu_count <= 2 else cpu_count - 2\n\n        if self._is_video and single_process:\n            self._num_threads = 1\n        elif self._is_video and not single_process:\n            assert self._meta.key_frames is not None\n            self._num_threads = min(self._num_threads, len(self._meta.key_frames))\n        else:\n            self._num_threads = max(self._num_threads, 32)\n        self._threads: list[MultiThread] = []\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    @property\n    def has_thumbs(self) -> bool:\n        \"\"\" bool: ``True`` if the underlying alignments file holds thumbnail images\n        otherwise ``False``. \"\"\"\n        return self._alignments.thumbnails.has_thumbnails\n\n    def generate_cache(self) -> None:\n        \"\"\" Extract the face thumbnails from a video or folder of images into the\n        alignments file. \"\"\"\n        self._pbar.pbar = tqdm(desc=\"Caching Thumbnails\",\n                               leave=False,\n                               total=len(self._frame_faces))\n        if self._is_video:\n            self._launch_video()\n        else:\n            self._launch_folder()\n        while True:\n            self._check_and_raise_error()\n            if all(not thread.is_alive() for thread in self._threads):\n                break\n            sleep(1)\n        self._join_threads()\n        self._pbar.pbar.close()\n        self._alignments.save()\n\n    # << PRIVATE METHODS >> #\n    def _check_and_raise_error(self) -> None:\n        \"\"\" Monitor the loading threads for errors and raise if any occur. \"\"\"\n        for thread in self._threads:\n            thread.check_and_raise_error()\n\n    def _join_threads(self) -> None:\n        \"\"\" Join the loading threads \"\"\"\n        logger.debug(\"Joining face viewer loading threads\")\n        for thread in self._threads:\n            thread.join()\n\n    def _launch_video(self) -> None:\n        \"\"\" Launch multiple :class:`lib.multithreading.MultiThread` objects to load faces from\n        a video file.\n\n        Splits the video into segments and passes each of these segments to separate background\n        threads for some speed up.\n        \"\"\"\n        key_frames = self._meta.key_frames\n        pts_times = self._meta.pts_times\n        assert key_frames is not None and pts_times is not None\n        key_frame_split = len(key_frames) // self._num_threads\n        for idx in range(self._num_threads):\n            is_final = idx == self._num_threads - 1\n            start_idx: int = idx * key_frame_split\n            keyframe_idx = len(key_frames) - 1 if is_final else start_idx + key_frame_split\n            end_idx = key_frames[keyframe_idx]\n            start_pts = pts_times[key_frames[start_idx]]\n            end_pts = False if idx + 1 == self._num_threads else pts_times[end_idx]\n            starting_index = pts_times.index(start_pts)\n            if end_pts:\n                segment_count = len(pts_times[key_frames[start_idx]:end_idx])\n            else:\n                segment_count = len(pts_times[key_frames[start_idx]:])\n            logger.debug(\"thread index: %s, start_idx: %s, end_idx: %s, start_pts: %s, \"\n                         \"end_pts: %s, starting_index: %s, segment_count: %s\", idx, start_idx,\n                         end_idx, start_pts, end_pts, starting_index, segment_count)\n            thread = MultiThread(self._load_from_video,\n                                 start_pts,\n                                 end_pts,\n                                 starting_index,\n                                 segment_count)\n            thread.start()\n            self._threads.append(thread)\n\n    def _launch_folder(self) -> None:\n        \"\"\" Launch :class:`lib.multithreading.MultiThread` to retrieve faces from a\n        folder of images.\n\n        Goes through the file list one at a time, passing each file to a separate background\n        thread for some speed up.\n        \"\"\"\n        reader = SingleFrameLoader(self._location)\n        num_threads = min(reader.count, self._num_threads)\n        frame_split = reader.count // self._num_threads\n        logger.debug(\"total images: %s, num_threads: %s, frames_per_thread: %s\",\n                     reader.count, num_threads, frame_split)\n        for idx in range(num_threads):\n            is_final = idx == num_threads - 1\n            start_idx = idx * frame_split\n            end_idx = reader.count if is_final else start_idx + frame_split\n            thread = MultiThread(self._load_from_folder, reader, start_idx, end_idx)\n            thread.start()\n            self._threads.append(thread)\n\n    def _load_from_video(self,\n                         pts_start: float,\n                         pts_end: float,\n                         start_index: int,\n                         segment_count: int) -> None:\n        \"\"\" Loads faces from video for the given segment of the source video.\n\n        Each segment of the video is extracted from in a different background thread.\n\n        Parameters\n        ----------\n        pts_start: float\n            The start time to cut the segment out of the video\n        pts_end: float\n            The end time to cut the segment out of the video\n        start_index: int\n            The frame index that this segment starts from. Used for calculating the actual frame\n            index of each frame extracted\n        segment_count: int\n            The number of frames that appear in this segment. Used for ending early in case more\n            frames come out of the segment than should appear (sometimes more frames are picked up\n            at the end of the segment, so these are discarded)\n        \"\"\"\n        logger.debug(\"pts_start: %s, pts_end: %s, start_index: %s, segment_count: %s\",\n                     pts_start, pts_end, start_index, segment_count)\n        reader = self._get_reader(pts_start, pts_end)\n        idx = 0\n        sample_filename, ext = os.path.splitext(next(fname for fname in self._alignments.data))\n        vidname = sample_filename[:sample_filename.rfind(\"_\")]\n        for idx, frame in enumerate(reader):\n            frame_idx = idx + start_index\n            filename = f\"{vidname}_{frame_idx + 1:06d}{ext}\"\n            self._set_thumbail(filename, frame[..., ::-1], frame_idx)\n            if idx == segment_count - 1:\n                # Sometimes extra frames are picked up at the end of a segment, so stop\n                # processing when segment frame count has been hit.\n                break\n        reader.close()\n        logger.debug(\"Segment complete: (starting_frame_index: %s, processed_count: %s)\",\n                     start_index, idx)\n\n    def _get_reader(self, pts_start: float, pts_end: float):\n        \"\"\" Get an imageio iterator for this thread's segment.\n\n        Parameters\n        ----------\n        pts_start: float\n            The start time to cut the segment out of the video\n        pts_end: float\n            The end time to cut the segment out of the video\n\n        Returns\n        -------\n        :class:`imageio.Reader`\n            A reader iterator for the requested segment of video\n        \"\"\"\n        input_params = [\"-ss\", str(pts_start)]\n        if pts_end:\n            input_params.extend([\"-to\", str(pts_end)])\n        logger.debug(\"pts_start: %s, pts_end: %s, input_params: %s\",\n                     pts_start, pts_end, input_params)\n        return imageio.get_reader(self._location,\n                                  \"ffmpeg\",  # type:ignore[arg-type]\n                                  input_params=input_params)\n\n    def _load_from_folder(self,\n                          reader: SingleFrameLoader,\n                          start_index: int,\n                          end_index: int) -> None:\n        \"\"\" Loads faces from the given range of frame indices from a folder of images.\n\n        Each frame range is extracted in a different background thread.\n\n        Parameters\n        ----------\n        reader: :class:`lib.image.SingleFrameLoader`\n            The reader that is used to retrieve the requested frame\n        start_index: int\n            The starting frame index for the images to extract faces from\n        end_index: int\n            The end frame index for the images to extract faces from\n        \"\"\"\n        logger.debug(\"reader: %s, start_index: %s, end_index: %s\",\n                     reader, start_index, end_index)\n        for frame_index in range(start_index, end_index):\n            filename, frame = reader.image_from_index(frame_index)\n            self._set_thumbail(filename, frame, frame_index)\n        logger.debug(\"Segment complete: (start_index: %s, processed_count: %s)\",\n                     start_index, end_index - start_index)\n\n    def _set_thumbail(self, filename: str, frame: np.ndarray, frame_index: int) -> None:\n        \"\"\" Extracts the faces from the frame and adds to alignments file\n\n        Parameters\n        ----------\n        filename: str\n            The filename of the frame within the alignments file\n        frame: :class:`numpy.ndarray`\n            The frame that contains the faces\n        frame_index: int\n            The frame index of this frame in the :attr:`_frame_faces`\n        \"\"\"\n        for face_idx, face in enumerate(self._frame_faces[frame_index]):\n            aligned = AlignedFace(face.landmarks_xy,\n                                  image=frame,\n                                  centering=\"head\",\n                                  size=96)\n            face.thumbnail = generate_thumbnail(aligned.face, size=96)\n            assert face.thumbnail is not None\n            self._alignments.thumbnails.add_thumbnail(filename, face_idx, face.thumbnail)\n        with self._pbar.lock:\n            assert self._pbar.pbar is not None\n            self._pbar.pbar.update(1)\n", "tools/manual/__init__.py": "", "tools/manual/faceviewer/interact.py": "#!/usr/bin/env python3\n\"\"\" Handles the viewport area for mouse hover actions and the active frame \"\"\"\nfrom __future__ import annotations\nimport logging\nimport tkinter as tk\nimport typing as T\nfrom dataclasses import dataclass\n\nimport numpy as np\n\nfrom lib.logger import parse_class_init\n\nif T.TYPE_CHECKING:\n    from lib.align import DetectedFace\n    from .viewport import Viewport\n\nlogger = logging.getLogger(__name__)\n\n\nclass HoverBox():\n    \"\"\" Handle the current mouse location when over the :class:`Viewport`.\n\n    Highlights the face currently underneath the cursor and handles actions when clicking\n    on a face.\n\n    Parameters\n    ----------\n    viewport: :class:`Viewport`\n        The viewport object for the :class:`~tools.manual.faceviewer.frame.FacesViewer` canvas\n    \"\"\"\n    def __init__(self, viewport: Viewport) -> None:\n        logger.debug(parse_class_init(locals()))\n        self._viewport = viewport\n        self._canvas = viewport._canvas\n        self._grid = viewport._canvas.layout\n        self._globals = viewport._canvas._globals\n        self._navigation = viewport._canvas._display_frame.navigation\n        self._box = self._canvas.create_rectangle(0.,  # type:ignore[call-overload]\n                                                  0.,\n                                                  float(self._size),\n                                                  float(self._size),\n                                                  outline=\"#0000ff\",\n                                                  width=2,\n                                                  state=\"hidden\",\n                                                  fill=\"#0000ff\",\n                                                  stipple=\"gray12\",\n                                                  tags=\"hover_box\")\n        self._current_frame_index = None\n        self._current_face_index = None\n        self._canvas.bind(\"<Leave>\", lambda e: self._clear())\n        self._canvas.bind(\"<Motion>\", self.on_hover)\n        self._canvas.bind(\"<ButtonPress-1>\", lambda e: self._select_frame())\n        logger.debug(\"Initialized: %s\", self.__class__.__name__)\n\n    @property\n    def _size(self) -> int:\n        \"\"\" int: the currently set viewport face size in pixels. \"\"\"\n        return self._viewport.face_size\n\n    def on_hover(self, event: tk.Event | None) -> None:\n        \"\"\" Highlight the face and set the mouse cursor for the mouse's current location.\n\n        Parameters\n        ----------\n        event: :class:`tkinter.Event` or ``None``\n            The tkinter mouse event. Provides the current location of the mouse cursor. If ``None``\n            is passed as the event (for example when this function is being called outside of a\n            mouse event) then the location of the cursor will be calculated\n        \"\"\"\n        if event is None:\n            pnts = np.array((self._canvas.winfo_pointerx(), self._canvas.winfo_pointery()))\n            pnts -= np.array((self._canvas.winfo_rootx(), self._canvas.winfo_rooty()))\n        else:\n            pnts = np.array((event.x, event.y))\n\n        coords = (int(self._canvas.canvasx(pnts[0])), int(self._canvas.canvasy(pnts[1])))\n        face = self._viewport.face_from_point(*coords)\n        frame_idx, face_idx = face[:2]\n\n        if frame_idx == self._current_frame_index and face_idx == self._current_face_index:\n            return\n\n        is_zoomed = self._globals.is_zoomed\n        if (-1 in face or (frame_idx == self._globals.frame_index\n                           and (not is_zoomed or\n                                (is_zoomed and face_idx == self._globals.tk_face_index.get())))):\n            self._clear()\n            self._canvas.config(cursor=\"\")\n            self._current_frame_index = None\n            self._current_face_index = None\n            return\n\n        logger.debug(\"Viewport hover: frame_idx: %s, face_idx: %s\", frame_idx, face_idx)\n\n        self._canvas.config(cursor=\"hand2\")\n        self._highlight(face[2:])\n        self._current_frame_index = frame_idx\n        self._current_face_index = face_idx\n\n    def _clear(self) -> None:\n        \"\"\" Hide the hover box when the mouse is not over a face. \"\"\"\n        if self._canvas.itemcget(self._box, \"state\") != \"hidden\":\n            self._canvas.itemconfig(self._box, state=\"hidden\")\n\n    def _highlight(self, top_left: np.ndarray) -> None:\n        \"\"\" Display the hover box around the face that the mouse is currently over.\n\n        Parameters\n        ----------\n        top_left: :class:`np.ndarray`\n            The top left point of the highlight box location\n        \"\"\"\n        coords = (*top_left, *[x + self._size for x in top_left])\n        self._canvas.coords(self._box, *coords)\n        self._canvas.itemconfig(self._box, state=\"normal\")\n        self._canvas.tag_raise(self._box)\n\n    def _select_frame(self) -> None:\n        \"\"\" Select the face and the subsequent frame (in the editor view) when a face is clicked\n        on in the :class:`Viewport`. \"\"\"\n        frame_id = self._current_frame_index\n        is_zoomed = self._globals.is_zoomed\n        logger.debug(\"Face clicked. Global frame index: %s, Current frame_id: %s, is_zoomed: %s\",\n                     self._globals.frame_index, frame_id, is_zoomed)\n        if frame_id is None or (frame_id == self._globals.frame_index and not is_zoomed):\n            return\n        face_idx = self._current_face_index if is_zoomed else 0\n        self._globals.tk_face_index.set(face_idx)\n        transport_id = self._grid.transport_index_from_frame(frame_id)\n        logger.trace(\"frame_index: %s, transport_id: %s, face_idx: %s\",\n                     frame_id, transport_id, face_idx)\n        if transport_id is None:\n            return\n        self._navigation.stop_playback()\n        self._globals.tk_transport_index.set(transport_id)\n        self._viewport.move_active_to_top()\n        self.on_hover(None)\n\n\n@dataclass\nclass Asset:\n    \"\"\" Holds all of the display assets identifiers for the active frame's face viewer objects\n\n    Parameters\n    ----------\n    images: list[int]\n        Indices for a frame's tk image ids displayed in the active frame\n    meshes: list[dict[Literal[\"polygon\", \"line\"], list[int]]]\n        Indices for a frame's tk line/polygon object ids displayed in the active frame\n    faces: list[:class:`~lib.align.detected_faces.DetectedFace`]\n        DetectedFace objects that exist in the current frame\n    boxes: list[int]\n        Indices for a frame's bounding box object ids displayed in the active frame\n    \"\"\"\n    images: list[int]\n    \"\"\"list[int]: Indices for a frame's tk image ids displayed in the active frame\"\"\"\n    meshes: list[dict[T.Literal[\"polygon\", \"line\"], list[int]]]\n    \"\"\"list[dict[Literal[\"polygon\", \"line\"], list[int]]]:  Indices for a frame's tk line/polygon\n    object ids displayed in the active frame\"\"\"\n    faces: list[DetectedFace]\n    \"\"\"list[:class:`~lib.align.detected_faces.DetectedFace`]: DetectedFace objects that exist\n    in the current frame\"\"\"\n    boxes: list[int]\n    \"\"\"list[int]: Indices for a frame's bounding box object ids displayed in the active\n    frame\"\"\"\n\n\nclass ActiveFrame():\n    \"\"\" Handles the display of faces and annotations for the currently active frame.\n\n    Parameters\n    ----------\n    canvas: :class:`tkinter.Canvas`\n        The :class:`~tools.manual.faceviewer.frame.FacesViewer` canvas\n    tk_edited_variable: :class:`tkinter.BooleanVar`\n        The tkinter callback variable indicating that a face has been edited\n    \"\"\"\n    def __init__(self, viewport: Viewport, tk_edited_variable: tk.BooleanVar) -> None:\n        logger.debug(parse_class_init(locals()))\n        self._objects = viewport._objects\n        self._viewport = viewport\n        self._grid = viewport._grid\n        self._tk_faces = viewport._tk_faces\n        self._canvas = viewport._canvas\n        self._globals = viewport._canvas._globals\n        self._navigation = viewport._canvas._display_frame.navigation\n        self._last_execution: dict[T.Literal[\"frame_index\", \"size\"],\n                                   int] = {\"frame_index\": -1, \"size\": viewport.face_size}\n        self._tk_vars: dict[T.Literal[\"selected_editor\", \"edited\"],\n                            tk.StringVar | tk.BooleanVar] = {\n            \"selected_editor\": self._canvas._display_frame.tk_selected_action,\n            \"edited\": tk_edited_variable}\n        self._assets: Asset = Asset([], [], [], [])\n\n        self._globals.tk_update_active_viewport.trace_add(\"write\",\n                                                          lambda *e: self._reload_callback())\n        tk_edited_variable.trace_add(\"write\", lambda *e: self._update_on_edit())\n        logger.debug(\"Initialized: %s\", self.__class__.__name__)\n\n    @property\n    def frame_index(self) -> int:\n        \"\"\" int: The frame index of the currently displayed frame. \"\"\"\n        return self._globals.frame_index\n\n    @property\n    def current_frame(self) -> np.ndarray:\n        \"\"\" :class:`numpy.ndarray`: A BGR version of the frame currently being displayed. \"\"\"\n        return self._globals.current_frame[\"image\"]\n\n    @property\n    def _size(self) -> int:\n        \"\"\" int: The size of the thumbnails displayed in the viewport, in pixels. \"\"\"\n        return self._viewport.face_size\n\n    @property\n    def _optional_annotations(self) -> dict[T.Literal[\"mesh\", \"mask\"], bool]:\n        \"\"\" dict[Literal[\"mesh\", \"mask\"], bool]: The currently selected optional\n        annotations \"\"\"\n        return self._canvas.optional_annotations\n\n    def _reload_callback(self) -> None:\n        \"\"\" If a frame has changed, triggering the variable, then update the active frame. Return\n        having done nothing if the variable is resetting. \"\"\"\n        if self._globals.tk_update_active_viewport.get():\n            self.reload_annotations()\n\n    def reload_annotations(self) -> None:\n        \"\"\" Handles the reloading of annotations for the currently active faces.\n\n        Highlights the faces within the viewport of those faces that exist in the currently\n        displaying frame. Applies annotations based on the optional annotations and current\n        editor selections.\n        \"\"\"\n        logger.trace(\"Reloading annotations\")  # type:ignore[attr-defined]\n        if self._assets.images:\n            self._clear_previous()\n\n        self._set_active_objects()\n        self._check_active_in_view()\n\n        if not self._assets.images:\n            logger.trace(\"No active faces. Returning\")  # type:ignore[attr-defined]\n            self._last_execution[\"frame_index\"] = self.frame_index\n            return\n\n        if self._last_execution[\"frame_index\"] != self.frame_index:\n            self.move_to_top()\n        self._create_new_boxes()\n\n        self._update_face()\n        self._canvas.tag_raise(\"active_highlighter\")\n        self._globals.tk_update_active_viewport.set(False)\n        self._last_execution[\"frame_index\"] = self.frame_index\n\n    def _clear_previous(self) -> None:\n        \"\"\" Reverts the previously selected annotations to their default state. \"\"\"\n        logger.trace(\"Clearing previous active frame\")  # type:ignore[attr-defined]\n        self._canvas.itemconfig(\"active_highlighter\", state=\"hidden\")\n\n        for key in T.get_args(T.Literal[\"polygon\", \"line\"]):\n            tag = f\"active_mesh_{key}\"\n            self._canvas.itemconfig(tag, **self._viewport.mesh_kwargs[key], width=1)\n            self._canvas.dtag(tag)\n\n        if self._viewport.selected_editor == \"mask\" and not self._optional_annotations[\"mask\"]:\n            for name, tk_face in self._tk_faces.items():\n                if name.startswith(f\"{self._last_execution['frame_index']}_\"):\n                    tk_face.update_mask(None)\n\n    def _set_active_objects(self) -> None:\n        \"\"\" Collect the objects that exist in the currently active frame from the main grid. \"\"\"\n        if self._grid.is_valid:\n            rows, cols = np.where(self._objects.visible_grid[0] == self.frame_index)\n            logger.trace(\"Setting active objects: (rows: %s, \"  # type:ignore[attr-defined]\n                         \"columns: %s)\", rows, cols)\n            self._assets.images = self._objects.images[rows, cols].tolist()\n            self._assets.meshes = self._objects.meshes[rows, cols].tolist()\n            self._assets.faces = self._objects.visible_faces[rows, cols].tolist()\n        else:\n            logger.trace(\"No valid grid. Clearing active objects\")  # type:ignore[attr-defined]\n            self._assets.images = []\n            self._assets.meshes = []\n            self._assets.faces = []\n\n    def _check_active_in_view(self) -> None:\n        \"\"\"  If the frame has changed, there are faces in the frame, but they don't appear in the\n        viewport, then bring the active faces to the top of the viewport. \"\"\"\n        if (not self._assets.images and\n                self._last_execution[\"frame_index\"] != self.frame_index and\n                self._grid.frame_has_faces(self.frame_index)):\n            y_coord = self._grid.y_coord_from_frame(self.frame_index)\n            logger.trace(\"Active not in view. Moving to: %s\", y_coord)  # type:ignore[attr-defined]\n            self._canvas.yview_moveto(y_coord / self._canvas.bbox(\"backdrop\")[3])\n            self._viewport.update()\n\n    def move_to_top(self) -> None:\n        \"\"\" Move the currently selected frame's faces to the top of the viewport if they are moving\n        off the bottom of the viewer. \"\"\"\n        height = self._canvas.bbox(\"backdrop\")[3]\n        bot = int(self._canvas.coords(self._assets.images[-1])[1] + self._size)\n\n        y_top, y_bot = (int(round(pnt * height)) for pnt in self._canvas.yview())\n\n        if y_top < bot < y_bot:  # bottom face is still in fully visible area\n            logger.trace(\"Active faces in frame. Returning\")  # type:ignore[attr-defined]\n            return\n\n        top = int(self._canvas.coords(self._assets.images[0])[1])\n        if y_top == top:\n            logger.trace(\"Top face already on top row. Returning\")  # type:ignore[attr-defined]\n            return\n\n        if self._canvas.winfo_height() > self._size:\n            logger.trace(\"Viewport taller than single face height. \"  # type:ignore[attr-defined]\n                         \"Moving Active faces to top: %s\", top)\n            self._canvas.yview_moveto(top / height)\n            self._viewport.update()\n        elif self._canvas.winfo_height() <= self._size and y_top != top:\n            logger.trace(\"Viewport shorter than single face height. \"  # type:ignore[attr-defined]\n                         \"Moving Active faces to top: %s\", top)\n            self._canvas.yview_moveto(top / height)\n            self._viewport.update()\n\n    def _create_new_boxes(self) -> None:\n        \"\"\" The highlight boxes (border around selected faces) are the only additional annotations\n        that are required for the highlighter. If more faces are displayed in the current frame\n        than highlight boxes are available, then new boxes are created to accommodate the\n        additional faces. \"\"\"\n        new_boxes_count = max(0, len(self._assets.images) - len(self._assets.boxes))\n        if new_boxes_count == 0:\n            return\n        logger.debug(\"new_boxes_count: %s\", new_boxes_count)\n        for _ in range(new_boxes_count):\n            box = self._canvas.create_rectangle(0.,  # type:ignore[call-overload]\n                                                0.,\n                                                float(self._viewport.face_size),\n                                                float(self._viewport.face_size),\n                                                outline=\"#00FF00\",\n                                                width=2,\n                                                state=\"hidden\",\n                                                tags=[\"active_highlighter\"])\n            logger.trace(\"Created new highlight_box: %s\", box)  # type:ignore[attr-defined]\n            self._assets.boxes.append(box)\n\n    def _update_on_edit(self) -> None:\n        \"\"\" Update the active faces on a frame edit. \"\"\"\n        if not self._tk_vars[\"edited\"].get():\n            return\n        self._set_active_objects()\n        self._update_face()\n        assert isinstance(self._tk_vars[\"edited\"], tk.BooleanVar)\n        self._tk_vars[\"edited\"].set(False)\n\n    def _update_face(self) -> None:\n        \"\"\" Update the highlighted annotations for faces in the currently selected frame. \"\"\"\n        for face_idx, (image_id, mesh_ids, box_id, det_face), in enumerate(\n                zip(self._assets.images,\n                    self._assets.meshes,\n                    self._assets.boxes,\n                    self._assets.faces)):\n            if det_face is None:\n                continue\n            top_left = self._canvas.coords(image_id)\n            coords = [*top_left, *[x + self._size for x in top_left]]\n            tk_face = self._viewport.get_tk_face(self.frame_index, face_idx, det_face)\n            self._canvas.itemconfig(image_id, image=tk_face.photo)\n            self._show_box(box_id, coords)\n            self._show_mesh(mesh_ids, face_idx, det_face, top_left)\n        self._last_execution[\"size\"] = self._viewport.face_size\n\n    def _show_box(self, item_id: int, coordinates: list[float]) -> None:\n        \"\"\" Display the highlight box around the given coordinates.\n\n        Parameters\n        ----------\n        item_id: int\n            The tkinter canvas object identifier for the highlight box\n        coordinates: list[float]\n            The (x, y, x1, y1) coordinates of the top left corner of the box\n        \"\"\"\n        self._canvas.coords(item_id, *coordinates)\n        self._canvas.itemconfig(item_id, state=\"normal\")\n\n    def _show_mesh(self,\n                   mesh_ids: dict[T.Literal[\"polygon\", \"line\"], list[int]],\n                   face_index: int,\n                   detected_face: DetectedFace,\n                   top_left: list[float]) -> None:\n        \"\"\" Display the mesh annotation for the given face, at the given location.\n\n        Parameters\n        ----------\n        mesh_ids: dict[Literal[\"polygon\", \"line\"], list[int]]\n            Dictionary containing the `polygon` and `line` tkinter canvas identifiers that make up\n            the mesh for the given face\n        face_index: int\n            The face index within the frame for the given face\n        detected_face: :class:`~lib.align.DetectedFace`\n            The detected face object that contains the landmarks for generating the mesh\n        top_left: list[float]\n            The (x, y) top left co-ordinates of the mesh's bounding box\n        \"\"\"\n        state = \"normal\" if (self._tk_vars[\"selected_editor\"].get() != \"Mask\" or\n                             self._optional_annotations[\"mesh\"]) else \"hidden\"\n        kwargs: dict[T.Literal[\"polygon\", \"line\"], dict[str, T.Any]] = {\n            \"polygon\": {\"fill\": \"\", \"width\": 2, \"outline\": self._canvas.control_colors[\"Mesh\"]},\n            \"line\": {\"fill\": self._canvas.control_colors[\"Mesh\"], \"width\": 2}}\n\n        assert isinstance(self._tk_vars[\"edited\"], tk.BooleanVar)\n        edited = (self._tk_vars[\"edited\"].get() and\n                  self._tk_vars[\"selected_editor\"].get() not in (\"Mask\", \"View\"))\n        landmarks = self._viewport.get_landmarks(self.frame_index,\n                                                 face_index,\n                                                 detected_face,\n                                                 top_left,\n                                                 edited)\n        for key, kwarg in kwargs.items():\n            if key not in mesh_ids:\n                continue\n            for idx, mesh_id in enumerate(mesh_ids[key]):\n                self._canvas.coords(mesh_id, *landmarks[key][idx].flatten())\n                self._canvas.itemconfig(mesh_id, state=state, **kwarg)\n                self._canvas.addtag_withtag(f\"active_mesh_{key}\", mesh_id)\n", "tools/manual/faceviewer/viewport.py": "#!/usr/bin/env python3\n\"\"\" Handles the visible area of the :class:`~tools.manual.faceviewer.frame.FacesViewer` canvas. \"\"\"\nfrom __future__ import annotations\nimport logging\nimport tkinter as tk\nimport typing as T\n\nimport cv2\nimport numpy as np\nfrom PIL import Image, ImageTk\n\nfrom lib.align import AlignedFace, LANDMARK_PARTS, LandmarkType\nfrom lib.logger import parse_class_init\n\nfrom .interact import ActiveFrame, HoverBox\n\nif T.TYPE_CHECKING:\n    from lib.align import CenteringType, DetectedFace\n    from .frame import FacesViewer\n\nlogger = logging.getLogger(__name__)\n\n\nclass Viewport():\n    \"\"\" Handles the display of faces and annotations in the currently viewable area of the canvas.\n\n    Parameters\n    ----------\n    canvas: :class:`tkinter.Canvas`\n        The :class:`~tools.manual.faceviewer.frame.FacesViewer` canvas\n    tk_edited_variable: :class:`tkinter.BooleanVar`\n        The variable that indicates that a face has been edited\n    \"\"\"\n    def __init__(self, canvas: FacesViewer, tk_edited_variable: tk.BooleanVar) -> None:\n        logger.debug(parse_class_init(locals()))\n        self._canvas = canvas\n        self._grid = canvas.layout\n        self._centering: CenteringType = \"face\"\n        self._tk_selected_editor = canvas._display_frame.tk_selected_action\n        self._landmarks: dict[str, dict[T.Literal[\"polygon\", \"line\"], list[np.ndarray]]] = {}\n        self._tk_faces: dict[str, TKFace] = {}\n        self._objects = VisibleObjects(self)\n        self._hoverbox = HoverBox(self)\n        self._active_frame = ActiveFrame(self, tk_edited_variable)\n        self._tk_selected_editor.trace(\n            \"w\", lambda *e: self._active_frame.reload_annotations())\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    @property\n    def face_size(self) -> int:\n        \"\"\" int: The pixel size of each thumbnail \"\"\"\n        return self._grid.face_size\n\n    @property\n    def mesh_kwargs(self) -> dict[T.Literal[\"polygon\", \"line\"], dict[str, T.Any]]:\n        \"\"\" dict[Literal[\"polygon\", \"line\"], str | int]: Dynamic keyword arguments defining the\n        color and state for the objects that make up a single face's mesh annotation based on the\n        current user selected options. Values are the keyword arguments for that given type. \"\"\"\n        state = \"normal\" if self._canvas.optional_annotations[\"mesh\"] else \"hidden\"\n        color = self._canvas.control_colors[\"Mesh\"]\n        return {\"polygon\": {\"fill\": \"\", \"outline\": color, \"state\": state},\n                \"line\": {\"fill\": color, \"state\": state}}\n\n    @property\n    def hover_box(self) -> HoverBox:\n        \"\"\" :class:`HoverBox`: The hover box for the viewport. \"\"\"\n        return self._hoverbox\n\n    @property\n    def selected_editor(self) -> str:\n        \"\"\" str: The currently selected editor. \"\"\"\n        return self._tk_selected_editor.get().lower()\n\n    def toggle_mesh(self, state: T.Literal[\"hidden\", \"normal\"]) -> None:\n        \"\"\" Toggles the mesh optional annotations on and off.\n\n        Parameters\n        ----------\n        state: Literal[\"hidden\", \"normal\"]\n            The state to set the mesh annotations to\n        \"\"\"\n        logger.debug(\"Toggling mesh annotations to: %s\", state)\n        self._canvas.itemconfig(\"viewport_mesh\", state=state)\n        self.update()\n\n    def toggle_mask(self, state: T.Literal[\"hidden\", \"normal\"], mask_type: str) -> None:\n        \"\"\" Toggles the mask optional annotation on and off.\n\n        Parameters\n        ----------\n        state: Literal[\"hidden\", \"normal\"]\n            Whether the mask should be displayed or hidden\n        mask_type: str\n            The type of mask to overlay onto the face\n        \"\"\"\n        logger.debug(\"Toggling mask annotations to: %s. mask_type: %s\", state, mask_type)\n        for (frame_idx, face_idx), det_face in zip(\n                self._objects.visible_grid[:2].transpose(1, 2, 0).reshape(-1, 2),\n                self._objects.visible_faces.flatten()):\n            if frame_idx == -1:\n                continue\n\n            key = \"_\".join([str(frame_idx), str(face_idx)])\n            mask = None if state == \"hidden\" else self._obtain_mask(det_face, mask_type)\n            self._tk_faces[key].update_mask(mask)\n        self.update()\n\n    @classmethod\n    def _obtain_mask(cls, detected_face: DetectedFace, mask_type: str) -> np.ndarray | None:\n        \"\"\" Obtain the mask for the correct \"face\" centering that is used in the thumbnail display.\n\n        Parameters\n        -----------\n        detected_face: :class:`lib.align.DetectedFace`\n            The Detected Face object to obtain the mask for\n        mask_type: str\n            The type of mask to obtain\n\n        Returns\n        -------\n        :class:`numpy.ndarray` or ``None``\n            The single channel mask of requested mask type, if it exists, otherwise ``None``\n        \"\"\"\n        mask = detected_face.mask.get(mask_type)\n        if not mask:\n            return None\n        if mask.stored_centering != \"face\":\n            face = AlignedFace(detected_face.landmarks_xy)\n            mask.set_sub_crop(face.pose.offset[mask.stored_centering],\n                              face.pose.offset[\"face\"],\n                              centering=\"face\")\n        return mask.mask.squeeze()\n\n    def reset(self) -> None:\n        \"\"\" Reset all the cached objects on a face size change. \"\"\"\n        self._landmarks = {}\n        self._tk_faces = {}\n\n    def update(self, refresh_annotations: bool = False) -> None:\n        \"\"\" Update the viewport.\n\n        Parameters\n        ----------\n        refresh_annotations: bool, optional\n            ``True`` if mesh annotations should be re-calculated otherwise ``False``.\n            Default: ``False``\n\n        Obtains the objects that are currently visible. Updates the visible area of the canvas\n        and reloads the active frame's annotations. \"\"\"\n        self._objects.update()\n        self._update_viewport(refresh_annotations)\n        self._active_frame.reload_annotations()\n\n    def _update_viewport(self, refresh_annotations: bool) -> None:\n        \"\"\" Update the viewport\n\n        Parameters\n        ----------\n        refresh_annotations: bool\n            ``True`` if mesh annotations should be re-calculated otherwise ``False``\n\n        Clear out cached objects that are not currently in view. Populate the cache for any\n        faces that are now in view. Populate the correct face image and annotations for each\n        object in the viewport based on current location. If optional mesh annotations are\n        enabled, then calculates newly displayed meshes. \"\"\"\n        if not self._grid.is_valid:\n            return\n        self._discard_tk_faces()\n\n        for collection in zip(self._objects.visible_grid.transpose(1, 2, 0),\n                              self._objects.images,\n                              self._objects.meshes,\n                              self._objects.visible_faces):\n            for (frame_idx, face_idx, pnt_x, pnt_y), image_id, mesh_ids, face in zip(*collection):\n                if frame_idx == self._active_frame.frame_index and not refresh_annotations:\n                    logger.trace(\"Skipping active frame: %s\",  # type:ignore[attr-defined]\n                                 frame_idx)\n                    continue\n                if frame_idx == -1:\n                    logger.trace(\"Blanking non-existant face\")  # type:ignore[attr-defined]\n                    self._canvas.itemconfig(image_id, image=\"\")\n                    for area in mesh_ids.values():\n                        for mesh_id in area:\n                            self._canvas.itemconfig(mesh_id, state=\"hidden\")\n                    continue\n\n                tk_face = self.get_tk_face(frame_idx, face_idx, face)\n                self._canvas.itemconfig(image_id, image=tk_face.photo)\n\n                if (self._canvas.optional_annotations[\"mesh\"]\n                        or frame_idx == self._active_frame.frame_index\n                        or refresh_annotations):\n                    landmarks = self.get_landmarks(frame_idx, face_idx, face, [pnt_x, pnt_y],\n                                                   refresh=True)\n                    self._locate_mesh(mesh_ids, landmarks)\n\n    def _discard_tk_faces(self) -> None:\n        \"\"\" Remove any :class:`TKFace` objects from the cache that are not currently displayed. \"\"\"\n        keys = [f\"{pnt_x}_{pnt_y}\"\n                for pnt_x, pnt_y in self._objects.visible_grid[:2].T.reshape(-1, 2)]\n        for key in list(self._tk_faces):\n            if key not in keys:\n                del self._tk_faces[key]\n        logger.trace(\"keys: %s allocated_faces: %s\",  # type:ignore[attr-defined]\n                     keys, len(self._tk_faces))\n\n    def get_tk_face(self, frame_index: int, face_index: int, face: DetectedFace) -> TKFace:\n        \"\"\" Obtain the :class:`TKFace` object for the given face from the cache. If the face does\n        not exist in the cache, then it is generated and added prior to returning.\n\n        Parameters\n        ----------\n        frame_index: int\n            The frame index to obtain the face for\n        face_index: int\n            The face index of the face within the requested frame\n        face: :class:`~lib.align.DetectedFace`\n            The detected face object, containing the thumbnail jpg\n\n        Returns\n        -------\n        :class:`TKFace`\n            An object for displaying in the faces viewer canvas populated with the aligned mesh\n            landmarks and face thumbnail\n        \"\"\"\n        is_active = frame_index == self._active_frame.frame_index\n        key = \"_\".join([str(frame_index), str(face_index)])\n        if key not in self._tk_faces or is_active:\n            logger.trace(\"creating new tk_face: (key: %s, \"  # type:ignore[attr-defined]\n                         \"is_active: %s)\", key, is_active)\n            if is_active:\n                image = AlignedFace(face.landmarks_xy,\n                                    image=self._active_frame.current_frame,\n                                    centering=self._centering,\n                                    size=self.face_size).face\n            else:\n                thumb = face.thumbnail\n                assert thumb is not None\n                image = AlignedFace(face.landmarks_xy,\n                                    image=cv2.imdecode(thumb, cv2.IMREAD_UNCHANGED),\n                                    centering=self._centering,\n                                    size=self.face_size,\n                                    is_aligned=True).face\n            assert image is not None\n            tk_face = self._get_tk_face_object(face, image, is_active)\n            self._tk_faces[key] = tk_face\n        else:\n            logger.trace(\"tk_face exists: %s\", key)  # type:ignore[attr-defined]\n            tk_face = self._tk_faces[key]\n        return tk_face\n\n    def _get_tk_face_object(self,\n                            face: DetectedFace,\n                            image: np.ndarray,\n                            is_active: bool) -> TKFace:\n        \"\"\" Obtain an existing unallocated, or a newly created :class:`TKFace` and populate it with\n        face information from the requested frame and face index.\n\n        If the face is currently active, then the face is generated from the currently displayed\n        frame, otherwise it is generated from the jpg thumbnail.\n\n        Parameters\n        ----------\n        face: :class:`lib.align.DetectedFace`\n            A detected face object to create the :class:`TKFace` from\n        image: :class:`numpy.ndarray`\n            The jpg thumbnail or the 3 channel image for the face\n        is_active: bool\n            ``True`` if the face in the currently active frame otherwise ``False``\n\n        Returns\n        -------\n        :class:`TKFace`\n            An object for displaying in the faces viewer canvas populated with the aligned face\n            image with a mask applied, if required.\n        \"\"\"\n        get_mask = (self._canvas.optional_annotations[\"mask\"] or\n                    (is_active and self.selected_editor == \"mask\"))\n        mask = self._obtain_mask(face, self._canvas.selected_mask) if get_mask else None\n        tk_face = TKFace(image, size=self.face_size, mask=mask)\n        logger.trace(\"face: %s, tk_face: %s\", face, tk_face)  # type:ignore[attr-defined]\n        return tk_face\n\n    def get_landmarks(self,\n                      frame_index: int,\n                      face_index: int,\n                      face: DetectedFace,\n                      top_left: list[float],\n                      refresh: bool = False\n                      ) -> dict[T.Literal[\"polygon\", \"line\"], list[np.ndarray]]:\n        \"\"\" Obtain the landmark points for each mesh annotation.\n\n        First tries to obtain the aligned landmarks from the cache. If the landmarks do not exist\n        in the cache, or a refresh has been requested, then the landmarks are calculated from the\n        detected face object.\n\n        Parameters\n        ----------\n        frame_index: int\n            The frame index to obtain the face for\n        face_index: int\n            The face index of the face within the requested frame\n        face: :class:`lib.align.DetectedFace`\n            The detected face object to obtain landmarks for\n        top_left: list[float]\n            The top left (x, y) points of the face's bounding box within the viewport\n        refresh: bool, optional\n            Whether to force a reload of the face's aligned landmarks, even if they already exist\n            within the cache. Default: ``False``\n\n        Returns\n        -------\n        dict\n            The key is the tkinter canvas object type for each part of the mesh annotation\n            (`polygon`, `line`). The value is a list containing the (x, y) coordinates of each\n            part of the mesh annotation, from the top left corner location.\n        \"\"\"\n        key = f\"{frame_index}_{face_index}\"\n        landmarks = self._landmarks.get(key, None)\n        if not landmarks or refresh:\n            aligned = AlignedFace(face.landmarks_xy,\n                                  centering=self._centering,\n                                  size=self.face_size)\n            landmarks = {\"polygon\": [], \"line\": []}\n            for start, end, fill in LANDMARK_PARTS[aligned.landmark_type].values():\n                points = aligned.landmarks[start:end] + top_left\n                shape: T.Literal[\"polygon\", \"line\"] = \"polygon\" if fill else \"line\"\n                landmarks[shape].append(points)\n            self._landmarks[key] = landmarks\n        return landmarks\n\n    def _locate_mesh(self, mesh_ids, landmarks):\n        \"\"\" Place the mesh annotation canvas objects in the correct location.\n\n        Parameters\n        ----------\n        mesh_ids: list\n            The list of mesh id objects to set coordinates for\n        landmarks: dict\n            The mesh point groupings and whether each group should be a line or a polygon\n        \"\"\"\n        for key, area in landmarks.items():\n            if key not in mesh_ids:\n                continue\n            for coords, mesh_id in zip(area, mesh_ids[key]):\n                self._canvas.coords(mesh_id, *coords.flatten())\n\n    def face_from_point(self, point_x: int, point_y: int) -> np.ndarray:\n        \"\"\" Given an (x, y) point on the :class:`Viewport`, obtain the face information at that\n        location.\n\n        Parameters\n        ----------\n        point_x: int\n            The x position on the canvas of the point to retrieve the face for\n        point_y: int\n            The y position on the canvas of the point to retrieve the face for\n\n        Returns\n        -------\n        :class:`numpy.ndarray`\n            Array of shape (4, ) containing the (`frame index`, `face index`, `x_point of top left\n            corner`, `y point of top left corner`) of the face at the given coordinates.\n\n            If the given coordinates are not over a face, then the frame and face indices will be\n            -1\n        \"\"\"\n        if not self._grid.is_valid or point_x > self._grid.dimensions[0]:\n            retval = np.array((-1, -1, -1, -1))\n        else:\n            x_idx = np.searchsorted(self._objects.visible_grid[2, 0, :], point_x, side=\"left\") - 1\n            y_idx = np.searchsorted(self._objects.visible_grid[3, :, 0], point_y, side=\"left\") - 1\n            if x_idx < 0 or y_idx < 0:\n                retval = np.array((-1, -1, -1, -1))\n            else:\n                retval = self._objects.visible_grid[:, y_idx, x_idx]\n        logger.trace(retval)  # type:ignore[attr-defined]\n        return retval\n\n    def move_active_to_top(self) -> None:\n        \"\"\" Check whether the active frame is going off the bottom of the viewport, if so: move it\n        to the top of the viewport. \"\"\"\n        self._active_frame.move_to_top()\n\n\nclass Recycler:\n    \"\"\" Tkinter can slow down when constantly creating new objects.\n\n    This class delivers recycled objects, if stale objects are available, otherwise creates a new\n    object\n\n    Parameters\n    ----------\n    :class:`~tools.manual.faceviewe.frame.FacesViewer`\n        The canvas that holds the faces display\n    \"\"\"\n    def __init__(self, canvas: FacesViewer) -> None:\n        self._canvas = canvas\n        self._assets: dict[T.Literal[\"image\", \"line\", \"polygon\"],\n                           list[int]] = {\"image\": [], \"line\": [], \"polygon\": []}\n        self._mesh_methods: dict[T.Literal[\"line\", \"polygon\"],\n                                 T.Callable] = {\"line\": canvas.create_line,\n                                                \"polygon\": canvas.create_polygon}\n\n    def recycle_assets(self, asset_ids: list[int]) -> None:\n        \"\"\" Recycle assets that are no longer required\n\n        Parameters\n        ----------\n        asset_ids: list[int]\n            The IDs of the assets to be recycled\n        \"\"\"\n        logger.trace(\"Recycling %s objects\", len(asset_ids))  # type:ignore[attr-defined]\n        for asset_id in asset_ids:\n            asset_type = self._canvas.type(asset_id)\n            assert asset_type in self._assets\n            coords = (0, 0, 0, 0) if asset_type == \"line\" else (0, 0)\n            self._canvas.coords(asset_id, *coords)\n\n            if asset_type == \"image\":\n                self._canvas.itemconfig(asset_id, image=\"\")\n\n            self._assets[asset_type].append(asset_id)\n        logger.trace(\"Recycled objects: %s\", self._assets)  # type:ignore[attr-defined]\n\n    def get_image(self, coordinates: tuple[float | int, float | int]) -> int:\n        \"\"\" Obtain a recycled or new image object ID\n\n        Parameters\n        ----------\n        coordinates: tuple[float | int, float | int]\n            The co-ordinates that the image should be displayed at\n\n        Returns\n        -------\n        int\n            The canvas object id for the created image\n        \"\"\"\n        if self._assets[\"image\"]:\n            retval = self._assets[\"image\"].pop()\n            self._canvas.coords(retval, *coordinates)\n            logger.trace(\"Recycled image: %s\", retval)  # type:ignore[attr-defined]\n        else:\n            retval = self._canvas.create_image(*coordinates,\n                                               anchor=tk.NW,\n                                               tags=[\"viewport\", \"viewport_image\"])\n            logger.trace(\"Created new image: %s\", retval)  # type:ignore[attr-defined]\n        return retval\n\n    def get_mesh(self, face: DetectedFace) -> dict[T.Literal[\"polygon\", \"line\"], list[int]]:\n        \"\"\" Get the mesh annotation for the landmarks. This is made up of a series of polygons\n        or lines, depending on which part of the face is being annotated. Creates a new series of\n        objects, or pulls existing objects from the recycled objects pool if they are available.\n\n        Parameters\n        ----------\n        face: :class:`~lib.align.detected_face.DetectedFace`\n            The detected face object to obrain the mesh for\n\n        Returns\n        -------\n        dict[Literal[\"polygon\", \"line\"], list[int]]\n            The dictionary of line and polygon tkinter canvas object ids for the mesh annotation\n        \"\"\"\n        mesh_kwargs = self._canvas.viewport.mesh_kwargs\n        mesh_parts = LANDMARK_PARTS[LandmarkType.from_shape(face.landmarks_xy.shape)]\n        retval: dict[T.Literal[\"polygon\", \"line\"], list[int]] = {}\n        for _, _, fill in mesh_parts.values():\n            asset_type: T.Literal[\"polygon\", \"line\"] = \"polygon\" if fill else \"line\"\n            kwargs = mesh_kwargs[asset_type]\n            if self._assets[asset_type]:\n                asset_id = self._assets[asset_type].pop()\n                self._canvas.itemconfig(asset_id, **kwargs)\n                logger.trace(\"Recycled mesh %s: %s\",  # type:ignore[attr-defined]\n                             asset_type, asset_id)\n            else:\n                coords = (0, 0) if asset_type == \"polygon\" else (0, 0, 0, 0)\n                tags = [\"viewport\", \"viewport_mesh\", f\"viewport_{asset_type}\"]\n                asset_id = self._mesh_methods[asset_type](coords, width=1, tags=tags, **kwargs)\n                logger.trace(\"Created new mesh %s: %s\",  # type:ignore[attr-defined]\n                             asset_type, asset_id)\n\n            retval.setdefault(asset_type, []).append(asset_id)\n        logger.trace(\"Got mesh: %s\", retval)  # type:ignore[attr-defined]\n        return retval\n\n\nclass VisibleObjects():\n    \"\"\" Holds the objects from the :class:`~tools.manual.faceviewer.frame.Grid` that appear in the\n    viewable area of the :class:`Viewport`.\n\n    Parameters\n    ----------\n    viewport: :class:`Viewport`\n        The viewport object for the :class:`~tools.manual.faceviewer.frame.FacesViewer` canvas\n    \"\"\"\n    def __init__(self, viewport: Viewport) -> None:\n        logger.debug(parse_class_init(locals()))\n        self._viewport = viewport\n        self._canvas = viewport._canvas\n        self._grid = viewport._grid\n        self._size = viewport.face_size\n\n        self._visible_grid = np.zeros((4, 0, 0))\n        self._visible_faces = np.zeros((0, 0))\n        self._recycler = Recycler(self._canvas)\n        self._images = np.zeros((0, 0), dtype=np.int64)\n        self._meshes = np.zeros((0, 0))\n        logger.debug(\"Initialized: %s\", self.__class__.__name__)\n\n    @property\n    def visible_grid(self) -> np.ndarray:\n        \"\"\" :class:`numpy.ndarray`: The currently visible section of the\n        :class:`~tools.manual.faceviewer.frame.Grid`\n\n        A numpy array of shape (`4`, `rows`, `columns`) corresponding to the viewable area of the\n        display grid. 1st dimension contains frame indices, 2nd dimension face indices. The 3rd and\n        4th dimension contain the x and y position of the top left corner of the face respectively.\n\n        Any locations that are not populated by a face will have a frame and face index of -1. \"\"\"\n        return self._visible_grid\n\n    @property\n    def visible_faces(self) -> np.ndarray:\n        \"\"\" :class:`numpy.ndarray`: The currently visible :class:`~lib.align.DetectedFace`\n        objects.\n\n        A numpy array of shape (`rows`, `columns`) corresponding to the viewable area of the\n        display grid and containing the detected faces at their currently viewable position.\n\n        Any locations that are not populated by a face will have ``None`` in it's place. \"\"\"\n        return self._visible_faces\n\n    @property\n    def images(self) -> np.ndarray:\n        \"\"\" :class:`numpy.ndarray`: The viewport's tkinter canvas image objects.\n\n        A numpy array of shape (`rows`, `columns`) corresponding to the viewable area of the\n        display grid and containing the tkinter canvas image object for the face at the\n        corresponding location. \"\"\"\n        return self._images\n\n    @property\n    def meshes(self) -> np.ndarray:\n        \"\"\" :class:`numpy.ndarray`: The viewport's tkinter canvas mesh annotation objects.\n\n        A numpy array of shape (`rows`, `columns`) corresponding to the viewable area of the\n        display grid and containing a dictionary of the corresponding tkinter polygon and line\n        objects required to build a face's mesh annotation for the face at the corresponding\n        location. \"\"\"\n        return self._meshes\n\n    @property\n    def _top_left(self) -> np.ndarray:\n        \"\"\" :class:`numpy.ndarray`: The canvas (`x`, `y`) position of the face currently in the\n        viewable area's top left position. \"\"\"\n        if not np.any(self._images):\n            retval = [0.0, 0.0]\n        else:\n            retval = self._canvas.coords(self._images[0][0])\n        return np.array(retval, dtype=\"int\")\n\n    def update(self) -> None:\n        \"\"\" Load and unload thumbnails in the visible area of the faces viewer. \"\"\"\n        if self._canvas.optional_annotations[\"mesh\"]:  # Display any hidden end of row meshes\n            self._canvas.itemconfig(\"viewport_mesh\", state=\"normal\")\n\n        self._visible_grid, self._visible_faces = self._grid.visible_area\n        if (np.any(self._images) and np.any(self._visible_grid)\n                and self._visible_grid.shape[1:] != self._images.shape):\n            self._reset_viewport()\n\n        required_rows = self._visible_grid.shape[1] if self._grid.is_valid else 0\n        existing_rows = len(self._images)\n        logger.trace(\"existing_rows: %s. required_rows: %s\",  # type:ignore[attr-defined]\n                     existing_rows, required_rows)\n\n        if existing_rows > required_rows:\n            self._remove_rows(existing_rows, required_rows)\n        if existing_rows < required_rows:\n            self._add_rows(existing_rows, required_rows)\n\n        self._shift()\n\n    def _reset_viewport(self) -> None:\n        \"\"\" Reset all objects in the viewport on a column count change. Reset the viewport size\n        to the newly specified face size. \"\"\"\n        logger.debug(\"Resetting Viewport\")\n        self._size = self._viewport.face_size\n        images = self._images.flatten().tolist()\n        meshes = [parts for mesh in [mesh.values() for mesh in self._meshes.flatten()]\n                  for parts in mesh]\n        mesh_ids = [asset for mesh in meshes for asset in mesh]\n        self._recycler.recycle_assets(images + mesh_ids)\n        self._images = np.zeros((0, 0), np.int64)\n        self._meshes = np.zeros((0, 0))\n\n    def _remove_rows(self, existing_rows: int, required_rows: int) -> None:\n        \"\"\" Remove and recycle rows from the viewport that are not in the view area.\n\n        Parameters\n        ----------\n        existing_rows: int\n            The number of existing rows within the viewport\n        required_rows: int\n            The number of rows required by the viewport\n        \"\"\"\n        logger.debug(\"Removing rows from viewport: (existing_rows: %s, required_rows: %s)\",\n                     existing_rows, required_rows)\n        images = self._images[required_rows: existing_rows].flatten().tolist()\n        meshes = [parts\n                  for mesh in [mesh.values()\n                               for mesh in self._meshes[required_rows: existing_rows].flatten()]\n                  for parts in mesh]\n        mesh_ids = [asset for mesh in meshes for asset in mesh]\n        self._recycler.recycle_assets(images + mesh_ids)\n        self._images = self._images[:required_rows]\n        self._meshes = self._meshes[:required_rows]\n        logger.trace(\"self._images: %s, self._meshes: %s\",  # type:ignore[attr-defined]\n                     self._images.shape, self._meshes.shape)\n\n    def _add_rows(self, existing_rows: int, required_rows: int) -> None:\n        \"\"\" Add rows to the viewport.\n\n        Parameters\n        ----------\n        existing_rows: int\n            The number of existing rows within the viewport\n        required_rows: int\n            The number of rows required by the viewport\n        \"\"\"\n        logger.debug(\"Adding rows to viewport: (existing_rows: %s, required_rows: %s)\",\n                     existing_rows, required_rows)\n        columns = self._grid.columns_rows[0]\n\n        base_coords: list[list[float | int]]\n\n        if not np.any(self._images):\n            base_coords = [[col * self._size, 0] for col in range(columns)]\n        else:\n            base_coords = [self._canvas.coords(item_id) for item_id in self._images[0]]\n        logger.trace(\"existing rows: %s, required_rows: %s, \"  # type:ignore[attr-defined]\n                     \"base_coords: %s\", existing_rows, required_rows, base_coords)\n        images = []\n        meshes = []\n        for row in range(existing_rows, required_rows):\n            y_coord = base_coords[0][1] + (row * self._size)\n            images.append([self._recycler.get_image((coords[0], y_coord))\n                           for coords in base_coords])\n            meshes.append([{} if face is None else self._recycler.get_mesh(face)\n                           for face in self._visible_faces[row]])\n\n        a_images = np.array(images)\n        a_meshes = np.array(meshes)\n\n        if not np.any(self._images):\n            logger.debug(\"Adding initial viewport objects: (image shapes: %s, mesh shapes: %s)\",\n                         a_images.shape, a_meshes.shape)\n            self._images = a_images\n            self._meshes = a_meshes\n        else:\n            logger.debug(\"Adding new viewport objects: (image shapes: %s, mesh shapes: %s)\",\n                         a_images.shape, a_meshes.shape)\n            self._images = np.concatenate((self._images, a_images))\n            self._meshes = np.concatenate((self._meshes, a_meshes))\n\n        logger.trace(\"self._images: %s, self._meshes: %s\",  # type:ignore[attr-defined]\n                     self._images.shape, self._meshes.shape)\n\n    def _shift(self) -> bool:\n        \"\"\" Shift the viewport in the y direction if required\n\n        Returns\n        -------\n        bool\n            ``True`` if the viewport was shifted otherwise ``False``\n        \"\"\"\n        current_y = self._top_left[1]\n        required_y = self.visible_grid[3, 0, 0] if self._grid.is_valid else 0\n        logger.trace(\"current_y: %s, required_y: %s\",  # type:ignore[attr-defined]\n                     current_y, required_y)\n        if current_y == required_y:\n            logger.trace(\"No move required\")  # type:ignore[attr-defined]\n            return False\n        shift_amount = required_y - current_y\n        logger.trace(\"Shifting viewport: %s\", shift_amount)  # type:ignore[attr-defined]\n        self._canvas.move(\"viewport\", 0, shift_amount)\n        return True\n\n\nclass TKFace():\n    \"\"\" An object that holds a single :class:`tkinter.PhotoImage` face, ready for placement in the\n    :class:`Viewport`, Handles the placement of and removal of masks for the face as well as\n    updates on any edits.\n\n    Parameters\n    ----------\n    face: :class:`numpy.ndarray`\n        The face, sized correctly as a 3 channel BGR image or an encoded jpg to create a\n        :class:`tkinter.PhotoImage` from\n    size: int, optional\n        The pixel size of the face image. Default: `128`\n    mask: :class:`numpy.ndarray` or ``None``, optional\n        The mask to be applied to the face image. Pass ``None`` if no mask is to be used.\n        Default ``None``\n    \"\"\"\n    def __init__(self, face: np.ndarray, size: int = 128, mask: np.ndarray | None = None) -> None:\n        logger.trace(parse_class_init(locals()))  # type:ignore[attr-defined]\n        self._size = size\n        if face.ndim == 2 and face.shape[1] == 1:\n            self._face = self._image_from_jpg(face)\n        else:\n            self._face = face[..., 2::-1]\n        self._photo = ImageTk.PhotoImage(self._generate_tk_face_data(mask))\n\n        logger.trace(\"Initialized %s\", self.__class__.__name__)  # type:ignore[attr-defined]\n\n    # << PUBLIC PROPERTIES >> #\n    @property\n    def photo(self) -> tk.PhotoImage:\n        \"\"\" :class:`tkinter.PhotoImage`: The face in a format that can be placed on the\n         :class:`~tools.manual.faceviewer.frame.FacesViewer` canvas. \"\"\"\n        return self._photo\n\n    # << PUBLIC METHODS >> #\n    def update(self, face: np.ndarray, mask: np.ndarray) -> None:\n        \"\"\" Update the :attr:`photo` with the given face and mask.\n\n        Parameters\n        ----------\n        face: :class:`numpy.ndarray`\n            The face, sized correctly as a 3 channel BGR image\n        mask: :class:`numpy.ndarray` or ``None``\n            The mask to be applied to the face image. Pass ``None`` if no mask is to be used\n        \"\"\"\n        self._face = face[..., 2::-1]\n        self._photo.paste(self._generate_tk_face_data(mask))\n\n    def update_mask(self, mask: np.ndarray | None) -> None:\n        \"\"\" Update the mask in the 4th channel of :attr:`photo` to the given mask.\n\n        Parameters\n        ----------\n        mask: :class:`numpy.ndarray` or ``None``\n            The mask to be applied to the face image. Pass ``None`` if no mask is to be used\n        \"\"\"\n        self._photo.paste(self._generate_tk_face_data(mask))\n\n    # << PRIVATE METHODS >> #\n    def _image_from_jpg(self, face: np.ndarray) -> np.ndarray:\n        \"\"\" Convert an encoded jpg into 3 channel BGR image.\n\n        Parameters\n        ----------\n        face: :class:`numpy.ndarray`\n            The encoded jpg as a two dimension numpy array\n\n        Returns\n        -------\n        :class:`numpy.ndarray`\n            The decoded jpg as a 3 channel BGR image\n        \"\"\"\n        face = cv2.imdecode(face, cv2.IMREAD_UNCHANGED)\n        interp = cv2.INTER_CUBIC if face.shape[0] < self._size else cv2.INTER_AREA\n        if face.shape[0] != self._size:\n            face = cv2.resize(face, (self._size, self._size), interpolation=interp)\n        return face[..., 2::-1]\n\n    def _generate_tk_face_data(self, mask: np.ndarray | None) -> tk.PhotoImage:\n        \"\"\" Create the :class:`tkinter.PhotoImage` from the currant :attr:`_face`.\n\n        Parameters\n        ----------\n        mask: :class:`numpy.ndarray` or ``None``\n            The mask to add to the image. ``None`` if a mask is not being used\n\n        Returns\n        -------\n        :class:`tkinter.PhotoImage`\n            The face formatted for the  :class:`~tools.manual.faceviewer.frame.FacesViewer` canvas.\n        \"\"\"\n        mask = np.ones(self._face.shape[:2], dtype=\"uint8\") * 255 if mask is None else mask\n        if mask.shape[0] != self._size:\n            mask = cv2.resize(mask, self._face.shape[:2], interpolation=cv2.INTER_AREA)\n        img = np.concatenate((self._face, mask[..., None]), axis=-1)\n        return Image.fromarray(img)\n", "tools/manual/faceviewer/__init__.py": "", "tools/manual/faceviewer/frame.py": "#!/usr/bin/env python3\n\"\"\" The Faces Viewer Frame and Canvas for Faceswap's Manual Tool. \"\"\"\nfrom __future__ import annotations\nimport colorsys\nimport gettext\nimport logging\nimport platform\nimport tkinter as tk\nfrom tkinter import ttk\nimport typing as T\nfrom math import floor, ceil\nfrom threading import Thread, Event\n\nimport numpy as np\n\nfrom lib.gui.custom_widgets import RightClickMenu, Tooltip\nfrom lib.gui.utils import get_config, get_images\nfrom lib.image import hex_to_rgb, rgb_to_hex\nfrom lib.logger import parse_class_init\n\nfrom .viewport import Viewport\n\nif T.TYPE_CHECKING:\n    from tools.manual.detected_faces import DetectedFaces\n    from tools.manual.frameviewer.frame import DisplayFrame\n    from tools.manual.manual import TkGlobals\n\nlogger = logging.getLogger(__name__)\n\n# LOCALES\n_LANG = gettext.translation(\"tools.manual\", localedir=\"locales\", fallback=True)\n_ = _LANG.gettext\n\n\nclass FacesFrame(ttk.Frame):  # pylint:disable=too-many-ancestors\n    \"\"\" The faces display frame (bottom section of GUI). This frame holds the faces viewport and\n    the tkinter objects.\n\n    Parameters\n    ----------\n    parent: :class:`ttk.PanedWindow`\n        The paned window that the faces frame resides in\n    tk_globals: :class:`~tools.manual.manual.TkGlobals`\n        The tkinter variables that apply to the whole of the GUI\n    detected_faces: :class:`~tools.manual.detected_faces.DetectedFaces`\n        The :class:`~lib.align.DetectedFace` objects for this video\n    display_frame: :class:`~tools.manual.frameviewer.frame.DisplayFrame`\n        The section of the Manual Tool that holds the frames viewer\n    \"\"\"\n    def __init__(self,\n                 parent: ttk.PanedWindow,\n                 tk_globals: TkGlobals,\n                 detected_faces: DetectedFaces,\n                 display_frame: DisplayFrame) -> None:\n        logger.debug(parse_class_init(locals()))\n        super().__init__(parent)\n        self.pack(side=tk.TOP, fill=tk.BOTH, expand=True)\n        self._actions_frame = FacesActionsFrame(self)\n\n        self._faces_frame = ttk.Frame(self)\n        self._faces_frame.pack_propagate(False)\n        self._faces_frame.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)\n        self._event = Event()\n        self._canvas = FacesViewer(self._faces_frame,\n                                   tk_globals,\n                                   self._actions_frame._tk_vars,\n                                   detected_faces,\n                                   display_frame,\n                                   self._event)\n        self._add_scrollbar()\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    def _add_scrollbar(self) -> None:\n        \"\"\" Add a scrollbar to the faces frame \"\"\"\n        logger.debug(\"Add Faces Viewer Scrollbar\")\n        scrollbar = ttk.Scrollbar(self._faces_frame, command=self._on_scroll)\n        scrollbar.pack(side=tk.RIGHT, fill=tk.Y)\n        self._canvas.config(yscrollcommand=scrollbar.set)\n        self.bind(\"<Configure>\", self._update_viewport)\n        logger.debug(\"Added Faces Viewer Scrollbar\")\n        self.update_idletasks()  # Update so scrollbar width is correct\n\n    def _on_scroll(self, *event: tk.Event) -> None:\n        \"\"\" Callback on scrollbar scroll. Updates the canvas location and displays/hides\n        thumbnail images.\n\n        Parameters\n        ----------\n        event :class:`tkinter.Event`\n            The scrollbar callback event\n        \"\"\"\n        self._canvas.yview(*event)\n        self._canvas.viewport.update()\n\n    def _update_viewport(self, event: tk.Event) -> None:  # pylint:disable=unused-argument\n        \"\"\" Update the faces viewport and scrollbar.\n\n        Parameters\n        ----------\n        event: :class:`tkinter.Event`\n            Unused but required\n        \"\"\"\n        self._canvas.viewport.update()\n        self._canvas.configure(scrollregion=self._canvas.bbox(\"backdrop\"))\n\n    def canvas_scroll(self, direction: T.Literal[\"up\", \"down\", \"page-up\", \"page-down\"]) -> None:\n        \"\"\" Scroll the canvas on an up/down or page-up/page-down key press.\n\n        Notes\n        -----\n        To protect against a held down key press stacking tasks and locking up the GUI\n        a background thread is launched and discards subsequent key presses whilst the\n        previous update occurs.\n\n        Parameters\n        ----------\n        direction: [\"up\", \"down\", \"page-up\", \"page-down\"]\n            The request page scroll direction and amount.\n        \"\"\"\n\n        if self._event.is_set():\n            logger.trace(\"Update already running. \"  # type:ignore[attr-defined]\n                         \"Aborting repeated keypress\")\n            return\n        logger.trace(\"Running update on received key press: %s\",  # type:ignore[attr-defined]\n                     direction)\n\n        amount = 1 if direction.endswith(\"down\") else -1\n        units = \"pages\" if direction.startswith(\"page\") else \"units\"\n        self._event.set()\n        thread = Thread(target=self._canvas.canvas_scroll,\n                        args=(amount, units, self._event))\n        thread.start()\n\n    def set_annotation_display(self, key: str) -> None:\n        \"\"\" Set the optional annotation overlay based on keyboard shortcut.\n\n        Parameters\n        ----------\n        key: str\n            The pressed key\n        \"\"\"\n        self._actions_frame.on_click(self._actions_frame.key_bindings[key])\n\n\nclass FacesActionsFrame(ttk.Frame):  # pylint:disable=too-many-ancestors\n    \"\"\" The left hand action frame holding the optional annotation buttons.\n\n    Parameters\n    ----------\n    parent: :class:`FacesFrame`\n        The Faces frame that this actions frame reside in\n    \"\"\"\n    def __init__(self, parent: FacesFrame) -> None:\n        logger.debug(parse_class_init(locals()))\n        super().__init__(parent)\n        self.pack(side=tk.LEFT, fill=tk.Y, padx=(2, 4), pady=2)\n        self._tk_vars: dict[T.Literal[\"mesh\", \"mask\"], tk.BooleanVar] = {}\n        self._configure_styles()\n        self._buttons = self._add_buttons()\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    @property\n    def key_bindings(self) -> dict[str, T.Literal[\"mask\", \"mesh\"]]:\n        \"\"\" dict: The mapping of key presses to optional annotations to display. Keyboard shortcuts\n        utilize the function keys. \"\"\"\n        return {f\"F{idx + 9}\": display\n                for idx, display in enumerate(T.get_args(T.Literal[\"mesh\", \"mask\"]))}\n\n    @property\n    def _helptext(self) -> dict[T.Literal[\"mask\", \"mesh\"], str]:\n        \"\"\" dict: `button key`: `button helptext`. The help text to display for each button. \"\"\"\n        inverse_keybindings = {val: key for key, val in self.key_bindings.items()}\n        retval: dict[T.Literal[\"mask\", \"mesh\"], str] = {\"mesh\": _('Display the landmarks mesh'),\n                                                        \"mask\": _('Display the mask')}\n        for item in retval:\n            retval[item] += f\" ({inverse_keybindings[item]})\"\n        return retval\n\n    def _configure_styles(self) -> None:\n        \"\"\" Configure the background color for button frame and the button styles. \"\"\"\n        style = ttk.Style()\n        style.configure(\"display.TFrame\", background='#d3d3d3')\n        style.configure(\"display_selected.TButton\", relief=\"flat\", background=\"#bedaf1\")\n        style.configure(\"display_deselected.TButton\", relief=\"flat\")\n        self.config(style=\"display.TFrame\")\n\n    def _add_buttons(self) -> dict[T.Literal[\"mesh\", \"mask\"], ttk.Button]:\n        \"\"\" Add the display buttons to the Faces window.\n\n        Returns\n        -------\n        dict[Literal[\"mesh\", \"mask\"], tk.Button]]\n            The display name and its associated button.\n        \"\"\"\n        frame = ttk.Frame(self)\n        frame.pack(side=tk.TOP, fill=tk.Y)\n        buttons = {}\n        for display in self.key_bindings.values():\n            var = tk.BooleanVar()\n            var.set(False)\n            self._tk_vars[display] = var\n\n            lookup = \"landmarks\" if display == \"mesh\" else display\n            button = ttk.Button(frame,\n                                image=get_images().icons[lookup],\n                                command=T.cast(T.Callable, lambda t=display: self.on_click(t)),\n                                style=\"display_deselected.TButton\")\n            button.state([\"!pressed\", \"!focus\"])\n            button.pack()\n            Tooltip(button, text=self._helptext[display])\n            buttons[display] = button\n        return buttons\n\n    def on_click(self, display: T.Literal[\"mesh\", \"mask\"]) -> None:\n        \"\"\" Click event for the optional annotation buttons. Loads and unloads the annotations from\n        the faces viewer.\n\n        Parameters\n        ----------\n        display: Literal[\"mesh\", \"mask\"]\n            The display name for the button that has called this event as exists in\n            :attr:`_buttons`\n        \"\"\"\n        is_pressed = not self._tk_vars[display].get()\n        style = \"display_selected.TButton\" if is_pressed else \"display_deselected.TButton\"\n        state = [\"pressed\", \"focus\"] if is_pressed else [\"!pressed\", \"!focus\"]\n        btn = self._buttons[display]\n        btn.configure(style=style)\n        btn.state(state)\n        self._tk_vars[display].set(is_pressed)\n\n\nclass FacesViewer(tk.Canvas):   # pylint:disable=too-many-ancestors\n    \"\"\" The :class:`tkinter.Canvas` that holds the faces viewer section of the Manual Tool.\n\n    Parameters\n    ----------\n    parent: :class:`tkinter.ttk.Frame`\n        The parent frame for the canvas\n    tk_globals: :class:`~tools.manual.manual.TkGlobals`\n        The tkinter variables that apply to the whole of the GUI\n    tk_action_vars: dict\n        The :class:`tkinter.BooleanVar` objects for selectable optional annotations\n        as set by the buttons in the :class:`FacesActionsFrame`\n    detected_faces: :class:`~tools.manual.detected_faces.DetectedFaces`\n        The :class:`~lib.align.DetectedFace` objects for this video\n    display_frame: :class:`~tools.manual.frameviewer.frame.DisplayFrame`\n        The section of the Manual Tool that holds the frames viewer\n    event: :class:`threading.Event`\n        The threading event object for repeated key press protection\n    \"\"\"\n    def __init__(self, parent: ttk.Frame,\n                 tk_globals: TkGlobals,\n                 tk_action_vars: dict[T.Literal[\"mesh\", \"mask\"], tk.BooleanVar],\n                 detected_faces: DetectedFaces,\n                 display_frame: DisplayFrame,\n                 event: Event) -> None:\n        logger.debug(parse_class_init(locals()))\n        super().__init__(parent,\n                         bd=0,\n                         highlightthickness=0,\n                         bg=get_config().user_theme[\"group_panel\"][\"panel_background\"])\n        self.pack(side=tk.LEFT, fill=tk.BOTH, expand=True, anchor=tk.E)\n        self._sizes = {\"tiny\": 32, \"small\": 64, \"medium\": 96, \"large\": 128, \"extralarge\": 192}\n\n        self._globals = tk_globals\n        self._tk_optional_annotations = tk_action_vars\n        self._event = event\n        self._display_frame = display_frame\n        self._grid = Grid(self, detected_faces)\n        self._view = Viewport(self, detected_faces.tk_edited)\n        self._annotation_colors = {\"mesh\": self.get_muted_color(\"Mesh\"),\n                                   \"box\": self.control_colors[\"ExtractBox\"]}\n\n        ContextMenu(self, detected_faces)\n        self._bind_mouse_wheel_scrolling()\n        self._set_tk_callbacks(detected_faces)\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    @property\n    def face_size(self) -> int:\n        \"\"\" int: The currently selected thumbnail size in pixels \"\"\"\n        scaling = get_config().scaling_factor\n        size = self._sizes[self._globals.tk_faces_size.get().lower().replace(\" \", \"\")]\n        scaled = size * scaling\n        return int(round(scaled / 2) * 2)\n\n    @property\n    def viewport(self) -> Viewport:\n        \"\"\" :class:`~tools.manual.faceviewer.viewport.Viewport`: The viewport area of the\n        faces viewer. \"\"\"\n        return self._view\n\n    @property\n    def layout(self) -> Grid:\n        \"\"\" :class:`Grid`: The grid for the current :class:`FacesViewer`. \"\"\"\n        return self._grid\n\n    @property\n    def optional_annotations(self) -> dict[T.Literal[\"mesh\", \"mask\"], bool]:\n        \"\"\" dict[Literal[\"mesh\", \"mask\"], bool]: The values currently set for the\n        selectable optional annotations. \"\"\"\n        return {opt: val.get() for opt, val in self._tk_optional_annotations.items()}\n\n    @property\n    def selected_mask(self) -> str:\n        \"\"\" str: The currently selected mask from the display frame control panel. \"\"\"\n        return self._display_frame.tk_selected_mask.get().lower()\n\n    @property\n    def control_colors(self) -> dict[str, str]:\n        \"\"\"dict[str, str]: The frame Editor name as key with the current user selected hex code as\n        value. \"\"\"\n        return ({key: val.get() for key, val in self._display_frame.tk_control_colors.items()})\n\n    # << CALLBACK FUNCTIONS >> #\n    def _set_tk_callbacks(self, detected_faces: DetectedFaces):\n        \"\"\" Set the tkinter variable call backs.\n\n        Parameters\n        ----------\n        detected_faces: :class:`~tools.manual.detected_faces.DetectedFaces`\n            The Manual Tool's Detected Faces object\n\n        Redraw the grid on a face size change, a filter change or on add/remove faces.\n        Updates the annotation colors when user amends a color drop down.\n        Updates the mask type when the user changes the selected mask types\n        Toggles the face viewer annotations on an optional annotation button press.\n        \"\"\"\n        for var in (self._globals.tk_faces_size, self._globals.tk_filter_mode):\n            var.trace_add(\"write\", lambda *e, v=var: self.refresh_grid(v))\n        var = detected_faces.tk_face_count_changed\n        var.trace_add(\"write\", lambda *e, v=var: self.refresh_grid(v, retain_position=True))\n\n        self._display_frame.tk_control_colors[\"Mesh\"].trace_add(\n            \"write\", lambda *e: self._update_mesh_color())\n        self._display_frame.tk_control_colors[\"ExtractBox\"].trace_add(\n            \"write\", lambda *e: self._update_box_color())\n        self._display_frame.tk_selected_mask.trace_add(\n            \"write\", lambda *e: self._update_mask_type())\n\n        for opt, var in self._tk_optional_annotations.items():\n            var.trace_add(\"write\", lambda *e, o=opt: self._toggle_annotations(o))\n\n        self.bind(\"<Configure>\", lambda *e: self._view.update())\n\n    def refresh_grid(self, trigger_var: tk.BooleanVar, retain_position: bool = False) -> None:\n        \"\"\" Recalculate the full grid and redraw. Used when the active filter pull down is used, a\n        face has been added or removed, or the face thumbnail size has changed.\n\n        Parameters\n        ----------\n        trigger_var: :class:`tkinter.BooleanVar`\n            The tkinter variable that has triggered the grid update. Will either be the variable\n            indicating that the face size have been changed, or the variable indicating that the\n            selected filter mode has been changed.\n        retain_position: bool, optional\n            ``True`` if the grid should be set back to the position it was at after the update has\n            been processed, otherwise ``False``. Default: ``False``.\n        \"\"\"\n        if not trigger_var.get():\n            return\n        size_change = isinstance(trigger_var, tk.StringVar)\n        move_to = self.yview()[0] if retain_position else 0.0\n        self._grid.update()\n        if move_to != 0.0:\n            self.yview_moveto(move_to)\n        if size_change:\n            self._view.reset()\n        self._view.update(refresh_annotations=retain_position)\n        if not size_change:\n            trigger_var.set(False)\n\n    def _update_mask_type(self) -> None:\n        \"\"\" Update the displayed mask in the :class:`FacesViewer` canvas when the user changes\n        the mask type. \"\"\"\n        state: T.Literal[\"normal\", \"hidden\"]\n        state = \"normal\" if self.optional_annotations[\"mask\"] else \"hidden\"\n        logger.debug(\"Updating mask type: (mask_type: %s. state: %s)\", self.selected_mask, state)\n        self._view.toggle_mask(state, self.selected_mask)\n\n    # << MOUSE HANDLING >>\n    def _bind_mouse_wheel_scrolling(self) -> None:\n        \"\"\" Bind mouse wheel to scroll the :class:`FacesViewer` canvas. \"\"\"\n        if platform.system() == \"Linux\":\n            self.bind(\"<Button-4>\", self._scroll)\n            self.bind(\"<Button-5>\", self._scroll)\n        else:\n            self.bind(\"<MouseWheel>\", self._scroll)\n\n    def _scroll(self, event: tk.Event) -> None:\n        \"\"\" Handle mouse wheel scrolling over the :class:`FacesViewer` canvas.\n\n        Update is run in a thread to avoid repeated scroll actions stacking and locking up the GUI.\n\n        Parameters\n        ----------\n        event: :class:`tkinter.Event`\n            The event fired by the mouse scrolling\n        \"\"\"\n        if self._event.is_set():\n            logger.trace(\"Update already running. \"  # type:ignore[attr-defined]\n                         \"Aborting repeated mousewheel\")\n            return\n        if platform.system() == \"Darwin\":\n            adjust = event.delta\n        elif platform.system() == \"Windows\":\n            adjust = int(event.delta / 120)\n        elif event.num == 5:\n            adjust = -1\n        else:\n            adjust = 1\n        self._event.set()\n        thread = Thread(target=self.canvas_scroll, args=(-1 * adjust, \"units\", self._event))\n        thread.start()\n\n    def canvas_scroll(self, amount: int, units: T.Literal[\"pages\", \"units\"], event: Event) -> None:\n        \"\"\" Scroll the canvas on an up/down or page-up/page-down key press.\n\n        Parameters\n        ----------\n        amount: int\n            The number of units to scroll the canvas\n        units: Literal[\"pages\", \"units\"]\n            The unit type to scroll by\n        event: :class:`threading.Event`\n            event to indicate to the calling process whether the scroll is still updating\n        \"\"\"\n        self.yview_scroll(int(amount), units)\n        self._view.update()\n        self._view.hover_box.on_hover(None)\n        event.clear()\n\n    # << OPTIONAL ANNOTATION METHODS >> #\n    def _update_mesh_color(self) -> None:\n        \"\"\" Update the mesh color when user updates the control panel. \"\"\"\n        color = self.get_muted_color(\"Mesh\")\n        if self._annotation_colors[\"mesh\"] == color:\n            return\n        highlight_color = self.control_colors[\"Mesh\"]\n\n        self.itemconfig(\"viewport_polygon\", outline=color)\n        self.itemconfig(\"viewport_line\", fill=color)\n        self.itemconfig(\"active_mesh_polygon\", outline=highlight_color)\n        self.itemconfig(\"active_mesh_line\", fill=highlight_color)\n        self._annotation_colors[\"mesh\"] = color\n\n    def _update_box_color(self) -> None:\n        \"\"\" Update the active box color when user updates the control panel. \"\"\"\n        color = self.control_colors[\"ExtractBox\"]\n\n        if self._annotation_colors[\"box\"] == color:\n            return\n        self.itemconfig(\"active_highlighter\", outline=color)\n        self._annotation_colors[\"box\"] = color\n\n    def get_muted_color(self, color_key: str) -> str:\n        \"\"\" Creates a muted version of the given annotation color for non-active faces.\n\n        Parameters\n        ----------\n        color_key: str\n            The annotation key to obtain the color for from :attr:`control_colors`\n\n        Returns\n        -------\n        str\n            The hex color code of the muted color\n        \"\"\"\n        scale = 0.65\n        hls = np.array(colorsys.rgb_to_hls(*hex_to_rgb(self.control_colors[color_key])))\n        scale = (1 - scale) + 1 if hls[1] < 120 else scale\n        hls[1] = max(0., min(256., scale * hls[1]))\n        rgb = np.clip(np.rint(colorsys.hls_to_rgb(*hls)).astype(\"uint8\"), 0, 255)\n        retval = rgb_to_hex(rgb)\n        return retval\n\n    def _toggle_annotations(self, annotation: T.Literal[\"mesh\", \"mask\"]) -> None:\n        \"\"\" Toggle optional annotations on or off after the user depresses an optional button.\n\n        Parameters\n        ----------\n        annotation: [\"mesh\", \"mask\"]\n            The optional annotation to toggle on or off\n        \"\"\"\n        state: T.Literal[\"hidden\", \"normal\"]\n        state = \"normal\" if self.optional_annotations[annotation] else \"hidden\"\n        logger.debug(\"Toggle annotation: (annotation: %s, state: %s)\", annotation, state)\n        if annotation == \"mesh\":\n            self._view.toggle_mesh(state)\n        if annotation == \"mask\":\n            self._view.toggle_mask(state, self.selected_mask)\n\n\nclass Grid():\n    \"\"\" Holds information on the current filtered grid layout.\n\n    The grid keeps information on frame indices, face indices, x and y positions and detected face\n    objects laid out in a numpy array to reflect the current full layout of faces within the face\n    viewer based on the currently selected filter and face thumbnail size.\n\n    Parameters\n    ----------\n    canvas: :class:`~FacesViewer`\n        The :class:`~tools.manual.faceviewer.frame.FacesViewer` canvas\n    detected_faces: :class:`~tools.manual.detected_faces.DetectedFaces`\n        The :class:`~lib.align.DetectedFace` objects for this video\n    \"\"\"\n    def __init__(self, canvas: FacesViewer, detected_faces: DetectedFaces):\n        logger.debug(parse_class_init(locals()))\n        self._canvas = canvas\n        self._detected_faces = detected_faces\n        self._raw_indices = detected_faces.filter.raw_indices\n        self._frames_list = detected_faces.filter.frames_list\n\n        self._is_valid: bool = False\n        self._face_size: int = 0\n        self._grid: np.ndarray | None = None\n        self._display_faces: np.ndarray | None = None\n\n        self._canvas.update_idletasks()\n        self._canvas.create_rectangle(0, 0, 0, 0, tags=[\"backdrop\"])\n        self.update()\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    @property\n    def face_size(self) -> int:\n        \"\"\" int: The pixel size of each thumbnail within the face viewer. \"\"\"\n        return self._face_size\n\n    @property\n    def is_valid(self) -> bool:\n        \"\"\" bool: ``True`` if the current filter means that the grid holds faces. ``False`` if\n        there are no faces displayed in the grid. \"\"\"\n        return self._is_valid\n\n    @property\n    def columns_rows(self) -> tuple[int, int]:\n        \"\"\" tuple: the (`columns`, `rows`) required to hold all display images. \"\"\"\n        if not self._is_valid:\n            return (0, 0)\n        assert self._grid is not None\n        retval = tuple(reversed(self._grid.shape[1:]))\n        return T.cast(tuple[int, int], retval)\n\n    @property\n    def dimensions(self) -> tuple[int, int]:\n        \"\"\" tuple: The (`width`, `height`) required to hold all display images. \"\"\"\n        if self._is_valid:\n            assert self._grid is not None\n            retval = tuple(dim * self._face_size for dim in reversed(self._grid.shape[1:]))\n            assert len(retval) == 2\n        else:\n            retval = (0, 0)\n        return T.cast(tuple[int, int], retval)\n\n    @property\n    def _visible_row_indices(self) -> tuple[int, int]:\n        \"\"\"tuple: A 1 dimensional array of the (`top_row_index`, `bottom_row_index`) of the grid\n        currently in the viewable area.\n        \"\"\"\n        height = self.dimensions[1]\n        visible = (max(0, floor(height * self._canvas.yview()[0]) - self._face_size),\n                   ceil(height * self._canvas.yview()[1]))\n        logger.trace(\"height: %s, yview: %s, face_size: %s, \"  # type:ignore[attr-defined]\n                     \"visible: %s\", height, self._canvas.yview(), self._face_size, visible)\n        assert self._grid is not None\n        y_points = self._grid[3, :, 1]\n        top = np.searchsorted(y_points, visible[0], side=\"left\")\n        bottom = np.searchsorted(y_points, visible[1], side=\"right\")\n        return int(top), int(bottom)\n\n    @property\n    def visible_area(self) -> tuple[np.ndarray, np.ndarray]:\n        \"\"\"tuple[:class:`numpy.ndarray`, :class:`numpy.ndarray`]: Tuple containing 2 arrays.\n\n        1st array contains an array of shape (`4`, `rows`, `columns`) corresponding\n        to the viewable area of the display grid. 1st dimension contains frame indices, 2nd\n        dimension face indices. The 3rd and 4th dimension contain the x and y position of the top\n        left corner of the face respectively.\n\n        2nd array contains :class:`~lib.align.DetectedFace` objects laid out in (rows, columns)\n\n        Any locations that are not populated by a face will have a frame and face index of -1\n        \"\"\"\n        if not self._is_valid:\n            retval = np.zeros((4, 0, 0)), np.zeros((0, 0))\n        else:\n            assert self._grid is not None\n            assert self._display_faces is not None\n            top, bottom = self._visible_row_indices\n            retval = self._grid[:, top:bottom, :], self._display_faces[top:bottom, :]\n        logger.trace([r if r is None else r.shape for r in retval])  # type:ignore[attr-defined]\n        return retval\n\n    def y_coord_from_frame(self, frame_index: int) -> int:\n        \"\"\" Return the y coordinate for the first face that appears in the given frame.\n\n        Parameters\n        ----------\n        frame_index: int\n            The frame index to locate in the grid\n\n        Returns\n        -------\n        int\n            The y coordinate of the first face for the given frame\n        \"\"\"\n        assert self._grid is not None\n        return min(self._grid[3][np.where(self._grid[0] == frame_index)])\n\n    def frame_has_faces(self, frame_index: int) -> bool | np.bool_:\n        \"\"\" Check whether the given frame index contains any faces.\n\n        Parameters\n        ----------\n        frame_index: int\n            The frame index to locate in the grid\n\n        Returns\n        -------\n        bool\n            ``True`` if there are faces in the given frame otherwise ``False``\n        \"\"\"\n        if not self._is_valid:\n            return False\n        assert self._grid is not None\n        return np.any(self._grid[0] == frame_index)\n\n    def update(self) -> None:\n        \"\"\" Update the underlying grid.\n\n        Called on initialization, on a filter change or on add/remove faces. Recalculates the\n        underlying grid for the current filter view and updates the attributes :attr:`_grid`,\n        :attr:`_display_faces`, :attr:`_raw_indices`, :attr:`_frames_list` and :attr:`is_valid`\n        \"\"\"\n        self._face_size = self._canvas.face_size\n        self._raw_indices = self._detected_faces.filter.raw_indices\n        self._frames_list = self._detected_faces.filter.frames_list\n        self._get_grid()\n        self._get_display_faces()\n        self._canvas.coords(\"backdrop\", 0, 0, *self.dimensions)\n        self._canvas.configure(scrollregion=self._canvas.bbox(\"backdrop\"))\n        self._canvas.yview_moveto(0.0)\n\n    def _get_grid(self) -> None:\n        \"\"\" Get the grid information for faces currently displayed in the :class:`FacesViewer`.\n        and set to :attr:`_grid`. Creates a numpy array of shape (`4`, `rows`, `columns`)\n        corresponding to the display grid. 1st dimension contains frame indices, 2nd dimension face\n        indices. The 3rd and 4th dimension contain the x and y position of the top left corner of\n        the face respectively.\n\n        Any locations that are not populated by a face will have a frame and face index of -1\"\"\"\n        labels = self._get_labels()\n        if not self._is_valid:\n            logger.debug(\"Setting grid to None for no faces.\")\n            self._grid = None\n            return\n        assert labels is not None\n        x_coords = np.linspace(0,\n                               labels.shape[2] * self._face_size,\n                               num=labels.shape[2],\n                               endpoint=False,\n                               dtype=\"int\")\n        y_coords = np.linspace(0,\n                               labels.shape[1] * self._face_size,\n                               num=labels.shape[1],\n                               endpoint=False,\n                               dtype=\"int\")\n        self._grid = np.array((*labels, *np.meshgrid(x_coords, y_coords)), dtype=\"int\")\n        logger.debug(self._grid.shape)\n\n    def _get_labels(self) -> np.ndarray | None:\n        \"\"\" Get the frame and face index for each grid position for the current filter.\n\n        Returns\n        -------\n        :class:`numpy.ndarray` | None\n            Array of dimensions (2, rows, columns) corresponding to the display grid, with frame\n            index as the first dimension and face index within the frame as the 2nd dimension.\n\n            Any remaining placeholders at the end of the grid which are not populated with a face\n            are given the index -1\n        \"\"\"\n        face_count = len(self._raw_indices[\"frame\"])\n        self._is_valid = face_count != 0\n        if not self._is_valid:\n            return None\n        columns = self._canvas.winfo_width() // self._face_size\n        rows = ceil(face_count / columns)\n        remainder = face_count % columns\n        padding = [] if remainder == 0 else [-1 for _ in range(columns - remainder)]\n        labels = np.array((self._raw_indices[\"frame\"] + padding,\n                           self._raw_indices[\"face\"] + padding),\n                          dtype=\"int\").reshape((2, rows, columns))\n        logger.debug(\"face-count: %s, columns: %s, rows: %s, remainder: %s, padding: %s, labels \"\n                     \"shape: %s\", face_count, columns, rows, remainder, padding, labels.shape)\n        return labels\n\n    def _get_display_faces(self):\n        \"\"\" Get the detected faces for the current filter, arrange to grid and set to\n        :attr:`_display_faces`. This is an array of dimensions (rows, columns) corresponding to the\n        display grid, containing the corresponding :class:`lib.align.DetectFace` object\n\n        Any remaining placeholders at the end of the grid which are not populated with a face are\n        replaced with ``None``\"\"\"\n        if not self._is_valid:\n            logger.debug(\"Setting display_faces to None for no faces.\")\n            self._display_faces = None\n            return\n        current_faces = self._detected_faces.current_faces\n        columns, rows = self.columns_rows\n        face_count = len(self._raw_indices[\"frame\"])\n        padding = [None for _ in range(face_count, columns * rows)]\n        self._display_faces = np.array([None if idx is None else current_faces[idx][face_idx]\n                                        for idx, face_idx\n                                        in zip(self._raw_indices[\"frame\"] + padding,\n                                               self._raw_indices[\"face\"] + padding)],\n                                       dtype=\"object\").reshape(rows, columns)\n        logger.debug(\"faces: (shape: %s, dtype: %s)\",\n                     self._display_faces.shape, self._display_faces.dtype)\n\n    def transport_index_from_frame(self, frame_index: int) -> int | None:\n        \"\"\" Return the main frame's transport index for the given frame index based on the current\n        filter criteria.\n\n        Parameters\n        ----------\n        frame_index: int\n            The absolute index for the frame within the full frames list\n\n        Returns\n        -------\n        int | None\n            The index of the requested frame within the filtered frames view. None if no valid\n            frames\n        \"\"\"\n        retval = self._frames_list.index(frame_index) if frame_index in self._frames_list else None\n        logger.trace(\"frame_index: %s, transport_index: %s\",  # type:ignore[attr-defined]\n                     frame_index, retval)\n        return retval\n\n\nclass ContextMenu():  # pylint:disable=too-few-public-methods\n    \"\"\"  Enables a right click context menu for the\n    :class:`~tools.manual.faceviewer.frame.FacesViewer`.\n\n    Parameters\n    ----------\n    canvas: :class:`tkinter.Canvas`\n        The :class:`FacesViewer` canvas\n    detected_faces: :class:`~tools.manual.detected_faces`\n        The manual tool's detected faces class\n    \"\"\"\n    def __init__(self, canvas, detected_faces):\n        logger.debug(\"Initializing: %s (canvas: %s, detected_faces: %s)\",\n                     self.__class__.__name__, canvas, detected_faces)\n        self._canvas = canvas\n        self._detected_faces = detected_faces\n        self._menu = RightClickMenu([\"Delete Face\"], [self._delete_face])\n        self._frame_index = None\n        self._face_index = None\n        self._canvas.bind(\"<Button-2>\" if platform.system() == \"Darwin\" else \"<Button-3>\",\n                          self._pop_menu)\n        logger.debug(\"Initialized: %s\", self.__class__.__name__)\n\n    def _pop_menu(self, event):\n        \"\"\" Pop up the context menu on a right click mouse event.\n\n        Parameters\n        ----------\n        event: :class:`tkinter.Event`\n            The mouse event that has triggered the pop up menu\n        \"\"\"\n        frame_idx, face_idx = self._canvas.viewport.face_from_point(\n            self._canvas.canvasx(event.x), self._canvas.canvasy(event.y))[:2]\n        if frame_idx == -1:\n            logger.trace(\"No valid item under mouse\")  # type:ignore[attr-defined]\n            self._frame_index = self._face_index = None\n            return\n        self._frame_index = frame_idx\n        self._face_index = face_idx\n        logger.trace(\"Popping right click menu\")  # type:ignore[attr-defined]\n        self._menu.popup(event)\n\n    def _delete_face(self):\n        \"\"\" Delete the selected face on a right click mouse delete action. \"\"\"\n        logger.trace(\"Right click delete received. frame_id: %s, \"  # type:ignore[attr-defined]\n                     \"face_id: %s\", self._frame_index, self._face_index)\n        self._detected_faces.update.delete(self._frame_index, self._face_index)\n        self._frame_index = self._face_index = None\n", "tools/manual/frameviewer/control.py": "#!/usr/bin/env python3\n\"\"\" Handles Navigation and Background Image for the Frame Viewer section of the manual\ntool GUI. \"\"\"\n\nimport logging\nimport tkinter as tk\n\nimport cv2\nimport numpy as np\nfrom PIL import Image, ImageTk\n\nfrom lib.align import AlignedFace\n\nlogger = logging.getLogger(__name__)\n\n\nclass Navigation():\n    \"\"\" Handles playback and frame navigation for the Frame Viewer Window.\n\n    Parameters\n    ----------\n    display_frame: :class:`DisplayFrame`\n        The parent frame viewer window\n    \"\"\"\n    def __init__(self, display_frame):\n        logger.debug(\"Initializing %s\", self.__class__.__name__)\n        self._display_frame = display_frame\n        self._globals = display_frame._globals\n        self._det_faces = display_frame._det_faces\n        self._nav = display_frame._nav\n        self._tk_is_playing = tk.BooleanVar()\n        self._tk_is_playing.set(False)\n        self._det_faces.tk_face_count_changed.trace(\"w\", self._update_total_frame_count)\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    @property\n    def _current_nav_frame_count(self):\n        \"\"\" int: The current frame count for the transport slider \"\"\"\n        return self._nav[\"scale\"].cget(\"to\") + 1\n\n    def nav_scale_callback(self, *args, reset_progress=True):  # pylint:disable=unused-argument\n        \"\"\" Adjust transport slider scale for different filters. Hide or display optional filter\n        controls.\n        \"\"\"\n        self._display_frame.pack_threshold_slider()\n        if reset_progress:\n            self.stop_playback()\n        frame_count = self._det_faces.filter.count\n        if self._current_nav_frame_count == frame_count:\n            logger.trace(\"Filtered count has not changed. Returning\")\n        if self._globals.tk_filter_mode.get() == \"Misaligned Faces\":\n            self._det_faces.tk_face_count_changed.set(True)\n        self._update_total_frame_count()\n        if reset_progress:\n            self._globals.tk_transport_index.set(0)\n\n    def _update_total_frame_count(self, *args):  # pylint:disable=unused-argument\n        \"\"\" Update the displayed number of total frames that meet the current filter criteria.\n\n        Parameters\n        ----------\n        args: tuple\n            Required for tkinter trace callback but unused\n        \"\"\"\n        frame_count = self._det_faces.filter.count\n        if self._current_nav_frame_count == frame_count:\n            logger.trace(\"Filtered count has not changed. Returning\")\n            return\n        max_frame = max(0, frame_count - 1)\n        logger.debug(\"Filtered frame count has changed. Updating from %s to %s\",\n                     self._current_nav_frame_count, frame_count)\n        self._nav[\"scale\"].config(to=max_frame)\n        self._nav[\"label\"].config(text=\"/{}\".format(max_frame))\n        state = \"disabled\" if max_frame == 0 else \"normal\"\n        self._nav[\"entry\"].config(state=state)\n\n    @property\n    def tk_is_playing(self):\n        \"\"\" :class:`tkinter.BooleanVar`: Whether the stream is currently playing. \"\"\"\n        return self._tk_is_playing\n\n    def handle_play_button(self):\n        \"\"\" Handle the play button.\n\n        Switches the :attr:`tk_is_playing` variable.\n        \"\"\"\n        is_playing = self.tk_is_playing.get()\n        self.tk_is_playing.set(not is_playing)\n\n    def stop_playback(self):\n        \"\"\" Stop play back if playing \"\"\"\n        if self.tk_is_playing.get():\n            logger.trace(\"Stopping playback\")\n            self.tk_is_playing.set(False)\n\n    def increment_frame(self, frame_count=None, is_playing=False):\n        \"\"\" Update The frame navigation position to the next frame based on filter. \"\"\"\n        if not is_playing:\n            self.stop_playback()\n        position = self._get_safe_frame_index()\n        face_count_change = not self._det_faces.filter.frame_meets_criteria\n        if face_count_change:\n            position -= 1\n        frame_count = self._det_faces.filter.count if frame_count is None else frame_count\n        if not face_count_change and (frame_count == 0 or position == frame_count - 1):\n            logger.debug(\"End of Stream. Not incrementing\")\n            self.stop_playback()\n            return\n        self._globals.tk_transport_index.set(min(position + 1, max(0, frame_count - 1)))\n\n    def decrement_frame(self):\n        \"\"\" Update The frame navigation position to the previous frame based on filter. \"\"\"\n        self.stop_playback()\n        position = self._get_safe_frame_index()\n        face_count_change = not self._det_faces.filter.frame_meets_criteria\n        if not face_count_change and (self._det_faces.filter.count == 0 or position == 0):\n            logger.debug(\"End of Stream. Not decrementing\")\n            return\n        self._globals.tk_transport_index.set(min(max(0, self._det_faces.filter.count - 1),\n                                                 max(0, position - 1)))\n\n    def _get_safe_frame_index(self):\n        \"\"\" Obtain the current frame position from the tk_transport_index variable in\n        a safe manner (i.e. handle for non-numeric)\n\n        Returns\n        -------\n        int\n            The current transport frame index\n        \"\"\"\n        try:\n            retval = self._globals.tk_transport_index.get()\n        except tk.TclError as err:\n            if \"expected floating-point\" not in str(err):\n                raise\n            val = str(err).split(\" \")[-1].replace(\"\\\"\", \"\")\n            retval = \"\".join(ch for ch in val if ch.isdigit())\n            retval = 0 if not retval else int(retval)\n            self._globals.tk_transport_index.set(retval)\n        return retval\n\n    def goto_first_frame(self):\n        \"\"\" Go to the first frame that meets the filter criteria. \"\"\"\n        self.stop_playback()\n        position = self._globals.tk_transport_index.get()\n        if position == 0:\n            return\n        self._globals.tk_transport_index.set(0)\n\n    def goto_last_frame(self):\n        \"\"\" Go to the last frame that meets the filter criteria. \"\"\"\n        self.stop_playback()\n        position = self._globals.tk_transport_index.get()\n        frame_count = self._det_faces.filter.count\n        if position == frame_count - 1:\n            return\n        self._globals.tk_transport_index.set(frame_count - 1)\n\n\nclass BackgroundImage():\n    \"\"\" The background image of the canvas \"\"\"\n    def __init__(self, canvas):\n        self._canvas = canvas\n        self._globals = canvas._globals\n        self._det_faces = canvas._det_faces\n        placeholder = np.ones((*reversed(self._globals.frame_display_dims), 3), dtype=\"uint8\")\n        self._tk_frame = ImageTk.PhotoImage(Image.fromarray(placeholder))\n        self._tk_face = ImageTk.PhotoImage(Image.fromarray(placeholder))\n        self._image = self._canvas.create_image(self._globals.frame_display_dims[0] / 2,\n                                                self._globals.frame_display_dims[1] / 2,\n                                                image=self._tk_frame,\n                                                anchor=tk.CENTER,\n                                                tags=\"main_image\")\n        self._zoomed_centering = \"face\"\n\n    @property\n    def _current_view_mode(self):\n        \"\"\" str: `frame` if global zoom mode variable is set to ``False`` other wise `face`. \"\"\"\n        retval = \"face\" if self._globals.is_zoomed else \"frame\"\n        logger.trace(retval)\n        return retval\n\n    def refresh(self, view_mode):\n        \"\"\" Update the displayed frame.\n\n        Parameters\n        ----------\n        view_mode: [\"frame\", \"face\"]\n            The currently active editor's selected view mode.\n        \"\"\"\n        self._switch_image(view_mode)\n        logger.trace(\"Updating background frame\")\n        getattr(self, \"_update_tk_{}\".format(self._current_view_mode))()\n\n    def _switch_image(self, view_mode):\n        \"\"\" Switch the image between the full frame image and the zoomed face image.\n\n        Parameters\n        ----------\n        view_mode: [\"frame\", \"face\"]\n            The currently active editor's selected view mode.\n        \"\"\"\n        if view_mode == self._current_view_mode and (\n                self._canvas.active_editor.zoomed_centering == self._zoomed_centering):\n            return\n        self._zoomed_centering = self._canvas.active_editor.zoomed_centering\n        logger.trace(\"Switching background image from '%s' to '%s'\",\n                     self._current_view_mode, view_mode)\n        img = getattr(self, \"_tk_{}\".format(view_mode))\n        self._canvas.itemconfig(self._image, image=img)\n        self._globals.tk_is_zoomed.set(view_mode == \"face\")\n        self._globals.tk_face_index.set(0)\n\n    def _update_tk_face(self):\n        \"\"\" Update the currently zoomed face. \"\"\"\n        face = self._get_zoomed_face()\n        padding = self._get_padding((min(self._globals.frame_display_dims),\n                                     min(self._globals.frame_display_dims)))\n        face = cv2.copyMakeBorder(face, *padding, cv2.BORDER_CONSTANT)\n        if self._tk_frame.height() != face.shape[0]:\n            self._resize_frame()\n\n        logger.trace(\"final shape: %s\", face.shape)\n        self._tk_face.paste(Image.fromarray(face))\n\n    def _get_zoomed_face(self):\n        \"\"\" Get the zoomed face or a blank image if no faces are available.\n\n        Returns\n        -------\n        :class:`numpy.ndarray`\n            The face sized to the shortest dimensions of the face viewer\n        \"\"\"\n        frame_idx = self._globals.frame_index\n        face_idx = self._globals.face_index\n        faces_in_frame = self._det_faces.face_count_per_index[frame_idx]\n        size = min(self._globals.frame_display_dims)\n\n        if face_idx + 1 > faces_in_frame:\n            logger.debug(\"Resetting face index to 0 for more faces in frame than current index: (\"\n                         \"faces_in_frame: %s, zoomed_face_index: %s\", faces_in_frame, face_idx)\n            self._globals.tk_face_index.set(0)\n\n        if faces_in_frame == 0:\n            face = np.ones((size, size, 3), dtype=\"uint8\")\n        else:\n            det_face = self._det_faces.current_faces[frame_idx][face_idx]\n            face = AlignedFace(det_face.landmarks_xy,\n                               image=self._globals.current_frame[\"image\"],\n                               centering=self._zoomed_centering,\n                               size=size).face\n        logger.trace(\"face shape: %s\", face.shape)\n        return face[..., 2::-1]\n\n    def _update_tk_frame(self):\n        \"\"\" Place the currently held frame into :attr:`_tk_frame`. \"\"\"\n        img = cv2.resize(self._globals.current_frame[\"image\"],\n                         self._globals.current_frame[\"display_dims\"],\n                         interpolation=self._globals.current_frame[\"interpolation\"])[..., 2::-1]\n        padding = self._get_padding(img.shape[:2])\n        if any(padding):\n            img = cv2.copyMakeBorder(img, *padding, cv2.BORDER_CONSTANT)\n        logger.trace(\"final shape: %s\", img.shape)\n\n        if self._tk_frame.height() != img.shape[0]:\n            self._resize_frame()\n\n        self._tk_frame.paste(Image.fromarray(img))\n\n    def _get_padding(self, size):\n        \"\"\" Obtain the Left, Top, Right, Bottom padding required to place the square face or frame\n        in to the Photo Image\n\n        Returns\n        -------\n        tuple\n            The (Left, Top, Right, Bottom) padding to apply to the face image in pixels\n        \"\"\"\n        pad_lt = ((self._globals.frame_display_dims[1] - size[0]) // 2,\n                  (self._globals.frame_display_dims[0] - size[1]) // 2)\n        padding = (pad_lt[0],\n                   self._globals.frame_display_dims[1] - size[0] - pad_lt[0],\n                   pad_lt[1],\n                   self._globals.frame_display_dims[0] - size[1] - pad_lt[1])\n        logger.debug(\"Frame dimensions: %s, size: %s, padding: %s\",\n                     self._globals.frame_display_dims, size, padding)\n        return padding\n\n    def _resize_frame(self):\n        \"\"\" Resize the :attr:`_tk_frame`, attr:`_tk_face` photo images, update the canvas to\n        offset the image correctly.\n        \"\"\"\n        logger.trace(\"Resizing video frame on resize event: %s\", self._globals.frame_display_dims)\n        placeholder = np.ones((*reversed(self._globals.frame_display_dims), 3), dtype=\"uint8\")\n        self._tk_frame = ImageTk.PhotoImage(Image.fromarray(placeholder))\n        self._tk_face = ImageTk.PhotoImage(Image.fromarray(placeholder))\n        self._canvas.coords(self._image,\n                            self._globals.frame_display_dims[0] / 2,\n                            self._globals.frame_display_dims[1] / 2)\n        img = self._tk_face if self._current_view_mode == \"face\" else self._tk_frame\n        self._canvas.itemconfig(self._image, image=img)\n", "tools/manual/frameviewer/__init__.py": "", "tools/manual/frameviewer/frame.py": "#!/usr/bin/env python3\n\"\"\" The frame viewer section of the manual tool GUI \"\"\"\nimport gettext\nimport logging\nimport tkinter as tk\nfrom tkinter import ttk, TclError\n\nfrom functools import partial\nfrom time import time\n\nfrom lib.gui.control_helper import set_slider_rounding\nfrom lib.gui.custom_widgets import Tooltip\nfrom lib.gui.utils import get_images\n\nfrom .control import Navigation, BackgroundImage\nfrom .editor import (BoundingBox, ExtractBox, Landmarks, Mask,  # noqa pylint:disable=unused-import\n                     Mesh, View)\n\nlogger = logging.getLogger(__name__)\n\n# LOCALES\n_LANG = gettext.translation(\"tools.manual\", localedir=\"locales\", fallback=True)\n_ = _LANG.gettext\n\n\nclass DisplayFrame(ttk.Frame):  # pylint:disable=too-many-ancestors\n    \"\"\" The main video display frame (top left section of GUI).\n\n    Parameters\n    ----------\n    parent: :class:`ttk.PanedWindow`\n        The paned window that the display frame resides in\n    tk_globals: :class:`~tools.manual.manual.TkGlobals`\n        The tkinter variables that apply to the whole of the GUI\n    detected_faces: :class:`tools.manual.detected_faces.DetectedFaces`\n        The detected faces stored in the alignments file\n    \"\"\"\n    def __init__(self, parent, tk_globals, detected_faces):\n        logger.debug(\"Initializing %s: (parent: %s, tk_globals: %s, detected_faces: %s)\",\n                     self.__class__.__name__, parent, tk_globals, detected_faces)\n        super().__init__(parent)\n\n        self._globals = tk_globals\n        self._det_faces = detected_faces\n        self._optional_widgets = {}\n\n        self._actions_frame = ActionsFrame(self)\n        main_frame = ttk.Frame(self)\n\n        self._transport_frame = ttk.Frame(main_frame)\n        self._nav = self._add_nav()\n        self._navigation = Navigation(self)\n        self._buttons = self._add_transport()\n        self._add_transport_tk_trace()\n\n        video_frame = ttk.Frame(main_frame)\n        video_frame.bind(\"<Configure>\", self._resize)\n\n        self._canvas = FrameViewer(video_frame,\n                                   self._globals,\n                                   self._det_faces,\n                                   self._actions_frame.actions,\n                                   self._actions_frame.tk_selected_action)\n\n        self._actions_frame.add_optional_buttons(self.editors)\n\n        self._transport_frame.pack(side=tk.BOTTOM, padx=5, fill=tk.X)\n        video_frame.pack(side=tk.TOP, expand=True, fill=tk.BOTH)\n        main_frame.pack(side=tk.RIGHT, expand=True, fill=tk.BOTH)\n        self.pack(side=tk.LEFT, anchor=tk.NW, expand=True, fill=tk.BOTH)\n\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    @property\n    def _helptext(self):\n        \"\"\" dict: {`name`: `help text`} Helptext lookup for navigation buttons \"\"\"\n        return {\n            \"play\": _(\"Play/Pause (SPACE)\"),\n            \"beginning\": _(\"Go to First Frame (HOME)\"),\n            \"prev\": _(\"Go to Previous Frame (Z)\"),\n            \"next\": _(\"Go to Next Frame (X)\"),\n            \"end\": _(\"Go to Last Frame (END)\"),\n            \"extract\": _(\"Extract the faces to a folder... (Ctrl+E)\"),\n            \"save\": _(\"Save the Alignments file (Ctrl+S)\"),\n            \"mode\": _(\"Filter Frames to only those Containing the Selected Item (F)\"),\n            \"distance\": _(\"Set the distance from an 'average face' to be considered misaligned. \"\n                          \"Higher distances are more restrictive\")}\n\n    @property\n    def _btn_action(self):\n        \"\"\" dict: {`name`: `action`} Command lookup for navigation buttons \"\"\"\n        actions = {\"play\": self._navigation.handle_play_button,\n                   \"beginning\": self._navigation.goto_first_frame,\n                   \"prev\": self._navigation.decrement_frame,\n                   \"next\": self._navigation.increment_frame,\n                   \"end\": self._navigation.goto_last_frame,\n                   \"extract\": self._det_faces.extract,\n                   \"save\": self._det_faces.save}\n        return actions\n\n    @property\n    def tk_selected_action(self):\n        \"\"\" :class:`tkinter.StringVar`: The variable holding the currently selected action \"\"\"\n        return self._actions_frame.tk_selected_action\n\n    @property\n    def active_editor(self):\n        \"\"\" :class:`Editor`: The current editor in use based on :attr:`selected_action`. \"\"\"\n        return self._canvas.active_editor\n\n    @property\n    def editors(self):\n        \"\"\" dict: All of the :class:`Editor` that the canvas holds \"\"\"\n        return self._canvas.editors\n\n    @property\n    def navigation(self):\n        \"\"\" :class:`~tools.manual.frameviewer.control.Navigation`: Class that handles frame\n        Navigation and transport. \"\"\"\n        return self._navigation\n\n    @property\n    def tk_control_colors(self):\n        \"\"\" :dict: Editor key with :class:`tkinter.StringVar` containing the selected color hex\n        code for each annotation \"\"\"\n        return {key: val[\"color\"].tk_var for key, val in self._canvas.annotation_formats.items()}\n\n    @property\n    def tk_selected_mask(self):\n        \"\"\" :dict: Editor key with :class:`tkinter.StringVar` containing the selected color hex\n        code for each annotation \"\"\"\n        return self._canvas.control_tk_vars[\"Mask\"][\"display\"][\"MaskType\"]\n\n    @property\n    def _filter_modes(self):\n        \"\"\" list: The filter modes combo box values \"\"\"\n        return [\"All Frames\", \"Has Face(s)\", \"No Faces\", \"Multiple Faces\", \"Misaligned Faces\"]\n\n    def _add_nav(self):\n        \"\"\" Add the slider to navigate through frames \"\"\"\n        max_frame = self._globals.frame_count - 1\n        frame = ttk.Frame(self._transport_frame)\n\n        frame.pack(side=tk.TOP, fill=tk.X, pady=(0, 5))\n        lbl_frame = ttk.Frame(frame)\n        lbl_frame.pack(side=tk.RIGHT)\n        tbox = ttk.Entry(lbl_frame,\n                         width=7,\n                         textvariable=self._globals.tk_transport_index,\n                         justify=tk.RIGHT)\n        tbox.pack(padx=0, side=tk.LEFT)\n        lbl = ttk.Label(lbl_frame, text=f\"/{max_frame}\")\n        lbl.pack(side=tk.RIGHT)\n\n        cmd = partial(set_slider_rounding,\n                      var=self._globals.tk_transport_index,\n                      d_type=int,\n                      round_to=1,\n                      min_max=(0, max_frame))\n\n        nav = ttk.Scale(frame,\n                        variable=self._globals.tk_transport_index,\n                        from_=0,\n                        to=max_frame,\n                        command=cmd)\n        nav.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)\n        self._globals.tk_transport_index.trace(\"w\", self._set_frame_index)\n        return {\"entry\": tbox, \"scale\": nav, \"label\": lbl}\n\n    def _set_frame_index(self, *args):  # pylint:disable=unused-argument\n        \"\"\" Set the actual frame index based on current slider position and filter mode. \"\"\"\n        try:\n            slider_position = self._globals.tk_transport_index.get()\n        except TclError:\n            # don't update the slider when the entry box has been cleared of any value\n            return\n        frames = self._det_faces.filter.frames_list\n        actual_position = max(0, min(len(frames) - 1, slider_position))\n        if actual_position != slider_position:\n            self._globals.tk_transport_index.set(actual_position)\n        frame_idx = frames[actual_position] if frames else -1\n        logger.trace(\"slider_position: %s, frame_idx: %s\", actual_position, frame_idx)\n        self._globals.tk_frame_index.set(frame_idx)\n\n    def _add_transport(self):\n        \"\"\" Add video transport controls \"\"\"\n        frame = ttk.Frame(self._transport_frame)\n        frame.pack(side=tk.BOTTOM, fill=tk.X)\n        icons = get_images().icons\n        buttons = {}\n        for action in (\"play\", \"beginning\", \"prev\", \"next\", \"end\", \"save\", \"extract\", \"mode\"):\n            padx = (0, 6) if action in (\"play\", \"prev\", \"mode\") else (0, 0)\n            side = tk.RIGHT if action in (\"extract\", \"save\", \"mode\") else tk.LEFT\n            state = [\"!disabled\"] if action != \"save\" else [\"disabled\"]\n            if action != \"mode\":\n                icon = action if action != \"extract\" else \"folder\"\n                wgt = ttk.Button(frame, image=icons[icon], command=self._btn_action[action])\n                wgt.state(state)\n            else:\n                wgt = self._add_filter_section(frame)\n            wgt.pack(side=side, padx=padx)\n            if action != \"mode\":\n                Tooltip(wgt, text=self._helptext[action])\n            buttons[action] = wgt\n        logger.debug(\"Transport buttons: %s\", buttons)\n        return buttons\n\n    def _add_transport_tk_trace(self):\n        \"\"\" Add the tkinter variable traces to buttons \"\"\"\n        self._navigation.tk_is_playing.trace(\"w\", self._play)\n        self._det_faces.tk_unsaved.trace(\"w\", self._toggle_save_state)\n\n    def _add_filter_section(self, frame):\n        \"\"\" Add the section that holds the filter mode combo and any optional filter widgets\n\n        Parameters\n        ----------\n        frame: :class:`tkinter.ttk.Frame`\n            The Frame that holds the filter section\n\n        Returns\n        -------\n        :class:`tkinter.ttk.Frame`\n            The filter section frame\n        \"\"\"\n        filter_frame = ttk.Frame(frame)\n        self._add_filter_mode_combo(filter_frame)\n        self._add_filter_threshold_slider(filter_frame)\n        filter_frame.pack(side=tk.RIGHT)\n        return filter_frame\n\n    def _add_filter_mode_combo(self, frame):\n        \"\"\" Add the navigation mode combo box to the filter frame.\n\n        Parameters\n        ----------\n        frame: :class:`tkinter.ttk.Frame`\n            The Filter Frame that holds the filter combo box\n        \"\"\"\n        self._globals.tk_filter_mode.set(\"All Frames\")\n        self._globals.tk_filter_mode.trace(\"w\", self._navigation.nav_scale_callback)\n        nav_frame = ttk.Frame(frame)\n        lbl = ttk.Label(nav_frame, text=\"Filter:\")\n        lbl.pack(side=tk.LEFT, padx=(0, 5))\n        combo = ttk.Combobox(\n            nav_frame,\n            textvariable=self._globals.tk_filter_mode,\n            state=\"readonly\",\n            values=self._filter_modes)\n        combo.pack(side=tk.RIGHT)\n        Tooltip(nav_frame, text=self._helptext[\"mode\"])\n        nav_frame.pack(side=tk.RIGHT)\n\n    def _add_filter_threshold_slider(self, frame):\n        \"\"\" Add the optional filter threshold slider for misaligned filter to the filter frame.\n\n        Parameters\n        ----------\n        frame: :class:`tkinter.ttk.Frame`\n            The Filter Frame that holds the filter threshold slider\n        \"\"\"\n        slider_frame = ttk.Frame(frame)\n        tk_var = self._globals.tk_filter_distance\n\n        min_max = (5, 20)\n        ctl_frame = ttk.Frame(slider_frame)\n        ctl_frame.pack(padx=2, side=tk.RIGHT)\n\n        lbl = ttk.Label(ctl_frame, text=\"Distance:\", anchor=tk.W)\n        lbl.pack(side=tk.LEFT, anchor=tk.N, expand=True)\n\n        tbox = ttk.Entry(ctl_frame, width=6, textvariable=tk_var, justify=tk.RIGHT)\n        tbox.pack(padx=(0, 5), side=tk.RIGHT)\n\n        ctl = ttk.Scale(\n            ctl_frame,\n            variable=tk_var,\n            command=lambda val, var=tk_var, dt=int, rn=1, mm=min_max:\n            set_slider_rounding(val, var, dt, rn, mm))\n        ctl[\"from_\"] = min_max[0]\n        ctl[\"to\"] = min_max[1]\n        ctl.pack(padx=5, fill=tk.X, expand=True)\n        for item in (tbox, ctl):\n            Tooltip(item,\n                    text=self._helptext[\"distance\"],\n                    wrap_length=200)\n        tk_var.trace(\"w\", self._navigation.nav_scale_callback)\n        self._optional_widgets[\"distance_slider\"] = slider_frame\n\n    def pack_threshold_slider(self):\n        \"\"\" Display or hide the threshold slider depending on the current filter mode. For\n        misaligned faces filter, display the slider. Hide for all other filters. \"\"\"\n        if self._globals.tk_filter_mode.get() == \"Misaligned Faces\":\n            self._optional_widgets[\"distance_slider\"].pack(side=tk.LEFT)\n        else:\n            self._optional_widgets[\"distance_slider\"].pack_forget()\n\n    def cycle_filter_mode(self):\n        \"\"\" Cycle the navigation mode combo entry \"\"\"\n        current_mode = self._globals.filter_mode\n        idx = (self._filter_modes.index(current_mode) + 1) % len(self._filter_modes)\n        self._globals.tk_filter_mode.set(self._filter_modes[idx])\n\n    def set_action(self, key):\n        \"\"\" Set the current action based on keyboard shortcut\n\n        Parameters\n        ----------\n        key: str\n            The pressed key\n        \"\"\"\n        # Allow key pad keys for numeric presses\n        key = key.replace(\"KP_\", \"\") if key.startswith(\"KP_\") else key\n        self._actions_frame.on_click(self._actions_frame.key_bindings[key])\n\n    def _resize(self, event):\n        \"\"\"  Resize the image to fit the frame, maintaining aspect ratio \"\"\"\n        framesize = (event.width, event.height)\n        logger.trace(\"Resizing video frame. Framesize: %s\", framesize)\n        self._globals.set_frame_display_dims(*framesize)\n        self._globals.tk_update.set(True)\n\n    # << TRANSPORT >> #\n    def _play(self, *args, frame_count=None):  # pylint:disable=unused-argument\n        \"\"\" Play the video file. \"\"\"\n        start = time()\n        is_playing = self._navigation.tk_is_playing.get()\n        icon = \"pause\" if is_playing else \"play\"\n        self._buttons[\"play\"].config(image=get_images().icons[icon])\n\n        if not is_playing:\n            logger.debug(\"Pause detected. Stopping.\")\n            return\n\n        # Populate the filtered frames count on first frame\n        frame_count = self._det_faces.filter.count if frame_count is None else frame_count\n        self._navigation.increment_frame(frame_count=frame_count, is_playing=True)\n        delay = 16  # Cap speed at approx 60fps max. Unlikely to hit, but just in case\n        duration = int((time() - start) * 1000)\n        delay = max(1, delay - duration)\n        self.after(delay, lambda f=frame_count: self._play(f))\n\n    def _toggle_save_state(self, *args):  # pylint:disable=unused-argument\n        \"\"\" Toggle the state of the save button when alignments are updated. \"\"\"\n        state = [\"!disabled\"] if self._det_faces.tk_unsaved.get() else [\"disabled\"]\n        self._buttons[\"save\"].state(state)\n\n\nclass ActionsFrame(ttk.Frame):  # pylint:disable=too-many-ancestors\n    \"\"\" The left hand action frame holding the action buttons.\n\n    Parameters\n    ----------\n    parent: :class:`DisplayFrame`\n        The Display frame that the Actions reside in\n    \"\"\"\n    def __init__(self, parent):\n        super().__init__(parent)\n        self.pack(side=tk.LEFT, fill=tk.Y, padx=(2, 4), pady=2)\n        self._globals = parent._globals\n        self._det_faces = parent._det_faces\n\n        self._configure_styles()\n        self._actions = (\"View\", \"BoundingBox\", \"ExtractBox\", \"Landmarks\", \"Mask\")\n        self._initial_action = \"View\"\n        self._buttons = self._add_buttons()\n        self._static_buttons = self._add_static_buttons()\n        self._selected_action = self._set_selected_action_tkvar()\n        self._optional_buttons = {}  # Has to be set from parent after canvas is initialized\n\n    @property\n    def actions(self):\n        \"\"\" tuple: The available action names as a tuple of strings. \"\"\"\n        return self._actions\n\n    @property\n    def tk_selected_action(self):\n        \"\"\" :class:`tkinter.StringVar`: The variable holding the currently selected action \"\"\"\n        return self._selected_action\n\n    @property\n    def key_bindings(self):\n        \"\"\" dict: {`key`: `action`}. The mapping of key presses to actions. Keyboard shortcut is\n        the first letter of each action. \"\"\"\n        return {f\"F{idx + 1}\": action for idx, action in enumerate(self._actions)}\n\n    @property\n    def _helptext(self):\n        \"\"\" dict: `button key`: `button helptext`. The help text to display for each button. \"\"\"\n        inverse_keybindings = {val: key for key, val in self.key_bindings.items()}\n        retval = {\"View\": _('View alignments'),\n                  \"BoundingBox\": _('Bounding box editor'),\n                  \"ExtractBox\": _(\"Location editor\"),\n                  \"Mask\": _(\"Mask editor\"),\n                  \"Landmarks\": _(\"Landmark point editor\")}\n        for item in retval:\n            retval[item] += f\" ({inverse_keybindings[item]})\"\n        return retval\n\n    def _configure_styles(self):\n        \"\"\" Configure background color for Actions widget \"\"\"\n        style = ttk.Style()\n        style.configure(\"actions.TFrame\", background='#d3d3d3')\n        style.configure(\"actions_selected.TButton\", relief=\"flat\", background=\"#bedaf1\")\n        style.configure(\"actions_deselected.TButton\", relief=\"flat\")\n        self.config(style=\"actions.TFrame\")\n\n    def _add_buttons(self):\n        \"\"\" Add the action buttons to the Display window.\n\n        Returns\n        -------\n        dict:\n            The action name and its associated button.\n        \"\"\"\n        frame = ttk.Frame(self)\n        frame.pack(side=tk.TOP, fill=tk.Y)\n        buttons = {}\n        for action in self.key_bindings.values():\n            if action == self._initial_action:\n                btn_style = \"actions_selected.TButton\"\n                state = ([\"pressed\", \"focus\"])\n            else:\n                btn_style = \"actions_deselected.TButton\"\n                state = ([\"!pressed\", \"!focus\"])\n\n            button = ttk.Button(frame,\n                                image=get_images().icons[action.lower()],\n                                command=lambda t=action: self.on_click(t),\n                                style=btn_style)\n            button.state(state)\n            button.pack()\n            Tooltip(button, text=self._helptext[action])\n            buttons[action] = button\n        return buttons\n\n    def on_click(self, action):\n        \"\"\" Click event for all of the main buttons.\n\n        Parameters\n        ----------\n        action: str\n            The action name for the button that has called this event as exists in :attr:`_buttons`\n        \"\"\"\n        for title, button in self._buttons.items():\n            if action == title:\n                button.configure(style=\"actions_selected.TButton\")\n                button.state([\"pressed\", \"focus\"])\n            else:\n                button.configure(style=\"actions_deselected.TButton\")\n                button.state([\"!pressed\", \"!focus\"])\n        self._selected_action.set(action)\n\n    def _set_selected_action_tkvar(self):\n        \"\"\" Set the tkinter string variable that holds the currently selected editor action.\n        Add traceback to display or hide editor specific optional buttons.\n\n        Returns\n        -------\n        :class:`tkinter.StringVar\n            The variable that holds the currently selected action\n        \"\"\"\n        var = tk.StringVar()\n        var.set(self._initial_action)\n        var.trace(\"w\", self._display_optional_buttons)\n        return var\n\n    def _add_static_buttons(self):\n        \"\"\" Add the buttons to copy alignments from previous and next frames \"\"\"\n        lookup = {\"copy_prev\": (_(\"Previous\"), \"C\"),\n                  \"copy_next\": (_(\"Next\"), \"V\"),\n                  \"reload\": (\"\", \"R\")}\n        frame = ttk.Frame(self)\n        frame.pack(side=tk.TOP, fill=tk.Y)\n        sep = ttk.Frame(frame, height=2, relief=tk.RIDGE)\n        sep.pack(fill=tk.X, pady=5, side=tk.TOP)\n        buttons = {}\n        tk_frame_index = self._globals.tk_frame_index\n        for action in (\"copy_prev\", \"copy_next\", \"reload\"):\n            if action == \"reload\":\n                icon = \"reload3\"\n                cmd = lambda f=tk_frame_index: self._det_faces.revert_to_saved(f.get())  # noqa:E731  # pylint:disable=line-too-long,unnecessary-lambda-assignment\n                helptext = _(\"Revert to saved Alignments ({})\").format(lookup[action][1])\n            else:\n                icon = action\n                direction = action.replace(\"copy_\", \"\")\n                cmd = lambda f=tk_frame_index, d=direction: self._det_faces.update.copy(  # noqa:E731  # pylint:disable=line-too-long,unnecessary-lambda-assignment\n                    f.get(), d)\n                helptext = _(\"Copy {} Alignments ({})\").format(*lookup[action])\n            state = [\"!disabled\"] if action == \"copy_next\" else [\"disabled\"]\n            button = ttk.Button(frame,\n                                image=get_images().icons[icon],\n                                command=cmd,\n                                style=\"actions_deselected.TButton\")\n            button.state(state)\n            button.pack()\n            Tooltip(button, text=helptext)\n            buttons[action] = button\n        self._globals.tk_frame_index.trace(\"w\", self._disable_enable_copy_buttons)\n        self._globals.tk_update.trace(\"w\", self._disable_enable_reload_button)\n        return buttons\n\n    def _disable_enable_copy_buttons(self, *args):  # pylint:disable=unused-argument\n        \"\"\" Disable or enable the static buttons \"\"\"\n        position = self._globals.frame_index\n        face_count_per_index = self._det_faces.face_count_per_index\n        prev_exists = position != -1 and any(count != 0\n                                             for count in face_count_per_index[:position])\n        next_exists = position != -1 and any(count != 0\n                                             for count in face_count_per_index[position + 1:])\n        states = {\"prev\": [\"!disabled\"] if prev_exists else [\"disabled\"],\n                  \"next\": [\"!disabled\"] if next_exists else [\"disabled\"]}\n        for direction in (\"prev\", \"next\"):\n            self._static_buttons[f\"copy_{direction}\"].state(states[direction])\n\n    def _disable_enable_reload_button(self, *args):  # pylint:disable=unused-argument\n        \"\"\" Disable or enable the static buttons \"\"\"\n        position = self._globals.frame_index\n        state = [\"!disabled\"] if (position != -1 and\n                                  self._det_faces.is_frame_updated(position)) else [\"disabled\"]\n        self._static_buttons[\"reload\"].state(state)\n\n    def add_optional_buttons(self, editors):\n        \"\"\" Add the optional editor specific action buttons \"\"\"\n        for name, editor in editors.items():\n            actions = editor.actions\n            if not actions:\n                self._optional_buttons[name] = None\n                continue\n            frame = ttk.Frame(self)\n            sep = ttk.Frame(frame, height=2, relief=tk.RIDGE)\n            sep.pack(fill=tk.X, pady=5, side=tk.TOP)\n            seen_groups = set()\n            for action in actions.values():\n                group = action[\"group\"]\n                if group is not None and group not in seen_groups:\n                    btn_style = \"actions_selected.TButton\"\n                    state = ([\"pressed\", \"focus\"])\n                    action[\"tk_var\"].set(True)\n                    seen_groups.add(group)\n                else:\n                    btn_style = \"actions_deselected.TButton\"\n                    state = ([\"!pressed\", \"!focus\"])\n                    action[\"tk_var\"].set(False)\n                button = ttk.Button(frame,\n                                    image=get_images().icons[action[\"icon\"]],\n                                    style=btn_style)\n                button.config(command=lambda b=button: self._on_optional_click(b))\n                button.state(state)\n                button.pack()\n\n                helptext = action[\"helptext\"]\n                hotkey = action[\"hotkey\"]\n                helptext += \"\" if hotkey is None else f\" ({hotkey.upper()})\"\n                Tooltip(button, text=helptext)\n                self._optional_buttons.setdefault(\n                    name, {})[button] = {\"hotkey\": hotkey,\n                                         \"group\": group,\n                                         \"tk_var\": action[\"tk_var\"]}\n            self._optional_buttons[name][\"frame\"] = frame\n        self._display_optional_buttons()\n\n    def _on_optional_click(self, button):\n        \"\"\" Click event for all of the optional buttons.\n\n        Parameters\n        ----------\n        button: str\n            The action name for the button that has called this event as exists in :attr:`_buttons`\n        \"\"\"\n        options = self._optional_buttons[self._selected_action.get()]\n        group = options[button][\"group\"]\n        for child in options[\"frame\"].winfo_children():\n            if child.winfo_class() != \"TButton\":\n                continue\n            child_group = options[child][\"group\"]\n            if child == button and group is not None:\n                child.configure(style=\"actions_selected.TButton\")\n                child.state([\"pressed\", \"focus\"])\n                options[child][\"tk_var\"].set(True)\n            elif child != button and group is not None and child_group == group:\n                child.configure(style=\"actions_deselected.TButton\")\n                child.state([\"!pressed\", \"!focus\"])\n                options[child][\"tk_var\"].set(False)\n            elif group is None and child_group is None:\n                if child.cget(\"style\") == \"actions_selected.TButton\":\n                    child.configure(style=\"actions_deselected.TButton\")\n                    child.state([\"!pressed\", \"!focus\"])\n                    options[child][\"tk_var\"].set(False)\n                else:\n                    child.configure(style=\"actions_selected.TButton\")\n                    child.state([\"pressed\", \"focus\"])\n                    options[child][\"tk_var\"].set(True)\n\n    def _display_optional_buttons(self, *args):  # pylint:disable=unused-argument\n        \"\"\" Pack or forget the optional buttons depending on active editor \"\"\"\n        self._unbind_optional_hotkeys()\n        for editor, option in self._optional_buttons.items():\n            if option is None:\n                continue\n            if editor == self._selected_action.get():\n                logger.debug(\"Displaying optional buttons for '%s'\", editor)\n                option[\"frame\"].pack(side=tk.TOP, fill=tk.Y)\n                for child in option[\"frame\"].winfo_children():\n                    if child.winfo_class() != \"TButton\":\n                        continue\n                    hotkey = option[child][\"hotkey\"]\n                    if hotkey is not None:\n                        logger.debug(\"Binding optional hotkey for editor '%s': %s\", editor, hotkey)\n                        self.winfo_toplevel().bind(hotkey.lower(),\n                                                   lambda e, b=child: self._on_optional_click(b))\n            elif option[\"frame\"].winfo_ismapped():\n                logger.debug(\"Hiding optional buttons for '%s'\", editor)\n                option[\"frame\"].pack_forget()\n\n    def _unbind_optional_hotkeys(self):\n        \"\"\" Unbind all mapped optional button hotkeys \"\"\"\n        for editor, option in self._optional_buttons.items():\n            if option is None or not option[\"frame\"].winfo_ismapped():\n                continue\n            for child in option[\"frame\"].winfo_children():\n                if child.winfo_class() != \"TButton\":\n                    continue\n                hotkey = option[child][\"hotkey\"]\n                if hotkey is not None:\n                    logger.debug(\"Unbinding optional hotkey for editor '%s': %s\", editor, hotkey)\n                    self.winfo_toplevel().unbind(hotkey.lower())\n\n\nclass FrameViewer(tk.Canvas):  # pylint:disable=too-many-ancestors\n    \"\"\" Annotation onto tkInter Canvas.\n\n    Parameters\n    ----------\n    parent: :class:`tkinter.ttk.Frame`\n        The parent frame for the canvas\n    tk_globals: :class:`~tools.manual.manual.TkGlobals`\n        The tkinter variables that apply to the whole of the GUI\n    detected_faces: :class:`AlignmentsData`\n        The alignments data for this manual session\n    actions: tuple\n        The available actions from :attr:`ActionFrame.actions`\n    tk_action_var: :class:`tkinter.StringVar`\n        The variable holding the currently selected action\n    \"\"\"\n    def __init__(self, parent, tk_globals, detected_faces, actions, tk_action_var):\n        logger.debug(\"Initializing %s: (parent: %s, tk_globals: %s, detected_faces: %s, \"\n                     \"actions: %s, tk_action_var: %s)\", self.__class__.__name__,\n                     parent, tk_globals, detected_faces, actions, tk_action_var)\n        super().__init__(parent, bd=0, highlightthickness=0, background=\"black\")\n        self.pack(side=tk.TOP, fill=tk.BOTH, expand=True, anchor=tk.E)\n        self._globals = tk_globals\n        self._det_faces = detected_faces\n        self._actions = actions\n        self._tk_action_var = tk_action_var\n        self._image = BackgroundImage(self)\n        self._editor_globals = {\"control_tk_vars\": {},\n                                \"annotation_formats\": {},\n                                \"key_bindings\": {}}\n        self._max_face_count = 0\n        self._editors = self._get_editors()\n        self._add_callbacks()\n        self._change_active_editor()\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    @property\n    def selected_action(self):\n        \"\"\"str: The name of the currently selected Editor action \"\"\"\n        return self._tk_action_var.get()\n\n    @property\n    def control_tk_vars(self):\n        \"\"\" dict: dictionary of tkinter variables as populated by the right hand control panel.\n        Tracking for all control panel variables, for access from all editors. \"\"\"\n        return self._editor_globals[\"control_tk_vars\"]\n\n    @property\n    def key_bindings(self):\n        \"\"\" dict: dictionary of key bindings for each editor for access from all editors. \"\"\"\n        return self._editor_globals[\"key_bindings\"]\n\n    @property\n    def annotation_formats(self):\n        \"\"\" dict: The selected formatting options for each annotation \"\"\"\n        return self._editor_globals[\"annotation_formats\"]\n\n    @property\n    def active_editor(self):\n        \"\"\" :class:`Editor`: The current editor in use based on :attr:`selected_action`. \"\"\"\n        return self._editors[self.selected_action]\n\n    @property\n    def editors(self):\n        \"\"\" dict: All of the :class:`Editor` objects that exist \"\"\"\n        return self._editors\n\n    @property\n    def editor_display(self):\n        \"\"\" dict: List of editors and any additional annotations they should display. \"\"\"\n        return {\"View\": [\"BoundingBox\", \"ExtractBox\", \"Landmarks\", \"Mesh\"],\n                \"BoundingBox\": [\"Mesh\"],\n                \"ExtractBox\": [\"Mesh\"],\n                \"Landmarks\": [\"ExtractBox\", \"Mesh\"],\n                \"Mask\": []}\n\n    @property\n    def offset(self):\n        \"\"\" tuple: The (`width`, `height`) offset of the canvas based on the size of the currently\n        displayed image \"\"\"\n        frame_dims = self._globals.current_frame[\"display_dims\"]\n        offset_x = (self._globals.frame_display_dims[0] - frame_dims[0]) / 2\n        offset_y = (self._globals.frame_display_dims[1] - frame_dims[1]) / 2\n        logger.trace(\"offset_x: %s, offset_y: %s\", offset_x, offset_y)\n        return offset_x, offset_y\n\n    def _get_editors(self):\n        \"\"\" Get the object editors for the canvas.\n\n        Returns\n        ------\n        dict\n            The {`action`: :class:`Editor`} dictionary of editors for :attr:`_actions` name.\n        \"\"\"\n        editors = {}\n        for editor_name in self._actions + (\"Mesh\", ):\n            editor = eval(editor_name)(self,  # pylint:disable=eval-used\n                                       self._det_faces)\n            editors[editor_name] = editor\n        logger.debug(editors)\n        return editors\n\n    def _add_callbacks(self):\n        \"\"\" Add the callback trace functions to the :class:`tkinter.Variable` s\n\n        Adds callbacks for:\n            :attr:`_globals.tk_update` Update the display for the current image\n            :attr:`__tk_action_var` Update the mouse display tracking for current action\n        \"\"\"\n        self._globals.tk_update.trace(\"w\", self._update_display)\n        self._tk_action_var.trace(\"w\", self._change_active_editor)\n\n    def _change_active_editor(self, *args):  # pylint:disable=unused-argument\n        \"\"\" Update the display for the active editor.\n\n        Hide the annotations that are not relevant for the selected editor.\n        Set the selected editor's cursor tracking.\n\n        Parameters\n        ----------\n        args: tuple, unused\n            Required for tkinter callback but unused\n        \"\"\"\n        to_display = [self.selected_action] + self.editor_display[self.selected_action]\n        to_hide = [editor for editor in self._editors if editor not in to_display]\n        for editor in to_hide:\n            self._editors[editor].hide_annotation()\n\n        self.active_editor.bind_mouse_motion()\n        self.active_editor.set_mouse_click_actions()\n        self._globals.tk_update.set(True)\n\n    def _update_display(self, *args):  # pylint:disable=unused-argument\n        \"\"\" Update the display on frame cache update\n\n        Notes\n        -----\n        A little hacky, but the editors to display or hide are processed in alphabetical\n        order, so that they are always processed in the same order (for tag lowering and raising)\n        \"\"\"\n        if not self._globals.tk_update.get():\n            return\n        zoomed_centering = self.active_editor.zoomed_centering\n        self._image.refresh(self.active_editor.view_mode)\n        to_display = sorted([self.selected_action] + self.editor_display[self.selected_action])\n        self._hide_additional_faces()\n        for editor in to_display:\n            self._editors[editor].update_annotation()\n        self._bind_unbind_keys()\n        if zoomed_centering != self.active_editor.zoomed_centering:\n            # Refresh the image if editor annotation has changed the zoom centering of the image\n            self._image.refresh(self.active_editor.view_mode)\n        self._globals.tk_update.set(False)\n        self.update_idletasks()\n\n    def _hide_additional_faces(self):\n        \"\"\" Hide additional faces if the number of faces on the canvas reduces on a frame\n        change. \"\"\"\n        if self._globals.is_zoomed:\n            current_face_count = 1\n        elif self._globals.frame_index == -1:\n            current_face_count = 0\n        else:\n            current_face_count = len(self._det_faces.current_faces[self._globals.frame_index])\n\n        if current_face_count > self._max_face_count:\n            # Most faces seen to date so nothing to hide. Update max count and return\n            logger.debug(\"Incrementing max face count from: %s to: %s\",\n                         self._max_face_count, current_face_count)\n            self._max_face_count = current_face_count\n            return\n        for idx in range(current_face_count, self._max_face_count):\n            tag = f\"face_{idx}\"\n            if any(self.itemcget(item_id, \"state\") != \"hidden\"\n                   for item_id in self.find_withtag(tag)):\n                logger.debug(\"Hiding face tag '%s'\", tag)\n                self.itemconfig(tag, state=\"hidden\")\n\n    def _bind_unbind_keys(self):\n        \"\"\" Bind or unbind this editor's hotkeys depending on whether it is active. \"\"\"\n        unbind_keys = [key for key, binding in self.key_bindings.items()\n                       if binding[\"bound_to\"] is not None\n                       and binding[\"bound_to\"] != self.selected_action]\n        for key in unbind_keys:\n            logger.debug(\"Unbinding key '%s'\", key)\n            self.winfo_toplevel().unbind(key)\n            self.key_bindings[key][\"bound_to\"] = None\n\n        bind_keys = {key: binding[self.selected_action]\n                     for key, binding in self.key_bindings.items()\n                     if self.selected_action in binding\n                     and binding[\"bound_to\"] != self.selected_action}\n        for key, method in bind_keys.items():\n            logger.debug(\"Binding key '%s' to method %s\", key, method)\n            self.winfo_toplevel().bind(key, method)\n            self.key_bindings[key][\"bound_to\"] = self.selected_action\n", "tools/manual/frameviewer/editor/extract_box.py": "#!/usr/bin/env python3\n\"\"\" Extract Box Editor for the manual adjustments tool \"\"\"\nimport gettext\nimport platform\n\nimport numpy as np\n\nfrom lib.align import AlignedFace\nfrom lib.gui.custom_widgets import RightClickMenu\nfrom lib.gui.utils import get_config\nfrom ._base import Editor, logger\n\n\n# LOCALES\n_LANG = gettext.translation(\"tools.manual\", localedir=\"locales\", fallback=True)\n_ = _LANG.gettext\n\n\nclass ExtractBox(Editor):\n    \"\"\" The Extract Box Editor.\n\n    Adjust the calculated Extract Box to shift all of the 68 point landmarks in place.\n\n    Parameters\n    ----------\n    canvas: :class:`tkinter.Canvas`\n        The canvas that holds the image and annotations\n    detected_faces: :class:`~tools.manual.detected_faces.DetectedFaces`\n        The _detected_faces data for this manual session\n    \"\"\"\n    def __init__(self, canvas, detected_faces):\n        self._right_click_menu = RightClickMenu([_(\"Delete Face\")],\n                                                [self._delete_current_face],\n                                                [\"Del\"])\n        control_text = _(\"Extract Box Editor\\nMove the extract box that has been generated by the \"\n                         \"aligner. Click and drag:\\n\\n\"\n                         \" - Inside the bounding box to relocate the landmarks.\\n\"\n                         \" - The corner anchors to resize the landmarks.\\n\"\n                         \" - Outside of the corners to rotate the landmarks.\")\n        key_bindings = {\"<Delete>\": self._delete_current_face}\n        super().__init__(canvas, detected_faces,\n                         control_text=control_text, key_bindings=key_bindings)\n\n    @property\n    def _corner_order(self):\n        \"\"\" dict: The position index of bounding box corners \"\"\"\n        return {0: (\"top\", \"left\"),\n                3: (\"top\", \"right\"),\n                2: (\"bottom\", \"right\"),\n                1: (\"bottom\", \"left\")}\n\n    def update_annotation(self):\n        \"\"\" Draw the latest Extract Boxes around the faces. \"\"\"\n        color = self._control_color\n        roi = self._zoomed_roi\n        for idx, face in enumerate(self._face_iterator):\n            logger.trace(\"Drawing Extract Box: (idx: %s)\", idx)\n            if self._globals.is_zoomed:\n                box = np.array((roi[0], roi[1], roi[2], roi[1], roi[2], roi[3], roi[0], roi[3]))\n            else:\n                aligned = AlignedFace(face.landmarks_xy, centering=\"face\")\n                box = self._scale_to_display(aligned.original_roi).flatten()\n            top_left = box[:2] - 10\n            kwargs = dict(fill=color, font=(\"Default\", 20, \"bold\"), text=str(idx))\n            self._object_tracker(\"eb_text\", \"text\", idx, top_left, kwargs)\n            kwargs = dict(fill=\"\", outline=color, width=1)\n            self._object_tracker(\"eb_box\", \"polygon\", idx, box, kwargs)\n            self._update_anchor_annotation(idx, box, color)\n        logger.trace(\"Updated extract box annotations\")\n\n    def _update_anchor_annotation(self, face_index, extract_box, color):\n        \"\"\" Update the anchor annotations for each corner of the extract box.\n\n        The anchors only display when the extract box editor is active.\n\n        Parameters\n        ----------\n        face_index: int\n            The index of the face being annotated\n        extract_box: :class:`numpy.ndarray`\n            The scaled extract box to get the corner anchors for\n        color: str\n            The hex color of the extract box line\n        \"\"\"\n        if not self._is_active or self._globals.is_zoomed:\n            self.hide_annotation(\"eb_anc_dsp\")\n            self.hide_annotation(\"eb_anc_grb\")\n            return\n        fill_color = \"gray\"\n        activefill_color = \"white\" if self._is_active else \"\"\n        anchor_points = self._get_anchor_points((extract_box[:2],\n                                                 extract_box[2:4],\n                                                 extract_box[4:6],\n                                                 extract_box[6:]))\n        for idx, (anc_dsp, anc_grb) in enumerate(zip(*anchor_points)):\n            dsp_kwargs = dict(outline=color, fill=fill_color, width=1)\n            grb_kwargs = dict(outline=\"\", fill=\"\", width=1, activefill=activefill_color)\n            dsp_key = \"eb_anc_dsp_{}\".format(idx)\n            grb_key = \"eb_anc_grb_{}\".format(idx)\n            self._object_tracker(dsp_key, \"oval\", face_index, anc_dsp, dsp_kwargs)\n            self._object_tracker(grb_key, \"oval\", face_index, anc_grb, grb_kwargs)\n        logger.trace(\"Updated extract box anchor annotations\")\n\n    # << MOUSE HANDLING >>\n    # Mouse cursor display\n    def _update_cursor(self, event):\n        \"\"\" Update the cursor when it is hovering over an extract box and update\n        :attr:`_mouse_location` with the current cursor position.\n\n        Parameters\n        ----------\n        event: :class:`tkinter.Event`\n            The current tkinter mouse event\n        \"\"\"\n        if self._check_cursor_anchors():\n            return\n        if self._check_cursor_box():\n            return\n        if self._check_cursor_rotate(event):\n            return\n        self._canvas.config(cursor=\"\")\n        self._mouse_location = None\n\n    def _check_cursor_anchors(self):\n        \"\"\" Check whether the cursor is over a corner anchor.\n\n        If it is, set the appropriate cursor type and set :attr:`_mouse_location` to\n        (\"anchor\", `face index`, `corner_index`)\n\n        Returns\n        -------\n        bool\n            ``True`` if cursor is over an anchor point otherwise ``False``\n        \"\"\"\n        anchors = set(self._canvas.find_withtag(\"eb_anc_grb\"))\n        item_ids = set(self._canvas.find_withtag(\"current\")).intersection(anchors)\n        if not item_ids:\n            return False\n        item_id = list(item_ids)[0]\n        tags = self._canvas.gettags(item_id)\n        face_idx = int(next(tag for tag in tags if tag.startswith(\"face_\")).split(\"_\")[-1])\n        corner_idx = int(next(tag for tag in tags\n                              if tag.startswith(\"eb_anc_grb_\")\n                              and \"face_\" not in tag).split(\"_\")[-1])\n\n        self._canvas.config(cursor=\"{}_{}_corner\".format(*self._corner_order[corner_idx]))\n        self._mouse_location = (\"anchor\", face_idx, corner_idx)\n        return True\n\n    def _check_cursor_box(self):\n        \"\"\" Check whether the cursor is inside an extract box.\n\n        If it is, set the appropriate cursor type and set :attr:`_mouse_location` to\n        (\"box\", `face index`)\n\n        Returns\n        -------\n        bool\n            ``True`` if cursor is over a rotate point otherwise ``False``\n        \"\"\"\n        extract_boxes = set(self._canvas.find_withtag(\"eb_box\"))\n        item_ids = set(self._canvas.find_withtag(\"current\")).intersection(extract_boxes)\n        if not item_ids:\n            return False\n        item_id = list(item_ids)[0]\n        self._canvas.config(cursor=\"fleur\")\n        self._mouse_location = (\"box\", next(int(tag.split(\"_\")[-1])\n                                            for tag in self._canvas.gettags(item_id)\n                                            if tag.startswith(\"face_\")))\n        return True\n\n    def _check_cursor_rotate(self, event):\n        \"\"\" Check whether the cursor is in an area to rotate the extract box.\n\n        If it is, set the appropriate cursor type and set :attr:`_mouse_location` to\n        (\"rotate\", `face index`)\n\n        Notes\n        -----\n        This code is executed after the check has been completed to see if the mouse is inside\n        the extract box. For this reason, we don't bother running a check to see if the mouse\n        is inside the box, as this code will never run if that is the case.\n\n        Parameters\n        ----------\n        event: :class:`tkinter.Event`\n            The current tkinter mouse event\n\n        Returns\n        -------\n        bool\n            ``True`` if cursor is over a rotate point otherwise ``False``\n        \"\"\"\n        distance = 30\n        boxes = np.array([np.array(self._canvas.coords(item_id)).reshape(4, 2)\n                          for item_id in self._canvas.find_withtag(\"eb_box\")\n                          if self._canvas.itemcget(item_id, \"state\") != \"hidden\"])\n        position = np.array((event.x, event.y)).astype(\"float32\")\n        for face_idx, points in enumerate(boxes):\n            if any(np.all(position > point - distance) and np.all(position < point + distance)\n                   for point in points):\n                self._canvas.config(cursor=\"exchange\")\n                self._mouse_location = (\"rotate\", face_idx)\n                return True\n        return False\n\n    # Mouse click actions\n    def set_mouse_click_actions(self):\n        \"\"\" Add context menu to OS specific right click action. \"\"\"\n        super().set_mouse_click_actions()\n        self._canvas.bind(\"<Button-2>\" if platform.system() == \"Darwin\" else \"<Button-3>\",\n                          self._context_menu)\n\n    def _drag_start(self, event):\n        \"\"\" The action to perform when the user starts clicking and dragging the mouse.\n\n        Selects the correct extract box action based on the initial cursor position.\n\n        Parameters\n        ----------\n        event: :class:`tkinter.Event`\n            The tkinter mouse event.\n        \"\"\"\n        if self._mouse_location is None:\n            self._drag_data = dict()\n            self._drag_callback = None\n            return\n        self._drag_data[\"current_location\"] = np.array((event.x, event.y))\n        callback = dict(anchor=self._resize, rotate=self._rotate, box=self._move)\n        self._drag_callback = callback[self._mouse_location[0]]\n\n    def _drag_stop(self, event):  # pylint:disable=unused-argument\n        \"\"\" Trigger a viewport thumbnail update on click + drag release\n\n        Parameters\n        ----------\n        event: :class:`tkinter.Event`\n            The tkinter mouse event. Required but unused.\n        \"\"\"\n        if self._mouse_location is None:\n            return\n        self._det_faces.update.post_edit_trigger(self._globals.frame_index,\n                                                 self._mouse_location[1])\n\n    def _move(self, event):\n        \"\"\" Updates the underlying detected faces landmarks based on mouse dragging delta,\n        which moves the Extract box on a drag event.\n\n        Parameters\n        ----------\n        event: :class:`tkinter.Event`\n            The tkinter mouse event.\n        \"\"\"\n        if not self._drag_data:\n            return\n        shift_x = event.x - self._drag_data[\"current_location\"][0]\n        shift_y = event.y - self._drag_data[\"current_location\"][1]\n        scaled_shift = self.scale_from_display(np.array((shift_x, shift_y)), do_offset=False)\n        self._det_faces.update.landmarks(self._globals.frame_index,\n                                         self._mouse_location[1],\n                                         *scaled_shift)\n        self._drag_data[\"current_location\"] = (event.x, event.y)\n\n    def _resize(self, event):\n        \"\"\" Resizes the landmarks contained within an extract box on a corner anchor drag event.\n\n        Parameters\n        ----------\n        event: :class:`tkinter.Event`\n            The tkinter mouse event.\n        \"\"\"\n        face_idx = self._mouse_location[1]\n        face_tag = \"eb_box_face_{}\".format(face_idx)\n        position = np.array((event.x, event.y))\n        box = np.array(self._canvas.coords(face_tag))\n        center = np.array((sum(box[0::2]) / 4, sum(box[1::2]) / 4))\n        if not self._check_in_bounds(center, box, position):\n            logger.trace(\"Drag out of bounds. Not updating\")\n            self._drag_data[\"current_location\"] = position\n            return\n\n        start = self._drag_data[\"current_location\"]\n        distance = ((np.linalg.norm(center - start) - np.linalg.norm(center - position))\n                    * get_config().scaling_factor)\n        size = ((box[2] - box[0]) ** 2 + (box[3] - box[1]) ** 2) ** 0.5\n        scale = 1 - (distance / size)\n        logger.trace(\"face_index: %s, center: %s, start: %s, position: %s, distance: %s, \"\n                     \"size: %s, scale: %s\", face_idx, center, start, position, distance, size,\n                     scale)\n        if size * scale < 20:\n            # Don't over shrink the box\n            logger.trace(\"Box would size to less than 20px. Not updating\")\n            self._drag_data[\"current_location\"] = position\n            return\n\n        self._det_faces.update.landmarks_scale(self._globals.frame_index,\n                                               face_idx,\n                                               scale,\n                                               self.scale_from_display(center))\n        self._drag_data[\"current_location\"] = position\n\n    def _check_in_bounds(self, center, box, position):\n        \"\"\" Ensure that a resize drag does is not going to cross the center point from it's initial\n        corner location.\n\n        Parameters\n        ----------\n        center: :class:`numpy.ndarray`\n            The (`x`, `y`) center point of the face extract box\n        box: :class:`numpy.ndarray`\n            The canvas coordinates of the extract box polygon's corners\n        position: : class:`numpy.ndarray`\n            The current (`x`, `y`) position of the mouse cursor\n\n        Returns\n        -------\n        bool\n            ``True`` if the drag operation does not cross the center point otherwise ``False``\n        \"\"\"\n        # Generate lines that span the full frame (x and y) along the center point\n        center_x = np.array(((center[0], 0), (center[0], self._globals.frame_display_dims[1])))\n        center_y = np.array(((0, center[1]), (self._globals.frame_display_dims[0], center[1])))\n\n        # Generate a line coming from the current corner location to the current cursor position\n        full_line = np.array((box[self._mouse_location[2] * 2:self._mouse_location[2] * 2 + 2],\n                              position))\n        logger.trace(\"center: %s, center_x_line: %s, center_y_line: %s, full_line: %s\",\n                     center, center_x, center_y, full_line)\n\n        # Check whether any of the generated lines intersect\n        for line in (center_x, center_y):\n            if (self._is_ccw(full_line[0], *line) != self._is_ccw(full_line[1], *line) and\n                    self._is_ccw(*full_line, line[0]) != self._is_ccw(*full_line, line[1])):\n                logger.trace(\"line: %s crosses center: %s\", full_line, center)\n                return False\n        return True\n\n    @staticmethod\n    def _is_ccw(point_a, point_b, point_c):\n        \"\"\" Check whether 3 points are counter clockwise from each other.\n\n        Parameters\n        ----------\n        point_a: :class:`numpy.ndarray`\n            The first (`x`, `y`) point to check for counter clockwise ordering\n        point_b: :class:`numpy.ndarray`\n            The second (`x`, `y`) point to check for counter clockwise ordering\n        point_c: :class:`numpy.ndarray`\n            The third (`x`, `y`) point to check for counter clockwise ordering\n\n        Returns\n        -------\n        bool\n            ``True`` if the 3 points are provided in counter clockwise order otherwise ``False``\n        \"\"\"\n        return ((point_c[1] - point_a[1]) * (point_b[0] - point_a[0]) >\n                (point_b[1] - point_a[1]) * (point_c[0] - point_a[0]))\n\n    def _rotate(self, event):\n        \"\"\" Rotates the landmarks contained within an extract box on a corner rotate drag event.\n\n        Parameters\n        ----------\n        event: :class:`tkinter.Event`\n            The tkinter mouse event.\n        \"\"\"\n        face_idx = self._mouse_location[1]\n        face_tag = \"eb_box_face_{}\".format(face_idx)\n        box = np.array(self._canvas.coords(face_tag))\n        position = np.array((event.x, event.y))\n\n        center = np.array((sum(box[0::2]) / 4, sum(box[1::2]) / 4))\n        init_to_center = self._drag_data[\"current_location\"] - center\n        new_to_center = position - center\n        angle = np.rad2deg(np.arctan2(*new_to_center) - np.arctan2(*init_to_center))\n        logger.trace(\"face_index: %s, box: %s, center: %s, init_to_center: %s, new_to_center: %s\"\n                     \"center: %s, angle: %s\", face_idx, box, center, init_to_center, new_to_center,\n                     center, angle)\n\n        self._det_faces.update.landmarks_rotate(self._globals.frame_index,\n                                                face_idx,\n                                                angle,\n                                                self.scale_from_display(center))\n        self._drag_data[\"current_location\"] = position\n\n    def _get_scale(self):\n        \"\"\" Obtain the scaling for the extract box resize \"\"\"\n\n    def _context_menu(self, event):\n        \"\"\" Create a right click context menu to delete the alignment that is being\n        hovered over. \"\"\"\n        if self._mouse_location is None or self._mouse_location[0] != \"box\":\n            return\n        self._right_click_menu.popup(event)\n\n    def _delete_current_face(self, *args):  # pylint:disable=unused-argument\n        \"\"\" Called by the right click delete event. Deletes the face that the mouse is currently\n        over.\n\n        Parameters\n        ----------\n        args: tuple (unused)\n            The event parameter is passed in by the hot key binding, so args is required\n        \"\"\"\n        if self._mouse_location is None or self._mouse_location[0] != \"box\":\n            return\n        self._det_faces.update.delete(self._globals.frame_index, self._mouse_location[1])\n", "tools/manual/frameviewer/editor/mask.py": "#!/usr/bin/env python3\n\"\"\" Mask Editor for the manual adjustments tool \"\"\"\nimport gettext\nimport tkinter as tk\n\nimport numpy as np\nimport cv2\nfrom PIL import Image, ImageTk\n\nfrom ._base import ControlPanelOption, Editor, logger\n\n# LOCALES\n_LANG = gettext.translation(\"tools.manual\", localedir=\"locales\", fallback=True)\n_ = _LANG.gettext\n\n\nclass Mask(Editor):\n    \"\"\" The mask Editor.\n\n    Edit a mask in the alignments file.\n\n    Parameters\n    ----------\n    canvas: :class:`tkinter.Canvas`\n        The canvas that holds the image and annotations\n    detected_faces: :class:`~tools.manual.detected_faces.DetectedFaces`\n        The _detected_faces data for this manual session\n    \"\"\"\n    def __init__(self, canvas, detected_faces):\n        self._meta = []\n        self._tk_faces = []\n        self._internal_size = 512\n        control_text = _(\"Mask Editor\\nEdit the mask.\"\n                         \"\\n - NB: For Landmark based masks (e.g. components/extended) it is \"\n                         \"better to make sure the landmarks are correct rather than editing the \"\n                         \"mask directly. Any change to the landmarks after editing the mask will \"\n                         \"override your manual edits.\")\n        key_bindings = {\"[\": lambda *e, i=False: self._adjust_brush_radius(increase=i),\n                        \"]\": lambda *e, i=True: self._adjust_brush_radius(increase=i)}\n        super().__init__(canvas, detected_faces,\n                         control_text=control_text, key_bindings=key_bindings)\n        # Bind control click for reverse painting\n        self._canvas.bind(\"<Control-ButtonPress-1>\", self._control_click)\n        self._mask_type = self._set_tk_mask_change_callback()\n        self._cursor_shape = self._set_tk_cursor_shape_change_callback()\n        self._mouse_location = [\n            self._get_cursor_shape(), False]\n\n    @property\n    def _opacity(self):\n        \"\"\" float: The mask opacity setting from the control panel from 0.0 - 1.0. \"\"\"\n        annotation = self.__class__.__name__\n        return self._annotation_formats[annotation][\"mask_opacity\"].get() / 100.0\n\n    @property\n    def _brush_radius(self):\n        \"\"\" int: The radius of the brush to use as set in control panel options \"\"\"\n        return self._control_vars[\"brush\"][\"BrushSize\"].get()\n\n    @property\n    def _edit_mode(self):\n        \"\"\" str: The currently selected edit mode based on optional action button.\n        One of \"draw\" or \"erase\" \"\"\"\n        action = [name for name, option in self._actions.items()\n                  if option[\"group\"] == \"paint\" and option[\"tk_var\"].get()]\n        return \"draw\" if not action else action[0]\n\n    @property\n    def _cursor_color(self):\n        \"\"\" str: The hex code for the selected cursor color \"\"\"\n        return self._control_vars[\"brush\"][\"CursorColor\"].get()\n\n    @property\n    def _cursor_shape_name(self):\n        \"\"\" str: The selected cursor shape \"\"\"\n        return self._control_vars[\"display\"][\"CursorShape\"].get()\n\n    def _add_actions(self):\n        \"\"\" Add the optional action buttons to the viewer. Current actions are Draw, Erase\n        and Zoom. \"\"\"\n        self._add_action(\"magnify\", \"zoom\", _(\"Magnify/Demagnify the View\"),\n                         group=None, hotkey=\"M\")\n        self._add_action(\"draw\", \"draw\", _(\"Draw Tool\"), group=\"paint\", hotkey=\"D\")\n        self._add_action(\"erase\", \"erase\", _(\"Erase Tool\"), group=\"paint\", hotkey=\"E\")\n        self._actions[\"magnify\"][\"tk_var\"].trace(\"w\", lambda *e: self._globals.tk_update.set(True))\n\n    def _add_controls(self):\n        \"\"\" Add the mask specific control panel controls.\n\n        Current controls are:\n          - the mask type to edit\n          - the size of brush to use\n          - the cursor display color\n        \"\"\"\n        masks = sorted(msk.title() for msk in list(self._det_faces.available_masks) + [\"None\"])\n        default = masks[0] if len(masks) == 1 else [mask for mask in masks if mask != \"None\"][0]\n        self._add_control(ControlPanelOption(\"Mask type\",\n                                             str,\n                                             group=\"Display\",\n                                             choices=masks,\n                                             default=default,\n                                             is_radio=True,\n                                             helptext=_(\"Select which mask to edit\")))\n        self._add_control(ControlPanelOption(\"Brush Size\",\n                                             int,\n                                             group=\"Brush\",\n                                             min_max=(1, 100),\n                                             default=10,\n                                             rounding=1,\n                                             helptext=_(\"Set the brush size. ([ - decrease, \"\n                                                        \"] - increase)\")))\n        self._add_control(ControlPanelOption(\"Cursor Color\",\n                                             str,\n                                             group=\"Brush\",\n                                             choices=\"colorchooser\",\n                                             default=\"#ffffff\",\n                                             helptext=_(\"Select the brush cursor color.\")))\n        self._add_control(ControlPanelOption(\"Cursor Shape\",\n                                             str,\n                                             group=\"Display\",\n                                             choices=[\"Circle\", \"Rectangle\"],\n                                             default=\"Circle\",\n                                             is_radio=True,\n                                             helptext=_(\"Select a shape for masking cursor.\")))\n\n    def _set_tk_mask_change_callback(self):\n        \"\"\" Add a trace to change the displayed mask on a mask type change. \"\"\"\n        var = self._control_vars[\"display\"][\"MaskType\"]\n        var.trace(\"w\", lambda *e: self._on_mask_type_change())\n        return var.get()\n\n    def _set_tk_cursor_shape_change_callback(self):\n        \"\"\" Add a trace to change the displayed cursor on a cursor shape type change. \"\"\"\n        var = self._control_vars[\"display\"][\"CursorShape\"]\n        var.trace(\"w\", lambda *e: self._on_cursor_shape_change())\n        return var.get()\n\n    def _on_cursor_shape_change(self):\n        self._mouse_location[0] = self._get_cursor_shape()\n\n    def _on_mask_type_change(self):\n        \"\"\" Update the displayed mask on a mask type change \"\"\"\n        mask_type = self._control_vars[\"display\"][\"MaskType\"].get()\n        if mask_type == self._mask_type:\n            return\n        self._meta = dict(position=self._globals.frame_index)\n        self._mask_type = mask_type\n        self._globals.tk_update.set(True)\n\n    def hide_annotation(self, tag=None):\n        \"\"\" Clear the mask :attr:`_meta` dict when hiding the annotation. \"\"\"\n        super().hide_annotation()\n        self._meta = dict()\n\n    def update_annotation(self):\n        \"\"\" Update the mask annotation with the latest mask. \"\"\"\n        position = self._globals.frame_index\n        if position != self._meta.get(\"position\", -1):\n            # Reset meta information when moving to a new frame\n            self._meta = dict(position=position)\n        key = self.__class__.__name__\n        mask_type = self._control_vars[\"display\"][\"MaskType\"].get().lower()\n        color = self._control_color[1:]\n        rgb_color = np.array(tuple(int(color[i:i + 2], 16) for i in (0, 2, 4)))\n        roi_color = self._annotation_formats[\"ExtractBox\"][\"color\"].get()\n        opacity = self._opacity\n        for idx, face in enumerate(self._face_iterator):\n            face_idx = self._globals.face_index if self._globals.is_zoomed else idx\n            mask = face.mask.get(mask_type, None)\n            if mask is None:\n                continue\n            self._set_face_meta_data(mask, face_idx)\n            self._update_mask_image(key.lower(), face_idx, rgb_color, opacity)\n            self._update_roi_box(mask, face_idx, roi_color)\n\n        self._canvas.tag_raise(self._mouse_location[0])  # Always keep brush cursor on top\n        logger.trace(\"Updated mask annotation\")\n\n    def _set_face_meta_data(self, mask, face_index):\n        \"\"\" Set the metadata for the current face if it has changed or is new.\n\n        Parameters\n        ----------\n        mask: :class:`numpy.ndarray`\n            The one channel mask cropped to the ROI\n        face_index: int\n            The index pertaining to the current face\n        \"\"\"\n        masks = self._meta.get(\"mask\", None)\n        if masks is not None and len(masks) - 1 == face_index:\n            logger.trace(\"Meta information already defined for face: %s\", face_index)\n            return\n\n        logger.debug(\"Defining meta information for face: %s\", face_index)\n        scale = self._internal_size / mask.stored_size\n        self._set_full_frame_meta(mask, scale)\n        dims = (self._internal_size, self._internal_size)\n        self._meta.setdefault(\"mask\", []).append(cv2.resize(mask.stored_mask,\n                                                            dims,\n                                                            interpolation=cv2.INTER_CUBIC))\n        if self.zoomed_centering != mask.stored_centering:\n            self.zoomed_centering = mask.stored_centering\n\n    def _set_full_frame_meta(self, mask, mask_scale):\n        \"\"\" Sets the meta information for displaying the mask in full frame mode.\n\n        Parameters\n        ----------\n        mask: :class:`lib.align.Mask`\n            The mask object\n        mask_scale: float\n            The scaling factor from the stored mask size to the internal mask size\n\n        Sets the following parameters to :attr:`_meta`:\n            - roi_mask: the rectangular ROI box from the full frame that contains the original ROI\n            for the full frame mask\n            - top_left: The location that the roi_mask should be placed in the display frame\n            - affine_matrix: The matrix for transposing the mask to a full frame\n            - interpolator: The cv2 interpolation method to use for transposing mask to a\n            full frame\n            - slices: The (`x`, `y`) slice objects required to extract the mask ROI\n            from the full frame\n        \"\"\"\n        frame_dims = self._globals.current_frame[\"display_dims\"]\n        scaled_mask_roi = np.rint(mask.original_roi *\n                                  self._globals.current_frame[\"scale\"]).astype(\"int32\")\n\n        # Scale and clip the ROI to fit within display frame boundaries\n        clipped_roi = scaled_mask_roi.clip(min=(0, 0), max=frame_dims)\n\n        # Obtain min and max points to get ROI as a rectangle\n        min_max = dict(min=clipped_roi.min(axis=0), max=clipped_roi.max(axis=0))\n\n        # Create a bounding box rectangle ROI\n        roi_dims = np.rint((min_max[\"max\"][1] - min_max[\"min\"][1],\n                            min_max[\"max\"][0] - min_max[\"min\"][0])).astype(\"uint16\")\n        roi = dict(mask=np.zeros(roi_dims, dtype=\"uint8\")[..., None],\n                   corners=np.expand_dims(scaled_mask_roi - min_max[\"min\"], axis=0))\n        # Block out areas outside of the actual mask ROI polygon\n        cv2.fillPoly(roi[\"mask\"], roi[\"corners\"], 255)\n        logger.trace(\"Setting Full Frame mask ROI. shape: %s\", roi[\"mask\"].shape)\n\n        # obtain the slices for cropping mask from full frame\n        xy_slices = (slice(int(round(min_max[\"min\"][1])), int(round(min_max[\"max\"][1]))),\n                     slice(int(round(min_max[\"min\"][0])), int(round(min_max[\"max\"][0]))))\n\n        # Adjust affine matrix for internal mask size and display dimensions\n        adjustments = (np.array([[mask_scale, 0., 0.], [0., mask_scale, 0.]]),\n                       np.array([[1 / self._globals.current_frame[\"scale\"], 0., 0.],\n                                 [0., 1 / self._globals.current_frame[\"scale\"], 0.],\n                                 [0., 0., 1.]]))\n        in_matrix = np.dot(adjustments[0],\n                           np.concatenate((mask.affine_matrix, np.array([[0., 0., 1.]]))))\n        affine_matrix = np.dot(in_matrix, adjustments[1])\n\n        # Get the size of the mask roi box in the frame\n        side_sizes = (scaled_mask_roi[1][0] - scaled_mask_roi[0][0],\n                      scaled_mask_roi[1][1] - scaled_mask_roi[0][1])\n        mask_roi_size = (side_sizes[0] ** 2 + side_sizes[1] ** 2) ** 0.5\n\n        self._meta.setdefault(\"roi_mask\", []).append(roi[\"mask\"])\n        self._meta.setdefault(\"affine_matrix\", []).append(affine_matrix)\n        self._meta.setdefault(\"interpolator\", []).append(mask.interpolator)\n        self._meta.setdefault(\"slices\", []).append(xy_slices)\n        self._meta.setdefault(\"top_left\", []).append(min_max[\"min\"] + self._canvas.offset)\n        self._meta.setdefault(\"mask_roi_size\", []).append(mask_roi_size)\n\n    def _update_mask_image(self, key, face_index, rgb_color, opacity):\n        \"\"\" Obtain a mask, overlay over image and add to canvas or update.\n\n        Parameters\n        ----------\n        key: str\n            The base annotation name for creating tags\n        face_index: int\n            The index of the face within the current frame\n        rgb_color: tuple\n            The color that the mask should be displayed as\n        opacity: float\n            The opacity to apply to the mask\n        \"\"\"\n        mask = (self._meta[\"mask\"][face_index] * opacity).astype(\"uint8\")\n        if self._globals.is_zoomed:\n            display_image = self._update_mask_image_zoomed(mask, rgb_color)\n            top_left = self._zoomed_roi[:2]\n            # Hide all masks and only display selected\n            self._canvas.itemconfig(\"Mask\", state=\"hidden\")\n            self._canvas.itemconfig(\"Mask_face_{}\".format(face_index), state=\"normal\")\n        else:\n            display_image = self._update_mask_image_full_frame(mask, rgb_color, face_index)\n            top_left = self._meta[\"top_left\"][face_index]\n\n        if len(self._tk_faces) < face_index + 1:\n            logger.trace(\"Adding new Photo Image for face index: %s\", face_index)\n            self._tk_faces.append(ImageTk.PhotoImage(display_image))\n        elif self._tk_faces[face_index].width() != display_image.width:\n            logger.trace(\"Replacing existing Photo Image on width change for face index: %s\",\n                         face_index)\n            self._tk_faces[face_index] = ImageTk.PhotoImage(display_image)\n        else:\n            logger.trace(\"Updating existing image\")\n            self._tk_faces[face_index].paste(display_image)\n\n        self._object_tracker(key,\n                             \"image\",\n                             face_index,\n                             top_left,\n                             dict(image=self._tk_faces[face_index], anchor=tk.NW))\n\n    def _update_mask_image_zoomed(self, mask, rgb_color):\n        \"\"\" Update the mask image when zoomed in.\n\n        Parameters\n        ----------\n        mask: :class:`numpy.ndarray`\n            The raw mask\n        rgb_color: tuple\n            The rgb color selected for the mask\n\n        Returns\n        -------\n        :class: `PIL.Image`\n            The zoomed mask image formatted for display\n        \"\"\"\n        rgb = np.tile(rgb_color, self._zoomed_dims + (1, )).astype(\"uint8\")\n        mask = cv2.resize(mask,\n                          tuple(reversed(self._zoomed_dims)),\n                          interpolation=cv2.INTER_CUBIC)[..., None]\n        rgba = np.concatenate((rgb, mask), axis=2)\n        return Image.fromarray(rgba)\n\n    def _update_mask_image_full_frame(self, mask, rgb_color, face_index):\n        \"\"\" Update the mask image when in full frame view.\n\n        Parameters\n        ----------\n        mask: :class:`numpy.ndarray`\n            The raw mask\n        rgb_color: tuple\n            The rgb color selected for the mask\n        face_index: int\n            The index of the face being displayed\n\n        Returns\n        -------\n        :class: `PIL.Image`\n            The full frame mask image formatted for display\n        \"\"\"\n        frame_dims = self._globals.current_frame[\"display_dims\"]\n        frame = np.zeros(frame_dims + (1, ), dtype=\"uint8\")\n        interpolator = self._meta[\"interpolator\"][face_index]\n        slices = self._meta[\"slices\"][face_index]\n        mask = cv2.warpAffine(mask,\n                              self._meta[\"affine_matrix\"][face_index],\n                              frame_dims,\n                              frame,\n                              flags=cv2.WARP_INVERSE_MAP | interpolator,\n                              borderMode=cv2.BORDER_CONSTANT)[slices[0], slices[1]]\n        mask = mask[..., None] if mask.ndim == 2 else mask\n        rgb = np.tile(rgb_color, mask.shape).astype(\"uint8\")\n        rgba = np.concatenate((rgb, np.minimum(mask, self._meta[\"roi_mask\"][face_index])), axis=2)\n        return Image.fromarray(rgba)\n\n    def _update_roi_box(self, mask, face_index, color):\n        \"\"\" Update the region of interest box for the current mask.\n\n        mask: :class:`~lib.align.Mask`\n            The current mask object to create an ROI box for\n        face_index: int\n            The index of the face within the current frame\n        color: str\n            The hex color code that the mask should be displayed as\n        \"\"\"\n        if self._globals.is_zoomed:\n            roi = self._zoomed_roi\n            box = np.array((roi[0], roi[1], roi[2], roi[1], roi[2], roi[3], roi[0], roi[3]))\n        else:\n            box = self._scale_to_display(mask.original_roi).flatten()\n        top_left = box[:2] - 10\n        kwargs = dict(fill=color, font=(\"Default\", 20, \"bold\"), text=str(face_index))\n        self._object_tracker(\"mask_text\", \"text\", face_index, top_left, kwargs)\n        kwargs = dict(fill=\"\", outline=color, width=1)\n        self._object_tracker(\"mask_roi\", \"polygon\", face_index, box, kwargs)\n        if self._globals.is_zoomed:\n            # Raise box above zoomed image\n            self._canvas.tag_raise(\"mask_roi_face_{}\".format(face_index))\n\n    # << MOUSE HANDLING >>\n    # Mouse cursor display\n    def _update_cursor(self, event):\n        \"\"\" Set the cursor action.\n\n        Update :attr:`_mouse_location` with the current cursor position and display appropriate\n        icon.\n\n        Checks whether the mouse is over a mask ROI box and pops the paint icon.\n\n        Parameters\n        ----------\n        event: :class:`tkinter.Event`\n            The current tkinter mouse event\n        \"\"\"\n        roi_boxes = self._canvas.find_withtag(\"mask_roi\")\n        item_ids = set(self._canvas.find_withtag(\"current\")).intersection(roi_boxes)\n        if not item_ids:\n            self._canvas.config(cursor=\"\")\n            self._canvas.itemconfig(self._mouse_location[0], state=\"hidden\")\n            self._mouse_location[1] = None\n            return\n        item_id = list(item_ids)[0]\n        tags = self._canvas.gettags(item_id)\n        face_idx = int(next(tag for tag in tags if tag.startswith(\"face_\")).split(\"_\")[-1])\n\n        radius = self._brush_radius\n        coords = (event.x - radius, event.y - radius, event.x + radius, event.y + radius)\n        self._canvas.config(cursor=\"none\")\n        self._canvas.coords(self._mouse_location[0], *coords)\n        self._canvas.itemconfig(self._mouse_location[0],\n                                state=\"normal\",\n                                outline=self._cursor_color)\n        self._mouse_location[1] = face_idx\n        self._canvas.update_idletasks()\n\n    def _control_click(self, event):\n        \"\"\" The action to perform when the user starts clicking and dragging the mouse whilst\n        pressing the control button.\n\n        For editing the mask this will activate the opposite action than what is currently selected\n        (e.g. it will erase if draw is set and it will draw if erase is set)\n\n        Parameters\n        ----------\n        event: :class:`tkinter.Event`\n            The tkinter mouse event.\n        \"\"\"\n        self._drag_start(event, control_click=True)\n\n    def _drag_start(self, event, control_click=False):  # pylint:disable=arguments-differ\n        \"\"\" The action to perform when the user starts clicking and dragging the mouse.\n\n        Paints on the mask with the appropriate draw or erase action.\n\n        Parameters\n        ----------\n        event: :class:`tkinter.Event`\n            The tkinter mouse event.\n        control_click: bool, optional\n            Indicates whether the control button is depressed when drag has commenced. If ``True``\n            then the opposite of the selected action is performed. Default: ``False``\n        \"\"\"\n        face_idx = self._mouse_location[1]\n        if face_idx is None:\n            self._drag_data = dict()\n            self._drag_callback = None\n        else:\n            self._drag_data[\"starting_location\"] = np.array((event.x, event.y))\n            self._drag_data[\"control_click\"] = control_click\n            self._drag_data[\"color\"] = np.array(tuple(int(self._control_color[1:][i:i + 2], 16)\n                                                      for i in (0, 2, 4)))\n            self._drag_data[\"opacity\"] = self._opacity\n            self._get_cursor_shape_mark(\n                self._meta[\"mask\"][face_idx],\n                np.array(((event.x, event.y), )),\n                face_idx)\n            self._drag_callback = self._paint\n\n    def _paint(self, event):\n        \"\"\" Paint or erase from Mask and update cursor on click and drag.\n\n        Parameters\n        ----------\n        event: :class:`tkinter.Event`\n            The tkinter mouse event.\n        \"\"\"\n        face_idx = self._mouse_location[1]\n        line = np.array((self._drag_data[\"starting_location\"], (event.x, event.y)))\n        line, scale = self._transform_points(face_idx, line)\n        brush_radius = int(round(self._brush_radius * scale))\n        color = 0 if self._edit_mode == \"erase\" else 255\n        # Reverse action on control click\n        color = abs(color - 255) if self._drag_data[\"control_click\"] else color\n        cv2.line(self._meta[\"mask\"][face_idx],\n                 tuple(line[0]),\n                 tuple(line[1]),\n                 color,\n                 brush_radius * 2)\n        self._update_mask_image(\"mask\",\n                                face_idx,\n                                self._drag_data[\"color\"],\n                                self._drag_data[\"opacity\"])\n        self._drag_data[\"starting_location\"] = np.array((event.x, event.y))\n        self._update_cursor(event)\n\n    def _transform_points(self, face_index, points):\n        \"\"\" Transform the edit points from a full frame or zoomed view back to the mask.\n\n        Parameters\n        ----------\n        face_index: int\n            The index of the face within the current frame\n        points: :class:`numpy.ndarray`\n            The points that are to be translated from the viewer to the underlying\n            Detected Face\n        \"\"\"\n        if self._globals.is_zoomed:\n            offset = self._zoomed_roi[:2]\n            scale = self._internal_size / self._zoomed_dims[0]\n            t_points = np.rint((points - offset) * scale).astype(\"int32\").squeeze()\n        else:\n            scale = self._internal_size / self._meta[\"mask_roi_size\"][face_index]\n            t_points = np.expand_dims(points - self._canvas.offset, axis=0)\n            t_points = cv2.transform(t_points, self._meta[\"affine_matrix\"][face_index]).squeeze()\n            t_points = np.rint(t_points).astype(\"int32\")\n        logger.trace(\"original points: %s, transformed points: %s, scale: %s\",\n                     points, t_points, scale)\n        return t_points, scale\n\n    def _drag_stop(self, event):\n        \"\"\" The action to perform when the user stops clicking and dragging the mouse.\n\n        If a line hasn't been drawn then draw a circle. Update alignments.\n\n        Parameters\n        ----------\n        event: :class:`tkinter.Event`\n            The tkinter mouse event. Unused but required\n        \"\"\"\n        if not self._drag_data:\n            return\n        face_idx = self._mouse_location[1]\n        location = np.array(((event.x, event.y), ))\n        if np.array_equal(self._drag_data[\"starting_location\"], location[0]):\n            self._get_cursor_shape_mark(self._meta[\"mask\"][face_idx], location, face_idx)\n        self._mask_to_alignments(face_idx)\n        self._drag_data = dict()\n        self._update_cursor(event)\n\n    def _get_cursor_shape_mark(self, img, location, face_idx):\n        \"\"\" Draw object depending on the cursor shape selection. Defaults to circle.\n\n        Parameters\n        ----------\n        img: Image to draw on (mask)\n        location: Cursor location coordinates that will be transformed to correct\n            coordinates\n        face_index: int\n            The index of the face within the current frame\n        \"\"\"\n        points, scale = self._transform_points(face_idx, location)\n        radius = int(round(self._brush_radius * scale))\n        color = 0 if self._edit_mode == \"erase\" else 255\n        # Reverse action on control click\n        color = abs(color - 255) if self._drag_data[\"control_click\"] else color\n\n        if self._cursor_shape_name == \"Rectangle\":\n            point2 = points.copy()\n            points[0] = points[0] - radius\n            points[1] = points[1] - radius\n            point2[0] = point2[0] + radius\n            point2[1] = point2[1] + radius\n            cv2.rectangle(img, tuple(points), tuple(point2), color, -1)\n        else:\n            cv2.circle(img, tuple(points), radius, color, thickness=-1)\n\n    def _get_cursor_shape(self, x1=0, y1=0, x2=0, y2=0, outline=\"black\", state=\"hidden\"):\n        if self._cursor_shape_name == \"Rectangle\":\n            return self._canvas.create_rectangle(x1, y1, x2, y2, outline=outline, state=state)\n        else:\n            return self._canvas.create_oval(x1, y1, x2, y2, outline=outline, state=state)\n\n    def _mask_to_alignments(self, face_index):\n        \"\"\" Update the annotated mask to alignments.\n\n        Parameters\n        ----------\n        face_index: int\n            The index of the face in the current frame\n        \"\"\"\n        mask_type = self._control_vars[\"display\"][\"MaskType\"].get().lower()\n        mask = self._meta[\"mask\"][face_index].astype(\"float32\") / 255.0\n        self._det_faces.update.mask(self._globals.frame_index, face_index, mask, mask_type)\n\n    def _adjust_brush_radius(self, increase=True):  # pylint:disable=unused-argument\n        \"\"\" Adjust the brush radius up or down by 2px.\n\n        Sets the control panel option for brush radius to 2 less or 2 more than its current value\n\n        Parameters\n        ----------\n        increase: bool, optional\n            ``True`` to increment brush radius, ``False`` to decrement. Default: ``True``\n        \"\"\"\n        radius_var = self._control_vars[\"brush\"][\"BrushSize\"]\n        current_val = radius_var.get()\n        new_val = min(100, current_val + 2) if increase else max(1, current_val - 2)\n        logger.trace(\"Adjusting brush radius from %s to %s\", current_val, new_val)\n        radius_var.set(new_val)\n\n        delta = new_val - current_val\n        if delta == 0:\n            return\n        current_coords = self._canvas.coords(self._mouse_location[0])\n        new_coords = tuple(coord - delta if idx < 2 else coord + delta\n                           for idx, coord in enumerate(current_coords))\n        logger.trace(\"Adjusting brush coordinates from %s to %s\", current_coords, new_coords)\n        self._canvas.coords(self._mouse_location[0], new_coords)\n", "tools/manual/frameviewer/editor/landmarks.py": "#!/usr/bin/env python3\n\"\"\" Landmarks Editor and Landmarks Mesh viewer for the manual adjustments tool \"\"\"\nimport gettext\nimport numpy as np\n\nfrom lib.align import AlignedFace, LANDMARK_PARTS, LandmarkType\nfrom ._base import Editor, logger\n\n# LOCALES\n_LANG = gettext.translation(\"tools.manual\", localedir=\"locales\", fallback=True)\n_ = _LANG.gettext\n\n\nclass Landmarks(Editor):\n    \"\"\" The Landmarks Editor.\n\n    Adjust individual landmark points and re-generate Extract Box.\n\n    Parameters\n    ----------\n    canvas: :class:`tkinter.Canvas`\n        The canvas that holds the image and annotations\n    detected_faces: :class:`~tools.manual.detected_faces.DetectedFaces`\n        The _detected_faces data for this manual session\n    \"\"\"\n    def __init__(self, canvas, detected_faces):\n        control_text = _(\"Landmark Point Editor\\nEdit the individual landmark points.\\n\\n\"\n                         \" - Click and drag individual points to relocate.\\n\"\n                         \" - Draw a box to select multiple points to relocate.\")\n        self._selection_box = canvas.create_rectangle(0, 0, 0, 0,\n                                                      dash=(2, 4),\n                                                      state=\"hidden\",\n                                                      outline=\"gray\",\n                                                      fill=\"blue\",\n                                                      stipple=\"gray12\")\n        super().__init__(canvas, detected_faces, control_text)\n        # Clear selection box on an editor or frame change\n        self._canvas._tk_action_var.trace(\"w\", lambda *e: self._reset_selection())\n        self._globals.tk_frame_index.trace(\"w\", lambda *e: self._reset_selection())\n\n    def _add_actions(self):\n        \"\"\" Add the optional action buttons to the viewer. Current actions are Point, Select\n        and Zoom. \"\"\"\n        self._add_action(\"magnify\", \"zoom\", _(\"Magnify/Demagnify the View\"),\n                         group=None, hotkey=\"M\")\n        self._actions[\"magnify\"][\"tk_var\"].trace(\"w\", self._toggle_zoom)\n\n    # CALLBACKS\n    def _toggle_zoom(self, *args):  # pylint:disable=unused-argument\n        \"\"\" Clear any selections when switching mode and perform an update.\n\n        Parameters\n        ----------\n        args: tuple\n            tkinter callback arguments. Required but unused.\n        \"\"\"\n        self._reset_selection()\n        self._globals.tk_update.set(True)\n\n    def _reset_selection(self, event=None):  # pylint:disable=unused-argument\n        \"\"\" Reset the selection box and the selected landmark annotations. \"\"\"\n        self._canvas.itemconfig(\"lm_selected\", outline=self._control_color)\n        self._canvas.dtag(\"lm_selected\")\n        self._canvas.itemconfig(self._selection_box,\n                                stipple=\"gray12\",\n                                fill=\"blue\",\n                                outline=\"gray\",\n                                state=\"hidden\")\n        self._canvas.coords(self._selection_box, 0, 0, 0, 0)\n        self._drag_data = {}\n        if event is not None:\n            self._drag_start(event)\n\n    def update_annotation(self):\n        \"\"\" Get the latest Landmarks points and update. \"\"\"\n        zoomed_offset = self._zoomed_roi[:2]\n        for face_idx, face in enumerate(self._face_iterator):\n            face_index = self._globals.face_index if self._globals.is_zoomed else face_idx\n            if self._globals.is_zoomed:\n                aligned = AlignedFace(face.landmarks_xy,\n                                      centering=\"face\",\n                                      size=min(self._globals.frame_display_dims))\n                landmarks = aligned.landmarks + zoomed_offset\n                # Hide all landmarks and only display selected\n                self._canvas.itemconfig(\"lm_dsp\", state=\"hidden\")\n                self._canvas.itemconfig(f\"lm_dsp_face_{face_index}\", state=\"normal\")\n            else:\n                landmarks = self._scale_to_display(face.landmarks_xy)\n            for lm_idx, landmark in enumerate(landmarks):\n                self._display_landmark(landmark, face_index, lm_idx)\n                self._label_landmark(landmark, face_index, lm_idx)\n                self._grab_landmark(landmark, face_index, lm_idx)\n        logger.trace(\"Updated landmark annotations\")\n\n    def _display_landmark(self, bounding_box, face_index, landmark_index):\n        \"\"\" Add an individual landmark display annotation to the canvas.\n\n        Parameters\n        ----------\n        bounding_box: :class:`numpy.ndarray`\n            The (left, top), (right, bottom) (x, y) coordinates of the oval bounding box for this\n            landmark\n        face_index: int\n            The index of the face within the current frame\n        landmark_index: int\n            The index point of this landmark\n        \"\"\"\n        radius = 1\n        color = self._control_color\n        bbox = (bounding_box[0] - radius, bounding_box[1] - radius,\n                bounding_box[0] + radius, bounding_box[1] + radius)\n        key = f\"lm_dsp_{landmark_index}\"\n        kwargs = {\"outline\": color, \"fill\": color, \"width\": radius}\n        self._object_tracker(key, \"oval\", face_index, bbox, kwargs)\n\n    def _label_landmark(self, bounding_box, face_index, landmark_index):\n        \"\"\" Add a text label for a landmark to the canvas.\n\n        Parameters\n        ----------\n        bounding_box: :class:`numpy.ndarray`\n            The (left, top), (right, bottom) (x, y) coordinates of the oval bounding box for this\n            landmark\n        face_index: int\n            The index of the face within the current frame\n        landmark_index: int\n            The index point of this landmark\n        \"\"\"\n        if not self._is_active:\n            return\n        top_left = np.array(bounding_box[:2]) - 20\n        # NB The text must be visible to be able to get the bounding box, so set to hidden\n        # after the bounding box has been retrieved\n\n        keys = [f\"lm_lbl_{landmark_index}\", f\"lm_lbl_bg_{landmark_index}\"]\n        text_kwargs = {\"fill\": \"black\", \"font\": (\"Default\", 10), \"text\": str(landmark_index + 1)}\n        bg_kwargs = {\"fill\": \"#ffffea\", \"outline\": \"black\"}\n\n        text_id = self._object_tracker(keys[0], \"text\", face_index, top_left, text_kwargs)\n        bbox = self._canvas.bbox(text_id)\n        bbox = [bbox[0] - 2, bbox[1] - 2, bbox[2] + 2, bbox[3] + 2]\n        bg_id = self._object_tracker(keys[1], \"rectangle\", face_index, bbox, bg_kwargs)\n        self._canvas.tag_lower(bg_id, text_id)\n        self._canvas.itemconfig(text_id, state=\"hidden\")\n        self._canvas.itemconfig(bg_id, state=\"hidden\")\n\n    def _grab_landmark(self, bounding_box, face_index, landmark_index):\n        \"\"\" Add an individual landmark grab anchor to the canvas.\n\n        Parameters\n        ----------\n        bounding_box: :class:`numpy.ndarray`\n            The (left, top), (right, bottom) (x, y) coordinates of the oval bounding box for this\n            landmark\n        face_index: int\n            The index of the face within the current frame\n        landmark_index: int\n            The index point of this landmark\n        \"\"\"\n        if not self._is_active:\n            return\n        radius = 7\n        bbox = (bounding_box[0] - radius, bounding_box[1] - radius,\n                bounding_box[0] + radius, bounding_box[1] + radius)\n        key = f\"lm_grb_{landmark_index}\"\n        kwargs = {\"outline\": \"\",\n                  \"fill\": \"\",\n                  \"width\": 1,\n                  \"dash\": (2, 4)}\n        self._object_tracker(key, \"oval\", face_index, bbox, kwargs)\n\n    # << MOUSE HANDLING >>\n    # Mouse cursor display\n    def _update_cursor(self, event):\n        \"\"\" Set the cursor action.\n\n        Launch the cursor update action for the currently selected edit mode.\n\n        Parameters\n        ----------\n        event: :class:`tkinter.Event`\n            The current tkinter mouse event\n        \"\"\"\n        self._hide_labels()\n        if self._drag_data:\n            self._update_cursor_select_mode(event)\n        else:\n            objs = self._canvas.find_withtag(f\"lm_grb_face_{self._globals.face_index}\"\n                                             if self._globals.is_zoomed else \"lm_grb\")\n            item_ids = set(self._canvas.find_overlapping(event.x - 6,\n                                                         event.y - 6,\n                                                         event.x + 6,\n                                                         event.y + 6)).intersection(objs)\n            bboxes = [self._canvas.bbox(idx) for idx in item_ids]\n            item_id = next((idx for idx, bbox in zip(item_ids, bboxes)\n                            if bbox[0] <= event.x <= bbox[2] and bbox[1] <= event.y <= bbox[3]),\n                           None)\n            if item_id:\n                self._update_cursor_point_mode(item_id)\n            else:\n                self._canvas.config(cursor=\"\")\n                self._mouse_location = None\n                return\n\n    def _hide_labels(self):\n        \"\"\" Clear all landmark text labels from display \"\"\"\n        self._canvas.itemconfig(\"lm_lbl\", state=\"hidden\")\n        self._canvas.itemconfig(\"lm_lbl_bg\", state=\"hidden\")\n        self._canvas.itemconfig(\"lm_grb\", fill=\"\", outline=\"\")\n\n    def _update_cursor_point_mode(self, item_id):\n        \"\"\" Update the cursor when the mouse is over an individual landmark's grab anchor. Displays\n        the landmark label for the landmark under the cursor. Updates :attr:`_mouse_location` with\n        the current cursor position.\n\n        Parameters\n        ----------\n        item_id: int\n            The tkinter canvas object id for the landmark point that the cursor is over\n        \"\"\"\n        self._canvas.itemconfig(item_id, outline=\"yellow\")\n        tags = self._canvas.gettags(item_id)\n        face_idx = int(next(tag for tag in tags if tag.startswith(\"face_\")).split(\"_\")[-1])\n        lm_idx = int(next(tag for tag in tags if tag.startswith(\"lm_grb_\")).split(\"_\")[-1])\n        obj_idx = (face_idx, lm_idx)\n\n        self._canvas.config(cursor=\"none\")\n        for prefix in (\"lm_lbl_\", \"lm_lbl_bg_\"):\n            tag = f\"{prefix}{lm_idx}_face_{face_idx}\"\n            logger.trace(\"Displaying: %s tag: %s\", self._canvas.type(tag), tag)\n            self._canvas.itemconfig(tag, state=\"normal\")\n        self._mouse_location = obj_idx\n\n    def _update_cursor_select_mode(self, event):\n        \"\"\" Update the mouse cursor when in select mode.\n\n        Standard cursor returned when creating a new selection box. Move cursor returned when over\n        an existing selection box\n\n        Parameters\n        ----------\n        event: :class:`tkinter.Event`\n            The current tkinter mouse event\n        \"\"\"\n        bbox = self._canvas.coords(self._selection_box)\n        if bbox[0] <= event.x <= bbox[2] and bbox[1] <= event.y <= bbox[3]:\n            self._canvas.config(cursor=\"fleur\")\n        else:\n            self._canvas.config(cursor=\"\")\n\n    # Mouse actions\n    def _drag_start(self, event):\n        \"\"\" The action to perform when the user starts clicking and dragging the mouse.\n\n        The underlying Detected Face's landmark is updated for the point being edited.\n\n        Parameters\n        ----------\n        event: :class:`tkinter.Event`\n            The tkinter mouse event.\n        \"\"\"\n        sel_box = self._canvas.coords(self._selection_box)\n        if self._mouse_location is not None:  # Point edit mode\n            self._drag_data[\"start_location\"] = (event.x, event.y)\n            self._drag_callback = self._move_point\n        elif not self._drag_data:  # Initial point selection box\n            self._drag_data[\"start_location\"] = (event.x, event.y)\n            self._drag_callback = self._select\n        elif sel_box[0] <= event.x <= sel_box[2] and sel_box[1] <= event.y <= sel_box[3]:\n            # Move point selection box\n            self._drag_data[\"start_location\"] = (event.x, event.y)\n            self._drag_callback = self._move_selection\n        else:  # Reset\n            self._drag_data = {}\n            self._drag_callback = None\n            self._reset_selection(event)\n\n    def _drag_stop(self, event):  # pylint:disable=unused-argument\n        \"\"\" In select mode, call the select mode callback.\n\n        In point mode: trigger a viewport thumbnail update on click + drag release\n\n        If there is drag data, and there are selected points in the drag data then\n        trigger the selected points stop code.\n\n        Otherwise reset the selection box and return\n\n        Parameters\n        ----------\n        event: :class:`tkinter.Event`\n            The tkinter mouse event. Required but unused.\n        \"\"\"\n        if self._mouse_location is not None:  # Point edit mode\n            self._det_faces.update.post_edit_trigger(self._globals.frame_index,\n                                                     self._mouse_location[0])\n            self._mouse_location = None\n            self._drag_data = {}\n        elif self._drag_data and self._drag_data.get(\"selected\", False):\n            self._drag_stop_selected()\n        else:\n            logger.debug(\"No selected data. Clearing. drag_data: %s\", self._drag_data)\n            self._reset_selection()\n\n    def _drag_stop_selected(self):\n        \"\"\" Action to perform when mouse drag is stopped in selected points editor mode.\n\n        If there is already a selection, update the viewport thumbnail\n\n        If this is a new selection, then obtain the selected points and track\n        \"\"\"\n        if \"face_index\" in self._drag_data:  # Selected data has been moved\n            self._det_faces.update.post_edit_trigger(self._globals.frame_index,\n                                                     self._drag_data[\"face_index\"])\n            return\n\n        # This is a new selection\n        face_idx = set()\n        landmark_indices = []\n\n        for item_id in self._canvas.find_withtag(\"lm_selected\"):\n            tags = self._canvas.gettags(item_id)\n            face_idx.add(next(int(tag.split(\"_\")[-1])\n                              for tag in tags if tag.startswith(\"face_\")))\n            landmark_indices.append(next(int(tag.split(\"_\")[-1])\n                                         for tag in tags\n                                         if tag.startswith(\"lm_dsp_\") and \"face\" not in tag))\n        if len(face_idx) != 1:\n            logger.trace(\"Not exactly 1 face in selection. Aborting. Face indices: %s\", face_idx)\n            self._reset_selection()\n            return\n\n        self._drag_data[\"face_index\"] = face_idx.pop()\n        self._drag_data[\"landmarks\"] = landmark_indices\n        self._canvas.itemconfig(self._selection_box, stipple=\"\", fill=\"\", outline=\"#ffff00\")\n        self._snap_selection_to_points()\n\n    def _snap_selection_to_points(self):\n        \"\"\" Snap the selection box to the selected points.\n\n        As the landmarks are calculated and redrawn, the selection box can drift. This is\n        particularly true in zoomed mode. The selection box is therefore redrawn to bind just\n        outside of the selected points.\n        \"\"\"\n        all_coords = np.array([self._canvas.coords(item_id)\n                               for item_id in self._canvas.find_withtag(\"lm_selected\")])\n        mins = np.min(all_coords, axis=0)\n        maxes = np.max(all_coords, axis=0)\n        box_coords = [np.min(mins[[0, 2]] - 5),\n                      np.min(mins[[1, 3]] - 5),\n                      np.max(maxes[[0, 2]] + 5),\n                      np.max(maxes[[1, 3]]) + 5]\n        self._canvas.coords(self._selection_box, *box_coords)\n\n    def _move_point(self, event):\n        \"\"\" Moves the selected landmark point box and updates the underlying landmark on a point\n        drag event.\n\n        Parameters\n        ----------\n        event: :class:`tkinter.Event`\n            The tkinter mouse event.\n        \"\"\"\n        face_idx, lm_idx = self._mouse_location\n        shift_x = event.x - self._drag_data[\"start_location\"][0]\n        shift_y = event.y - self._drag_data[\"start_location\"][1]\n\n        if self._globals.is_zoomed:\n            scaled_shift = np.array((shift_x, shift_y))\n        else:\n            scaled_shift = self.scale_from_display(np.array((shift_x, shift_y)), do_offset=False)\n        self._det_faces.update.landmark(self._globals.frame_index,\n                                        face_idx,\n                                        lm_idx,\n                                        *scaled_shift,\n                                        self._globals.is_zoomed)\n        self._drag_data[\"start_location\"] = (event.x, event.y)\n\n    def _select(self, event):\n        \"\"\" Create a selection box on mouse drag event when in \"select\" mode\n\n        Parameters\n        ----------\n        event: :class:`tkinter.Event`\n            The tkinter mouse event.\n        \"\"\"\n        if self._canvas.itemcget(self._selection_box, \"state\") == \"hidden\":\n            self._canvas.itemconfig(self._selection_box, state=\"normal\")\n        coords = (*self._drag_data[\"start_location\"], event.x, event.y)\n        self._canvas.coords(self._selection_box, *coords)\n        enclosed = set(self._canvas.find_enclosed(*coords))\n        landmarks = set(self._canvas.find_withtag(\"lm_dsp\"))\n\n        for item_id in list(enclosed.intersection(landmarks)):\n            self._canvas.addtag_withtag(\"lm_selected\", item_id)\n        self._canvas.itemconfig(\"lm_selected\", outline=\"#ffff00\")\n        self._drag_data[\"selected\"] = True\n\n    def _move_selection(self, event):\n        \"\"\" Move a selection box and the landmarks contained when in \"select\" mode and a selection\n        box has been drawn. \"\"\"\n        shift_x = event.x - self._drag_data[\"start_location\"][0]\n        shift_y = event.y - self._drag_data[\"start_location\"][1]\n        if self._globals.is_zoomed:\n            scaled_shift = np.array((shift_x, shift_y))\n        else:\n            scaled_shift = self.scale_from_display(np.array((shift_x, shift_y)), do_offset=False)\n        self._canvas.move(self._selection_box, shift_x, shift_y)\n\n        self._det_faces.update.landmark(self._globals.frame_index,\n                                        self._drag_data[\"face_index\"],\n                                        self._drag_data[\"landmarks\"],\n                                        *scaled_shift,\n                                        self._globals.is_zoomed)\n        self._snap_selection_to_points()\n        self._drag_data[\"start_location\"] = (event.x, event.y)\n\n\nclass Mesh(Editor):\n    \"\"\" The Landmarks Mesh Display.\n\n    There are no editing options for Mesh editor. It is purely aesthetic and updated when other\n    editors are used.\n\n    Parameters\n    ----------\n    canvas: :class:`tkinter.Canvas`\n        The canvas that holds the image and annotations\n    detected_faces: :class:`~tools.manual.detected_faces.DetectedFaces`\n        The _detected_faces data for this manual session\n    \"\"\"\n    def __init__(self, canvas, detected_faces):\n        super().__init__(canvas, detected_faces, None)\n\n    def update_annotation(self):\n        \"\"\" Get the latest Landmarks and update the mesh.\"\"\"\n        key = \"mesh\"\n        color = self._control_color\n        zoomed_offset = self._zoomed_roi[:2]\n        for face_idx, face in enumerate(self._face_iterator):\n            face_index = self._globals.face_index if self._globals.is_zoomed else face_idx\n            if self._globals.is_zoomed:\n                aligned = AlignedFace(face.landmarks_xy,\n                                      centering=\"face\",\n                                      size=min(self._globals.frame_display_dims))\n                landmarks = aligned.landmarks + zoomed_offset\n                landmark_mapping = LANDMARK_PARTS[aligned.landmark_type]\n                # Hide all meshes and only display selected\n                self._canvas.itemconfig(\"Mesh\", state=\"hidden\")\n                self._canvas.itemconfig(f\"Mesh_face_{face_index}\", state=\"normal\")\n            else:\n                landmarks = self._scale_to_display(face.landmarks_xy)\n                landmark_mapping = LANDMARK_PARTS[LandmarkType.from_shape(landmarks.shape)]\n            logger.trace(\"Drawing Landmarks Mesh: (landmarks: %s, color: %s)\", landmarks, color)\n            for idx, (start, end, fill) in enumerate(landmark_mapping.values()):\n                key = f\"mesh_{idx}\"\n                pts = landmarks[start:end].flatten()\n                if fill:\n                    kwargs = {\"fill\": \"\", \"outline\": color, \"width\": 1}\n                    asset = \"polygon\"\n                else:\n                    kwargs = {\"fill\": color, \"width\": 1}\n                    asset = \"line\"\n                self._object_tracker(key, asset, face_index, pts, kwargs)\n        # Place mesh as bottom annotation\n        self._canvas.tag_raise(self.__class__.__name__, \"main_image\")\n", "tools/manual/frameviewer/editor/_base.py": "#!/usr/bin/env python3\n\"\"\" Editor objects for the manual adjustments tool \"\"\"\n\nimport gettext\nimport logging\nimport tkinter as tk\n\nfrom collections import OrderedDict\n\nimport numpy as np\n\nfrom lib.gui.control_helper import ControlPanelOption\n\nlogger = logging.getLogger(__name__)\n\n# LOCALES\n_LANG = gettext.translation(\"tools.manual\", localedir=\"locales\", fallback=True)\n_ = _LANG.gettext\n\n\nclass Editor():\n    \"\"\" Parent Class for Object Editors.\n\n    Editors allow the user to use a variety of tools to manipulate alignments from the main\n    display frame.\n\n    Parameters\n    ----------\n    canvas: :class:`tkinter.Canvas`\n        The canvas that holds the image and annotations\n    detected_faces: :class:`~tools.manual.detected_faces.DetectedFaces`\n        The _detected_faces data for this manual session\n    control_text: str\n        The text that is to be displayed at the top of the Editor's control panel.\n    \"\"\"\n    def __init__(self, canvas, detected_faces, control_text=\"\", key_bindings=None):\n        logger.debug(\"Initializing %s: (canvas: '%s', detected_faces: %s, control_text: %s)\",\n                     self.__class__.__name__, canvas, detected_faces, control_text)\n        self.zoomed_centering = \"face\"  # Override for different zoomed centering per editor\n        self._canvas = canvas\n        self._globals = canvas._globals\n        self._det_faces = detected_faces\n\n        self._current_color = dict()\n        self._actions = OrderedDict()\n        self._controls = dict(header=control_text, controls=[])\n        self._add_key_bindings(key_bindings)\n\n        self._add_actions()\n        self._add_controls()\n        self._add_annotation_format_controls()\n\n        self._mouse_location = None\n        self._drag_data = dict()\n        self._drag_callback = None\n        self.bind_mouse_motion()\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    @property\n    def _default_colors(self):\n        \"\"\" dict: The default colors for each annotation \"\"\"\n        return {\"BoundingBox\": \"#0000ff\",\n                \"ExtractBox\": \"#00ff00\",\n                \"Landmarks\": \"#ff00ff\",\n                \"Mask\": \"#ff0000\",\n                \"Mesh\": \"#00ffff\"}\n\n    @property\n    def _is_active(self):\n        \"\"\" bool: ``True`` if this editor is currently active otherwise ``False``.\n\n        Notes\n        -----\n        When initializing, the active_editor parameter will not be set in the parent,\n        so return ``False`` in this instance\n        \"\"\"\n        return hasattr(self._canvas, \"active_editor\") and self._canvas.active_editor == self\n\n    @property\n    def view_mode(self):\n        \"\"\" [\"frame\", \"face\"]: The view mode for the currently selected editor. If the editor does\n        not have a view mode that can be updated, then `\"frame\"` will be returned. \"\"\"\n        tk_var = self._actions.get(\"magnify\", dict()).get(\"tk_var\", None)\n        retval = \"frame\" if tk_var is None or not tk_var.get() else \"face\"\n        return retval\n\n    @property\n    def _zoomed_roi(self):\n        \"\"\" :class:`numpy.ndarray`: The (`left`, `top`, `right`, `bottom`) roi of the zoomed face\n        in the display frame. \"\"\"\n        half_size = min(self._globals.frame_display_dims) / 2\n        left = self._globals.frame_display_dims[0] / 2 - half_size\n        top = 0\n        right = self._globals.frame_display_dims[0] / 2 + half_size\n        bottom = self._globals.frame_display_dims[1]\n        retval = np.rint(np.array((left, top, right, bottom))).astype(\"int32\")\n        logger.trace(\"Zoomed ROI: %s\", retval)\n        return retval\n\n    @property\n    def _zoomed_dims(self):\n        \"\"\" tuple: The (`width`, `height`) of the zoomed ROI. \"\"\"\n        roi = self._zoomed_roi\n        return (roi[2] - roi[0], roi[3] - roi[1])\n\n    @property\n    def _control_vars(self):\n        \"\"\" dict: The tk control panel variables for the currently selected editor. \"\"\"\n        return self._canvas.control_tk_vars.get(self.__class__.__name__, dict())\n\n    @property\n    def controls(self):\n        \"\"\" dict: The control panel options and header text for the current editor \"\"\"\n        return self._controls\n\n    @property\n    def _control_color(self):\n        \"\"\" str: The hex color code set in the control panel for the current editor. \"\"\"\n        annotation = self.__class__.__name__\n        return self._annotation_formats[annotation][\"color\"].get()\n\n    @property\n    def _annotation_formats(self):\n        \"\"\" dict: The format (color, opacity etc.) of each editor's annotation display. \"\"\"\n        return self._canvas.annotation_formats\n\n    @property\n    def actions(self):\n        \"\"\" list: The optional action buttons for the actions frame in the GUI for the\n        current editor \"\"\"\n        return self._actions\n\n    @property\n    def _face_iterator(self):\n        \"\"\" list: The detected face objects to be iterated. This will either be all faces in the\n        frame (normal view) or the single zoomed in face (zoom mode). \"\"\"\n        if self._globals.frame_index == -1:\n            faces = []\n        else:\n            faces = self._det_faces.current_faces[self._globals.frame_index]\n            faces = ([faces[self._globals.face_index]]\n                     if self._globals.is_zoomed and faces else faces)\n        return faces\n\n    def _add_key_bindings(self, key_bindings):\n        \"\"\" Add the editor specific key bindings for the currently viewed editor.\n\n        Parameters\n        ----------\n        key_bindings: dict\n            The key binding to method dictionary for this editor.\n        \"\"\"\n        if key_bindings is None:\n            return\n        for key, method in key_bindings.items():\n            logger.debug(\"Binding key '%s' to method %s for editor '%s'\",\n                         key, method, self.__class__.__name__)\n            self._canvas.key_bindings.setdefault(key, dict())[\"bound_to\"] = None\n            self._canvas.key_bindings[key][self.__class__.__name__] = method\n\n    @staticmethod\n    def _get_anchor_points(bounding_box):\n        \"\"\" Retrieve the (x, y) co-ordinates for each of the 4 corners of a bounding box's anchors\n        for both the displayed anchors and the anchor grab locations.\n\n        Parameters\n        ----------\n        bounding_box: tuple\n            The (`top-left`, `top-right`, `bottom-right`, `bottom-left`) (x, y) coordinates of the\n            bounding box\n\n        Returns\n            display_anchors: tuple\n                The (`top`, `left`, `bottom`, `right`) co-ordinates for each circle at each point\n                of the bounding box corners, sized for display\n            grab_anchors: tuple\n                The (`top`, `left`, `bottom`, `right`) co-ordinates for each circle at each point\n                of the bounding box corners, at a larger size for grabbing with a mouse\n        \"\"\"\n        radius = 3\n        grab_radius = radius * 3\n        display_anchors = tuple((cnr[0] - radius, cnr[1] - radius,\n                                 cnr[0] + radius, cnr[1] + radius)\n                                for cnr in bounding_box)\n        grab_anchors = tuple((cnr[0] - grab_radius, cnr[1] - grab_radius,\n                              cnr[0] + grab_radius, cnr[1] + grab_radius)\n                             for cnr in bounding_box)\n        return display_anchors, grab_anchors\n\n    def update_annotation(self):  # pylint:disable=no-self-use\n        \"\"\" Update the display annotations for the current objects.\n\n        Override for specific editors.\n        \"\"\"\n        logger.trace(\"Default annotations. Not storing Objects\")\n\n    def hide_annotation(self, tag=None):\n        \"\"\" Hide annotations for this editor.\n\n        Parameters\n        ----------\n        tag: str, optional\n            The specific tag to hide annotations for. If ``None`` then all annotations for this\n            editor are hidden, otherwise only the annotations specified by the given tag are\n            hidden. Default: ``None``\n        \"\"\"\n        tag = self.__class__.__name__ if tag is None else tag\n        logger.trace(\"Hiding annotations for tag: %s\", tag)\n        self._canvas.itemconfig(tag, state=\"hidden\")\n\n    def _object_tracker(self, key, object_type, face_index,\n                        coordinates, object_kwargs):\n        \"\"\" Create an annotation object and add it to :attr:`_objects` or update an existing\n        annotation if it has already been created.\n\n        Parameters\n        ----------\n        key: str\n            The key for this annotation in :attr:`_objects`\n        object_type: str\n            This can be any string that is a natural extension to :class:`tkinter.Canvas.create_`\n        face_index: int\n            The index of the face within the current frame\n        coordinates: tuple or list\n            The bounding box coordinates for this object\n        object_kwargs: dict\n            The keyword arguments for this object\n\n        Returns\n        -------\n        int:\n            The tkinter canvas item identifier for the created object\n        \"\"\"\n        object_color_keys = self._get_object_color_keys(key, object_type)\n        tracking_id = \"_\".join((key, str(face_index)))\n        face_tag = \"face_{}\".format(face_index)\n        face_objects = set(self._canvas.find_withtag(face_tag))\n        annotation_objects = set(self._canvas.find_withtag(key))\n        existing_object = tuple(face_objects.intersection(annotation_objects))\n        if not existing_object:\n            item_id = self._add_new_object(key,\n                                           object_type,\n                                           face_index,\n                                           coordinates,\n                                           object_kwargs)\n            update_color = bool(object_color_keys)\n        else:\n            item_id = existing_object[0]\n            update_color = self._update_existing_object(\n                existing_object[0],\n                coordinates,\n                object_kwargs,\n                tracking_id,\n                object_color_keys)\n        if update_color:\n            self._current_color[tracking_id] = object_kwargs[object_color_keys[0]]\n        return item_id\n\n    @staticmethod\n    def _get_object_color_keys(key, object_type):\n        \"\"\" The canvas object's parameter that needs to be adjusted for color varies based on\n        the type of object that is being used. Returns the correct parameter based on object.\n\n        Parameters\n        ----------\n        key: str\n            The key for this annotation's tag creation\n        object_type: str\n            This can be any string that is a natural extension to :class:`tkinter.Canvas.create_`\n\n        Returns\n        -------\n        list:\n            The list of keyword arguments for this objects color parameter(s) or an empty list\n            if it is not relevant for this object\n        \"\"\"\n        if object_type in (\"line\", \"text\"):\n            retval = [\"fill\"]\n        elif object_type == \"image\":\n            retval = []\n        elif object_type == \"oval\" and key.startswith(\"lm_dsp_\"):\n            retval = [\"fill\", \"outline\"]\n        else:\n            retval = [\"outline\"]\n        logger.trace(\"returning %s for key: %s, object_type: %s\", retval, key, object_type)\n        return retval\n\n    def _add_new_object(self, key, object_type, face_index, coordinates, object_kwargs):\n        \"\"\" Add a new object to the canvas.\n\n        Parameters\n        ----------\n        key: str\n            The key for this annotation's tag creation\n        object_type: str\n            This can be any string that is a natural extension to :class:`tkinter.Canvas.create_`\n        face_index: int\n            The index of the face within the current frame\n        coordinates: tuple or list\n            The bounding box coordinates for this object\n        object_kwargs: dict\n            The keyword arguments for this object\n\n        Returns\n        -------\n        int:\n            The tkinter canvas item identifier for the created object\n        \"\"\"\n        logger.debug(\"Adding object: (key: '%s', object_type: '%s', face_index: %s, \"\n                     \"coordinates: %s, object_kwargs: %s)\", key, object_type, face_index,\n                     coordinates, object_kwargs)\n        object_kwargs[\"tags\"] = self._set_object_tags(face_index, key)\n        item_id = getattr(self._canvas,\n                          \"create_{}\".format(object_type))(*coordinates, **object_kwargs)\n        return item_id\n\n    def _set_object_tags(self, face_index, key):\n        \"\"\" Create the tkinter object tags for the incoming object.\n\n        Parameters\n        ----------\n        face_index: int\n            The face index within the current frame for the face that tags are being created for\n        key: str\n            The base tag for this object, for which additional tags will be generated\n\n        Returns\n        -------\n        list\n            The generated tags for the current object\n        \"\"\"\n        tags = [\"face_{}\".format(face_index),\n                self.__class__.__name__,\n                \"{}_face_{}\".format(self.__class__.__name__, face_index),\n                key,\n                \"{}_face_{}\".format(key, face_index)]\n        if \"_\" in key:\n            split_key = key.split(\"_\")\n            if split_key[-1].isdigit():\n                base_tag = \"_\".join(split_key[:-1])\n                tags.append(base_tag)\n                tags.append(\"{}_face_{}\".format(base_tag, face_index))\n        return tags\n\n    def _update_existing_object(self, item_id, coordinates, object_kwargs,\n                                tracking_id, object_color_keys):\n        \"\"\" Update an existing tracked object.\n\n        Parameters\n        ----------\n        item_id: int\n            The canvas object item_id to be updated\n        coordinates: tuple or list\n            The bounding box coordinates for this object\n        object_kwargs: dict\n            The keyword arguments for this object\n        tracking_id: str\n            The tracking identifier for this object's color\n        object_color_keys: list\n            The list of keyword arguments for this object to update for color\n\n        Returns\n        -------\n        bool\n            ``True`` if :attr:`_current_color` should be updated otherwise ``False``\n        \"\"\"\n        update_color = (object_color_keys and\n                        object_kwargs[object_color_keys[0]] != self._current_color[tracking_id])\n        update_kwargs = dict(state=object_kwargs.get(\"state\", \"normal\"))\n        if update_color:\n            for key in object_color_keys:\n                update_kwargs[key] = object_kwargs[object_color_keys[0]]\n        if self._canvas.type(item_id) == \"image\" and \"image\" in object_kwargs:\n            update_kwargs[\"image\"] = object_kwargs[\"image\"]\n        logger.trace(\"Updating coordinates: (item_id: '%s', object_kwargs: %s, \"\n                     \"coordinates: %s, update_kwargs: %s\", item_id, object_kwargs,\n                     coordinates, update_kwargs)\n        self._canvas.itemconfig(item_id, **update_kwargs)\n        self._canvas.coords(item_id, *coordinates)\n        return update_color\n\n    # << MOUSE CALLBACKS >>\n    # Mouse cursor display\n    def bind_mouse_motion(self):\n        \"\"\" Binds the mouse motion for the current editor's mouse <Motion> event to the editor's\n        :func:`_update_cursor` function.\n\n        Called on initialization and active editor update.\n        \"\"\"\n        self._canvas.bind(\"<Motion>\", self._update_cursor)\n\n    def _update_cursor(self, event):  # pylint:disable=unused-argument\n        \"\"\" The mouse cursor display as bound to the mouse's <Motion> event..\n\n        The default is to always return a standard cursor, so this method should be overridden for\n        editor specific cursor update.\n\n        Parameters\n        ----------\n        event: :class:`tkinter.Event`\n            The tkinter mouse event. Unused for default tracking, but available for specific editor\n            tracking.\n        \"\"\"\n        self._canvas.config(cursor=\"\")\n\n    # Mouse click and drag actions\n    def set_mouse_click_actions(self):\n        \"\"\" Add the bindings for left mouse button click and drag actions.\n\n        This binds the mouse to the :func:`_drag_start`, :func:`_drag` and :func:`_drag_stop`\n        methods.\n\n        By default these methods do nothing (except for :func:`_drag_stop` which resets\n        :attr:`_drag_data`.\n\n        This bindings should be added for all editors. To add additional bindings,\n        `super().set_mouse_click_actions` should be called prior to adding them..\n        \"\"\"\n        logger.debug(\"Setting mouse bindings\")\n        self._canvas.bind(\"<ButtonPress-1>\", self._drag_start)\n        self._canvas.bind(\"<ButtonRelease-1>\", self._drag_stop)\n        self._canvas.bind(\"<B1-Motion>\", self._drag)\n\n    def _drag_start(self, event):  # pylint:disable=unused-argument\n        \"\"\" The action to perform when the user starts clicking and dragging the mouse.\n\n        The default does nothing except reset the attr:`drag_data` and attr:`drag_callback`.\n        Override for Editor specific click and drag start actions.\n\n        Parameters\n        ----------\n        event: :class:`tkinter.Event`\n            The tkinter mouse event. Unused but for default action, but available for editor\n            specific actions\n        \"\"\"\n        self._drag_data = dict()\n        self._drag_callback = None\n\n    def _drag(self, event):\n        \"\"\" The default callback for the drag part of a mouse click and drag action.\n\n        :attr:`_drag_callback` should be set in :func:`self._drag_start`. This callback will then\n        be executed on a mouse drag event.\n\n        Parameters\n        ----------\n        event: :class:`tkinter.Event`\n            The tkinter mouse event.\n        \"\"\"\n        if self._drag_callback is None:\n            return\n        self._drag_callback(event)\n\n    def _drag_stop(self, event):  # pylint:disable=unused-argument\n        \"\"\" The action to perform when the user stops clicking and dragging the mouse.\n\n        Default is to set :attr:`_drag_data` to `dict`. Override for Editor specific stop actions.\n\n        Parameters\n        ----------\n        event: :class:`tkinter.Event`\n            The tkinter mouse event. Unused but required\n        \"\"\"\n        self._drag_data = dict()\n\n    def _scale_to_display(self, points):\n        \"\"\" Scale and offset the given points to the current display scale and offset values.\n\n        Parameters\n        ----------\n        points: :class:`numpy.ndarray`\n            Array of x, y co-ordinates to adjust\n\n        Returns\n        -------\n        :class:`numpy.ndarray`\n            The adjusted x, y co-ordinates for display purposes rounded to the nearest integer\n        \"\"\"\n        retval = np.rint((points * self._globals.current_frame[\"scale\"])\n                         + self._canvas.offset).astype(\"int32\")\n        logger.trace(\"Original points: %s, scaled points: %s\", points, retval)\n        return retval\n\n    def scale_from_display(self, points, do_offset=True):\n        \"\"\" Scale and offset the given points from the current display to the correct original\n        values.\n\n        Parameters\n        ----------\n        points: :class:`numpy.ndarray`\n            Array of x, y co-ordinates to adjust\n        offset: bool, optional\n            ``True`` if the offset should be calculated otherwise ``False``. Default: ``True``\n\n        Returns\n        -------\n        :class:`numpy.ndarray`\n            The adjusted x, y co-ordinates to the original frame location rounded to the nearest\n            integer\n        \"\"\"\n        offset = self._canvas.offset if do_offset else (0, 0)\n        retval = np.rint((points - offset) / self._globals.current_frame[\"scale\"]).astype(\"int32\")\n        logger.trace(\"Original points: %s, scaled points: %s\", points, retval)\n        return retval\n\n    # << ACTION CONTROL PANEL OPTIONS >>\n    def _add_actions(self):\n        \"\"\" Add the Action buttons for this editor's optional left hand side action sections.\n\n        The default does nothing. Override for editor specific actions.\n        \"\"\"\n        self._actions = self._actions\n\n    def _add_action(self, title, icon, helptext, group=None, hotkey=None):\n        \"\"\" Add an action dictionary to :attr:`_actions`. This will create a button in the optional\n        actions frame to the left hand side of the frames viewer.\n\n        Parameters\n        ----------\n        title: str\n            The title of the action to be generated\n        icon: str\n            The name of the icon that is used to display this action's button\n        helptext: str\n            The tooltip text to display for this action\n        group: str, optional\n            If a group is passed in, then any buttons belonging to that group will be linked (i.e.\n            only one button can be active at a time.). If ``None`` is passed in then the button\n            will act independently. Default: ``None``\n        hotkey: str, optional\n            The hotkey binding for this action. Set to ``None`` if there is no hotkey binding.\n            Default: ``None``\n        \"\"\"\n        var = tk.BooleanVar()\n        action = dict(icon=icon, helptext=helptext, group=group, tk_var=var, hotkey=hotkey)\n        logger.debug(\"Adding action: %s\", action)\n        self._actions[title] = action\n\n    def _add_controls(self):\n        \"\"\" Add the controls for this editor's control panel.\n\n        The default does nothing. Override for editor specific controls.\n        \"\"\"\n        self._controls = self._controls\n\n    def _add_control(self, option, global_control=False):\n        \"\"\" Add a control panel control to :attr:`_controls` and add a trace to the variable\n        to update display.\n\n        Parameters\n        ----------\n        option: :class:`lib.gui.control_helper.ControlPanelOption'\n            The control panel option to add to this editor's control\n        global_control: bool, optional\n            Whether the given control is a global control (i.e. annotation formatting).\n            Default: ``False``\n        \"\"\"\n        self._controls[\"controls\"].append(option)\n        if global_control:\n            logger.debug(\"Added global control: '%s' for editor: '%s'\",\n                         option.title, self.__class__.__name__)\n            return\n        logger.debug(\"Added local control: '%s' for editor: '%s'\",\n                     option.title, self.__class__.__name__)\n        editor_key = self.__class__.__name__\n        group_key = option.group.replace(\" \", \"\").lower()\n        group_key = \"none\" if group_key == \"_master\" else group_key\n        annotation_key = option.title.replace(\" \", \"\")\n        self._canvas.control_tk_vars.setdefault(\n            editor_key, dict()).setdefault(group_key, dict())[annotation_key] = option.tk_var\n\n    def _add_annotation_format_controls(self):\n        \"\"\" Add the annotation display (color/size) controls to :attr:`_annotation_formats`.\n\n        These should be universal and available for all editors.\n        \"\"\"\n        editors = (\"Bounding Box\", \"Extract Box\", \"Landmarks\", \"Mask\", \"Mesh\")\n        if not self._annotation_formats:\n            opacity = ControlPanelOption(\"Mask Opacity\",\n                                         int,\n                                         group=\"Color\",\n                                         min_max=(0, 100),\n                                         default=40,\n                                         rounding=1,\n                                         helptext=\"Set the mask opacity\")\n            for editor in editors:\n                annotation_key = editor.replace(\" \", \"\")\n                logger.debug(\"Adding to global format controls: '%s'\", editor)\n                colors = ControlPanelOption(editor,\n                                            str,\n                                            group=\"Color\",\n                                            subgroup=\"colors\",\n                                            choices=\"colorchooser\",\n                                            default=self._default_colors[annotation_key],\n                                            helptext=\"Set the annotation color\")\n                colors.set(self._default_colors[annotation_key])\n                self._annotation_formats.setdefault(annotation_key, dict())[\"color\"] = colors\n                self._annotation_formats[annotation_key][\"mask_opacity\"] = opacity\n\n        for editor in editors:\n            annotation_key = editor.replace(\" \", \"\")\n            for group, ctl in self._annotation_formats[annotation_key].items():\n                logger.debug(\"Adding global format control to editor: (editor:'%s', group: '%s')\",\n                             editor, group)\n                self._add_control(ctl, global_control=True)\n\n\nclass View(Editor):\n    \"\"\" The view Editor.\n\n    Does not allow any editing, just used for previewing annotations.\n\n    This is the default start-up editor.\n\n    Parameters\n    ----------\n    canvas: :class:`tkinter.Canvas`\n        The canvas that holds the image and annotations\n    detected_faces: :class:`~tools.manual.detected_faces.DetectedFaces`\n        The _detected_faces data for this manual session\n    \"\"\"\n    def __init__(self, canvas, detected_faces):\n        control_text = \"Viewer\\nPreview the frame's annotations.\"\n        super().__init__(canvas, detected_faces, control_text)\n\n    def _add_actions(self):\n        \"\"\" Add the optional action buttons to the viewer. Current actions are Zoom. \"\"\"\n        self._add_action(\"magnify\", \"zoom\", _(\"Magnify/Demagnify the View\"),\n                         group=None, hotkey=\"M\")\n        self._actions[\"magnify\"][\"tk_var\"].trace(\"w\", lambda *e: self._globals.tk_update.set(True))\n", "tools/manual/frameviewer/editor/__init__.py": "#!/usr/bin/env python3\n\"\"\" The Frame Viewer for Faceswap's Manual Tool. \"\"\"\n\nfrom ._base import View\nfrom .bounding_box import BoundingBox\nfrom .extract_box import ExtractBox\nfrom .landmarks import Landmarks, Mesh\nfrom .mask import Mask\n", "tools/manual/frameviewer/editor/bounding_box.py": "#!/usr/bin/env python3\n\"\"\" Bounding Box Editor for the manual adjustments tool \"\"\"\n\nimport gettext\nimport platform\nfrom functools import partial\n\nimport numpy as np\n\nfrom lib.gui.custom_widgets import RightClickMenu\nfrom ._base import ControlPanelOption, Editor, logger\n\n\n# LOCALES\n_LANG = gettext.translation(\"tools.manual\", localedir=\"locales\", fallback=True)\n_ = _LANG.gettext\n\n\nclass BoundingBox(Editor):\n    \"\"\" The Bounding Box Editor.\n\n    Adjusting the bounding box feeds the aligner to generate new 68 point landmarks.\n\n    Parameters\n    ----------\n    canvas: :class:`tkinter.Canvas`\n        The canvas that holds the image and annotations\n    detected_faces: :class:`~tools.manual.detected_faces.DetectedFaces`\n        The _detected_faces data for this manual session\n    \"\"\"\n    def __init__(self, canvas, detected_faces):\n        self._tk_aligner = None\n        self._right_click_menu = RightClickMenu([_(\"Delete Face\")],\n                                                [self._delete_current_face],\n                                                [\"Del\"])\n        control_text = _(\"Bounding Box Editor\\nEdit the bounding box being fed into the aligner \"\n                         \"to recalculate the landmarks.\\n\\n\"\n                         \" - Grab the corner anchors to resize the bounding box.\\n\"\n                         \" - Click and drag the bounding box to relocate.\\n\"\n                         \" - Click in empty space to create a new bounding box.\\n\"\n                         \" - Right click a bounding box to delete a face.\")\n        key_bindings = {\"<Delete>\": self._delete_current_face}\n        super().__init__(canvas, detected_faces,\n                         control_text=control_text, key_bindings=key_bindings)\n\n    @property\n    def _corner_order(self):\n        \"\"\" dict: The position index of bounding box corners \"\"\"\n        return {0: (\"top\", \"left\"),\n                1: (\"top\", \"right\"),\n                2: (\"bottom\", \"right\"),\n                3: (\"bottom\", \"left\")}\n\n    @property\n    def _bounding_boxes(self):\n        \"\"\" list: The :func:`tkinter.Canvas.coords` for all displayed bounding boxes. \"\"\"\n        item_ids = self._canvas.find_withtag(\"bb_box\")\n        return [self._canvas.coords(item_id) for item_id in item_ids\n                if self._canvas.itemcget(item_id, \"state\") != \"hidden\"]\n\n    def _add_controls(self):\n        \"\"\" Controls for feeding the Aligner. Exposes Normalization Method as a parameter. \"\"\"\n        align_ctl = ControlPanelOption(\n            \"Aligner\",\n            str,\n            group=\"Aligner\",\n            choices=[\"cv2-dnn\", \"FAN\"],\n            default=\"FAN\",\n            is_radio=True,\n            helptext=_(\"Aligner to use. FAN will obtain better alignments, but cv2-dnn can be \"\n                       \"useful if FAN cannot get decent alignments and you want to set a base to \"\n                       \"edit from.\"))\n        self._tk_aligner = align_ctl.tk_var\n        self._add_control(align_ctl)\n\n        norm_ctl = ControlPanelOption(\n            \"Normalization method\",\n            str,\n            group=\"Aligner\",\n            choices=[\"none\", \"clahe\", \"hist\", \"mean\"],\n            default=\"hist\",\n            is_radio=True,\n            helptext=_(\"Normalization method to use for feeding faces to the aligner. This can \"\n                       \"help the aligner better align faces with difficult lighting conditions. \"\n                       \"Different methods will yield different results on different sets. NB: \"\n                       \"This does not impact the output face, just the input to the aligner.\"\n                       \"\\n\\tnone: Don't perform normalization on the face.\"\n                       \"\\n\\tclahe: Perform Contrast Limited Adaptive Histogram Equalization on \"\n                       \"the face.\"\n                       \"\\n\\thist: Equalize the histograms on the RGB channels.\"\n                       \"\\n\\tmean: Normalize the face colors to the mean.\"))\n        var = norm_ctl.tk_var\n        var.trace(\"w\",\n                  lambda *e, v=var: self._det_faces.extractor.set_normalization_method(v.get()))\n        self._add_control(norm_ctl)\n\n    def update_annotation(self):\n        \"\"\" Get the latest bounding box data from alignments and update. \"\"\"\n        if self._globals.is_zoomed:\n            logger.trace(\"Image is zoomed. Hiding Bounding Box.\")\n            self.hide_annotation()\n            return\n        key = \"bb_box\"\n        color = self._control_color\n        for idx, face in enumerate(self._face_iterator):\n            box = np.array([(face.left, face.top), (face.right, face.bottom)])\n            box = self._scale_to_display(box).astype(\"int32\").flatten()\n            kwargs = dict(outline=color, width=1)\n            logger.trace(\"frame_index: %s, face_index: %s, box: %s, kwargs: %s\",\n                         self._globals.frame_index, idx, box, kwargs)\n            self._object_tracker(key, \"rectangle\", idx, box, kwargs)\n            self._update_anchor_annotation(idx, box, color)\n        logger.trace(\"Updated bounding box annotations\")\n\n    def _update_anchor_annotation(self, face_index, bounding_box, color):\n        \"\"\" Update the anchor annotations for each corner of the bounding box.\n\n        The anchors only display when the bounding box editor is active.\n\n        Parameters\n        ----------\n        face_index: int\n            The index of the face being annotated\n        bounding_box: :class:`numpy.ndarray`\n            The scaled bounding box to get the corner anchors for\n        color: str\n            The hex color of the bounding box line\n        \"\"\"\n        if not self._is_active:\n            self.hide_annotation(\"bb_anc_dsp\")\n            self.hide_annotation(\"bb_anc_grb\")\n            return\n        fill_color = \"gray\"\n        activefill_color = \"white\" if self._is_active else \"\"\n        anchor_points = self._get_anchor_points(((bounding_box[0], bounding_box[1]),\n                                                 (bounding_box[2], bounding_box[1]),\n                                                 (bounding_box[2], bounding_box[3]),\n                                                 (bounding_box[0], bounding_box[3])))\n        for idx, (anc_dsp, anc_grb) in enumerate(zip(*anchor_points)):\n            dsp_kwargs = dict(outline=color, fill=fill_color, width=1)\n            grb_kwargs = dict(outline=\"\", fill=\"\", width=1, activefill=activefill_color)\n            dsp_key = \"bb_anc_dsp_{}\".format(idx)\n            grb_key = \"bb_anc_grb_{}\".format(idx)\n            self._object_tracker(dsp_key, \"oval\", face_index, anc_dsp, dsp_kwargs)\n            self._object_tracker(grb_key, \"oval\", face_index, anc_grb, grb_kwargs)\n        logger.trace(\"Updated bounding box anchor annotations\")\n\n    # << MOUSE HANDLING >>\n    # Mouse cursor display\n    def _update_cursor(self, event):\n        \"\"\" Set the cursor action.\n\n        Update :attr:`_mouse_location` with the current cursor position and display appropriate\n        icon.\n\n        If the cursor is over a corner anchor, then pop resize icon.\n        If the cursor is over a bounding box, then pop move icon.\n        If the cursor is over the image, then pop add icon.\n\n        Parameters\n        ----------\n        event: :class:`tkinter.Event`\n            The current tkinter mouse event\n        \"\"\"\n        if self._check_cursor_anchors():\n            return\n        if self._check_cursor_bounding_box(event):\n            return\n        if self._check_cursor_image(event):\n            return\n\n        self._canvas.config(cursor=\"\")\n        self._mouse_location = None\n\n    def _check_cursor_anchors(self):\n        \"\"\" Check whether the cursor is over a corner anchor.\n\n        If it is, set the appropriate cursor type and set :attr:`_mouse_location` to\n        (\"anchor\", (`face index`, `anchor index`)\n\n        Returns\n        -------\n        bool\n            ``True`` if cursor is over an anchor point otherwise ``False``\n        \"\"\"\n        anchors = set(self._canvas.find_withtag(\"bb_anc_grb\"))\n        item_ids = set(self._canvas.find_withtag(\"current\")).intersection(anchors)\n        if not item_ids:\n            return False\n        item_id = list(item_ids)[0]\n        tags = self._canvas.gettags(item_id)\n        face_idx = int(next(tag for tag in tags if tag.startswith(\"face_\")).split(\"_\")[-1])\n        corner_idx = int(next(tag for tag in tags\n                              if tag.startswith(\"bb_anc_grb_\")\n                              and \"face_\" not in tag).split(\"_\")[-1])\n        self._canvas.config(cursor=\"{}_{}_corner\".format(*self._corner_order[corner_idx]))\n        self._mouse_location = (\"anchor\", \"{}_{}\".format(face_idx, corner_idx))\n        return True\n\n    def _check_cursor_bounding_box(self, event):\n        \"\"\" Check whether the cursor is over a bounding box.\n\n        If it is, set the appropriate cursor type and set :attr:`_mouse_location` to:\n        (\"box\", `face index`)\n\n        Parameters\n        ----------\n        event: :class:`tkinter.Event`\n            The tkinter mouse event\n\n        Returns\n        -------\n        bool\n            ``True`` if cursor is over a bounding box otherwise ``False``\n\n        Notes\n        -----\n        We can't use tags on unfilled rectangles as the interior of the rectangle is not tagged.\n        \"\"\"\n        for face_idx, bbox in enumerate(self._bounding_boxes):\n            if bbox[0] <= event.x <= bbox[2] and bbox[1] <= event.y <= bbox[3]:\n                self._canvas.config(cursor=\"fleur\")\n                self._mouse_location = (\"box\", str(face_idx))\n                return True\n        return False\n\n    def _check_cursor_image(self, event):\n        \"\"\" Check whether the cursor is over the image.\n\n        If it is, set the appropriate cursor type and set :attr:`_mouse_location` to:\n        (\"image\", )\n\n        Parameters\n        ----------\n        event: :class:`tkinter.Event`\n            The tkinter mouse event\n\n        Returns\n        -------\n        bool\n            ``True`` if cursor is over a bounding box otherwise ``False``\n        \"\"\"\n        if self._globals.frame_index == -1:\n            return False\n        display_dims = self._globals.current_frame[\"display_dims\"]\n        if (self._canvas.offset[0] <= event.x <= display_dims[0] + self._canvas.offset[0] and\n                self._canvas.offset[1] <= event.y <= display_dims[1] + self._canvas.offset[1]):\n            self._canvas.config(cursor=\"plus\")\n            self._mouse_location = (\"image\", )\n            return True\n        return False\n\n    # Mouse Actions\n    def set_mouse_click_actions(self):\n        \"\"\" Add context menu to OS specific right click action. \"\"\"\n        super().set_mouse_click_actions()\n        self._canvas.bind(\"<Button-2>\" if platform.system() == \"Darwin\" else \"<Button-3>\",\n                          self._context_menu)\n\n    def _drag_start(self, event):\n        \"\"\" The action to perform when the user starts clicking and dragging the mouse.\n\n        If :attr:`_mouse_location` indicates a corner anchor, then the bounding box is resized\n        based on the adjusted corner, and the alignments re-generated.\n\n        If :attr:`_mouse_location` indicates a bounding box, then the bounding box is moved, and\n        the alignments re-generated.\n\n        If :attr:`_mouse_location` indicates being over the main image, then a new bounding box is\n        created, and alignments generated.\n\n        Parameters\n        ----------\n        event: :class:`tkinter.Event`\n            The tkinter mouse event.\n        \"\"\"\n        if self._mouse_location is None:\n            self._drag_data = dict()\n            self._drag_callback = None\n            return\n        if self._mouse_location[0] == \"anchor\":\n            corner_idx = int(self._mouse_location[1].split(\"_\")[-1])\n            self._drag_data[\"corner\"] = self._corner_order[corner_idx]\n            self._drag_callback = self._resize\n        elif self._mouse_location[0] == \"box\":\n            self._drag_data[\"current_location\"] = (event.x, event.y)\n            self._drag_callback = self._move\n        elif self._mouse_location[0] == \"image\":\n            self._create_new_bounding_box(event)\n            # Refresh cursor and _mouse_location for new bounding box and reset _drag_start\n            self._update_cursor(event)\n            self._drag_start(event)\n\n    def _drag_stop(self, event):  # pylint:disable=unused-argument\n        \"\"\" Trigger a viewport thumbnail update on click + drag release\n\n        Parameters\n        ----------\n        event: :class:`tkinter.Event`\n            The tkinter mouse event. Required but unused.\n        \"\"\"\n        if self._mouse_location is None:\n            return\n        face_idx = int(self._mouse_location[1].split(\"_\")[0])\n        self._det_faces.update.post_edit_trigger(self._globals.frame_index, face_idx)\n\n    def _create_new_bounding_box(self, event):\n        \"\"\" Create a new bounding box when user clicks on image, outside of existing boxes.\n\n        The bounding box is created as a square located around the click location, with dimensions\n        1 quarter the size of the frame's shortest side\n\n        Parameters\n        ----------\n        event: :class:`tkinter.Event`\n            The tkinter mouse event\n        \"\"\"\n        size = min(self._globals.current_frame[\"display_dims\"]) // 8\n        box = (event.x - size, event.y - size, event.x + size, event.y + size)\n        logger.debug(\"Creating new bounding box: %s \", box)\n        self._det_faces.update.add(self._globals.frame_index, *self._coords_to_bounding_box(box))\n\n    def _resize(self, event):\n        \"\"\" Resizes a bounding box on a corner anchor drag event.\n\n        Parameters\n        ----------\n        event: :class:`tkinter.Event`\n            The tkinter mouse event.\n        \"\"\"\n        face_idx = int(self._mouse_location[1].split(\"_\")[0])\n        face_tag = \"bb_box_face_{}\".format(face_idx)\n        box = self._canvas.coords(face_tag)\n        logger.trace(\"Face Index: %s, Corner Index: %s. Original ROI: %s\",\n                     face_idx, self._drag_data[\"corner\"], box)\n        # Switch top/bottom and left/right and set partial so indices match and we don't\n        # need branching logic for min/max.\n        limits = (partial(min, box[2] - 20),\n                  partial(min, box[3] - 20),\n                  partial(max, box[0] + 20),\n                  partial(max, box[1] + 20))\n        rect_xy_indices = [(\"left\", \"top\", \"right\", \"bottom\").index(pnt)\n                           for pnt in self._drag_data[\"corner\"]]\n        box[rect_xy_indices[1]] = limits[rect_xy_indices[1]](event.x)\n        box[rect_xy_indices[0]] = limits[rect_xy_indices[0]](event.y)\n        logger.trace(\"New ROI: %s\", box)\n        self._det_faces.update.bounding_box(self._globals.frame_index,\n                                            face_idx,\n                                            *self._coords_to_bounding_box(box),\n                                            aligner=self._tk_aligner.get())\n\n    def _move(self, event):\n        \"\"\" Moves the bounding box on a bounding box drag event.\n\n        Parameters\n        ----------\n        event: :class:`tkinter.Event`\n            The tkinter mouse event.\n        \"\"\"\n        logger.trace(\"event: %s, mouse_location: %s\", event, self._mouse_location)\n        face_idx = int(self._mouse_location[1])\n        shift = (event.x - self._drag_data[\"current_location\"][0],\n                 event.y - self._drag_data[\"current_location\"][1])\n        face_tag = \"bb_box_face_{}\".format(face_idx)\n        coords = np.array(self._canvas.coords(face_tag)) + (*shift, *shift)\n        logger.trace(\"face_tag: %s, shift: %s, new co-ords: %s\", face_tag, shift, coords)\n        self._det_faces.update.bounding_box(self._globals.frame_index,\n                                            face_idx,\n                                            *self._coords_to_bounding_box(coords),\n                                            aligner=self._tk_aligner.get())\n        self._drag_data[\"current_location\"] = (event.x, event.y)\n\n    def _coords_to_bounding_box(self, coords):\n        \"\"\" Converts tkinter coordinates to :class:`lib.align.DetectedFace` bounding\n        box format, scaled up and offset for feeding the model.\n\n        Returns\n        -------\n        tuple\n            The (`x`, `width`, `y`, `height`) integer points of the bounding box.\n        \"\"\"\n        logger.trace(\"in: %s\", coords)\n        coords = self.scale_from_display(\n            np.array(coords).reshape((2, 2))).flatten().astype(\"int32\")\n        logger.trace(\"out: %s\", coords)\n        return (coords[0], coords[2] - coords[0], coords[1], coords[3] - coords[1])\n\n    def _context_menu(self, event):\n        \"\"\" Create a right click context menu to delete the alignment that is being\n        hovered over. \"\"\"\n        if self._mouse_location is None or self._mouse_location[0] != \"box\":\n            return\n        self._right_click_menu.popup(event)\n\n    def _delete_current_face(self, *args):  # pylint:disable=unused-argument\n        \"\"\" Called by the right click delete event. Deletes the face that the mouse is currently\n        over.\n\n        Parameters\n        ----------\n        args: tuple (unused)\n            The event parameter is passed in by the hot key binding, so args is required\n        \"\"\"\n        if self._mouse_location is None or self._mouse_location[0] != \"box\":\n            logger.debug(\"Delete called without valid location. _mouse_location: %s\",\n                         self._mouse_location)\n            return\n        logger.debug(\"Deleting face. _mouse_location: %s\", self._mouse_location)\n        self._det_faces.update.delete(self._globals.frame_index, int(self._mouse_location[1]))\n", "tools/preview/control_panels.py": "#!/usr/bin/env python3\n\"\"\" Manages the widgets that hold the bottom 'control' area of the preview tool \"\"\"\nfrom __future__ import annotations\nimport gettext\nimport logging\nimport typing as T\n\nimport tkinter as tk\n\nfrom tkinter import ttk\nfrom configparser import ConfigParser\n\nfrom lib.gui.custom_widgets import Tooltip\nfrom lib.gui.control_helper import ControlPanel, ControlPanelOption\nfrom lib.gui.utils import get_images\nfrom plugins.plugin_loader import PluginLoader\nfrom plugins.convert._config import Config\n\nif T.TYPE_CHECKING:\n    from collections.abc import Callable\n    from .preview import Preview\n\nlogger = logging.getLogger(__name__)\n\n# LOCALES\n_LANG = gettext.translation(\"tools.preview\", localedir=\"locales\", fallback=True)\n_ = _LANG.gettext\n\n\nclass ConfigTools():\n    \"\"\" Tools for loading, saving, setting and retrieving configuration file values.\n\n    Attributes\n    ----------\n    tk_vars: dict\n        Global tkinter variables. `Refresh` and `Busy` :class:`tkinter.BooleanVar`\n    \"\"\"\n    def __init__(self) -> None:\n        self._config = Config(None)\n        self.tk_vars: dict[str, dict[str, tk.BooleanVar | tk.StringVar | tk.IntVar | tk.DoubleVar]\n                           ] = {}\n        self._config_dicts = self._get_config_dicts()  # Holds currently saved config\n\n    @property\n    def config(self) -> Config:\n        \"\"\" :class:`plugins.convert._config.Config` The convert configuration \"\"\"\n        return self._config\n\n    @property\n    def config_dicts(self) -> dict[str, T.Any]:\n        \"\"\" dict: The convert configuration options in dictionary form.\"\"\"\n        return self._config_dicts\n\n    @property\n    def sections(self) -> list[str]:\n        \"\"\" list: The sorted section names that exist within the convert Configuration options. \"\"\"\n        return sorted(set(plugin.split(\".\")[0] for plugin in self._config.config.sections()\n                          if plugin.split(\".\")[0] != \"writer\"))\n\n    @property\n    def plugins_dict(self) -> dict[str, list[str]]:\n        \"\"\" dict: Dictionary of configuration option sections as key with a list of containing\n        plugins as the value \"\"\"\n        return {section: sorted([plugin.split(\".\")[1] for plugin in self._config.config.sections()\n                                 if plugin.split(\".\")[0] == section])\n                for section in self.sections}\n\n    def update_config(self) -> None:\n        \"\"\" Update :attr:`config` with the currently selected values from the GUI. \"\"\"\n        for section, items in self.tk_vars.items():\n            for item, value in items.items():\n                try:\n                    new_value = str(value.get())\n                except tk.TclError as err:\n                    # When manually filling in text fields, blank values will\n                    # raise an error on numeric data types so return 0\n                    logger.debug(\"Error getting value. Defaulting to 0. Error: %s\", str(err))\n                    new_value = str(0)\n                old_value = self._config.config[section][item]\n                if new_value != old_value:\n                    logger.trace(\"Updating config: %s, %s from %s to %s\",  # type: ignore\n                                 section, item, old_value, new_value)\n                    self._config.config[section][item] = new_value\n\n    def _get_config_dicts(self) -> dict[str, dict[str, T.Any]]:\n        \"\"\" Obtain a custom configuration dictionary for convert configuration items in use\n        by the preview tool formatted for control helper.\n\n        Returns\n        -------\n        dict\n            Each configuration section as keys, with the values as a dict of option:\n            :class:`lib.gui.control_helper.ControlOption` pairs. \"\"\"\n        logger.debug(\"Formatting Config for GUI\")\n        config_dicts: dict[str, dict[str, T.Any]] = {}\n        for section in self._config.config.sections():\n            if section.startswith(\"writer.\"):\n                continue\n            for key, val in self._config.defaults[section].items.items():\n                if key == \"helptext\":\n                    config_dicts.setdefault(section, {})[key] = val\n                    continue\n                cp_option = ControlPanelOption(title=key,\n                                               dtype=val.datatype,\n                                               group=val.group,\n                                               default=val.default,\n                                               initial_value=self._config.get(section, key),\n                                               choices=val.choices,\n                                               is_radio=val.gui_radio,\n                                               rounding=val.rounding,\n                                               min_max=val.min_max,\n                                               helptext=val.helptext)\n                self.tk_vars.setdefault(section, {})[key] = cp_option.tk_var\n                config_dicts.setdefault(section, {})[key] = cp_option\n        logger.debug(\"Formatted Config for GUI: %s\", config_dicts)\n        return config_dicts\n\n    def reset_config_to_saved(self, section: str | None = None) -> None:\n        \"\"\" Reset the GUI parameters to their saved values within the configuration file.\n\n        Parameters\n        ----------\n        section: str, optional\n            The configuration section to reset the values for, If ``None`` provided then all\n            sections are reset. Default: ``None``\n        \"\"\"\n        logger.debug(\"Resetting to saved config: %s\", section)\n        sections = [section] if section is not None else list(self.tk_vars.keys())\n        for config_section in sections:\n            for item, options in self._config_dicts[config_section].items():\n                if item == \"helptext\":\n                    continue\n                val = options.value\n                if val != self.tk_vars[config_section][item].get():\n                    self.tk_vars[config_section][item].set(val)\n                    logger.debug(\"Setting %s - %s to saved value %s\", config_section, item, val)\n        logger.debug(\"Reset to saved config: %s\", section)\n\n    def reset_config_to_default(self, section: str | None = None) -> None:\n        \"\"\" Reset the GUI parameters to their default configuration values.\n\n        Parameters\n        ----------\n        section: str, optional\n            The configuration section to reset the values for, If ``None`` provided then all\n            sections are reset. Default: ``None``\n        \"\"\"\n        logger.debug(\"Resetting to default: %s\", section)\n        sections = [section] if section is not None else list(self.tk_vars.keys())\n        for config_section in sections:\n            for item, options in self._config_dicts[config_section].items():\n                if item == \"helptext\":\n                    continue\n                default = options.default\n                if default != self.tk_vars[config_section][item].get():\n                    self.tk_vars[config_section][item].set(default)\n                    logger.debug(\"Setting %s - %s to default value %s\",\n                                 config_section, item, default)\n        logger.debug(\"Reset to default: %s\", section)\n\n    def save_config(self, section: str | None = None) -> None:\n        \"\"\" Save the configuration ``.ini`` file with the currently stored values.\n\n        Notes\n        -----\n        We cannot edit the existing saved config as comments tend to get removed, so we create\n        a new config and populate that.\n\n        Parameters\n        ----------\n        section: str, optional\n            The configuration section to save, If ``None`` provided then all sections are saved.\n            Default: ``None``\n        \"\"\"\n        logger.debug(\"Saving %s config\", section)\n\n        new_config = ConfigParser(allow_no_value=True)\n\n        for section_name, sect in self._config.defaults.items():\n            logger.debug(\"Adding section: '%s')\", section_name)\n            self._config.insert_config_section(section_name,\n                                               sect.helptext,\n                                               config=new_config)\n            for item, options in sect.items.items():\n                if item == \"helptext\":\n                    continue  # helptext already written at top\n                if ((section is not None and section_name != section)\n                        or section_name not in self.tk_vars):\n                    # retain saved values that have not been updated\n                    new_opt = self._config.get(section_name, item)\n                    logger.debug(\"Retaining option: (item: '%s', value: '%s')\", item, new_opt)\n                else:\n                    new_opt = self.tk_vars[section_name][item].get()\n                    logger.debug(\"Setting option: (item: '%s', value: '%s')\", item, new_opt)\n\n                    # Set config_dicts value to new saved value\n                    self._config_dicts[section_name][item].set_initial_value(new_opt)\n\n                helptext = self._config.format_help(options.helptext, is_section=False)\n                new_config.set(section_name, helptext)\n                new_config.set(section_name, item, str(new_opt))\n\n        self._config.config = new_config\n        self._config.save_config()\n        logger.info(\"Saved config: '%s'\", self._config.configfile)\n\n\nclass BusyProgressBar():\n    \"\"\" An infinite progress bar for when a thread is running to swap/patch a group of samples \"\"\"\n    def __init__(self, parent: ttk.Frame) -> None:\n        self._progress_bar = self._add_busy_indicator(parent)\n\n    def _add_busy_indicator(self, parent: ttk.Frame) -> ttk.Progressbar:\n        \"\"\" Place progress bar into bottom bar to indicate when processing.\n\n        Parameters\n        ----------\n        parent: tkinter object\n            The tkinter object that holds the busy indicator\n\n        Returns\n        -------\n        ttk.Progressbar\n            A Progress bar to indicate that the Preview tool is busy\n        \"\"\"\n        logger.debug(\"Placing busy indicator\")\n        pbar = ttk.Progressbar(parent, mode=\"indeterminate\")\n        pbar.pack(side=tk.LEFT)\n        pbar.pack_forget()\n        return pbar\n\n    def stop(self) -> None:\n        \"\"\" Stop and hide progress bar \"\"\"\n        logger.debug(\"Stopping busy indicator\")\n        if not self._progress_bar.winfo_ismapped():\n            logger.debug(\"busy indicator already hidden\")\n            return\n        self._progress_bar.stop()\n        self._progress_bar.pack_forget()\n\n    def start(self) -> None:\n        \"\"\" Start and display progress bar \"\"\"\n        logger.debug(\"Starting busy indicator\")\n        if self._progress_bar.winfo_ismapped():\n            logger.debug(\"busy indicator already started\")\n            return\n\n        self._progress_bar.pack(side=tk.LEFT, padx=5, pady=(5, 10), fill=tk.X, expand=True)\n        self._progress_bar.start(25)\n\n\nclass ActionFrame(ttk.Frame):  # pylint:disable=too-many-ancestors\n    \"\"\" Frame that holds the left hand side options panel containing the command line options.\n\n    Parameters\n    ----------\n    app: :class:`Preview`\n        The main tkinter Preview app\n    parent: tkinter object\n        The parent tkinter object that holds the Action Frame\n    \"\"\"\n    def __init__(self, app: Preview, parent: ttk.Frame) -> None:\n        logger.debug(\"Initializing %s: (app: %s, parent: %s)\",\n                     self.__class__.__name__, app, parent)\n        self._app = app\n\n        super().__init__(parent)\n        self.pack(side=tk.LEFT, anchor=tk.N, fill=tk.Y)\n        self._tk_vars: dict[str, tk.StringVar] = {}\n\n        self._options = {\n            \"color\": app._patch.converter.cli_arguments.color_adjustment.replace(\"-\", \"_\"),\n            \"mask_type\": app._patch.converter.cli_arguments.mask_type.replace(\"-\", \"_\"),\n            \"face_scale\": app._patch.converter.cli_arguments.face_scale}\n        defaults = {opt: self._format_to_display(val) if opt != \"face_scale\" else val\n                    for opt, val in self._options.items()}\n        self._busy_bar = self._build_frame(defaults,\n                                           app._samples.generate,\n                                           app._refresh,\n                                           app._samples.available_masks,\n                                           app._samples.predictor.has_predicted_mask)\n\n    @property\n    def convert_args(self) -> dict[str, T.Any]:\n        \"\"\" dict: Currently selected Command line arguments from the :class:`ActionFrame`. \"\"\"\n        retval = {opt if opt != \"color\" else \"color_adjustment\":\n                  self._format_from_display(self._tk_vars[opt].get())\n                  for opt in self._options if opt != \"face_scale\"}\n        retval[\"face_scale\"] = self._tk_vars[\"face_scale\"].get()\n        return retval\n\n    @property\n    def busy_progress_bar(self) -> BusyProgressBar:\n        \"\"\" :class:`BusyProgressBar`: The progress bar that appears on the left hand side whilst a\n        swap/patch is being applied \"\"\"\n        return self._busy_bar\n\n    @staticmethod\n    def _format_from_display(var: str) -> str:\n        \"\"\" Format a variable from the display version to the command line action version.\n\n        Parameters\n        ----------\n        var: str\n            The variable name to format\n\n        Returns\n        -------\n        str\n            The formatted variable name\n        \"\"\"\n        return var.replace(\" \", \"_\").lower()\n\n    @staticmethod\n    def _format_to_display(var: str) -> str:\n        \"\"\" Format a variable from the command line action version to the display version.\n        Parameters\n        ----------\n        var: str\n            The variable name to format\n\n        Returns\n        -------\n        str\n            The formatted variable name\n        \"\"\"\n        return var.replace(\"_\", \" \").replace(\"-\", \" \").title()\n\n    def _build_frame(self,\n                     defaults: dict[str, T.Any],\n                     refresh_callback: Callable[[], None],\n                     patch_callback: Callable[[], None],\n                     available_masks: list[str],\n                     has_predicted_mask: bool) -> BusyProgressBar:\n        \"\"\" Build the :class:`ActionFrame`.\n\n        Parameters\n        ----------\n        defaults: dict\n            The default command line options\n        patch_callback: python function\n            The function to execute when a patch callback is received\n        refresh_callback: python function\n            The function to execute when a refresh callback is received\n        available_masks: list\n            The available masks that exist within the alignments file\n        has_predicted_mask: bool\n            Whether the model was trained with a mask\n\n        Returns\n        -------\n        ttk.Progressbar\n            A Progress bar to indicate that the Preview tool is busy\n        \"\"\"\n        logger.debug(\"Building Action frame\")\n\n        bottom_frame = ttk.Frame(self)\n        bottom_frame.pack(side=tk.BOTTOM, fill=tk.X, anchor=tk.S)\n        top_frame = ttk.Frame(self)\n        top_frame.pack(side=tk.TOP, fill=tk.BOTH, anchor=tk.N, expand=True)\n\n        self._add_cli_choices(top_frame, defaults, available_masks, has_predicted_mask)\n\n        busy_indicator = BusyProgressBar(bottom_frame)\n        self._add_refresh_button(bottom_frame, refresh_callback)\n        self._add_patch_callback(patch_callback)\n        self._add_actions(bottom_frame)\n        logger.debug(\"Built Action frame\")\n        return busy_indicator\n\n    def _add_cli_choices(self,\n                         parent: ttk.Frame,\n                         defaults: dict[str, T.Any],\n                         available_masks: list[str],\n                         has_predicted_mask: bool) -> None:\n        \"\"\" Create :class:`lib.gui.control_helper.ControlPanel` object for the command\n        line options.\n\n        parent: :class:`ttk.Frame`\n            The frame to hold the command line choices\n        defaults: dict\n            The default command line options\n        available_masks: list\n            The available masks that exist within the alignments file\n        has_predicted_mask: bool\n            Whether the model was trained with a mask\n        \"\"\"\n        cp_options = self._get_control_panel_options(defaults, available_masks, has_predicted_mask)\n        panel_kwargs = {\"blank_nones\": False, \"label_width\": 10, \"style\": \"CPanel\"}\n        ControlPanel(parent, cp_options, header_text=None, **panel_kwargs)\n\n    def _get_control_panel_options(self,\n                                   defaults: dict[str, T.Any],\n                                   available_masks: list[str],\n                                   has_predicted_mask: bool) -> list[ControlPanelOption]:\n        \"\"\" Create :class:`lib.gui.control_helper.ControlPanelOption` objects for the command\n        line options.\n\n        defaults: dict\n            The default command line options\n        available_masks: list\n            The available masks that exist within the alignments file\n        has_predicted_mask: bool\n            Whether the model was trained with a mask\n\n        Returns\n        -------\n        list\n            The list of `lib.gui.control_helper.ControlPanelOption` objects for the Action Frame\n        \"\"\"\n        cp_options: list[ControlPanelOption] = []\n        for opt in self._options:\n            if opt == \"face_scale\":\n                cp_option = ControlPanelOption(title=opt,\n                                               dtype=float,\n                                               default=0.0,\n                                               rounding=2,\n                                               min_max=(-10., 10.),\n                                               group=\"Command Line Choices\")\n            else:\n                if opt == \"mask_type\":\n                    choices = self._create_mask_choices(defaults,\n                                                        available_masks,\n                                                        has_predicted_mask)\n                else:\n                    choices = PluginLoader.get_available_convert_plugins(opt, True)\n                cp_option = ControlPanelOption(title=opt,\n                                               dtype=str,\n                                               default=defaults[opt],\n                                               initial_value=defaults[opt],\n                                               choices=choices,\n                                               group=\"Command Line Choices\",\n                                               is_radio=False)\n            self._tk_vars[opt] = cp_option.tk_var\n            cp_options.append(cp_option)\n        return cp_options\n\n    def _create_mask_choices(self,\n                             defaults: dict[str, T.Any],\n                             available_masks: list[str],\n                             has_predicted_mask: bool) -> list[str]:\n        \"\"\" Set the mask choices and default mask based on available masks.\n\n        Parameters\n        ----------\n        defaults: dict\n            The default command line options\n        available_masks: list\n            The available masks that exist within the alignments file\n        has_predicted_mask: bool\n            Whether the model was trained with a mask\n\n        Returns\n        -------\n        list\n            The masks that are available to use from the alignments file\n        \"\"\"\n        logger.debug(\"Initial mask choices: %s\", available_masks)\n        if has_predicted_mask:\n            available_masks += [\"predicted\"]\n        if \"none\" not in available_masks:\n            available_masks += [\"none\"]\n        if self._format_from_display(defaults[\"mask_type\"]) not in available_masks:\n            logger.debug(\"Setting default mask to first available: %s\", available_masks[0])\n            defaults[\"mask_type\"] = available_masks[0]\n        logger.debug(\"Final mask choices: %s\", available_masks)\n        return available_masks\n\n    @classmethod\n    def _add_refresh_button(cls,\n                            parent: ttk.Frame,\n                            refresh_callback: Callable[[], None]) -> None:\n        \"\"\" Add a button to refresh the images.\n\n        Parameters\n        ----------\n        refresh_callback: python function\n            The function to execute when the refresh button is pressed\n        \"\"\"\n        btn = ttk.Button(parent, text=\"Update Samples\", command=refresh_callback)\n        btn.pack(padx=5, pady=5, side=tk.TOP, fill=tk.X, anchor=tk.N)\n\n    def _add_patch_callback(self, patch_callback: Callable[[], None]) -> None:\n        \"\"\" Add callback to re-patch images on action option change.\n\n        Parameters\n        ----------\n        patch_callback: python function\n            The function to execute when the images require patching\n        \"\"\"\n        for tk_var in self._tk_vars.values():\n            tk_var.trace(\"w\", patch_callback)\n\n    def _add_actions(self, parent: ttk.Frame) -> None:\n        \"\"\" Add Action Buttons to the :class:`ActionFrame`\n\n        Parameters\n        ----------\n        parent: tkinter object\n            The tkinter object that holds the action buttons\n        \"\"\"\n        logger.debug(\"Adding util buttons\")\n        frame = ttk.Frame(parent)\n        frame.pack(padx=5, pady=(5, 10), side=tk.RIGHT, fill=tk.X, anchor=tk.E)\n\n        for utl in (\"save\", \"clear\", \"reload\"):\n            logger.debug(\"Adding button: '%s'\", utl)\n            img = get_images().icons[utl]\n            if utl == \"save\":\n                text = _(\"Save full config\")\n                action = self._app.config_tools.save_config\n            elif utl == \"clear\":\n                text = _(\"Reset full config to default values\")\n                action = self._app.config_tools.reset_config_to_default\n            elif utl == \"reload\":\n                text = _(\"Reset full config to saved values\")\n                action = self._app.config_tools.reset_config_to_saved\n\n            btnutl = ttk.Button(frame,\n                                image=img,\n                                command=action)\n            btnutl.pack(padx=2, side=tk.RIGHT)\n            Tooltip(btnutl, text=text, wrap_length=200)\n        logger.debug(\"Added util buttons\")\n\n\nclass OptionsBook(ttk.Notebook):  # pylint:disable=too-many-ancestors\n    \"\"\" The notebook that holds the Convert configuration options.\n\n    Parameters\n    ----------\n    parent: tkinter object\n        The parent tkinter object that holds the Options book\n    config_tools: :class:`ConfigTools`\n        Tools for loading and saving configuration files\n    patch_callback: python function\n        The function to execute when a patch callback is received\n\n    Attributes\n    ----------\n    config_tools: :class:`ConfigTools`\n        Tools for loading and saving configuration files\n    \"\"\"\n    def __init__(self,\n                 parent: ttk.Frame,\n                 config_tools: ConfigTools,\n                 patch_callback: Callable[[], None]) -> None:\n        logger.debug(\"Initializing %s: (parent: %s, config: %s)\",\n                     self.__class__.__name__, parent, config_tools)\n        super().__init__(parent)\n        self.pack(side=tk.RIGHT, anchor=tk.N, fill=tk.BOTH, expand=True)\n        self.config_tools = config_tools\n\n        self._tabs: dict[str, dict[str, ttk.Notebook | ConfigFrame]] = {}\n        self._build_tabs()\n        self._build_sub_tabs()\n        self._add_patch_callback(patch_callback)\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    def _build_tabs(self) -> None:\n        \"\"\" Build the notebook tabs for the each configuration section. \"\"\"\n        logger.debug(\"Build Tabs\")\n        for section in self.config_tools.sections:\n            tab = ttk.Notebook(self)\n            self._tabs[section] = {\"tab\": tab}\n            self.add(tab, text=section.replace(\"_\", \" \").title())\n\n    def _build_sub_tabs(self) -> None:\n        \"\"\" Build the notebook sub tabs for each convert section's plugin. \"\"\"\n        for section, plugins in self.config_tools.plugins_dict.items():\n            for plugin in plugins:\n                config_key = \".\".join((section, plugin))\n                config_dict = self.config_tools.config_dicts[config_key]\n                tab = ConfigFrame(self, config_key, config_dict)\n                self._tabs[section][plugin] = tab\n                text = plugin.replace(\"_\", \" \").title()\n                T.cast(ttk.Notebook, self._tabs[section][\"tab\"]).add(tab, text=text)\n\n    def _add_patch_callback(self, patch_callback: Callable[[], None]) -> None:\n        \"\"\" Add callback to re-patch images on configuration option change.\n\n        Parameters\n        ----------\n        patch_callback: python function\n            The function to execute when the images require patching\n        \"\"\"\n        for plugins in self.config_tools.tk_vars.values():\n            for tk_var in plugins.values():\n                tk_var.trace(\"w\", patch_callback)\n\n\nclass ConfigFrame(ttk.Frame):  # pylint:disable=too-many-ancestors\n    \"\"\" Holds the configuration options for a convert plugin inside the :class:`OptionsBook`.\n\n    Parameters\n    ----------\n    parent: tkinter object\n        The tkinter object that will hold this configuration frame\n    config_key: str\n        The section/plugin key for these configuration options\n    options: dict\n        The options for this section/plugin\n    \"\"\"\n\n    def __init__(self,\n                 parent: OptionsBook,\n                 config_key: str,\n                 options: dict[str, T.Any]):\n        logger.debug(\"Initializing %s\", self.__class__.__name__)\n        super().__init__(parent)\n        self.pack(side=tk.TOP, fill=tk.BOTH, expand=True)\n\n        self._options = options\n\n        self._action_frame = ttk.Frame(self)\n        self._action_frame.pack(padx=0, pady=(0, 5), side=tk.BOTTOM, fill=tk.X, anchor=tk.E)\n        self._add_frame_separator()\n\n        self._build_frame(parent, config_key)\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    def _build_frame(self, parent: OptionsBook, config_key: str) -> None:\n        \"\"\" Build the options frame for this command\n\n        Parameters\n        ----------\n        parent: tkinter object\n            The tkinter object that will hold this configuration frame\n        config_key: str\n            The section/plugin key for these configuration options\n        \"\"\"\n        logger.debug(\"Add Config Frame\")\n        panel_kwargs = {\"columns\": 2, \"option_columns\": 2, \"blank_nones\": False, \"style\": \"CPanel\"}\n        frame = ttk.Frame(self)\n        frame.pack(side=tk.TOP, fill=tk.BOTH, expand=True)\n        cp_options = [opt for key, opt in self._options.items() if key != \"helptext\"]\n        ControlPanel(frame, cp_options, header_text=None, **panel_kwargs)\n        self._add_actions(parent, config_key)\n        logger.debug(\"Added Config Frame\")\n\n    def _add_frame_separator(self) -> None:\n        \"\"\" Add a separator between top and bottom frames. \"\"\"\n        logger.debug(\"Add frame seperator\")\n        sep = ttk.Frame(self._action_frame, height=2, relief=tk.RIDGE)\n        sep.pack(fill=tk.X, pady=5, side=tk.TOP)\n        logger.debug(\"Added frame seperator\")\n\n    def _add_actions(self, parent: OptionsBook, config_key: str) -> None:\n        \"\"\" Add Action Buttons.\n\n        Parameters\n        ----------\n        parent: tkinter object\n            The tkinter object that will hold this configuration frame\n        config_key: str\n            The section/plugin key for these configuration options\n        \"\"\"\n        logger.debug(\"Adding util buttons\")\n\n        title = config_key.split(\".\")[1].replace(\"_\", \" \").title()\n        btn_frame = ttk.Frame(self._action_frame)\n        btn_frame.pack(padx=5, side=tk.BOTTOM, fill=tk.X)\n        for utl in (\"save\", \"clear\", \"reload\"):\n            logger.debug(\"Adding button: '%s'\", utl)\n            img = get_images().icons[utl]\n            if utl == \"save\":\n                text = _(f\"Save {title} config\")\n                action = parent.config_tools.save_config\n            elif utl == \"clear\":\n                text = _(f\"Reset {title} config to default values\")\n                action = parent.config_tools.reset_config_to_default\n            elif utl == \"reload\":\n                text = _(f\"Reset {title} config to saved values\")\n                action = parent.config_tools.reset_config_to_saved\n\n            btnutl = ttk.Button(btn_frame,\n                                image=img,\n                                command=lambda cmd=action: cmd(config_key))  # type: ignore\n            btnutl.pack(padx=2, side=tk.RIGHT)\n            Tooltip(btnutl, text=text, wrap_length=200)\n        logger.debug(\"Added util buttons\")\n", "tools/preview/viewer.py": "#!/usr/bin/env python3\n\"\"\" Manages the widgets that hold the top 'viewer' area of the preview tool \"\"\"\nfrom __future__ import annotations\nimport logging\nimport os\nimport tkinter as tk\nimport typing as T\n\nfrom tkinter import ttk\nfrom dataclasses import dataclass, field\n\nimport cv2\nimport numpy as np\nfrom PIL import Image, ImageTk\n\nfrom lib.align import transform_image\nfrom lib.align.aligned_face import CenteringType\nfrom scripts.convert import ConvertItem\n\n\nif T.TYPE_CHECKING:\n    from .preview import Preview\n\nlogger = logging.getLogger(__name__)\n\n\n@dataclass\nclass _Faces:\n    \"\"\" Dataclass for holding faces \"\"\"\n    filenames: list[str] = field(default_factory=list)\n    matrix: list[np.ndarray] = field(default_factory=list)\n    src: list[np.ndarray] = field(default_factory=list)\n    dst: list[np.ndarray] = field(default_factory=list)\n\n\nclass FacesDisplay():\n    \"\"\" Compiles the 2 rows of sample faces (original and swapped) into a single image\n\n    Parameters\n    ----------\n    app: :class:`Preview`\n        The main tkinter Preview app\n    size: int\n        The size of each individual face sample in pixels\n    padding: int\n        The amount of extra padding to apply to the outside of the face\n\n    Attributes\n    ----------\n    update_source: bool\n        Flag to indicate that the source images for the preview have been updated, so the preview\n        should be recompiled.\n    source: list\n        The list of :class:`numpy.ndarray` source preview images for top row of display\n    destination: list\n        The list of :class:`numpy.ndarray` swapped and patched preview images for bottom row of\n        display\n    \"\"\"\n    def __init__(self, app: Preview, size: int, padding: int) -> None:\n        logger.trace(\"Initializing %s: (app: %s, size: %s, padding: %s)\",  # type: ignore\n                     self.__class__.__name__, app, size, padding)\n        self._size = size\n        self._display_dims = (1, 1)\n        self._app = app\n        self._padding = padding\n\n        self._faces = _Faces()\n        self._centering: CenteringType | None = None\n        self._faces_source: np.ndarray = np.array([])\n        self._faces_dest: np.ndarray = np.array([])\n        self._tk_image: ImageTk.PhotoImage | None = None\n\n        # Set from Samples\n        self.update_source = False\n        self.source: list[ConvertItem] = []  # Source images, filenames + detected faces\n        # Set from Patch\n        self.destination: list[np.ndarray] = []  # Swapped + patched images\n\n        logger.trace(\"Initialized %s\", self.__class__.__name__)  # type: ignore\n\n    @property\n    def tk_image(self) -> ImageTk.PhotoImage | None:\n        \"\"\" :class:`PIL.ImageTk.PhotoImage`: The compiled preview display in tkinter display\n        format \"\"\"\n        return self._tk_image\n\n    @property\n    def _total_columns(self) -> int:\n        \"\"\" int: The total number of images that are being displayed \"\"\"\n        return len(self.source)\n\n    def set_centering(self, centering: CenteringType) -> None:\n        \"\"\" The centering that the model uses is not known at initialization time.\n        Set :attr:`_centering` when the model has been loaded.\n\n        Parameters\n        ----------\n        centering: str\n            The centering that the model was trained on\n        \"\"\"\n        self._centering = centering\n\n    def set_display_dimensions(self, dimensions: tuple[int, int]) -> None:\n        \"\"\" Adjust the size of the frame that will hold the preview samples.\n\n        Parameters\n        ----------\n        dimensions: tuple\n            The (`width`, `height`) of the frame that holds the preview\n        \"\"\"\n        self._display_dims = dimensions\n\n    def update_tk_image(self) -> None:\n        \"\"\" Build the full preview images and compile :attr:`tk_image` for display. \"\"\"\n        logger.trace(\"Updating tk image\")  # type: ignore\n        self._build_faces_image()\n        img = np.vstack((self._faces_source, self._faces_dest))\n        size = self._get_scale_size(img)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        pilimg = Image.fromarray(img)\n        pilimg = pilimg.resize(size, Image.ANTIALIAS)\n        self._tk_image = ImageTk.PhotoImage(pilimg)\n        logger.trace(\"Updated tk image\")  # type: ignore\n\n    def _get_scale_size(self, image: np.ndarray) -> tuple[int, int]:\n        \"\"\" Get the size that the full preview image should be resized to fit in the\n        display window.\n\n        Parameters\n        ----------\n        image: :class:`numpy.ndarray`\n            The full sized compiled preview image\n\n        Returns\n        -------\n        tuple\n            The (`width`, `height`) that the display image should be sized to fit in the display\n            window\n        \"\"\"\n        frameratio = float(self._display_dims[0]) / float(self._display_dims[1])\n        imgratio = float(image.shape[1]) / float(image.shape[0])\n\n        if frameratio <= imgratio:\n            scale = self._display_dims[0] / float(image.shape[1])\n            size = (self._display_dims[0], max(1, int(image.shape[0] * scale)))\n        else:\n            scale = self._display_dims[1] / float(image.shape[0])\n            size = (max(1, int(image.shape[1] * scale)), self._display_dims[1])\n        logger.trace(\"scale: %s, size: %s\", scale, size)  # type: ignore\n        return size\n\n    def _build_faces_image(self) -> None:\n        \"\"\" Compile the source and destination rows of the preview image. \"\"\"\n        logger.trace(\"Building Faces Image\")  # type: ignore\n        update_all = self.update_source\n        self._faces_from_frames()\n        if update_all:\n            header = self._header_text()\n            source = np.hstack([self._draw_rect(face) for face in self._faces.src])\n            self._faces_source = np.vstack((header, source))\n        self._faces_dest = np.hstack([self._draw_rect(face) for face in self._faces.dst])\n        logger.debug(\"source row shape: %s, swapped row shape: %s\",\n                     self._faces_dest.shape, self._faces_source.shape)\n\n    def _faces_from_frames(self) -> None:\n        \"\"\" Extract the preview faces from the source frames and apply the requisite padding. \"\"\"\n        logger.debug(\"Extracting faces from frames: Number images: %s\", len(self.source))\n        if self.update_source:\n            self._crop_source_faces()\n        self._crop_destination_faces()\n        logger.debug(\"Extracted faces from frames: %s\",\n                     {k: len(v) for k, v in self._faces.__dict__.items()})\n\n    def _crop_source_faces(self) -> None:\n        \"\"\" Extract the source faces from the source frames, along with their filenames and the\n        transformation matrix used to extract the faces. \"\"\"\n        logger.debug(\"Updating source faces\")\n        self._faces = _Faces()  # Init new class\n        for item in self.source:\n            detected_face = item.inbound.detected_faces[0]\n            src_img = item.inbound.image\n            detected_face.load_aligned(src_img,\n                                       size=self._size,\n                                       centering=T.cast(CenteringType, self._centering))\n            matrix = detected_face.aligned.matrix\n            self._faces.filenames.append(os.path.splitext(item.inbound.filename)[0])\n            self._faces.matrix.append(matrix)\n            self._faces.src.append(transform_image(src_img, matrix, self._size, self._padding))\n        self.update_source = False\n        logger.debug(\"Updated source faces\")\n\n    def _crop_destination_faces(self) -> None:\n        \"\"\" Extract the swapped faces from the swapped frames using the source face destination\n        matrices. \"\"\"\n        logger.debug(\"Updating destination faces\")\n        self._faces.dst = []\n        destination = self.destination if self.destination else [np.ones_like(src.inbound.image)\n                                                                 for src in self.source]\n        for idx, image in enumerate(destination):\n            self._faces.dst.append(transform_image(image,\n                                                   self._faces.matrix[idx],\n                                                   self._size,\n                                                   self._padding))\n        logger.debug(\"Updated destination faces\")\n\n    def _header_text(self) -> np.ndarray:\n        \"\"\" Create the header text displaying the frame name for each preview column.\n\n        Returns\n        -------\n        :class:`numpy.ndarray`\n            The header row of the preview image containing the frame names for each column\n        \"\"\"\n        font_scale = self._size / 640\n        height = self._size // 8\n        font = cv2.FONT_HERSHEY_SIMPLEX\n        # Get size of placed text for positioning\n        text_sizes = [cv2.getTextSize(self._faces.filenames[idx],\n                                      font,\n                                      font_scale,\n                                      1)[0]\n                      for idx in range(self._total_columns)]\n        # Get X and Y co-ordinates for each text item\n        text_y = int((height + text_sizes[0][1]) / 2)\n        text_x = [int((self._size - text_sizes[idx][0]) / 2) + self._size * idx\n                  for idx in range(self._total_columns)]\n        logger.debug(\"filenames: %s, text_sizes: %s, text_x: %s, text_y: %s\",\n                     self._faces.filenames, text_sizes, text_x, text_y)\n        header_box = np.ones((height, self._size * self._total_columns, 3), np.uint8) * 255\n        for idx, text in enumerate(self._faces.filenames):\n            cv2.putText(header_box,\n                        text,\n                        (text_x[idx], text_y),\n                        font,\n                        font_scale,\n                        (0, 0, 0),\n                        1,\n                        lineType=cv2.LINE_AA)\n        logger.debug(\"header_box.shape: %s\", header_box.shape)\n        return header_box\n\n    def _draw_rect(self, image: np.ndarray) -> np.ndarray:\n        \"\"\" Place a white border around a given image.\n\n        Parameters\n        ----------\n        image: :class:`numpy.ndarray`\n            The image to place a border on to\n        Returns\n        -------\n        :class:`numpy.ndarray`\n            The given image with a border drawn around the outside\n        \"\"\"\n        cv2.rectangle(image, (0, 0), (self._size - 1, self._size - 1), (255, 255, 255), 1)\n        image = np.clip(image, 0.0, 255.0)\n        return image.astype(\"uint8\")\n\n\nclass ImagesCanvas(ttk.Frame):  # pylint:disable=too-many-ancestors\n    \"\"\" tkinter Canvas that holds the preview images.\n\n    Parameters\n    ----------\n    app: :class:`Preview`\n        The main tkinter Preview app\n    parent: tkinter object\n        The parent tkinter object that holds the canvas\n    \"\"\"\n    def __init__(self, app: Preview, parent: ttk.PanedWindow) -> None:\n        logger.debug(\"Initializing %s: (app: %s, parent: %s)\",\n                     self.__class__.__name__, app, parent)\n        super().__init__(parent)\n        self.pack(expand=True, fill=tk.BOTH, padx=2, pady=2)\n\n        self._display: FacesDisplay = parent.preview_display  # type: ignore\n        self._canvas = tk.Canvas(self, bd=0, highlightthickness=0)\n        self._canvas.pack(side=tk.TOP, fill=tk.BOTH, expand=True)\n        self._displaycanvas = self._canvas.create_image(0, 0,\n                                                        image=self._display.tk_image,\n                                                        anchor=tk.NW)\n        self.bind(\"<Configure>\", self._resize)\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    def _resize(self, event: tk.Event) -> None:\n        \"\"\" Resize the image to fit the frame, maintaining aspect ratio \"\"\"\n        logger.debug(\"Resizing preview image\")\n        framesize = (event.width, event.height)\n        self._display.set_display_dimensions(framesize)\n        self.reload()\n\n    def reload(self) -> None:\n        \"\"\" Update the images in the canvas and redraw \"\"\"\n        logger.debug(\"Reloading preview image\")\n        self._display.update_tk_image()\n        self._canvas.itemconfig(self._displaycanvas, image=self._display.tk_image)\n        logger.debug(\"Reloaded preview image\")\n", "tools/preview/cli.py": "#!/usr/bin/env python3\n\"\"\" Command Line Arguments for tools \"\"\"\nimport argparse\nimport gettext\nimport typing as T\n\nfrom lib.cli.args import FaceSwapArgs\nfrom lib.cli.actions import DirOrFileFullPaths, DirFullPaths, FileFullPaths\n\n# LOCALES\n_LANG = gettext.translation(\"tools.preview\", localedir=\"locales\", fallback=True)\n_ = _LANG.gettext\n\n\n_HELPTEXT = _(\"This command allows you to preview swaps to tweak convert settings.\")\n\n\nclass PreviewArgs(FaceSwapArgs):\n    \"\"\" Class to parse the command line arguments for Preview (Convert Settings) tool \"\"\"\n\n    @staticmethod\n    def get_info() -> str:\n        \"\"\" Return command information\n\n        Returns\n        -------\n        str\n            Top line information about the Preview tool\n        \"\"\"\n        return _(\"Preview tool\\nAllows you to configure your convert settings with a live preview\")\n\n    @staticmethod\n    def get_argument_list() -> list[dict[str, T.Any]]:\n        \"\"\" Put the arguments in a list so that they are accessible from both argparse and gui\n\n        Returns\n        -------\n        list[dict[str, Any]]\n            Top command line options for the preview tool\n        \"\"\"\n        argument_list = []\n        argument_list.append({\n            \"opts\": (\"-i\", \"--input-dir\"),\n            \"action\": DirOrFileFullPaths,\n            \"filetypes\": \"video\",\n            \"dest\": \"input_dir\",\n            \"group\": _(\"data\"),\n            \"required\": True,\n            \"help\": _(\n                \"Input directory or video. Either a directory containing the image files you wish \"\n                \"to process or path to a video file.\")})\n        argument_list.append({\n            \"opts\": (\"-a\", \"--alignments\"),\n            \"action\": FileFullPaths,\n            \"filetypes\": \"alignments\",\n            \"type\": str,\n            \"group\": _(\"data\"),\n            \"dest\": \"alignments_path\",\n            \"help\": _(\n                \"Path to the alignments file for the input, if not at the default location\")})\n        argument_list.append({\n            \"opts\": (\"-m\", \"--model-dir\"),\n            \"action\": DirFullPaths,\n            \"dest\": \"model_dir\",\n            \"group\": _(\"data\"),\n            \"required\": True,\n            \"help\": _(\n                \"Model directory. A directory containing the trained model you wish to process.\")})\n        argument_list.append({\n            \"opts\": (\"-s\", \"--swap-model\"),\n            \"action\": \"store_true\",\n            \"dest\": \"swap_model\",\n            \"default\": False,\n            \"help\": _(\"Swap the model. Instead of A -> B, swap B -> A\")})\n        # Deprecated multi-character switches\n        argument_list.append({\n            \"opts\": (\"-al\", ),\n            \"type\": str,\n            \"dest\": \"depr_alignments_al_a\",\n            \"help\": argparse.SUPPRESS})\n        return argument_list\n", "tools/preview/preview.py": "#!/usr/bin/env python3\n\"\"\" Tool to preview swaps and tweak configuration prior to running a convert \"\"\"\nfrom __future__ import annotations\nimport gettext\nimport logging\nimport random\nimport tkinter as tk\nimport typing as T\n\nfrom tkinter import ttk\nimport os\nimport sys\n\nfrom threading import Event, Lock, Thread\n\nimport numpy as np\n\nfrom lib.align import DetectedFace\nfrom lib.cli.args_extract_convert import ConvertArgs\nfrom lib.gui.utils import get_images, get_config, initialize_config, initialize_images\nfrom lib.convert import Converter\nfrom lib.utils import FaceswapError, handle_deprecated_cliopts\nfrom lib.queue_manager import queue_manager\nfrom scripts.fsmedia import Alignments, Images\nfrom scripts.convert import Predict, ConvertItem\n\nfrom plugins.extract import ExtractMedia\n\nfrom .control_panels import ActionFrame, ConfigTools, OptionsBook\nfrom .viewer import FacesDisplay, ImagesCanvas\n\nif T.TYPE_CHECKING:\n    from argparse import Namespace\n    from lib.queue_manager import EventQueue\n    from .control_panels import BusyProgressBar\n\nlogger = logging.getLogger(__name__)\n\n# LOCALES\n_LANG = gettext.translation(\"tools.preview\", localedir=\"locales\", fallback=True)\n_ = _LANG.gettext\n\n\nclass Preview(tk.Tk):\n    \"\"\" This tool is part of the Faceswap Tools suite and should be called from\n    ``python tools.py preview`` command.\n\n    Loads up 5 semi-random face swaps and displays them, cropped, in place in the final frame.\n    Allows user to live tweak settings, before saving the final config to\n    :file:`./config/convert.ini`\n\n    Parameters\n    ----------\n    arguments: :class:`argparse.Namespace`\n        The :mod:`argparse` arguments as passed in from :mod:`tools.py`\n    \"\"\"\n    _w: str\n\n    def __init__(self, arguments: Namespace) -> None:\n        logger.debug(\"Initializing %s: (arguments: '%s'\", self.__class__.__name__, arguments)\n        super().__init__()\n        arguments = handle_deprecated_cliopts(arguments)\n        self._config_tools = ConfigTools()\n        self._lock = Lock()\n        self._dispatcher = Dispatcher(self)\n        self._display = FacesDisplay(self, 256, 64)\n        self._samples = Samples(self, arguments, 5)\n        self._patch = Patch(self, arguments)\n\n        self._initialize_tkinter()\n        self._image_canvas: ImagesCanvas | None = None\n        self._opts_book: OptionsBook | None = None\n        self._cli_frame: ActionFrame | None = None  # cli frame holds cli options\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    @property\n    def config_tools(self) -> \"ConfigTools\":\n        \"\"\" :class:`ConfigTools`: The object responsible for parsing configuration options and\n        updating to/from the GUI \"\"\"\n        return self._config_tools\n\n    @property\n    def dispatcher(self) -> \"Dispatcher\":\n        \"\"\" :class:`Dispatcher`: The object responsible for triggering events and variables and\n        handling global GUI state \"\"\"\n        return self._dispatcher\n\n    @property\n    def display(self) -> FacesDisplay:\n        \"\"\" :class:`~tools.preview.viewer.FacesDisplay`: The object that holds the sample,\n        converted and patched faces \"\"\"\n        return self._display\n\n    @property\n    def lock(self) -> Lock:\n        \"\"\" :class:`threading.Lock`: The threading lock object for the Preview GUI \"\"\"\n        return self._lock\n\n    @property\n    def progress_bar(self) -> BusyProgressBar:\n        \"\"\" :class:`~tools.preview.control_panels.BusyProgressBar`: The progress bar that indicates\n        a swap/patch thread is running \"\"\"\n        assert self._cli_frame is not None\n        return self._cli_frame.busy_progress_bar\n\n    def update_display(self):\n        \"\"\" Update the images in the canvas and redraw \"\"\"\n        if not hasattr(self, \"_image_canvas\"):  # On first call object not yet created\n            return\n        assert self._image_canvas is not None\n        self._image_canvas.reload()\n\n    def _initialize_tkinter(self) -> None:\n        \"\"\" Initialize a standalone tkinter instance. \"\"\"\n        logger.debug(\"Initializing tkinter\")\n        initialize_config(self, None, None)\n        initialize_images()\n        get_config().set_geometry(940, 600, fullscreen=False)\n        self.title(\"Faceswap.py - Convert Settings\")\n        self.tk.call(\n            \"wm\",\n            \"iconphoto\",\n            self._w,\n            get_images().icons[\"favicon\"])  # pylint:disable=protected-access\n        logger.debug(\"Initialized tkinter\")\n\n    def process(self) -> None:\n        \"\"\" The entry point for the Preview tool from :file:`lib.tools.cli`.\n\n        Launch the tkinter preview Window and run main loop.\n        \"\"\"\n        self._build_ui()\n        self.mainloop()\n\n    def _refresh(self, *args) -> None:\n        \"\"\" Patch faces with current convert settings.\n\n        Parameters\n        ----------\n        *args: tuple\n            Unused, but required for tkinter callback.\n        \"\"\"\n        logger.debug(\"Patching swapped faces. args: %s\", args)\n        self._dispatcher.set_busy()\n        self._config_tools.update_config()\n        with self._lock:\n            assert self._cli_frame is not None\n            self._patch.converter_arguments = self._cli_frame.convert_args\n\n        self._dispatcher.set_needs_patch()\n        logger.debug(\"Patched swapped faces\")\n\n    def _build_ui(self) -> None:\n        \"\"\" Build the elements for displaying preview images and options panels. \"\"\"\n        container = ttk.PanedWindow(self,\n                                    orient=tk.VERTICAL)\n        container.pack(fill=tk.BOTH, expand=True)\n        setattr(container, \"preview_display\", self._display)  # TODO subclass not setattr\n        self._image_canvas = ImagesCanvas(self, container)\n        container.add(self._image_canvas, weight=3)\n\n        options_frame = ttk.Frame(container)\n        self._cli_frame = ActionFrame(self, options_frame)\n        self._opts_book = OptionsBook(options_frame,\n                                      self._config_tools,\n                                      self._refresh)\n        container.add(options_frame, weight=1)\n        self.update_idletasks()\n        container.sashpos(0, int(400 * get_config().scaling_factor))\n\n\nclass Dispatcher():\n    \"\"\" Handles the app level tk.Variables and the threading events. Dispatches events to the\n    correct location and handles GUI state whilst events are handled\n\n    Parameters\n    ----------\n    app: :class:`Preview`\n        The main tkinter Preview app\n    \"\"\"\n    def __init__(self, app: Preview):\n        logger.debug(\"Initializing %s: (app: %s)\", self.__class__.__name__, app)\n        self._app = app\n        self._tk_busy = tk.BooleanVar(value=False)\n        self._evnt_needs_patch = Event()\n        self._is_updating = False\n        self._stacked_event = False\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    @property\n    def needs_patch(self) -> Event:\n        \"\"\":class:`threading.Event`. Set by the parent and cleared by the child. Informs the child\n        patching thread that a run needs to be processed \"\"\"\n        return self._evnt_needs_patch\n\n    # TKInter Variables\n    def set_busy(self) -> None:\n        \"\"\" Set the tkinter busy variable to ``True`` and display the busy progress bar \"\"\"\n        if self._tk_busy.get():\n            logger.debug(\"Busy event is already set. Doing nothing\")\n            return\n        if not hasattr(self._app, \"progress_bar\"):\n            logger.debug(\"Not setting busy during initial startup\")\n            return\n\n        logger.debug(\"Setting busy event to True\")\n        self._tk_busy.set(True)\n        self._app.progress_bar.start()\n        self._app.update_idletasks()\n\n    def _unset_busy(self) -> None:\n        \"\"\" Set the tkinter busy variable to ``False`` and hide the busy progress bar \"\"\"\n        self._is_updating = False\n        if not self._tk_busy.get():\n            logger.debug(\"busy unset when already unset. Doing nothing\")\n            return\n        logger.debug(\"Setting busy event to False\")\n        self._tk_busy.set(False)\n        self._app.progress_bar.stop()\n        self._app.update_idletasks()\n\n    # Threading Events\n    def _wait_for_patch(self) -> None:\n        \"\"\" Wait for a patch thread to complete before triggering a display refresh and unsetting\n        the busy indicators \"\"\"\n        logger.debug(\"Checking for patch completion...\")\n        if self._evnt_needs_patch.is_set():\n            logger.debug(\"Samples not patched. Waiting...\")\n            self._app.after(1000, self._wait_for_patch)\n            return\n\n        logger.debug(\"Patch completion detected\")\n        self._app.update_display()\n        self._unset_busy()\n\n        if self._stacked_event:\n            logger.debug(\"Processing last stacked event\")\n            self.set_busy()\n            self._stacked_event = False\n            self.set_needs_patch()\n            return\n\n    def set_needs_patch(self) -> None:\n        \"\"\" Sends a trigger to the patching thread that it needs to be run. Waits for the patching\n        to complete prior to triggering a display refresh and unsetting the busy indicators \"\"\"\n        if self._is_updating:\n            logger.debug(\"Request to run patch when it is already running. Adding stacked event.\")\n            self._stacked_event = True\n            return\n        self._is_updating = True\n        logger.debug(\"Triggering patch\")\n        self._evnt_needs_patch.set()\n        self._wait_for_patch()\n\n\nclass Samples():\n    \"\"\" The display samples.\n\n    Obtains and holds :attr:`sample_size` semi random test faces for displaying in the\n    preview GUI.\n\n    The file list is split into evenly sized groups of :attr:`sample_size`. When a display set is\n    generated, a random image from each of the groups is selected to provide an array of images\n    across the length of the video.\n\n    Parameters\n    ----------\n    app: :class:`Preview`\n        The main tkinter Preview app\n    arguments: :class:`argparse.Namespace`\n        The :mod:`argparse` arguments as passed in from :mod:`tools.py`\n    sample_size: int\n        The number of samples to take from the input video/images\n    \"\"\"\n\n    def __init__(self, app: Preview, arguments: Namespace, sample_size: int) -> None:\n        logger.debug(\"Initializing %s: (app: %s, arguments: '%s', sample_size: %s)\",\n                     self.__class__.__name__, app, arguments, sample_size)\n        self._sample_size = sample_size\n        self._app = app\n        self._input_images: list[ConvertItem] = []\n        self._predicted_images: list[tuple[ConvertItem, np.ndarray]] = []\n\n        self._images = Images(arguments)\n        self._alignments = Alignments(arguments,\n                                      is_extract=False,\n                                      input_is_video=self._images.is_video)\n        if self._alignments.version == 1.0:\n            logger.error(\"The alignments file format has been updated since the given alignments \"\n                         \"file was generated. You need to update the file to proceed.\")\n            logger.error(\"To do this run the 'Alignments Tool' > 'Extract' Job.\")\n            sys.exit(1)\n\n        if not self._alignments.have_alignments_file:\n            logger.error(\"Alignments file not found at: '%s'\", self._alignments.file)\n            sys.exit(1)\n\n        if self._images.is_video:\n            assert isinstance(self._images.input_images, str)\n            self._alignments.update_legacy_has_source(os.path.basename(self._images.input_images))\n\n        self._filelist = self._get_filelist()\n        self._indices = self._get_indices()\n\n        self._predictor = Predict(self._sample_size, arguments)\n        self._predictor.launch(queue_manager.get_queue(\"preview_predict_in\"))\n        self._app._display.set_centering(self._predictor.centering)\n        self.generate()\n\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    @property\n    def available_masks(self) -> list[str]:\n        \"\"\" list: The mask names that are available for every face in the alignments file \"\"\"\n        retval = [key\n                  for key, val in self.alignments.mask_summary.items()\n                  if val == self.alignments.faces_count]\n        return retval\n\n    @property\n    def sample_size(self) -> int:\n        \"\"\" int: The number of samples to take from the input video/images \"\"\"\n        return self._sample_size\n\n    @property\n    def predicted_images(self) -> list[tuple[ConvertItem, np.ndarray]]:\n        \"\"\" list: The predicted faces output from the Faceswap model \"\"\"\n        return self._predicted_images\n\n    @property\n    def alignments(self) -> Alignments:\n        \"\"\" :class:`~lib.align.Alignments`: The alignments for the preview faces \"\"\"\n        return self._alignments\n\n    @property\n    def predictor(self) -> Predict:\n        \"\"\" :class:`~scripts.convert.Predict`: The Predictor for the Faceswap model \"\"\"\n        return self._predictor\n\n    @property\n    def _random_choice(self) -> list[int]:\n        \"\"\" list: Random indices from the :attr:`_indices` group \"\"\"\n        retval = [random.choice(indices) for indices in self._indices]\n        logger.debug(retval)\n        return retval\n\n    def _get_filelist(self) -> list[str]:\n        \"\"\" Get a list of files for the input, filtering out those frames which do\n        not contain faces.\n\n        Returns\n        -------\n        list\n            A list of filenames of frames that contain faces.\n        \"\"\"\n        logger.debug(\"Filtering file list to frames with faces\")\n        if isinstance(self._images.input_images, str):\n            vid_name, ext = os.path.splitext(self._images.input_images)\n            filelist = [f\"{vid_name}_{frame_no:06d}{ext}\"\n                        for frame_no in range(1, self._images.images_found + 1)]\n        else:\n            filelist = self._images.input_images\n\n        retval = [filename for filename in filelist\n                  if self._alignments.frame_has_faces(os.path.basename(filename))]\n        logger.debug(\"Filtered out frames: %s\", self._images.images_found - len(retval))\n        try:\n            assert retval\n        except AssertionError as err:\n            msg = (\"No faces were found in any of the frames passed in. Make sure you are passing \"\n                   \"in a frames source rather than extracted faces, and that you have provided \"\n                   \"the correct alignments file.\")\n            raise FaceswapError(msg) from err\n        return retval\n\n    def _get_indices(self) -> list[list[int]]:\n        \"\"\" Get indices for each sample group.\n\n        Obtain :attr:`self.sample_size` evenly sized groups of indices\n        pertaining to the filtered :attr:`self._file_list`\n\n        Returns\n        -------\n        list\n            list of indices relating to the filtered file list, split into groups\n        \"\"\"\n        # Remove start and end values to get a list divisible by self.sample_size\n        no_files = len(self._filelist)\n        self._sample_size = min(self._sample_size, no_files)\n        crop = no_files % self._sample_size\n        top_tail = list(range(no_files))[\n            crop // 2:no_files - (crop - (crop // 2))]\n        # Partition the indices\n        size = len(top_tail)\n        retval = [top_tail[start:start + size // self._sample_size]\n                  for start in range(0, size, size // self._sample_size)]\n        logger.debug(\"Indices pools: %s\", [f\"{idx}: (start: {min(pool)}, \"\n                                           f\"end: {max(pool)}, size: {len(pool)})\"\n                                           for idx, pool in enumerate(retval)])\n        return retval\n\n    def generate(self) -> None:\n        \"\"\" Generate a sample set.\n\n        Selects :attr:`sample_size` random faces. Runs them through prediction to obtain the\n        swap, then trigger the patch event to run the faces through patching.\n        \"\"\"\n        logger.debug(\"Generating new random samples\")\n        self._app.dispatcher.set_busy()\n        self._load_frames()\n        self._predict()\n        self._app.dispatcher.set_needs_patch()\n        logger.debug(\"Generated new random samples\")\n\n    def _load_frames(self) -> None:\n        \"\"\" Load a sample of random frames.\n\n        * Picks a random face from each indices group.\n\n        * Takes the first face from the image (if there are multiple faces). Adds the images to \\\n        :attr:`self._input_images`.\n\n        * Sets :attr:`_display.source` to the input images and flags that the display should be \\\n        updated\n        \"\"\"\n        self._input_images = []\n        for selection in self._random_choice:\n            filename = os.path.basename(self._filelist[selection])\n            image = self._images.load_one_image(self._filelist[selection])\n            # Get first face only\n            face = self._alignments.get_faces_in_frame(filename)[0]\n            detected_face = DetectedFace()\n            detected_face.from_alignment(face, image=image)\n            inbound = ExtractMedia(filename=filename, image=image, detected_faces=[detected_face])\n            self._input_images.append(ConvertItem(inbound=inbound))\n        self._app.display.source = self._input_images\n        self._app.display.update_source = True\n        logger.debug(\"Selected frames: %s\",\n                     [frame.inbound.filename for frame in self._input_images])\n\n    def _predict(self) -> None:\n        \"\"\" Predict from the loaded frames.\n\n        With a threading lock (to prevent stacking), run the selected faces through the Faceswap\n        model predict function and add the output to :attr:`predicted`\n        \"\"\"\n        with self._app.lock:\n            self._predicted_images = []\n            for frame in self._input_images:\n                self._predictor.in_queue.put(frame)\n            idx = 0\n            while idx < self._sample_size:\n                logger.debug(\"Predicting face %s of %s\", idx + 1, self._sample_size)\n                items: (T.Literal[\"EOF\"] |\n                        list[tuple[ConvertItem, np.ndarray]]) = self._predictor.out_queue.get()\n                if items == \"EOF\":\n                    logger.debug(\"Received EOF\")\n                    break\n                for item in items:\n                    self._predicted_images.append(item)\n                    logger.debug(\"Predicted face %s of %s\", idx + 1, self._sample_size)\n                    idx += 1\n        logger.debug(\"Predicted faces\")\n\n\nclass Patch():\n    \"\"\" The Patch pipeline\n\n    Runs in it's own thread. Takes the output from the Faceswap model predictor and runs the faces\n    through the convert pipeline using the currently selected options.\n\n    Parameters\n    ----------\n    app: :class:`Preview`\n        The main tkinter Preview app\n    arguments: :class:`argparse.Namespace`\n        The :mod:`argparse` arguments as passed in from :mod:`tools.py`\n\n    Attributes\n    ----------\n    converter_arguments: dict\n        The currently selected converter command line arguments for the patch queue\n    \"\"\"\n    def __init__(self, app: Preview, arguments: Namespace) -> None:\n        logger.debug(\"Initializing %s: (app: %s, arguments: '%s')\",\n                     self.__class__.__name__, app, arguments)\n        self._app = app\n        self._queue_patch_in = queue_manager.get_queue(\"preview_patch_in\")\n        self.converter_arguments: dict[str, T.Any] | None = None  # Updated converter args\n\n        configfile = arguments.configfile if hasattr(arguments, \"configfile\") else None\n        self._converter = Converter(output_size=app._samples.predictor.output_size,\n                                    coverage_ratio=app._samples.predictor.coverage_ratio,\n                                    centering=app._samples.predictor.centering,\n                                    draw_transparent=False,\n                                    pre_encode=None,\n                                    arguments=self._generate_converter_arguments(\n                                        arguments,\n                                        app._samples.available_masks),\n                                    configfile=configfile)\n        self._thread = Thread(target=self._process,\n                              name=\"patch_thread\",\n                              args=(self._queue_patch_in,\n                                    self._app.dispatcher.needs_patch,\n                                    app._samples),\n                              daemon=True)\n        self._thread.start()\n        logger.debug(\"Initializing %s\", self.__class__.__name__)\n\n    @property\n    def converter(self) -> Converter:\n        \"\"\" :class:`lib.convert.Converter`: The converter to use for patching the images. \"\"\"\n        return self._converter\n\n    @staticmethod\n    def _generate_converter_arguments(arguments: Namespace,\n                                      available_masks: list[str]) -> Namespace:\n        \"\"\" Add the default converter arguments to the initial arguments. Ensure the mask selection\n        is available.\n\n        Parameters\n        ----------\n        arguments: :class:`argparse.Namespace`\n            The :mod:`argparse` arguments as passed in from :mod:`tools.py`\n        available_masks: list\n            The masks that are available for convert\n        Returns\n        ----------\n        arguments: :class:`argparse.Namespace`\n            The :mod:`argparse` arguments as passed in with converter default\n            arguments added\n        \"\"\"\n        valid_masks = available_masks + [\"none\"]\n        converter_arguments = ConvertArgs(None, \"convert\").get_optional_arguments()  # type: ignore\n        for item in converter_arguments:\n            value = item.get(\"default\", None)\n            # Skip options without a default value\n            if value is None:\n                continue\n            option = item.get(\"dest\", item[\"opts\"][1].replace(\"--\", \"\"))\n            if option == \"mask_type\" and value not in valid_masks:\n                logger.debug(\"Amending default mask from '%s' to '%s'\", value, valid_masks[0])\n                value = valid_masks[0]\n            # Skip options already in arguments\n            if hasattr(arguments, option):\n                continue\n            # Add option to arguments\n            setattr(arguments, option, value)\n        logger.debug(arguments)\n        return arguments\n\n    def _process(self,\n                 patch_queue_in: EventQueue,\n                 trigger_event: Event,\n                 samples: Samples) -> None:\n        \"\"\" The face patching process.\n\n        Runs in a thread, and waits for an event to be set. Once triggered, runs a patching\n        cycle and sets the :class:`Display` destination images.\n\n        Parameters\n        ----------\n        patch_queue_in: :class:`~lib.queue_manager.EventQueue`\n            The input queue for the patching process\n        trigger_event: :class:`threading.Event`\n            The event that indicates a patching run needs to be processed\n        samples: :class:`Samples`\n            The Samples for display.\n        \"\"\"\n        logger.debug(\"Launching patch process thread: (patch_queue_in: %s, trigger_event: %s, \"\n                     \"samples: %s)\", patch_queue_in, trigger_event, samples)\n        patch_queue_out = queue_manager.get_queue(\"preview_patch_out\")\n        while True:\n            trigger = trigger_event.wait(1)\n            if not trigger:\n                continue\n            logger.debug(\"Patch Triggered\")\n            queue_manager.flush_queue(\"preview_patch_in\")\n            self._feed_swapped_faces(patch_queue_in, samples)\n            with self._app.lock:\n                self._update_converter_arguments()\n                self._converter.reinitialize(config=self._app.config_tools.config)\n            swapped = self._patch_faces(patch_queue_in, patch_queue_out, samples.sample_size)\n            with self._app.lock:\n                self._app.display.destination = swapped\n\n            logger.debug(\"Patch complete\")\n            trigger_event.clear()\n\n        logger.debug(\"Closed patch process thread\")\n\n    def _update_converter_arguments(self) -> None:\n        \"\"\" Update the converter arguments to the currently selected values. \"\"\"\n        logger.debug(\"Updating Converter cli arguments\")\n        if self.converter_arguments is None:\n            logger.debug(\"No arguments to update\")\n            return\n        for key, val in self.converter_arguments.items():\n            logger.debug(\"Updating %s to %s\", key, val)\n            setattr(self._converter.cli_arguments, key, val)\n        logger.debug(\"Updated Converter cli arguments\")\n\n    @staticmethod\n    def _feed_swapped_faces(patch_queue_in: EventQueue, samples: Samples) -> None:\n        \"\"\" Feed swapped faces to the converter's in-queue.\n\n        Parameters\n        ----------\n        patch_queue_in: :class:`~lib.queue_manager.EventQueue`\n            The input queue for the patching process\n        samples: :class:`Samples`\n            The Samples for display.\n        \"\"\"\n        logger.debug(\"feeding swapped faces to converter\")\n        for item in samples.predicted_images:\n            patch_queue_in.put(item)\n        logger.debug(\"fed %s swapped faces to converter\",\n                     len(samples.predicted_images))\n        logger.debug(\"Putting EOF to converter\")\n        patch_queue_in.put(\"EOF\")\n\n    def _patch_faces(self,\n                     queue_in: EventQueue,\n                     queue_out: EventQueue,\n                     sample_size: int) -> list[np.ndarray]:\n        \"\"\" Patch faces.\n\n        Run the convert process on the swapped faces and return the patched faces.\n\n        patch_queue_in: :class:`~lib.queue_manager.EventQueue`\n            The input queue for the patching process\n        queue_out: :class:`~lib.queue_manager.EventQueue`\n            The output queue from the patching process\n        sample_size: int\n            The number of samples to be displayed\n\n        Returns\n        -------\n        list\n            The swapped faces patched with the selected convert settings\n        \"\"\"\n        logger.debug(\"Patching faces\")\n        self._converter.process(queue_in, queue_out)\n        swapped = []\n        idx = 0\n        while idx < sample_size:\n            logger.debug(\"Patching image %s of %s\", idx + 1, sample_size)\n            item = queue_out.get()\n            swapped.append(item[1])\n            logger.debug(\"Patched image %s of %s\", idx + 1, sample_size)\n            idx += 1\n        logger.debug(\"Patched faces\")\n        return swapped\n", "tools/preview/__init__.py": "", "scripts/fsmedia.py": "#!/usr/bin/env python3\n\"\"\" Helper functions for :mod:`~scripts.extract` and :mod:`~scripts.convert`.\n\nHolds the classes for the 2 main Faceswap 'media' objects: Images and Alignments.\n\nHolds optional pre/post processing functions for convert and extract.\n\"\"\"\nfrom __future__ import annotations\nimport logging\nimport os\nimport sys\nimport typing as T\n\nfrom collections.abc import Iterator\n\nimport cv2\nimport numpy as np\nimport imageio\n\nfrom lib.align import Alignments as AlignmentsBase, get_centered_size\nfrom lib.image import count_frames, read_image\nfrom lib.utils import (camel_case_split, get_image_paths, VIDEO_EXTENSIONS)\n\nif T.TYPE_CHECKING:\n    from collections.abc import Generator\n    from argparse import Namespace\n    from lib.align import AlignedFace\n    from plugins.extract import ExtractMedia\n\nlogger = logging.getLogger(__name__)\n\n\ndef finalize(images_found: int, num_faces_detected: int, verify_output: bool) -> None:\n    \"\"\" Output summary statistics at the end of the extract or convert processes.\n\n    Parameters\n    ----------\n    images_found: int\n        The number of images/frames that were processed\n    num_faces_detected: int\n        The number of faces that have been detected\n    verify_output: bool\n        ``True`` if multiple faces were detected in frames otherwise ``False``.\n     \"\"\"\n    logger.info(\"-------------------------\")\n    logger.info(\"Images found:        %s\", images_found)\n    logger.info(\"Faces detected:      %s\", num_faces_detected)\n    logger.info(\"-------------------------\")\n\n    if verify_output:\n        logger.info(\"Note:\")\n        logger.info(\"Multiple faces were detected in one or more pictures.\")\n        logger.info(\"Double check your results.\")\n        logger.info(\"-------------------------\")\n\n    logger.info(\"Process Successfully Completed. Shutting Down...\")\n\n\nclass Alignments(AlignmentsBase):\n    \"\"\" Override :class:`lib.align.Alignments` to add custom loading based on command\n    line arguments.\n\n    Parameters\n    ----------\n    arguments: :class:`argparse.Namespace`\n        The command line arguments that were passed to Faceswap\n    is_extract: bool\n        ``True`` if the process calling this class is extraction otherwise ``False``\n    input_is_video: bool, optional\n        ``True`` if the input to the process is a video, ``False`` if it is a folder of images.\n        Default: False\n    \"\"\"\n    def __init__(self,\n                 arguments: Namespace,\n                 is_extract: bool,\n                 input_is_video: bool = False) -> None:\n        logger.debug(\"Initializing %s: (is_extract: %s, input_is_video: %s)\",\n                     self.__class__.__name__, is_extract, input_is_video)\n        self._args = arguments\n        self._is_extract = is_extract\n        folder, filename = self._set_folder_filename(input_is_video)\n        super().__init__(folder, filename=filename)\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    def _set_folder_filename(self, input_is_video: bool) -> tuple[str, str]:\n        \"\"\" Return the folder and the filename for the alignments file.\n\n        If the input is a video, the alignments file will be stored in the same folder\n        as the video, with filename `<videoname>_alignments`.\n\n        If the input is a folder of images, the alignments file will be stored in folder with\n        the images and just be called 'alignments'\n\n        Parameters\n        ----------\n        input_is_video: bool, optional\n            ``True`` if the input to the process is a video, ``False`` if it is a folder of images.\n\n        Returns\n        -------\n        folder: str\n            The folder where the alignments file will be stored\n        filename: str\n            The filename of the alignments file\n        \"\"\"\n        if self._args.alignments_path:\n            logger.debug(\"Alignments File provided: '%s'\", self._args.alignments_path)\n            folder, filename = os.path.split(str(self._args.alignments_path))\n        elif input_is_video:\n            logger.debug(\"Alignments from Video File: '%s'\", self._args.input_dir)\n            folder, filename = os.path.split(self._args.input_dir)\n            filename = f\"{os.path.splitext(filename)[0]}_alignments.fsa\"\n        else:\n            logger.debug(\"Alignments from Input Folder: '%s'\", self._args.input_dir)\n            folder = str(self._args.input_dir)\n            filename = \"alignments\"\n        logger.debug(\"Setting Alignments: (folder: '%s' filename: '%s')\", folder, filename)\n        return folder, filename\n\n    def _load(self) -> dict[str, T.Any]:\n        \"\"\" Override the parent :func:`~lib.align.Alignments._load` to handle skip existing\n        frames and faces on extract.\n\n        If skip existing has been selected, existing alignments are loaded and returned to the\n        calling script.\n\n        Returns\n        -------\n        dict\n            Any alignments that have already been extracted if skip existing has been selected\n            otherwise an empty dictionary\n        \"\"\"\n        data: dict[str, T.Any] = {}\n        if not self._is_extract and not self.have_alignments_file:\n            return data\n        if not self._is_extract:\n            data = super()._load()\n            return data\n\n        skip_existing = hasattr(self._args, 'skip_existing') and self._args.skip_existing\n        skip_faces = hasattr(self._args, 'skip_faces') and self._args.skip_faces\n\n        if not skip_existing and not skip_faces:\n            logger.debug(\"No skipping selected. Returning empty dictionary\")\n            return data\n\n        if not self.have_alignments_file and (skip_existing or skip_faces):\n            logger.warning(\"Skip Existing/Skip Faces selected, but no alignments file found!\")\n            return data\n\n        data = super()._load()\n\n        if skip_faces:\n            # Remove items from alignments that have no faces so they will\n            # be re-detected\n            del_keys = [key for key, val in data.items() if not val[\"faces\"]]\n            logger.debug(\"Frames with no faces selected for redetection: %s\", len(del_keys))\n            for key in del_keys:\n                if key in data:\n                    logger.trace(\"Selected for redetection: '%s'\",  # type:ignore[attr-defined]\n                                 key)\n                    del data[key]\n        return data\n\n\nclass Images():\n    \"\"\" Handles the loading of frames from a folder of images or a video file for extract\n    and convert processes.\n\n    Parameters\n    ----------\n    arguments: :class:`argparse.Namespace`\n        The command line arguments that were passed to Faceswap\n    \"\"\"\n    def __init__(self, arguments: Namespace) -> None:\n        logger.debug(\"Initializing %s\", self.__class__.__name__)\n        self._args = arguments\n        self._is_video = self._check_input_folder()\n        self._input_images = self._get_input_images()\n        self._images_found = self._count_images()\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    @property\n    def is_video(self) -> bool:\n        \"\"\"bool: ``True`` if the input is a video file otherwise ``False``. \"\"\"\n        return self._is_video\n\n    @property\n    def input_images(self) -> str | list[str]:\n        \"\"\"str or list: Path to the video file if the input is a video otherwise list of\n        image paths. \"\"\"\n        return self._input_images\n\n    @property\n    def images_found(self) -> int:\n        \"\"\"int: The number of frames that exist in the video file, or the folder of images. \"\"\"\n        return self._images_found\n\n    def _count_images(self) -> int:\n        \"\"\" Get the number of Frames from a video file or folder of images.\n\n        Returns\n        -------\n        int\n            The number of frames in the image source\n        \"\"\"\n        if self._is_video:\n            retval = int(count_frames(self._args.input_dir, fast=True))\n        else:\n            retval = len(self._input_images)\n        return retval\n\n    def _check_input_folder(self) -> bool:\n        \"\"\" Check whether the input is a folder or video.\n\n        Returns\n        -------\n        bool\n            ``True`` if the input is a video otherwise ``False``\n        \"\"\"\n        if not os.path.exists(self._args.input_dir):\n            logger.error(\"Input location %s not found.\", self._args.input_dir)\n            sys.exit(1)\n        if (os.path.isfile(self._args.input_dir) and\n                os.path.splitext(self._args.input_dir)[1].lower() in VIDEO_EXTENSIONS):\n            logger.info(\"Input Video: %s\", self._args.input_dir)\n            retval = True\n        else:\n            logger.info(\"Input Directory: %s\", self._args.input_dir)\n            retval = False\n        return retval\n\n    def _get_input_images(self) -> str | list[str]:\n        \"\"\" Return the list of images or path to video file that is to be processed.\n\n        Returns\n        -------\n        str or list\n            Path to the video file if the input is a video otherwise list of image paths.\n        \"\"\"\n        if self._is_video:\n            input_images = self._args.input_dir\n        else:\n            input_images = get_image_paths(self._args.input_dir)\n\n        return input_images\n\n    def load(self) -> Generator[tuple[str, np.ndarray], None, None]:\n        \"\"\" Generator to load frames from a folder of images or from a video file.\n\n        Yields\n        ------\n        filename: str\n            The filename of the current frame\n        image: :class:`numpy.ndarray`\n            A single frame\n        \"\"\"\n        iterator = self._load_video_frames if self._is_video else self._load_disk_frames\n        for filename, image in iterator():\n            yield filename, image\n\n    def _load_disk_frames(self) -> Generator[tuple[str, np.ndarray], None, None]:\n        \"\"\" Generator to load frames from a folder of images.\n\n        Yields\n        ------\n        filename: str\n            The filename of the current frame\n        image: :class:`numpy.ndarray`\n            A single frame\n        \"\"\"\n        logger.debug(\"Input is separate Frames. Loading images\")\n        for filename in self._input_images:\n            image = read_image(filename, raise_error=False)\n            if image is None:\n                continue\n            yield filename, image\n\n    def _load_video_frames(self) -> Generator[tuple[str, np.ndarray], None, None]:\n        \"\"\" Generator to load frames from a video file.\n\n        Yields\n        ------\n        filename: str\n            The filename of the current frame\n        image: :class:`numpy.ndarray`\n            A single frame\n        \"\"\"\n        logger.debug(\"Input is video. Capturing frames\")\n        vidname, ext = os.path.splitext(os.path.basename(self._args.input_dir))\n        reader = imageio.get_reader(self._args.input_dir, \"ffmpeg\")  # type:ignore[arg-type]\n        for i, frame in enumerate(T.cast(Iterator[np.ndarray], reader)):\n            # Convert to BGR for cv2 compatibility\n            frame = frame[:, :, ::-1]\n            filename = f\"{vidname}_{i + 1:06d}{ext}\"\n            logger.trace(\"Loading video frame: '%s'\", filename)  # type:ignore[attr-defined]\n            yield filename, frame\n        reader.close()\n\n    def load_one_image(self, filename) -> np.ndarray:\n        \"\"\" Obtain a single image for the given filename.\n\n        Parameters\n        ----------\n        filename: str\n            The filename to return the image for\n\n        Returns\n        ------\n        :class:`numpy.ndarray`\n            The image for the requested filename,\n\n        \"\"\"\n        logger.trace(\"Loading image: '%s'\", filename)  # type:ignore[attr-defined]\n        if self._is_video:\n            if filename.isdigit():\n                frame_no = filename\n            else:\n                frame_no = os.path.splitext(filename)[0][filename.rfind(\"_\") + 1:]\n                logger.trace(  # type:ignore[attr-defined]\n                    \"Extracted frame_no %s from filename '%s'\", frame_no, filename)\n            retval = self._load_one_video_frame(int(frame_no))\n        else:\n            retval = read_image(filename, raise_error=True)\n        return retval\n\n    def _load_one_video_frame(self, frame_no: int) -> np.ndarray:\n        \"\"\" Obtain a single frame from a video file.\n\n        Parameters\n        ----------\n        frame_no: int\n            The frame index for the required frame\n\n        Returns\n        ------\n        :class:`numpy.ndarray`\n            The image for the requested frame index,\n        \"\"\"\n        logger.trace(\"Loading video frame: %s\", frame_no)  # type:ignore[attr-defined]\n        reader = imageio.get_reader(self._args.input_dir, \"ffmpeg\")  # type:ignore[arg-type]\n        reader.set_image_index(frame_no - 1)\n        frame = reader.get_next_data()[:, :, ::-1]  # type:ignore[index]\n        reader.close()\n        return frame\n\n\nclass PostProcess():\n    \"\"\" Optional pre/post processing tasks for convert and extract.\n\n    Builds a pipeline of actions that have optionally been requested to be performed\n    in this session.\n\n    Parameters\n    ----------\n    arguments: :class:`argparse.Namespace`\n        The command line arguments that were passed to Faceswap\n    \"\"\"\n    def __init__(self, arguments: Namespace) -> None:\n        logger.debug(\"Initializing %s\", self.__class__.__name__)\n        self._args = arguments\n        self._actions = self._set_actions()\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    def _set_actions(self) -> list[PostProcessAction]:\n        \"\"\" Compile the requested actions to be performed into a list\n\n        Returns\n        -------\n        list\n            The list of :class:`PostProcessAction` to be performed\n        \"\"\"\n        postprocess_items = self._get_items()\n        actions: list[\"PostProcessAction\"] = []\n        for action, options in postprocess_items.items():\n            options = {} if options is None else options\n            args = options.get(\"args\", tuple())\n            kwargs = options.get(\"kwargs\", {})\n            args = args if isinstance(args, tuple) else tuple()\n            kwargs = kwargs if isinstance(kwargs, dict) else {}\n            task = globals()[action](*args, **kwargs)\n            if task.valid:\n                logger.debug(\"Adding Postprocess action: '%s'\", task)\n                actions.append(task)\n\n        for ppaction in actions:\n            action_name = camel_case_split(ppaction.__class__.__name__)\n            logger.info(\"Adding post processing item: %s\", \" \".join(action_name))\n\n        return actions\n\n    def _get_items(self) -> dict[str, dict[str, tuple | dict] | None]:\n        \"\"\" Check the passed in command line arguments for requested actions,\n\n        For any requested actions, add the item to the actions list along with\n        any relevant arguments and keyword arguments.\n\n        Returns\n        -------\n        dict\n            The name of the action to be performed as the key. Any action specific\n            arguments and keyword arguments as the value.\n        \"\"\"\n        postprocess_items: dict[str, dict[str, tuple | dict] | None] = {}\n        # Debug Landmarks\n        if (hasattr(self._args, 'debug_landmarks') and self._args.debug_landmarks):\n            postprocess_items[\"DebugLandmarks\"] = None\n\n        logger.debug(\"Postprocess Items: %s\", postprocess_items)\n        return postprocess_items\n\n    def do_actions(self, extract_media: ExtractMedia) -> None:\n        \"\"\" Perform the requested optional post-processing actions on the given image.\n\n        Parameters\n        ----------\n        extract_media: :class:`~plugins.extract.extract_media.ExtractMedia`\n            The :class:`~plugins.extract.extract_media.ExtractMedia` object to perform the\n            action on.\n\n        Returns\n        -------\n        :class:`~plugins.extract.extract_media.ExtractMedia`\n            The original :class:`~plugins.extract.extract_media.ExtractMedia` with any actions\n            applied\n        \"\"\"\n        for action in self._actions:\n            logger.debug(\"Performing postprocess action: '%s'\", action.__class__.__name__)\n            action.process(extract_media)\n\n\nclass PostProcessAction():\n    \"\"\" Parent class for Post Processing Actions.\n\n    Usable in Extract or Convert or both depending on context. Any post-processing actions should\n    inherit from this class.\n\n    Parameters\n    -----------\n    args: tuple\n        Varies for specific post process action\n    kwargs: dict\n        Varies for specific post process action\n    \"\"\"\n    def __init__(self, *args, **kwargs) -> None:\n        logger.debug(\"Initializing %s: (args: %s, kwargs: %s)\",\n                     self.__class__.__name__, args, kwargs)\n        self._valid = True  # Set to False if invalid parameters passed in to disable\n        logger.debug(\"Initialized base class %s\", self.__class__.__name__)\n\n    @property\n    def valid(self) -> bool:\n        \"\"\"bool: ``True`` if the action if the parameters passed in for this action are valid,\n        otherwise ``False`` \"\"\"\n        return self._valid\n\n    def process(self, extract_media: ExtractMedia) -> None:\n        \"\"\" Override for specific post processing action\n\n        Parameters\n        ----------\n        extract_media: :class:`~plugins.extract.extract_media.ExtractMedia`\n            The :class:`~plugins.extract.extract_media.ExtractMedia` object to perform the\n            action on.\n        \"\"\"\n        raise NotImplementedError\n\n\nclass DebugLandmarks(PostProcessAction):\n    \"\"\" Draw debug landmarks on face output. Extract Only \"\"\"\n    def __init__(self, *args, **kwargs) -> None:\n        super().__init__(self, *args, **kwargs)\n        self._face_size = 0\n        self._legacy_size = 0\n        self._font = cv2.FONT_HERSHEY_SIMPLEX\n        self._font_scale = 0.0\n        self._font_pad = 0\n\n    def _initialize_font(self, size: int) -> None:\n        \"\"\" Set the font scaling sizes on first call\n\n        Parameters\n        ----------\n        size: int\n            The pixel size of the saved aligned face\n        \"\"\"\n        self._font_scale = size / 512\n        self._font_pad = size // 64\n\n    def _border_text(self,\n                     image: np.ndarray,\n                     text: str,\n                     color: tuple[int, int, int],\n                     position: tuple[int, int]) -> None:\n        \"\"\" Create text on an image with a black border\n\n        Parameters\n        ----------\n        image: :class:`numpy.ndarray`\n            The image to put bordered text on to\n        text: str\n            The text to place the image\n        color: tuple\n            The color of the text\n        position: tuple\n            The (x, y) co-ordinates to place the text\n        \"\"\"\n        thickness = 2\n        for idx in range(2):\n            text_color = (0, 0, 0) if idx == 0 else color\n            cv2.putText(image,\n                        text,\n                        position,\n                        self._font,\n                        self._font_scale,\n                        text_color,\n                        thickness,\n                        lineType=cv2.LINE_AA)\n            thickness //= 2\n\n    def _annotate_face_box(self, face: AlignedFace) -> None:\n        \"\"\" Annotate the face extract box and print the original size in pixels\n\n        face: :class:`~lib.align.AlignedFace`\n            The object containing the aligned face to annotate\n        \"\"\"\n        assert face.face is not None\n        color = (0, 255, 0)\n        roi = face.get_cropped_roi(face.size, self._face_size, \"face\")\n        cv2.rectangle(face.face, tuple(roi[:2]), tuple(roi[2:]), color, 1)\n\n        # Size in top right corner\n        roi_pnts = np.array([[roi[0], roi[1]],\n                             [roi[0], roi[3]],\n                             [roi[2], roi[3]],\n                             [roi[2], roi[1]]])\n        orig_roi = face.transform_points(roi_pnts, invert=True)\n        size = int(round(((orig_roi[1][0] - orig_roi[0][0]) ** 2 +\n                          (orig_roi[1][1] - orig_roi[0][1]) ** 2) ** 0.5))\n        text_img = face.face.copy()\n        text = f\"{size}px\"\n        text_size = cv2.getTextSize(text, self._font, self._font_scale, 1)[0]\n        pos_x = roi[2] - (text_size[0] + self._font_pad)\n        pos_y = roi[1] + text_size[1] + self._font_pad\n\n        self._border_text(text_img, text, color, (pos_x, pos_y))\n        cv2.addWeighted(text_img, 0.75, face.face, 0.25, 0, face.face)\n\n    def _print_stats(self, face: AlignedFace) -> None:\n        \"\"\" Print various metrics on the output face images\n\n        Parameters\n        ----------\n        face: :class:`~lib.align.AlignedFace`\n            The loaded aligned face\n        \"\"\"\n        assert face.face is not None\n        text_image = face.face.copy()\n        texts = [f\"pitch: {face.pose.pitch:.2f}\",\n                 f\"yaw: {face.pose.yaw:.2f}\",\n                 f\"roll: {face.pose.roll: .2f}\",\n                 f\"distance: {face.average_distance:.2f}\"]\n        colors = [(255, 0, 0), (0, 0, 255), (0, 255, 0), (255, 255, 255)]\n        text_sizes = [cv2.getTextSize(text, self._font, self._font_scale, 1)[0] for text in texts]\n\n        final_y = face.size - text_sizes[-1][1]\n        pos_y = [(size[1] + self._font_pad) * (idx + 1)\n                 for idx, size in enumerate(text_sizes)][:-1] + [final_y]\n        pos_x = self._font_pad\n\n        for idx, text in enumerate(texts):\n            self._border_text(text_image, text, colors[idx], (pos_x, pos_y[idx]))\n\n        # Apply text to face\n        cv2.addWeighted(text_image, 0.75, face.face, 0.25, 0, face.face)\n\n    def process(self, extract_media: ExtractMedia) -> None:\n        \"\"\" Draw landmarks on a face.\n\n        Parameters\n        ----------\n        extract_media: :class:`~plugins.extract.extract_media.ExtractMedia`\n            The :class:`~plugins.extract.extract_media.ExtractMedia` object that contains the faces\n            to draw the landmarks on to\n        \"\"\"\n        frame = os.path.splitext(os.path.basename(extract_media.filename))[0]\n        for idx, face in enumerate(extract_media.detected_faces):\n            if not self._face_size:\n                self._face_size = get_centered_size(face.aligned.centering,\n                                                    \"face\",\n                                                    face.aligned.size)\n                logger.debug(\"set face size: %s\", self._face_size)\n            if not self._legacy_size:\n                self._legacy_size = get_centered_size(face.aligned.centering,\n                                                      \"legacy\",\n                                                      face.aligned.size)\n                logger.debug(\"set legacy size: %s\", self._legacy_size)\n            if not self._font_scale:\n                self._initialize_font(face.aligned.size)\n\n            logger.trace(\"Drawing Landmarks. Frame: '%s'. Face: %s\",  # type:ignore[attr-defined]\n                         frame, idx)\n            # Landmarks\n            assert face.aligned.face is not None\n            for (pos_x, pos_y) in face.aligned.landmarks.astype(\"int32\"):\n                cv2.circle(face.aligned.face, (pos_x, pos_y), 1, (0, 255, 255), -1)\n            # Pose\n            center = (face.aligned.size // 2, face.aligned.size // 2)\n            points = (face.aligned.pose.xyz_2d * face.aligned.size).astype(\"int32\")\n            cv2.line(face.aligned.face, center, tuple(points[1]), (0, 255, 0), 1)\n            cv2.line(face.aligned.face, center, tuple(points[0]), (255, 0, 0), 1)\n            cv2.line(face.aligned.face, center, tuple(points[2]), (0, 0, 255), 1)\n            # Face centering\n            self._annotate_face_box(face.aligned)\n            # Legacy centering\n            roi = face.aligned.get_cropped_roi(face.aligned.size, self._legacy_size, \"legacy\")\n            cv2.rectangle(face.aligned.face, tuple(roi[:2]), tuple(roi[2:]), (0, 0, 255), 1)\n            self._print_stats(face.aligned)\n", "scripts/train.py": "#!/usr/bin python3\n\"\"\" Main entry point to the training process of FaceSwap \"\"\"\nfrom __future__ import annotations\nimport logging\nimport os\nimport sys\nimport typing as T\n\nfrom time import sleep\nfrom threading import Event\n\nimport cv2\nimport numpy as np\n\nfrom lib.gui.utils.image import TRAININGPREVIEW\nfrom lib.image import read_image_meta\nfrom lib.keypress import KBHit\nfrom lib.multithreading import MultiThread, FSThread\nfrom lib.training import Preview, PreviewBuffer, TriggerType\nfrom lib.utils import (get_folder, get_image_paths, handle_deprecated_cliopts,\n                       FaceswapError, IMAGE_EXTENSIONS)\nfrom plugins.plugin_loader import PluginLoader\n\nif T.TYPE_CHECKING:\n    import argparse\n    from collections.abc import Callable\n    from plugins.train.model._base import ModelBase\n    from plugins.train.trainer._base import TrainerBase\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass Train():\n    \"\"\" The Faceswap Training Process.\n\n    The training process is responsible for training a model on a set of source faces and a set of\n    destination faces.\n\n    The training process is self contained and should not be referenced by any other scripts, so it\n    contains no public properties.\n\n    Parameters\n    ----------\n    arguments: argparse.Namespace\n        The arguments to be passed to the training process as generated from Faceswap's command\n        line arguments\n    \"\"\"\n    def __init__(self, arguments: argparse.Namespace) -> None:\n        logger.debug(\"Initializing %s: (args: %s\", self.__class__.__name__, arguments)\n        self._args = handle_deprecated_cliopts(arguments)\n\n        if self._args.summary:\n            # If just outputting summary we don't need to initialize everything\n            return\n\n        self._images = self._get_images()\n        self._timelapse = self._set_timelapse()\n        gui_cache = os.path.join(\n            os.path.realpath(os.path.dirname(sys.argv[0])), \"lib\", \"gui\", \".cache\")\n        self._gui_triggers: dict[T.Literal[\"mask\", \"refresh\"], str] = {\n            \"mask\": os.path.join(gui_cache, \".preview_mask_toggle\"),\n            \"refresh\": os.path.join(gui_cache, \".preview_trigger\")}\n        self._stop: bool = False\n        self._save_now: bool = False\n        self._preview = PreviewInterface(self._args.preview)\n\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    def _get_images(self) -> dict[T.Literal[\"a\", \"b\"], list[str]]:\n        \"\"\" Check the image folders exist and contains valid extracted faces. Obtain image paths.\n\n        Returns\n        -------\n        dict\n            The image paths for each side. The key is the side, the value is the list of paths\n            for that side.\n        \"\"\"\n        logger.debug(\"Getting image paths\")\n        images = {}\n        for side in (\"a\", \"b\"):\n            side = T.cast(T.Literal[\"a\", \"b\"], side)\n            image_dir = getattr(self._args, f\"input_{side}\")\n            if not os.path.isdir(image_dir):\n                logger.error(\"Error: '%s' does not exist\", image_dir)\n                sys.exit(1)\n\n            images[side] = get_image_paths(image_dir, \".png\")\n            if not images[side]:\n                logger.error(\"Error: '%s' contains no images\", image_dir)\n                sys.exit(1)\n            # Validate the first image is a detected face\n            test_image = next(img for img in images[side])\n            meta = read_image_meta(test_image)\n            logger.debug(\"Test file: (filename: %s, metadata: %s)\", test_image, meta)\n            if \"itxt\" not in meta or \"alignments\" not in meta[\"itxt\"]:\n                logger.error(\"The input folder '%s' contains images that are not extracted faces.\",\n                             image_dir)\n                logger.error(\"You can only train a model on faces generated from Faceswap's \"\n                             \"extract process. Please check your sources and try again.\")\n                sys.exit(1)\n\n            logger.info(\"Model %s Directory: '%s' (%s images)\",\n                        side.upper(), image_dir, len(images[side]))\n        logger.debug(\"Got image paths: %s\", [(key, str(len(val)) + \" images\")\n                                             for key, val in images.items()])\n        self._validate_image_counts(images)\n        return images\n\n    @classmethod\n    def _validate_image_counts(cls, images: dict[T.Literal[\"a\", \"b\"], list[str]]) -> None:\n        \"\"\" Validate that there are sufficient images to commence training without raising an\n        error.\n\n        Confirms that there are at least 24 images in each folder. Whilst this is not enough images\n        to train a Neural Network to any successful degree, it should allow the process to train\n        without raising errors when generating previews.\n\n        A warning is raised if there are fewer than 250 images on any side.\n\n        Parameters\n        ----------\n        images: dict\n            The image paths for each side. The key is the side, the value is the list of paths\n            for that side.\n        \"\"\"\n        counts = {side: len(paths) for side, paths in images.items()}\n        msg = (\"You need to provide a significant number of images to successfully train a Neural \"\n               \"Network. Aim for between 500 - 5000 images per side.\")\n        if any(count < 25 for count in counts.values()):\n            logger.error(\"At least one of your input folders contains fewer than 25 images.\")\n            logger.error(msg)\n            sys.exit(1)\n        if any(count < 250 for count in counts.values()):\n            logger.warning(\"At least one of your input folders contains fewer than 250 images. \"\n                           \"Results are likely to be poor.\")\n            logger.warning(msg)\n\n    def _set_timelapse(self) -> dict[T.Literal[\"input_a\", \"input_b\", \"output\"], str]:\n        \"\"\" Set time-lapse paths if requested.\n\n        Returns\n        -------\n        dict\n            The time-lapse keyword arguments for passing to the trainer\n\n        \"\"\"\n        if (not self._args.timelapse_input_a and\n                not self._args.timelapse_input_b and\n                not self._args.timelapse_output):\n            return {}\n        if (not self._args.timelapse_input_a or\n                not self._args.timelapse_input_b or\n                not self._args.timelapse_output):\n            raise FaceswapError(\"To enable the timelapse, you have to supply all the parameters \"\n                                \"(--timelapse-input-A, --timelapse-input-B and \"\n                                \"--timelapse-output).\")\n\n        timelapse_output = get_folder(self._args.timelapse_output)\n\n        for side in (\"a\", \"b\"):\n            side = T.cast(T.Literal[\"a\", \"b\"], side)\n            folder = getattr(self._args, f\"timelapse_input_{side}\")\n            if folder is not None and not os.path.isdir(folder):\n                raise FaceswapError(f\"The Timelapse path '{folder}' does not exist\")\n\n            training_folder = getattr(self._args, f\"input_{side}\")\n            if folder == training_folder:\n                continue  # Time-lapse folder is training folder\n\n            filenames = [fname for fname in os.listdir(folder)\n                         if os.path.splitext(fname)[-1].lower() in IMAGE_EXTENSIONS]\n            if not filenames:\n                raise FaceswapError(f\"The Timelapse path '{folder}' does not contain any valid \"\n                                    \"images\")\n\n            # Time-lapse images must appear in the training set, as we need access to alignment and\n            # mask info. Check filenames are there to save failing much later in the process.\n            training_images = [os.path.basename(img) for img in self._images[side]]\n            if not all(img in training_images for img in filenames):\n                raise FaceswapError(f\"All images in the Timelapse folder '{folder}' must exist in \"\n                                    f\"the training folder '{training_folder}'\")\n\n        TKey = T.Literal[\"input_a\", \"input_b\", \"output\"]\n        kwargs = {T.cast(TKey, \"input_a\"): self._args.timelapse_input_a,\n                  T.cast(TKey, \"input_b\"): self._args.timelapse_input_b,\n                  T.cast(TKey, \"output\"): timelapse_output}\n        logger.debug(\"Timelapse enabled: %s\", kwargs)\n        return kwargs\n\n    def process(self) -> None:\n        \"\"\" The entry point for triggering the Training Process.\n\n        Should only be called from  :class:`lib.cli.launcher.ScriptExecutor`\n        \"\"\"\n        if self._args.summary:\n            self._load_model()\n            return\n        logger.debug(\"Starting Training Process\")\n        logger.info(\"Training data directory: %s\", self._args.model_dir)\n        thread = self._start_thread()\n        # from lib.queue_manager import queue_manager; queue_manager.debug_monitor(1)\n        err = self._monitor(thread)\n        self._end_thread(thread, err)\n        logger.debug(\"Completed Training Process\")\n\n    def _start_thread(self) -> MultiThread:\n        \"\"\" Put the :func:`_training` into a background thread so we can keep control.\n\n        Returns\n        -------\n        :class:`lib.multithreading.MultiThread`\n            The background thread for running training\n        \"\"\"\n        logger.debug(\"Launching Trainer thread\")\n        thread = MultiThread(target=self._training)\n        thread.start()\n        logger.debug(\"Launched Trainer thread\")\n        return thread\n\n    def _end_thread(self, thread: MultiThread, err: bool) -> None:\n        \"\"\" Output message and join thread back to main on termination.\n\n        Parameters\n        ----------\n        thread: :class:`lib.multithreading.MultiThread`\n            The background training thread\n        err: bool\n            Whether an error has been detected in :func:`_monitor`\n        \"\"\"\n        logger.debug(\"Ending Training thread\")\n        if err:\n            msg = \"Error caught! Exiting...\"\n            log = logger.critical\n        else:\n            msg = (\"Exit requested! The trainer will complete its current cycle, \"\n                   \"save the models and quit (This can take a couple of minutes \"\n                   \"depending on your training speed).\")\n            if not self._args.redirect_gui:\n                msg += \" If you want to kill it now, press Ctrl + c\"\n            log = logger.info\n        log(msg)\n        self._stop = True\n        thread.join()\n        sys.stdout.flush()\n        logger.debug(\"Ended training thread\")\n\n    def _training(self) -> None:\n        \"\"\" The training process to be run inside a thread. \"\"\"\n        try:\n            sleep(0.5)  # Let preview instructions flush out to logger\n            logger.debug(\"Commencing Training\")\n            logger.info(\"Loading data, this may take a while...\")\n            model = self._load_model()\n            trainer = self._load_trainer(model)\n            if trainer.exit_early:\n                self._stop = True\n                return\n            self._run_training_cycle(model, trainer)\n        except KeyboardInterrupt:\n            try:\n                logger.debug(\"Keyboard Interrupt Caught. Saving Weights and exiting\")\n                model.io.save(is_exit=True)\n                trainer.clear_tensorboard()\n            except KeyboardInterrupt:\n                logger.info(\"Saving model weights has been cancelled!\")\n            sys.exit(0)\n        except Exception as err:\n            raise err\n\n    def _load_model(self) -> ModelBase:\n        \"\"\" Load the model requested for training.\n\n        Returns\n        -------\n        :file:`plugins.train.model` plugin\n            The requested model plugin\n        \"\"\"\n        logger.debug(\"Loading Model\")\n        model_dir = get_folder(self._args.model_dir)\n        model: ModelBase = PluginLoader.get_model(self._args.trainer)(\n            model_dir,\n            self._args,\n            predict=False)\n        model.build()\n        logger.debug(\"Loaded Model\")\n        return model\n\n    def _load_trainer(self, model: ModelBase) -> TrainerBase:\n        \"\"\" Load the trainer requested for training.\n\n        Parameters\n        ----------\n        model: :file:`plugins.train.model` plugin\n            The requested model plugin\n\n        Returns\n        -------\n        :file:`plugins.train.trainer` plugin\n            The requested model trainer plugin\n        \"\"\"\n        logger.debug(\"Loading Trainer\")\n        base = PluginLoader.get_trainer(model.trainer)\n        trainer: TrainerBase = base(model,\n                                    self._images,\n                                    self._args.batch_size,\n                                    self._args.configfile)\n        logger.debug(\"Loaded Trainer\")\n        return trainer\n\n    def _run_training_cycle(self, model: ModelBase, trainer: TrainerBase) -> None:\n        \"\"\" Perform the training cycle.\n\n        Handles the background training, updating previews/time-lapse on each save interval,\n        and saving the model.\n\n        Parameters\n        ----------\n        model: :file:`plugins.train.model` plugin\n            The requested model plugin\n        trainer: :file:`plugins.train.trainer` plugin\n            The requested model trainer plugin\n        \"\"\"\n        logger.debug(\"Running Training Cycle\")\n        update_preview_images = False\n        if self._args.write_image or self._args.redirect_gui or self._args.preview:\n            display_func: Callable | None = self._show\n        else:\n            display_func = None\n\n        for iteration in range(1, self._args.iterations + 1):\n            logger.trace(\"Training iteration: %s\", iteration)  # type:ignore\n            save_iteration = iteration % self._args.save_interval == 0 or iteration == 1\n            gui_triggers = self._process_gui_triggers()\n\n            if self._preview.should_toggle_mask or gui_triggers[\"mask\"]:\n                trainer.toggle_mask()\n                update_preview_images = True\n\n            if self._preview.should_refresh or gui_triggers[\"refresh\"] or update_preview_images:\n                viewer = display_func\n                update_preview_images = False\n            else:\n                viewer = None\n\n            timelapse = self._timelapse if save_iteration else {}\n            trainer.train_one_step(viewer, timelapse)\n\n            if viewer is not None and not save_iteration:\n                # Spammy but required by GUI to know to update window\n                print(\"\")\n                logger.info(\"[Preview Updated]\")\n\n            if self._stop:\n                logger.debug(\"Stop received. Terminating\")\n                break\n\n            if save_iteration or self._save_now:\n                logger.debug(\"Saving (save_iterations: %s, save_now: %s) Iteration: \"\n                             \"(iteration: %s)\", save_iteration, self._save_now, iteration)\n                model.io.save(is_exit=False)\n                self._save_now = False\n                update_preview_images = True\n\n        logger.debug(\"Training cycle complete\")\n        model.io.save(is_exit=True)\n        trainer.clear_tensorboard()\n        self._stop = True\n\n    def _output_startup_info(self) -> None:\n        \"\"\" Print the startup information to the console. \"\"\"\n        logger.debug(\"Launching Monitor\")\n        logger.info(\"===================================================\")\n        logger.info(\"  Starting\")\n        if self._args.preview:\n            logger.info(\"  Using live preview\")\n        if sys.stdout.isatty():\n            logger.info(\"  Press '%s' to save and quit\",\n                        \"Stop\" if self._args.redirect_gui else \"ENTER\")\n        if not self._args.redirect_gui and sys.stdout.isatty():\n            logger.info(\"  Press 'S' to save model weights immediately\")\n        logger.info(\"===================================================\")\n\n    def _check_keypress(self, keypress: KBHit) -> bool:\n        \"\"\" Check if a keypress has been detected.\n\n        Parameters\n        ----------\n        keypress: :class:`lib.keypress.KBHit`\n            The keypress monitor\n\n        Returns\n        -------\n        bool\n            ``True`` if an exit keypress has been detected otherwise ``False``\n        \"\"\"\n        retval = False\n        if keypress.kbhit():\n            console_key = keypress.getch()\n            if console_key in (\"\\n\", \"\\r\"):\n                logger.debug(\"Exit requested\")\n                retval = True\n            if console_key in (\"s\", \"S\"):\n                logger.info(\"Save requested\")\n                self._save_now = True\n        return retval\n\n    def _process_gui_triggers(self) -> dict[T.Literal[\"mask\", \"refresh\"], bool]:\n        \"\"\" Check whether a file drop has occurred from the GUI to manually update the preview.\n\n        Returns\n        -------\n        dict\n            The trigger name as key and boolean as value\n        \"\"\"\n        retval: dict[T.Literal[\"mask\", \"refresh\"], bool] = {key: False\n                                                            for key in self._gui_triggers}\n        if not self._args.redirect_gui:\n            return retval\n\n        for trigger, filename in self._gui_triggers.items():\n            if os.path.isfile(filename):\n                logger.debug(\"GUI Trigger received for: '%s'\", trigger)\n                retval[trigger] = True\n                logger.debug(\"Removing gui trigger file: %s\", filename)\n                os.remove(filename)\n                if trigger == \"refresh\":\n                    print(\"\")  # Let log print on different line from loss output\n                    logger.info(\"Refresh preview requested...\")\n        return retval\n\n    def _monitor(self, thread: MultiThread) -> bool:\n        \"\"\" Monitor the background :func:`_training` thread for key presses and errors.\n\n        Parameters\n        ----------\n        thread: :class:~`lib.multithreading.MultiThread`\n            The thread containing the training loop\n\n        Returns\n        -------\n        bool\n            ``True`` if there has been an error in the background thread otherwise ``False``\n        \"\"\"\n        self._output_startup_info()\n        keypress = KBHit(is_gui=self._args.redirect_gui)\n        err = False\n        while True:\n            try:\n                if thread.has_error:\n                    logger.debug(\"Thread error detected\")\n                    err = True\n                    break\n                if self._stop:\n                    logger.debug(\"Stop received\")\n                    break\n\n                # Preview Monitor\n                if self._preview.should_quit:\n                    break\n                if self._preview.should_save:\n                    self._save_now = True\n\n                # Console Monitor\n                if self._check_keypress(keypress):\n                    break  # Exit requested\n\n                sleep(1)\n            except KeyboardInterrupt:\n                logger.debug(\"Keyboard Interrupt received\")\n                break\n        self._preview.shutdown()\n        keypress.set_normal_term()\n        logger.debug(\"Closed Monitor\")\n        return err\n\n    def _show(self, image: np.ndarray, name: str = \"\") -> None:\n        \"\"\" Generate the preview and write preview file output.\n\n        Handles the output and display of preview images.\n\n        Parameters\n        ----------\n        image: :class:`numpy.ndarray`\n            The preview image to be displayed and/or written out\n        name: str, optional\n            The name of the image for saving or display purposes. If an empty string is passed\n            then it will automatically be named. Default: \"\"\n        \"\"\"\n        logger.debug(\"Updating preview: (name: %s)\", name)\n        try:\n            scriptpath = os.path.realpath(os.path.dirname(sys.argv[0]))\n            if self._args.write_image:\n                logger.debug(\"Saving preview to disk\")\n                img = \"training_preview.png\"\n                imgfile = os.path.join(scriptpath, img)\n                cv2.imwrite(imgfile, image)  # pylint:disable=no-member\n                logger.debug(\"Saved preview to: '%s'\", img)\n            if self._args.redirect_gui:\n                logger.debug(\"Generating preview for GUI\")\n                img = TRAININGPREVIEW\n                imgfile = os.path.join(scriptpath, \"lib\", \"gui\", \".cache\", \"preview\", img)\n                cv2.imwrite(imgfile, image)  # pylint:disable=no-member\n                logger.debug(\"Generated preview for GUI: '%s'\", imgfile)\n            if self._args.preview:\n                logger.debug(\"Generating preview for display: '%s'\", name)\n                self._preview.buffer.add_image(name, image)\n                logger.debug(\"Generated preview for display: '%s'\", name)\n        except Exception as err:\n            logging.error(\"could not preview sample\")\n            raise err\n        logger.debug(\"Updated preview: (name: %s)\", name)\n\n\nclass PreviewInterface():\n    \"\"\" Run the preview window in a thread and interface with it\n\n    Parameters\n    ----------\n    use_preview: bool\n        ``True`` if pop-up preview window has been requested otherwise ``False``\n    \"\"\"\n    def __init__(self, use_preview: bool) -> None:\n        self._active = use_preview\n        self._triggers: TriggerType = {\"toggle_mask\": Event(),\n                                       \"refresh\": Event(),\n                                       \"save\": Event(),\n                                       \"quit\": Event(),\n                                       \"shutdown\": Event()}\n        self._buffer = PreviewBuffer()\n        self._thread = self._launch_thread()\n\n    @property\n    def buffer(self) -> PreviewBuffer:\n        \"\"\" :class:`PreviewBuffer`: The thread save preview image object \"\"\"\n        return self._buffer\n\n    @property\n    def should_toggle_mask(self) -> bool:\n        \"\"\" bool: Check whether the mask should be toggled and return the value. If ``True`` is\n        returned then resets mask toggle back to ``False`` \"\"\"\n        if not self._active:\n            return False\n        retval = self._triggers[\"toggle_mask\"].is_set()\n        if retval:\n            logger.debug(\"Sending toggle mask\")\n            self._triggers[\"toggle_mask\"].clear()\n        return retval\n\n    @property\n    def should_refresh(self) -> bool:\n        \"\"\" bool: Check whether the preview should be updated and return the value. If ``True`` is\n        returned then resets the refresh trigger back to ``False`` \"\"\"\n        if not self._active:\n            return False\n        retval = self._triggers[\"refresh\"].is_set()\n        if retval:\n            logger.debug(\"Sending should refresh\")\n            self._triggers[\"refresh\"].clear()\n        return retval\n\n    @property\n    def should_save(self) -> bool:\n        \"\"\" bool: Check whether a save request has been made. If ``True`` is returned then save\n        trigger is set back to ``False`` \"\"\"\n        if not self._active:\n            return False\n        retval = self._triggers[\"save\"].is_set()\n        if retval:\n            logger.debug(\"Sending should save\")\n            self._triggers[\"save\"].clear()\n        return retval\n\n    @property\n    def should_quit(self) -> bool:\n        \"\"\" bool: Check whether an exit request has been made. ``True`` if an exit request has\n        been made otherwise ``False``.\n\n        Raises\n        ------\n        Error\n            Re-raises any error within the preview thread\n         \"\"\"\n        if self._thread is None:\n            return False\n\n        self._thread.check_and_raise_error()\n\n        retval = self._triggers[\"quit\"].is_set()\n        if retval:\n            logger.debug(\"Sending should stop\")\n        return retval\n\n    def _launch_thread(self) -> FSThread | None:\n        \"\"\" Launch the preview viewer in it's own thread if preview has been selected\n\n        Returns\n        -------\n        :class:`lib.multithreading.FSThread` or ``None``\n            The thread that holds the preview viewer if preview is selected otherwise ``None``\n        \"\"\"\n        if not self._active:\n            return None\n        thread = FSThread(target=Preview,\n                          name=\"preview\",\n                          args=(self._buffer, ),\n                          kwargs={\"triggers\": self._triggers})\n        thread.start()\n        return thread\n\n    def shutdown(self) -> None:\n        \"\"\" Send a signal to shutdown the preview window. \"\"\"\n        if not self._active:\n            return\n        logger.debug(\"Sending shutdown to preview viewer\")\n        self._triggers[\"shutdown\"].set()\n", "scripts/extract.py": "#!/usr/bin python3\n\"\"\" Main entry point to the extract process of FaceSwap \"\"\"\n\nfrom __future__ import annotations\nimport logging\nimport os\nimport sys\nimport typing as T\n\nfrom argparse import Namespace\nfrom multiprocessing import Process\n\nimport numpy as np\nfrom tqdm import tqdm\nfrom lib.align.alignments import PNGHeaderDict\n\nfrom lib.image import encode_image, generate_thumbnail, ImagesLoader, ImagesSaver, read_image_meta\nfrom lib.multithreading import MultiThread\nfrom lib.utils import get_folder, handle_deprecated_cliopts, IMAGE_EXTENSIONS, VIDEO_EXTENSIONS\nfrom plugins.extract import ExtractMedia, Extractor\nfrom scripts.fsmedia import Alignments, PostProcess, finalize\n\nif T.TYPE_CHECKING:\n    from lib.align.alignments import PNGHeaderAlignmentsDict\n\n# tqdm.monitor_interval = 0  # workaround for TqdmSynchronisationWarning  # TODO?\nlogger = logging.getLogger(__name__)\n\n\nclass Extract():\n    \"\"\" The Faceswap Face Extraction Process.\n\n    The extraction process is responsible for detecting faces in a series of images/video, aligning\n    these faces and then generating a mask.\n\n    It leverages a series of user selected plugins, chained together using\n    :mod:`plugins.extract.pipeline`.\n\n    The extract process is self contained and should not be referenced by any other scripts, so it\n    contains no public properties.\n\n    Parameters\n    ----------\n    arguments: :class:`argparse.Namespace`\n        The arguments to be passed to the extraction process as generated from Faceswap's command\n        line arguments\n    \"\"\"\n    def __init__(self, arguments: Namespace) -> None:\n        logger.debug(\"Initializing %s: (args: %s\", self.__class__.__name__, arguments)\n        self._args = handle_deprecated_cliopts(arguments)\n        self._input_locations = self._get_input_locations()\n        self._validate_batchmode()\n\n        configfile = self._args.configfile if hasattr(self._args, \"configfile\") else None\n        normalization = None if self._args.normalization == \"none\" else self._args.normalization\n        maskers = [\"components\", \"extended\"]\n        maskers += self._args.masker if self._args.masker else []\n        recognition = (\"vgg_face2\"\n                       if arguments.identity or arguments.filter or arguments.nfilter\n                       else None)\n        self._extractor = Extractor(self._args.detector,\n                                    self._args.aligner,\n                                    maskers,\n                                    recognition=recognition,\n                                    configfile=configfile,\n                                    multiprocess=not self._args.singleprocess,\n                                    exclude_gpus=self._args.exclude_gpus,\n                                    rotate_images=self._args.rotate_images,\n                                    min_size=self._args.min_size,\n                                    normalize_method=normalization,\n                                    re_feed=self._args.re_feed,\n                                    re_align=self._args.re_align)\n        self._filter = Filter(self._args.ref_threshold,\n                              self._args.filter,\n                              self._args.nfilter,\n                              self._extractor)\n\n    def _get_input_locations(self) -> list[str]:\n        \"\"\" Obtain the full path to input locations. Will be a list of locations if batch mode is\n        selected, or a containing a single location if batch mode is not selected.\n\n        Returns\n        -------\n        list:\n            The list of input location paths\n        \"\"\"\n        if not self._args.batch_mode or os.path.isfile(self._args.input_dir):\n            return [self._args.input_dir]  # Not batch mode or a single file\n\n        retval = [os.path.join(self._args.input_dir, fname)\n                  for fname in os.listdir(self._args.input_dir)\n                  if (os.path.isdir(os.path.join(self._args.input_dir, fname))  # folder images\n                      and any(os.path.splitext(iname)[-1].lower() in IMAGE_EXTENSIONS\n                              for iname in os.listdir(os.path.join(self._args.input_dir, fname))))\n                  or os.path.splitext(fname)[-1].lower() in VIDEO_EXTENSIONS]  # video\n\n        logger.debug(\"Input locations: %s\", retval)\n        return retval\n\n    def _validate_batchmode(self) -> None:\n        \"\"\" Validate the command line arguments.\n\n        If batch-mode selected and there is only one object to extract from, then batch mode is\n        disabled\n\n        If processing in batch mode, some of the given arguments may not make sense, in which case\n        a warning is shown and those options are reset.\n\n        \"\"\"\n        if not self._args.batch_mode:\n            return\n\n        if os.path.isfile(self._args.input_dir):\n            logger.warning(\"Batch mode selected but input is not a folder. Switching to normal \"\n                           \"mode\")\n            self._args.batch_mode = False\n\n        if not self._input_locations:\n            logger.error(\"Batch mode selected, but no valid files found in input location: '%s'. \"\n                         \"Exiting.\", self._args.input_dir)\n            sys.exit(1)\n\n        if self._args.alignments_path:\n            logger.warning(\"Custom alignments path not supported for batch mode. \"\n                           \"Reverting to default.\")\n            self._args.alignments_path = None\n\n    def _output_for_input(self, input_location: str) -> str:\n        \"\"\" Obtain the path to an output folder for faces for a given input location.\n\n        If not running in batch mode, then the user supplied output location will be returned,\n        otherwise a sub-folder within the user supplied output location will be returned based on\n        the input filename\n\n        Parameters\n        ----------\n        input_location: str\n            The full path to an input video or folder of images\n        \"\"\"\n        if not self._args.batch_mode:\n            return self._args.output_dir\n\n        retval = os.path.join(self._args.output_dir,\n                              os.path.splitext(os.path.basename(input_location))[0])\n        logger.debug(\"Returning output: '%s' for input: '%s'\", retval, input_location)\n        return retval\n\n    def process(self) -> None:\n        \"\"\" The entry point for triggering the Extraction Process.\n\n        Should only be called from  :class:`lib.cli.launcher.ScriptExecutor`\n        \"\"\"\n        logger.info('Starting, this may take a while...')\n        if self._args.batch_mode:\n            logger.info(\"Batch mode selected processing: %s\", self._input_locations)\n        for job_no, location in enumerate(self._input_locations):\n            if self._args.batch_mode:\n                logger.info(\"Processing job %s of %s: '%s'\",\n                            job_no + 1, len(self._input_locations), location)\n                arguments = Namespace(**self._args.__dict__)\n                arguments.input_dir = location\n                arguments.output_dir = self._output_for_input(location)\n            else:\n                arguments = self._args\n            extract = _Extract(self._extractor, arguments)\n            if sys.platform == \"linux\" and len(self._input_locations) > 1:\n                # TODO - Running this in a process is hideously hacky. However, there is a memory\n                # leak in some instances when running in batch mode. Many days have been spent\n                # trying to track this down to no avail (most likely coming from C-code.) Running\n                # the extract job inside a process prevents the memory leak in testing. This should\n                # be replaced if/when the memory leak is found\n                # Only done for Linux as not reported elsewhere and this new process won't work in\n                # Windows because it can't fork.\n                proc = Process(target=extract.process)\n                proc.start()\n                proc.join()\n            else:\n                extract.process()\n            self._extractor.reset_phase_index()\n\n\nclass Filter():\n    \"\"\" Obtains and holds face identity embeddings for any filter/nfilter image files\n    passed in from the command line.\n\n    Parameters\n    ----------\n    filter_files: list or ``None``\n        The list of filter file(s) passed in as command line arguments\n    nfilter_files: list or ``None``\n        The list of nfilter file(s) passed in as command line arguments\n    extractor: :class:`~plugins.extract.pipeline.Extractor`\n        The extractor pipeline for obtaining face identity from images\n    \"\"\"\n    def __init__(self,\n                 threshold: float,\n                 filter_files: list[str] | None,\n                 nfilter_files: list[str] | None,\n                 extractor: Extractor) -> None:\n        logger.debug(\"Initializing %s: (threshold: %s, filter_files: %s, nfilter_files: %s \"\n                     \"extractor: %s)\", self.__class__.__name__, threshold, filter_files,\n                     nfilter_files, extractor)\n        self._threshold = threshold\n        self._filter_files, self._nfilter_files = self._validate_inputs(filter_files,\n                                                                        nfilter_files)\n\n        if not self._filter_files and not self._nfilter_files:\n            logger.debug(\"Filter not selected. Exiting %s\", self.__class__.__name__)\n            return\n\n        self._embeddings: list[np.ndarray] = [np.array([]) for _ in self._filter_files]\n        self._nembeddings: list[np.ndarray] = [np.array([]) for _ in self._nfilter_files]\n        self._extractor = extractor\n\n        self._get_embeddings()\n        self._extractor.recognition.add_identity_filters(self.embeddings,\n                                                         self.n_embeddings,\n                                                         self._threshold)\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    @property\n    def active(self):\n        \"\"\" bool: ``True`` if filter files have been passed in command line arguments. ``False`` if\n        no filter files have been provided \"\"\"\n        return bool(self._filter_files) or bool(self._nfilter_files)\n\n    @property\n    def embeddings(self) -> np.ndarray:\n        \"\"\" :class:`numpy.ndarray`: The filter embeddings\"\"\"\n        if self._embeddings and all(np.any(e) for e in self._embeddings):\n            retval = np.concatenate(self._embeddings, axis=0)\n        else:\n            retval = np.array([])\n        return retval\n\n    @property\n    def n_embeddings(self) -> np.ndarray:\n        \"\"\" :class:`numpy.ndarray`: The n-filter embeddings\"\"\"\n        if self._nembeddings and all(np.any(e) for e in self._nembeddings):\n            retval = np.concatenate(self._nembeddings, axis=0)\n        else:\n            retval = np.array([])\n        return retval\n\n    @classmethod\n    def _files_from_folder(cls, input_location: list[str]) -> list[str]:\n        \"\"\" Test whether the input location is a folder and if so, return the list of contained\n        image files, otherwise return the original input location\n\n        Parameters\n        ---------\n        input_files: list\n            A list of full paths to individual files or to a folder location\n\n        Returns\n        -------\n        bool\n            Either the original list of files provided, or the image files that exist in the\n            provided folder location\n        \"\"\"\n        if not input_location or len(input_location) > 1:\n            return input_location\n\n        test_folder = input_location[0]\n        if not os.path.isdir(test_folder):\n            logger.debug(\"'%s' is not a folder. Returning original list\", test_folder)\n            return input_location\n\n        retval = [os.path.join(test_folder, fname)\n                  for fname in os.listdir(test_folder)\n                  if os.path.splitext(fname)[-1].lower() in IMAGE_EXTENSIONS]\n        logger.info(\"Collected files from folder '%s': %s\", test_folder,\n                    [os.path.basename(f) for f in retval])\n        return retval\n\n    def _validate_inputs(self,\n                         filter_files: list[str] | None,\n                         nfilter_files: list[str] | None) -> tuple[list[str], list[str]]:\n        \"\"\" Validates that the given filter/nfilter files exist, are image files and are unique\n\n        Parameters\n        ----------\n        filter_files: list or ``None``\n            The list of filter file(s) passed in as command line arguments\n        nfilter_files: list or ``None``\n            The list of nfilter file(s) passed in as command line arguments\n\n        Returns\n        -------\n        filter_files: list\n            List of full paths to filter files\n        nfilter_files: list\n            List of full paths to nfilter files\n        \"\"\"\n        error = False\n        retval: list[list[str]] = []\n\n        for files in (filter_files, nfilter_files):\n            filt_files = [] if files is None else self._files_from_folder(files)\n            for file in filt_files:\n                if (not os.path.isfile(file) or\n                        os.path.splitext(file)[-1].lower() not in IMAGE_EXTENSIONS):\n                    logger.warning(\"Filter file '%s' does not exist or is not an image file\", file)\n                    error = True\n            retval.append(filt_files)\n\n        filters = retval[0]\n        nfilters = retval[1]\n        f_fnames = set(os.path.basename(fname) for fname in filters)\n        n_fnames = set(os.path.basename(fname) for fname in nfilters)\n        if f_fnames.intersection(n_fnames):\n            error = True\n            logger.warning(\"filter and nfilter filenames should be unique. The following \"\n                           \"filenames exist in both folders: %s\", f_fnames.intersection(n_fnames))\n\n        if error:\n            logger.error(\"There was a problem processing filter files. See the above warnings for \"\n                         \"details\")\n            sys.exit(1)\n        logger.debug(\"filter_files: %s, nfilter_files: %s\", retval[0], retval[1])\n\n        return filters, nfilters\n\n    @classmethod\n    def _identity_from_extracted(cls, filename) -> tuple[np.ndarray, bool]:\n        \"\"\" Test whether the given image is a faceswap extracted face and contains identity\n        information. If so, return the identity embedding\n\n        Parameters\n        ----------\n        filename: str\n            Full path to the image file to load\n\n        Returns\n        -------\n        :class:`numpy.ndarray`\n            The identity embeddings, if they can be obtained from the image header, otherwise an\n            empty array\n        bool\n            ``True`` if the image is a faceswap extracted image otherwise ``False``\n        \"\"\"\n        if os.path.splitext(filename)[-1].lower() != \".png\":\n            logger.debug(\"'%s' not a png. Returning empty array\", filename)\n            return np.array([]), False\n\n        meta = read_image_meta(filename)\n        if \"itxt\" not in meta or \"alignments\" not in meta[\"itxt\"]:\n            logger.debug(\"'%s' does not contain faceswap data. Returning empty array\", filename)\n            return np.array([]), False\n\n        align: \"PNGHeaderAlignmentsDict\" = meta[\"itxt\"][\"alignments\"]\n        if \"identity\" not in align or \"vggface2\" not in align[\"identity\"]:\n            logger.debug(\"'%s' does not contain identity data. Returning empty array\", filename)\n            return np.array([]), True\n\n        retval = np.array(align[\"identity\"][\"vggface2\"])\n        logger.debug(\"Obtained identity for '%s'. Shape: %s\", filename, retval.shape)\n\n        return retval, True\n\n    def _process_extracted(self, item: ExtractMedia) -> None:\n        \"\"\" Process the output from the extraction pipeline.\n\n        If no face has been detected, or multiple faces are detected for the inclusive filter,\n        embeddings and filenames are removed from the filter.\n\n        if a single face is detected or multiple faces are detected for the exclusive filter,\n        embeddings are added to the relevent filter list\n\n        Parameters\n        ----------\n        item: :class:`plugins.extract.Pipeline.ExtracMedia`\n            The output from the extraction pipeline containing the identity encodings\n        \"\"\"\n        is_filter = item.filename in self._filter_files\n        lbl = \"filter\" if is_filter else \"nfilter\"\n        filelist = self._filter_files if is_filter else self._nfilter_files\n        embeddings = self._embeddings if is_filter else self._nembeddings\n        identities = np.array([face.identity[\"vggface2\"] for face in item.detected_faces])\n        idx = filelist.index(item.filename)\n\n        if len(item.detected_faces) == 0:\n            logger.warning(\"No faces detected for %s in file '%s'. Image will not be used\",\n                           lbl, os.path.basename(item.filename))\n            filelist.pop(idx)\n            embeddings.pop(idx)\n            return\n\n        if len(item.detected_faces) == 1:\n            logger.debug(\"Adding identity for %s from file '%s'\", lbl, item.filename)\n            embeddings[idx] = identities\n            return\n\n        if len(item.detected_faces) > 1 and is_filter:\n            logger.warning(\"%s faces detected for filter in '%s'. These identies will not be used\",\n                           len(item.detected_faces), os.path.basename(item.filename))\n            filelist.pop(idx)\n            embeddings.pop(idx)\n            return\n\n        if len(item.detected_faces) > 1 and not is_filter:\n            logger.warning(\"%s faces detected for nfilter in '%s'. All of these identies will be \"\n                           \"used\", len(item.detected_faces), os.path.basename(item.filename))\n            embeddings[idx] = identities\n            return\n\n    def _identity_from_extractor(self, file_list: list[str], aligned: list[str]) -> None:\n        \"\"\" Obtain the identity embeddings from the extraction pipeline\n\n        Parameters\n        ----------\n        filesile_list: list\n            List of full path to images to run through the extraction pipeline\n        aligned: list\n            List of full path to images that exist in attr:`filelist` that are faceswap aligned\n            images\n        \"\"\"\n        logger.info(\"Extracting faces to obtain identity from images\")\n        logger.debug(\"Files requiring full extraction: %s\",\n                     [fname for fname in file_list if fname not in aligned])\n        logger.debug(\"Aligned files requiring identity info: %s\", aligned)\n\n        loader = PipelineLoader(file_list, self._extractor, aligned_filenames=aligned)\n        loader.launch()\n\n        for phase in range(self._extractor.passes):\n            is_final = self._extractor.final_pass\n            detected_faces: dict[str, ExtractMedia] = {}\n            self._extractor.launch()\n            desc = \"Obtaining reference face Identity\"\n            if self._extractor.passes > 1:\n                desc = (f\"{desc } pass {phase + 1} of {self._extractor.passes}: \"\n                        f\"{self._extractor.phase_text}\")\n            for extract_media in tqdm(self._extractor.detected_faces(),\n                                      total=len(file_list),\n                                      file=sys.stdout,\n                                      desc=desc):\n                if is_final:\n                    self._process_extracted(extract_media)\n                else:\n                    extract_media.remove_image()\n                    # cache extract_media for next run\n                    detected_faces[extract_media.filename] = extract_media\n\n            if not is_final:\n                logger.debug(\"Reloading images\")\n                loader.reload(detected_faces)\n\n        self._extractor.reset_phase_index()\n\n    def _get_embeddings(self) -> None:\n        \"\"\" Obtain the embeddings for the given filter lists \"\"\"\n        needs_extraction: list[str] = []\n        aligned: list[str] = []\n\n        for files, embed in zip((self._filter_files, self._nfilter_files),\n                                (self._embeddings, self._nembeddings)):\n            for idx, file in enumerate(files):\n                identity, is_aligned = self._identity_from_extracted(file)\n                if np.any(identity):\n                    logger.debug(\"Obtained identity from png header: '%s'\", file)\n                    embed[idx] = identity[None, ...]\n                    continue\n\n                needs_extraction.append(file)\n                if is_aligned:\n                    aligned.append(file)\n\n        if needs_extraction:\n            self._identity_from_extractor(needs_extraction, aligned)\n\n        if not self._nfilter_files and not self._filter_files:\n            logger.error(\"No faces were detected from your selected identity filter files\")\n            sys.exit(1)\n\n        logger.debug(\"Filter: (filenames: %s, shape: %s), nFilter: (filenames: %s, shape: %s)\",\n                     [os.path.basename(f) for f in self._filter_files],\n                     self.embeddings.shape,\n                     [os.path.basename(f) for f in self._nfilter_files],\n                     self.n_embeddings.shape)\n\n\nclass PipelineLoader():\n    \"\"\" Handles loading and reloading images into the extraction pipeline.\n\n    Parameters\n    ----------\n    path: str or list of str\n        Full path to a folder of images or a video file or a list of image files\n    extractor: :class:`~plugins.extract.pipeline.Extractor`\n        The extractor pipeline for obtaining face identity from images\n    aligned_filenames: list, optional\n        Used for when the loader is used for getting face filter embeddings. List of full path to\n        image files that exist in :attr:`path` that are aligned faceswap images\n    \"\"\"\n    def __init__(self,\n                 path: str | list[str],\n                 extractor: Extractor,\n                 aligned_filenames: list[str] | None = None) -> None:\n        logger.debug(\"Initializing %s: (path: %s, extractor: %s, aligned_filenames: %s)\",\n                     self.__class__.__name__, path, extractor, aligned_filenames)\n        self._images = ImagesLoader(path, fast_count=True)\n        self._extractor = extractor\n        self._threads: list[MultiThread] = []\n        self._aligned_filenames = [] if aligned_filenames is None else aligned_filenames\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    @property\n    def is_video(self) -> bool:\n        \"\"\" bool: ``True`` if the input location is a video file, ``False`` if it is a folder of\n        images \"\"\"\n        return self._images.is_video\n\n    @property\n    def file_list(self) -> list[str]:\n        \"\"\" list: A full list of files in the source location. If the input is a video\n        then this is a list of dummy filenames as corresponding to an alignments file \"\"\"\n        return self._images.file_list\n\n    @property\n    def process_count(self) -> int:\n        \"\"\" int: The number of images or video frames to be processed (IE the total count less\n        items that are to be skipped from the :attr:`skip_list`)\"\"\"\n        return self._images.process_count\n\n    def add_skip_list(self, skip_list: list[int]) -> None:\n        \"\"\" Add a skip list to the :class:`ImagesLoader`\n\n        Parameters\n        ----------\n        skip_list: list\n            A list of indices corresponding to the frame indices that should be skipped by the\n            :func:`load` function.\n        \"\"\"\n        self._images.add_skip_list(skip_list)\n\n    def launch(self) -> None:\n        \"\"\" Launch the image loading pipeline \"\"\"\n        self._threaded_redirector(\"load\")\n\n    def reload(self, detected_faces: dict[str, ExtractMedia]) -> None:\n        \"\"\" Reload images for multiple pipeline passes \"\"\"\n        self._threaded_redirector(\"reload\", (detected_faces, ))\n\n    def check_thread_error(self) -> None:\n        \"\"\" Check if any errors have occurred in the running threads and raise their errors \"\"\"\n        for thread in self._threads:\n            thread.check_and_raise_error()\n\n    def join(self) -> None:\n        \"\"\" Join all open loader threads \"\"\"\n        for thread in self._threads:\n            thread.join()\n\n    def _threaded_redirector(self, task: str, io_args: tuple | None = None) -> None:\n        \"\"\" Redirect image input/output tasks to relevant queues in background thread\n\n        Parameters\n        ----------\n        task: str\n            The name of the task to be put into a background thread\n        io_args: tuple, optional\n            Any arguments that need to be provided to the background function\n        \"\"\"\n        logger.debug(\"Threading task: (Task: '%s')\", task)\n        io_args = tuple() if io_args is None else io_args\n        func = getattr(self, f\"_{task}\")\n        io_thread = MultiThread(func, *io_args, thread_count=1)\n        io_thread.start()\n        self._threads.append(io_thread)\n\n    def _load(self) -> None:\n        \"\"\" Load the images\n\n        Loads images from :class:`lib.image.ImagesLoader`, formats them into a dict compatible\n        with :class:`plugins.extract.Pipeline.Extractor` and passes them into the extraction queue.\n        \"\"\"\n        logger.debug(\"Load Images: Start\")\n        load_queue = self._extractor.input_queue\n        for filename, image in self._images.load():\n            if load_queue.shutdown.is_set():\n                logger.debug(\"Load Queue: Stop signal received. Terminating\")\n                break\n            is_aligned = filename in self._aligned_filenames\n            item = ExtractMedia(filename, image[..., :3], is_aligned=is_aligned)\n            load_queue.put(item)\n        load_queue.put(\"EOF\")\n        logger.debug(\"Load Images: Complete\")\n\n    def _reload(self, detected_faces: dict[str, ExtractMedia]) -> None:\n        \"\"\" Reload the images and pair to detected face\n\n        When the extraction pipeline is running in serial mode, images are reloaded from disk,\n        paired with their extraction data and passed back into the extraction queue\n\n        Parameters\n        ----------\n        detected_faces: dict\n            Dictionary of :class:`~plugins.extract.extract_media.ExtractMedia` with the filename as\n            the key for repopulating the image attribute.\n        \"\"\"\n        logger.debug(\"Reload Images: Start. Detected Faces Count: %s\", len(detected_faces))\n        load_queue = self._extractor.input_queue\n        for filename, image in self._images.load():\n            if load_queue.shutdown.is_set():\n                logger.debug(\"Reload Queue: Stop signal received. Terminating\")\n                break\n            logger.trace(\"Reloading image: '%s'\", filename)  # type: ignore\n            extract_media = detected_faces.pop(filename, None)\n            if not extract_media:\n                logger.warning(\"Couldn't find faces for: %s\", filename)\n                continue\n            extract_media.set_image(image)\n            load_queue.put(extract_media)\n        load_queue.put(\"EOF\")\n        logger.debug(\"Reload Images: Complete\")\n\n\nclass _Extract():\n    \"\"\" The Actual extraction process.\n\n    This class is called by the parent :class:`Extract` process\n\n    Parameters\n    ----------\n    extractor: :class:`~plugins.extract.pipeline.Extractor`\n        The extractor pipeline for running extractions\n    arguments: :class:`argparse.Namespace`\n        The arguments to be passed to the extraction process as generated from Faceswap's command\n        line arguments\n    \"\"\"\n    def __init__(self,\n                 extractor: Extractor,\n                 arguments: Namespace) -> None:\n        logger.debug(\"Initializing %s: (extractor: %s, args: %s)\", self.__class__.__name__,\n                     extractor, arguments)\n        self._args = arguments\n        self._output_dir = None if self._args.skip_saving_faces else get_folder(\n            self._args.output_dir)\n\n        logger.info(\"Output Directory: %s\", self._output_dir)\n        self._loader = PipelineLoader(self._args.input_dir, extractor)\n\n        self._alignments = Alignments(self._args, True, self._loader.is_video)\n        self._extractor = extractor\n        self._extractor.import_data(self._args.input_dir)\n\n        self._existing_count = 0\n        self._set_skip_list()\n\n        self._post_process = PostProcess(arguments)\n        self._verify_output = False\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    @property\n    def _save_interval(self) -> int | None:\n        \"\"\" int: The number of frames to be processed between each saving of the alignments file if\n        it has been provided, otherwise ``None`` \"\"\"\n        if hasattr(self._args, \"save_interval\"):\n            return self._args.save_interval\n        return None\n\n    @property\n    def _skip_num(self) -> int:\n        \"\"\" int: Number of frames to skip if extract_every_n has been provided \"\"\"\n        return self._args.extract_every_n if hasattr(self._args, \"extract_every_n\") else 1\n\n    def _set_skip_list(self) -> None:\n        \"\"\" Add the skip list to the image loader\n\n        Checks against `extract_every_n` and the existence of alignments data (can exist if\n        `skip_existing` or `skip_existing_faces` has been provided) and compiles a list of frame\n        indices that should not be processed, providing these to :class:`lib.image.ImagesLoader`.\n        \"\"\"\n        if self._skip_num == 1 and not self._alignments.data:\n            logger.debug(\"No frames to be skipped\")\n            return\n        skip_list = []\n        for idx, filename in enumerate(self._loader.file_list):\n            if idx % self._skip_num != 0:\n                logger.trace(\"Adding image '%s' to skip list due to \"  # type: ignore\n                             \"extract_every_n = %s\", filename, self._skip_num)\n                skip_list.append(idx)\n            # Items may be in the alignments file if skip-existing[-faces] is selected\n            elif os.path.basename(filename) in self._alignments.data:\n                self._existing_count += 1\n                logger.trace(\"Removing image: '%s' due to previously existing\",  # type: ignore\n                             filename)\n                skip_list.append(idx)\n        if self._existing_count != 0:\n            logger.info(\"Skipping %s frames due to skip_existing/skip_existing_faces.\",\n                        self._existing_count)\n        logger.debug(\"Adding skip list: %s\", skip_list)\n        self._loader.add_skip_list(skip_list)\n\n    def process(self) -> None:\n        \"\"\" The entry point for triggering the Extraction Process.\n\n        Should only be called from  :class:`lib.cli.launcher.ScriptExecutor`\n        \"\"\"\n        # from lib.queue_manager import queue_manager ; queue_manager.debug_monitor(3)\n        self._loader.launch()\n        self._run_extraction()\n        self._loader.join()\n        self._alignments.save()\n        finalize(self._loader.process_count + self._existing_count,\n                 self._alignments.faces_count,\n                 self._verify_output)\n\n    def _run_extraction(self) -> None:\n        \"\"\" The main Faceswap Extraction process\n\n        Receives items from :class:`plugins.extract.Pipeline.Extractor` and either saves out the\n        faces and data (if on the final pass) or reprocesses data through the pipeline for serial\n        processing.\n        \"\"\"\n        size = self._args.size if hasattr(self._args, \"size\") else 256\n        saver = None if self._args.skip_saving_faces else ImagesSaver(self._output_dir,\n                                                                      as_bytes=True)\n        for phase in range(self._extractor.passes):\n            is_final = self._extractor.final_pass\n            detected_faces: dict[str, ExtractMedia] = {}\n            self._extractor.launch()\n            self._loader.check_thread_error()\n            ph_desc = \"Extraction\" if self._extractor.passes == 1 else self._extractor.phase_text\n            desc = f\"Running pass {phase + 1} of {self._extractor.passes}: {ph_desc}\"\n            for idx, extract_media in enumerate(tqdm(self._extractor.detected_faces(),\n                                                     total=self._loader.process_count,\n                                                     file=sys.stdout,\n                                                     desc=desc,\n                                                     leave=False)):\n                self._loader.check_thread_error()\n                if is_final:\n                    self._output_processing(extract_media, size)\n                    self._output_faces(saver, extract_media)\n                    if self._save_interval and (idx + 1) % self._save_interval == 0:\n                        self._alignments.save()\n                else:\n                    extract_media.remove_image()\n                    # cache extract_media for next run\n                    detected_faces[extract_media.filename] = extract_media\n\n            if not is_final:\n                logger.debug(\"Reloading images\")\n                self._loader.reload(detected_faces)\n        if saver is not None:\n            saver.close()\n\n    def _output_processing(self, extract_media: ExtractMedia, size: int) -> None:\n        \"\"\" Prepare faces for output\n\n        Loads the aligned face, generate the thumbnail, perform any processing actions and verify\n        the output.\n\n        Parameters\n        ----------\n        extract_media: :class:`~plugins.extract.extract_media.ExtractMedia`\n            Output from :class:`plugins.extract.pipeline.Extractor`\n        size: int\n            The size that the aligned face should be created at\n        \"\"\"\n        for face in extract_media.detected_faces:\n            face.load_aligned(extract_media.image,\n                              size=size,\n                              centering=\"head\")\n            face.thumbnail = generate_thumbnail(face.aligned.face, size=96, quality=60)\n        self._post_process.do_actions(extract_media)\n        extract_media.remove_image()\n\n        faces_count = len(extract_media.detected_faces)\n        if faces_count == 0:\n            logger.verbose(\"No faces were detected in image: %s\",  # type: ignore\n                           os.path.basename(extract_media.filename))\n\n        if not self._verify_output and faces_count > 1:\n            self._verify_output = True\n\n    def _output_faces(self, saver: ImagesSaver | None, extract_media: ExtractMedia) -> None:\n        \"\"\" Output faces to save thread\n\n        Set the face filename based on the frame name and put the face to the\n        :class:`~lib.image.ImagesSaver` save queue and add the face information to the alignments\n        data.\n\n        Parameters\n        ----------\n        saver: :class:`lib.images.ImagesSaver` or ``None``\n            The background saver for saving the image or ``None`` if faces are not to be saved\n        extract_media: :class:`~plugins.extract.extract_media.ExtractMedia`\n            The output from :class:`~plugins.extract.Pipeline.Extractor`\n        \"\"\"\n        logger.trace(\"Outputting faces for %s\", extract_media.filename)  # type: ignore\n        final_faces = []\n        filename = os.path.splitext(os.path.basename(extract_media.filename))[0]\n\n        skip_idx = 0\n        for face_id, face in enumerate(extract_media.detected_faces):\n            real_face_id = face_id - skip_idx\n            output_filename = f\"{filename}_{real_face_id}.png\"\n            aligned = face.aligned.face\n            assert aligned is not None\n            meta: PNGHeaderDict = {\n                \"alignments\": face.to_png_meta(),\n                \"source\": {\"alignments_version\": self._alignments.version,\n                           \"original_filename\": output_filename,\n                           \"face_index\": real_face_id,\n                           \"source_filename\": os.path.basename(extract_media.filename),\n                           \"source_is_video\": self._loader.is_video,\n                           \"source_frame_dims\": extract_media.image_size}}\n            image = encode_image(aligned, \".png\", metadata=meta)\n\n            sub_folder = extract_media.sub_folders[face_id]\n            # Binned faces shouldn't risk filename clash, so just use original id\n            out_name = output_filename if not sub_folder else f\"{filename}_{face_id}.png\"\n\n            if saver is not None:\n                saver.save(out_name, image, sub_folder)\n\n            if sub_folder:  # This is a filtered out face being binned\n                skip_idx += 1\n                continue\n            final_faces.append(face.to_alignment())\n\n        self._alignments.data[os.path.basename(extract_media.filename)] = {\"faces\": final_faces,\n                                                                           \"video_meta\": {}}\n        del extract_media\n", "scripts/gui.py": "#!/usr/bin python3\n\"\"\" The optional GUI for faceswap \"\"\"\n\nimport logging\nimport sys\nimport tkinter as tk\nfrom tkinter import messagebox, ttk\n\nfrom lib.gui import (TaskBar, CliOptions, CommandNotebook, ConsoleOut, DisplayNotebook,\n                     get_images, initialize_images, initialize_config, LastSession,\n                     MainMenuBar, preview_trigger, ProcessWrapper, StatusBar)\n\nlogger = logging.getLogger(__name__)\n\n\nclass FaceswapGui(tk.Tk):\n    \"\"\" The Graphical User Interface \"\"\"\n\n    def __init__(self, debug):\n        logger.debug(\"Initializing %s\", self.__class__.__name__)\n        super().__init__()\n\n        self._init_args = dict(debug=debug)\n        self._config = self.initialize_globals()\n        self.set_fonts()\n        self._config.set_geometry(1200, 640, self._config.user_config_dict[\"fullscreen\"])\n\n        self.wrapper = ProcessWrapper()\n        self.objects = dict()\n\n        get_images().delete_preview()\n        preview_trigger().clear(trigger_type=None)\n        self.protocol(\"WM_DELETE_WINDOW\", self.close_app)\n        self.build_gui()\n        self._last_session = LastSession(self._config)\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    def initialize_globals(self):\n        \"\"\" Initialize config and images global constants \"\"\"\n        cliopts = CliOptions()\n        statusbar = StatusBar(self)\n        config = initialize_config(self, cliopts, statusbar)\n        initialize_images()\n        return config\n\n    def set_fonts(self):\n        \"\"\" Set global default font \"\"\"\n        tk.font.nametofont(\"TkFixedFont\").configure(size=self._config.default_font[1])\n        for font in (\"TkDefaultFont\", \"TkHeadingFont\", \"TkMenuFont\"):\n            tk.font.nametofont(font).configure(family=self._config.default_font[0],\n                                               size=self._config.default_font[1])\n\n    def build_gui(self, rebuild=False):\n        \"\"\" Build the GUI \"\"\"\n        logger.debug(\"Building GUI\")\n        if not rebuild:\n            self.tk.call('wm', 'iconphoto', self._w, get_images().icons[\"favicon\"])\n            self.configure(menu=MainMenuBar(self))\n\n        if rebuild:\n            objects = list(self.objects.keys())\n            for obj in objects:\n                self.objects[obj].destroy()\n                del self.objects[obj]\n\n        self.objects[\"taskbar\"] = TaskBar(self)\n        self.add_containers()\n\n        self.objects[\"command\"] = CommandNotebook(self.objects[\"container_top\"])\n        self.objects[\"display\"] = DisplayNotebook(self.objects[\"container_top\"])\n        self.objects[\"console\"] = ConsoleOut(self.objects[\"container_bottom\"],\n                                             self._init_args[\"debug\"])\n        self.set_initial_focus()\n        self.set_layout()\n        self._config.set_default_options()\n        logger.debug(\"Built GUI\")\n\n    def add_containers(self):\n        \"\"\" Add the paned window containers that\n            hold each main area of the gui \"\"\"\n        logger.debug(\"Adding containers\")\n        maincontainer = ttk.PanedWindow(self,\n                                        orient=tk.VERTICAL,\n                                        name=\"pw_main\")\n        maincontainer.pack(fill=tk.BOTH, expand=True)\n\n        topcontainer = ttk.PanedWindow(maincontainer,\n                                       orient=tk.HORIZONTAL,\n                                       name=\"pw_top\")\n        maincontainer.add(topcontainer)\n\n        bottomcontainer = ttk.Frame(maincontainer, name=\"frame_bottom\")\n        maincontainer.add(bottomcontainer)\n        self.objects[\"container_main\"] = maincontainer\n        self.objects[\"container_top\"] = topcontainer\n        self.objects[\"container_bottom\"] = bottomcontainer\n\n        logger.debug(\"Added containers\")\n\n    def set_initial_focus(self):\n        \"\"\" Set the tab focus from settings \"\"\"\n        tab = self._config.user_config_dict[\"tab\"]\n        logger.debug(\"Setting focus for tab: %s\", tab)\n        self._config.set_active_tab_by_name(tab)\n        logger.debug(\"Focus set to: %s\", tab)\n\n    def set_layout(self):\n        \"\"\" Set initial layout \"\"\"\n        self.update_idletasks()\n        config_opts = self._config.user_config_dict\n        r_width = self.winfo_width()\n        r_height = self.winfo_height()\n        w_ratio = config_opts[\"options_panel_width\"] / 100.0\n        h_ratio = 1 - (config_opts[\"console_panel_height\"] / 100.0)\n        width = round(r_width * w_ratio)\n        height = round(r_height * h_ratio)\n        logger.debug(\"Setting Initial Layout: (root_width: %s, root_height: %s, width_ratio: %s, \"\n                     \"height_ratio: %s, width: %s, height: %s\", r_width, r_height, w_ratio,\n                     h_ratio, width, height)\n        self.objects[\"container_top\"].sashpos(0, width)\n        self.objects[\"container_main\"].sashpos(0, height)\n        self.update_idletasks()\n\n    def rebuild(self):\n        \"\"\" Rebuild the GUI on config change \"\"\"\n        logger.debug(\"Redrawing GUI\")\n        session_state = self._last_session.to_dict()\n        self._config.refresh_config()\n        get_images().__init__()\n        self.set_fonts()\n        self.build_gui(rebuild=True)\n        if session_state is not None:\n            self._last_session.from_dict(session_state)\n        logger.debug(\"GUI Redrawn\")\n\n    def close_app(self, *args):  # pylint:disable=unused-argument\n        \"\"\" Close Python. This is here because the graph\n            animation function continues to run even when\n            tkinter has gone away \"\"\"\n        logger.debug(\"Close Requested\")\n\n        if not self._confirm_close_on_running_task():\n            return\n        if not self._config.project.confirm_close():\n            return\n\n        if self._config.tk_vars.running_task.get():\n            self.wrapper.task.terminate()\n\n        self._last_session.save()\n        get_images().delete_preview()\n        preview_trigger().clear(trigger_type=None)\n        self.quit()\n        logger.debug(\"Closed GUI\")\n        sys.exit(0)\n\n    def _confirm_close_on_running_task(self):\n        \"\"\" Pop a confirmation box to close the GUI if a task is running\n\n        Returns\n        -------\n        bool: ``True`` if user confirms close, ``False`` if user cancels close\n        \"\"\"\n        if not self._config.tk_vars.running_task.get():\n            logger.debug(\"No tasks currently running\")\n            return True\n\n        confirmtxt = \"Processes are still running.\\n\\nAre you sure you want to exit?\"\n        if not messagebox.askokcancel(\"Close\", confirmtxt, default=\"cancel\", icon=\"warning\"):\n            logger.debug(\"Close Cancelled\")\n            return False\n        logger.debug(\"Close confirmed\")\n        return True\n\n\nclass Gui():\n    \"\"\" The GUI process. \"\"\"\n    def __init__(self, arguments):\n        self.root = FaceswapGui(arguments.debug)\n\n    def process(self):\n        \"\"\" Builds the GUI \"\"\"\n        self.root.mainloop()\n", "scripts/convert.py": "#!/usr/bin python3\n\"\"\" Main entry point to the convert process of FaceSwap \"\"\"\nfrom __future__ import annotations\nfrom dataclasses import dataclass, field\nimport logging\nimport re\nimport os\nimport sys\nimport typing as T\nfrom threading import Event\nfrom time import sleep\n\nimport cv2\nimport numpy as np\nfrom tqdm import tqdm\n\nfrom scripts.fsmedia import Alignments, PostProcess, finalize\nfrom lib.serializer import get_serializer\nfrom lib.convert import Converter\nfrom lib.align import AlignedFace, DetectedFace, update_legacy_png_header\nfrom lib.gpu_stats import GPUStats\nfrom lib.image import read_image_meta_batch, ImagesLoader\nfrom lib.multithreading import MultiThread, total_cpus\nfrom lib.queue_manager import queue_manager\nfrom lib.utils import FaceswapError, get_folder, get_image_paths, handle_deprecated_cliopts\nfrom plugins.extract import ExtractMedia, Extractor\nfrom plugins.plugin_loader import PluginLoader\n\nif T.TYPE_CHECKING:\n    from argparse import Namespace\n    from collections.abc import Callable\n    from plugins.convert.writer._base import Output\n    from plugins.train.model._base import ModelBase\n    from lib.align.aligned_face import CenteringType\n    from lib.queue_manager import EventQueue\n\n\nlogger = logging.getLogger(__name__)\n\n\n@dataclass\nclass ConvertItem:\n    \"\"\" A single frame with associated objects passing through the convert process.\n\n    Parameters\n    ----------\n    input: :class:`~plugins.extract.extract_media.ExtractMedia`\n        The ExtractMedia object holding the :attr:`filename`, :attr:`image` and attr:`list` of\n        :class:`~lib.align.DetectedFace` objects loaded from disk\n    feed_faces: list, Optional\n        list of :class:`lib.align.AlignedFace` objects for feeding into the model's predict\n        function\n    reference_faces: list, Optional\n        list of :class:`lib.align.AlignedFace` objects at model output sized for using as reference\n        in the convert functionfor feeding into the model's predict\n    swapped_faces: :class:`np.ndarray`\n        The swapped faces returned from the model's predict function\n    \"\"\"\n    inbound: ExtractMedia\n    feed_faces: list[AlignedFace] = field(default_factory=list)\n    reference_faces: list[AlignedFace] = field(default_factory=list)\n    swapped_faces: np.ndarray = np.array([])\n\n\nclass Convert():\n    \"\"\" The Faceswap Face Conversion Process.\n\n    The conversion process is responsible for swapping the faces on source frames with the output\n    from a trained model.\n\n    It leverages a series of user selected post-processing plugins, executed from\n    :class:`lib.convert.Converter`.\n\n    The convert process is self contained and should not be referenced by any other scripts, so it\n    contains no public properties.\n\n    Parameters\n    ----------\n    arguments: :class:`argparse.Namespace`\n        The arguments to be passed to the convert process as generated from Faceswap's command\n        line arguments\n    \"\"\"\n    def __init__(self, arguments: Namespace) -> None:\n        logger.debug(\"Initializing %s: (args: %s)\", self.__class__.__name__, arguments)\n        self._args = handle_deprecated_cliopts(arguments)\n\n        self._images = ImagesLoader(self._args.input_dir, fast_count=True)\n        self._alignments = self._get_alignments()\n        self._opts = OptionalActions(self._args, self._images.file_list, self._alignments)\n\n        self._add_queues()\n        self._predictor = Predict(self._queue_size, arguments)\n        self._disk_io = DiskIO(self._alignments, self._images, self._predictor, arguments)\n        self._predictor.launch(self._disk_io.load_queue)\n        self._validate()\n        get_folder(self._args.output_dir)\n\n        configfile = self._args.configfile if hasattr(self._args, \"configfile\") else None\n        self._converter = Converter(self._predictor.output_size,\n                                    self._predictor.coverage_ratio,\n                                    self._predictor.centering,\n                                    self._disk_io.draw_transparent,\n                                    self._disk_io.pre_encode,\n                                    arguments,\n                                    configfile=configfile)\n        self._patch_threads = self._get_threads()\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    @property\n    def _queue_size(self) -> int:\n        \"\"\" int: Size of the converter queues. 2 for single process otherwise 4 \"\"\"\n        retval = 2 if self._args.singleprocess or self._args.jobs == 1 else 4\n        logger.debug(retval)\n        return retval\n\n    @property\n    def _pool_processes(self) -> int:\n        \"\"\" int: The number of threads to run in parallel. Based on user options and number of\n        available processors. \"\"\"\n        if self._args.singleprocess:\n            retval = 1\n        elif self._args.jobs > 0:\n            retval = min(self._args.jobs, total_cpus(), self._images.count)\n        else:\n            retval = min(total_cpus(), self._images.count)\n        retval = 1 if retval == 0 else retval\n        logger.debug(retval)\n        return retval\n\n    def _get_alignments(self) -> Alignments:\n        \"\"\" Perform validation checks and legacy updates and return alignemnts object\n\n        Returns\n        -------\n        :class:`~lib.align.alignments.Alignments`\n            The alignments file for the extract job\n        \"\"\"\n        retval = Alignments(self._args, False, self._images.is_video)\n        if retval.version == 1.0:\n            logger.error(\"The alignments file format has been updated since the given alignments \"\n                         \"file was generated. You need to update the file to proceed.\")\n            logger.error(\"To do this run the 'Alignments Tool' > 'Extract' Job.\")\n            sys.exit(1)\n\n        retval.update_legacy_has_source(os.path.basename(self._args.input_dir))\n        return retval\n\n    def _validate(self) -> None:\n        \"\"\" Validate the Command Line Options.\n\n        Ensure that certain cli selections are valid and won't result in an error. Checks:\n            * If frames have been passed in with video output, ensure user supplies reference\n            video.\n            * If \"on-the-fly\" and a Neural Network mask is selected, warn and switch to 'extended'\n            * If a mask-type is selected, ensure it exists in the alignments file.\n            * If a predicted mask-type is selected, ensure model has been trained with a mask\n            otherwise attempt to select first available masks, otherwise raise error.\n\n        Raises\n        ------\n        FaceswapError\n            If an invalid selection has been found.\n\n        \"\"\"\n        if (self._args.writer == \"ffmpeg\" and\n                not self._images.is_video and\n                self._args.reference_video is None):\n            raise FaceswapError(\"Output as video selected, but using frames as input. You must \"\n                                \"provide a reference video ('-ref', '--reference-video').\")\n\n        if (self._args.on_the_fly and\n                self._args.mask_type not in (\"none\", \"extended\", \"components\")):\n            logger.warning(\"You have selected an incompatible mask type ('%s') for On-The-Fly \"\n                           \"conversion. Switching to 'extended'\", self._args.mask_type)\n            self._args.mask_type = \"extended\"\n\n        if (not self._args.on_the_fly and\n                self._args.mask_type not in (\"none\", \"predicted\") and\n                not self._alignments.mask_is_valid(self._args.mask_type)):\n            msg = (f\"You have selected the Mask Type `{self._args.mask_type}` but at least one \"\n                   \"face does not have this mask stored in the Alignments File.\\nYou should \"\n                   \"generate the required masks with the Mask Tool or set the Mask Type option to \"\n                   \"an existing Mask Type.\\nA summary of existing masks is as follows:\\nTotal \"\n                   f\"faces: {self._alignments.faces_count}, \"\n                   f\"Masks: {self._alignments.mask_summary}\")\n            raise FaceswapError(msg)\n\n        if self._args.mask_type == \"predicted\" and not self._predictor.has_predicted_mask:\n            available_masks = [k for k, v in self._alignments.mask_summary.items()\n                               if k != \"none\" and v == self._alignments.faces_count]\n            if not available_masks:\n                msg = (\"Predicted Mask selected, but the model was not trained with a mask and no \"\n                       \"masks are stored in the Alignments File.\\nYou should generate the \"\n                       \"required masks with the Mask Tool or set the Mask Type to `none`.\")\n                raise FaceswapError(msg)\n            mask_type = available_masks[0]\n            logger.warning(\"Predicted Mask selected, but the model was not trained with a \"\n                           \"mask. Selecting first available mask: '%s'\", mask_type)\n            self._args.mask_type = mask_type\n\n    def _add_queues(self) -> None:\n        \"\"\" Add the queues for in, patch and out. \"\"\"\n        logger.debug(\"Adding queues. Queue size: %s\", self._queue_size)\n        for qname in (\"convert_in\", \"convert_out\", \"patch\"):\n            queue_manager.add_queue(qname, self._queue_size)\n\n    def _get_threads(self) -> MultiThread:\n        \"\"\" Get the threads for patching the converted faces onto the frames.\n\n        Returns\n        :class:`lib.multithreading.MultiThread`\n            The threads that perform the patching of swapped faces onto the output frames\n        \"\"\"\n        save_queue = queue_manager.get_queue(\"convert_out\")\n        patch_queue = queue_manager.get_queue(\"patch\")\n        return MultiThread(self._converter.process, patch_queue, save_queue,\n                           thread_count=self._pool_processes, name=\"patch\")\n\n    def process(self) -> None:\n        \"\"\" The entry point for triggering the Conversion Process.\n\n        Should only be called from  :class:`lib.cli.launcher.ScriptExecutor`\n\n        Raises\n        ------\n        FaceswapError\n            Error raised if the process runs out of memory\n        \"\"\"\n        logger.debug(\"Starting Conversion\")\n        # queue_manager.debug_monitor(5)\n        try:\n            self._convert_images()\n            self._disk_io.save_thread.join()\n            queue_manager.terminate_queues()\n\n            finalize(self._images.count,\n                     self._predictor.faces_count,\n                     self._predictor.verify_output)\n            logger.debug(\"Completed Conversion\")\n        except MemoryError as err:\n            msg = (\"Faceswap ran out of RAM running convert. Conversion is very system RAM \"\n                   \"heavy, so this can happen in certain circumstances when you have a lot of \"\n                   \"cpus but not enough RAM to support them all.\"\n                   \"\\nYou should lower the number of processes in use by either setting the \"\n                   \"'singleprocess' flag (-sp) or lowering the number of parallel jobs (-j).\")\n            raise FaceswapError(msg) from err\n\n    def _convert_images(self) -> None:\n        \"\"\" Start the multi-threaded patching process, monitor all threads for errors and join on\n        completion. \"\"\"\n        logger.debug(\"Converting images\")\n        self._patch_threads.start()\n        while True:\n            self._check_thread_error()\n            if self._disk_io.completion_event.is_set():\n                logger.debug(\"DiskIO completion event set. Joining Pool\")\n                break\n            if self._patch_threads.completed():\n                logger.debug(\"All patch threads completed\")\n                break\n            sleep(1)\n        self._patch_threads.join()\n\n        logger.debug(\"Putting EOF\")\n        queue_manager.get_queue(\"convert_out\").put(\"EOF\")\n        logger.debug(\"Converted images\")\n\n    def _check_thread_error(self) -> None:\n        \"\"\" Monitor all running threads for errors, and raise accordingly.\n\n        Raises\n        ------\n        Error\n            Re-raises any error encountered within any of the running threads\n        \"\"\"\n        for thread in (self._predictor.thread,\n                       self._disk_io.load_thread,\n                       self._disk_io.save_thread,\n                       self._patch_threads):\n            thread.check_and_raise_error()\n\n\nclass DiskIO():\n    \"\"\" Disk Input/Output for the converter process.\n\n    Background threads to:\n        * Load images from disk and get the detected faces\n        * Save images back to disk\n\n    Parameters\n    ----------\n    alignments: :class:`lib.alignmnents.Alignments`\n        The alignments for the input video\n    images: :class:`lib.image.ImagesLoader`\n        The input images\n    predictor: :class:`Predict`\n        The object for generating predictions from the model\n    arguments: :class:`argparse.Namespace`\n        The arguments that were passed to the convert process as generated from Faceswap's command\n        line arguments\n    \"\"\"\n\n    def __init__(self,\n                 alignments: Alignments,\n                 images: ImagesLoader,\n                 predictor: Predict,\n                 arguments: Namespace) -> None:\n        logger.debug(\"Initializing %s: (alignments: %s, images: %s, predictor: %s, arguments: %s)\",\n                     self.__class__.__name__, alignments, images, predictor, arguments)\n        self._alignments = alignments\n        self._images = images\n        self._args = arguments\n        self._pre_process = PostProcess(arguments)\n        self._completion_event = Event()\n\n        # For frame skipping\n        self._imageidxre = re.compile(r\"(\\d+)(?!.*\\d\\.)(?=\\.\\w+$)\")\n        self._frame_ranges = self._get_frame_ranges()\n        self._writer = self._get_writer(predictor)\n\n        # Extractor for on the fly detection\n        self._extractor = self._load_extractor()\n\n        self._queues: dict[T.Literal[\"load\", \"save\"], EventQueue] = {}\n        self._threads: dict[T.Literal[\"load\", \"save\"], MultiThread] = {}\n        self._init_threads()\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    @property\n    def completion_event(self) -> Event:\n        \"\"\" :class:`event.Event`: Event is set when the DiskIO Save task is complete \"\"\"\n        return self._completion_event\n\n    @property\n    def draw_transparent(self) -> bool:\n        \"\"\" bool: ``True`` if the selected writer's Draw_transparent configuration item is set\n        otherwise ``False`` \"\"\"\n        return self._writer.config.get(\"draw_transparent\", False)\n\n    @property\n    def pre_encode(self) -> Callable[[np.ndarray, T.Any], list[bytes]] | None:\n        \"\"\" python function: Selected writer's pre-encode function, if it has one,\n        otherwise ``None`` \"\"\"\n        dummy = np.zeros((20, 20, 3), dtype=\"uint8\")\n        test = self._writer.pre_encode(dummy)\n        retval: Callable | None = None if test is None else self._writer.pre_encode\n        logger.debug(\"Writer pre_encode function: %s\", retval)\n        return retval\n\n    @property\n    def save_thread(self) -> MultiThread:\n        \"\"\" :class:`lib.multithreading.MultiThread`: The thread that is running the image writing\n        operation. \"\"\"\n        return self._threads[\"save\"]\n\n    @property\n    def load_thread(self) -> MultiThread:\n        \"\"\" :class:`lib.multithreading.MultiThread`: The thread that is running the image loading\n        operation. \"\"\"\n        return self._threads[\"load\"]\n\n    @property\n    def load_queue(self) -> EventQueue:\n        \"\"\" :class:`~lib.queue_manager.EventQueue`: The queue that images and detected faces are \"\n        \"loaded into. \"\"\"\n        return self._queues[\"load\"]\n\n    @property\n    def _total_count(self) -> int:\n        \"\"\" int: The total number of frames to be converted \"\"\"\n        if self._frame_ranges and not self._args.keep_unchanged:\n            retval = sum(fr[1] - fr[0] + 1 for fr in self._frame_ranges)\n        else:\n            retval = self._images.count\n        logger.debug(retval)\n        return retval\n\n    # Initialization\n    def _get_writer(self, predictor: Predict) -> Output:\n        \"\"\" Load the selected writer plugin.\n\n        Parameters\n        ----------\n        predictor: :class:`Predict`\n            The object for generating predictions from the model\n\n        Returns\n        -------\n        :mod:`plugins.convert.writer` plugin\n            The requested writer plugin\n        \"\"\"\n        args = [self._args.output_dir]\n        if self._args.writer in (\"ffmpeg\", \"gif\"):\n            args.extend([self._total_count, self._frame_ranges])\n        if self._args.writer == \"ffmpeg\":\n            if self._images.is_video:\n                args.append(self._args.input_dir)\n            else:\n                args.append(self._args.reference_video)\n        if self._args.writer == \"patch\":\n            args.append(predictor.output_size)\n        logger.debug(\"Writer args: %s\", args)\n        configfile = self._args.configfile if hasattr(self._args, \"configfile\") else None\n        return PluginLoader.get_converter(\"writer\", self._args.writer)(*args,\n                                                                       configfile=configfile)\n\n    def _get_frame_ranges(self) -> list[tuple[int, int]] | None:\n        \"\"\" Obtain the frame ranges that are to be converted.\n\n        If frame ranges have been specified, then split the command line formatted arguments into\n        ranges that can be used.\n\n        Returns\n        list or ``None``\n            A list of  frames to be processed, or ``None`` if the command line argument was not\n            used\n        \"\"\"\n        if not self._args.frame_ranges:\n            logger.debug(\"No frame range set\")\n            return None\n\n        minframe, maxframe = None, None\n        if self._images.is_video:\n            minframe, maxframe = 1, self._images.count\n        else:\n            indices = [int(self._imageidxre.findall(os.path.basename(filename))[0])\n                       for filename in self._images.file_list]\n            if indices:\n                minframe, maxframe = min(indices), max(indices)\n        logger.debug(\"minframe: %s, maxframe: %s\", minframe, maxframe)\n\n        if minframe is None or maxframe is None:\n            raise FaceswapError(\"Frame Ranges specified, but could not determine frame numbering \"\n                                \"from filenames\")\n\n        retval = []\n        for rng in self._args.frame_ranges:\n            if \"-\" not in rng:\n                raise FaceswapError(\"Frame Ranges not specified in the correct format\")\n            start, end = rng.split(\"-\")\n            retval.append((max(int(start), minframe), min(int(end), maxframe)))\n        logger.debug(\"frame ranges: %s\", retval)\n        return retval\n\n    def _load_extractor(self) -> Extractor | None:\n        \"\"\" Load the CV2-DNN Face Extractor Chain.\n\n        For On-The-Fly conversion we use a CPU based extractor to avoid stacking the GPU.\n        Results are poor.\n\n        Returns\n        -------\n        :class:`plugins.extract.Pipeline.Extractor`\n            The face extraction chain to be used for on-the-fly conversion\n        \"\"\"\n        if not self._alignments.have_alignments_file and not self._args.on_the_fly:\n            logger.error(\"No alignments file found. Please provide an alignments file for your \"\n                         \"destination video (recommended) or enable on-the-fly conversion (not \"\n                         \"recommended).\")\n            sys.exit(1)\n        if self._alignments.have_alignments_file:\n            if self._args.on_the_fly:\n                logger.info(\"On-The-Fly conversion selected, but an alignments file was found. \"\n                            \"Using pre-existing alignments file: '%s'\", self._alignments.file)\n            else:\n                logger.debug(\"Alignments file found: '%s'\", self._alignments.file)\n            return None\n\n        logger.debug(\"Loading extractor\")\n        logger.warning(\"On-The-Fly conversion selected. This will use the inferior cv2-dnn for \"\n                       \"extraction and will produce poor results.\")\n        logger.warning(\"It is recommended to generate an alignments file for your destination \"\n                       \"video with Extract first for superior results.\")\n        extractor = Extractor(detector=\"cv2-dnn\",\n                              aligner=\"cv2-dnn\",\n                              masker=self._args.mask_type,\n                              multiprocess=True,\n                              rotate_images=None,\n                              min_size=20)\n        extractor.launch()\n        logger.debug(\"Loaded extractor\")\n        return extractor\n\n    def _init_threads(self) -> None:\n        \"\"\" Initialize queues and threads.\n\n        Creates the load and save queues and the load and save threads. Starts the threads.\n        \"\"\"\n        logger.debug(\"Initializing DiskIO Threads\")\n        for task in T.get_args(T.Literal[\"load\", \"save\"]):\n            self._add_queue(task)\n            self._start_thread(task)\n        logger.debug(\"Initialized DiskIO Threads\")\n\n    def _add_queue(self, task: T.Literal[\"load\", \"save\"]) -> None:\n        \"\"\" Add the queue to queue_manager and to :attr:`self._queues` for the given task.\n\n        Parameters\n        ----------\n        task: {\"load\", \"save\"}\n            The task that the queue is to be added for\n        \"\"\"\n        logger.debug(\"Adding queue for task: '%s'\", task)\n        if task == \"load\":\n            q_name = \"convert_in\"\n        elif task == \"save\":\n            q_name = \"convert_out\"\n        else:\n            q_name = task\n        self._queues[task] = queue_manager.get_queue(q_name)\n        logger.debug(\"Added queue for task: '%s'\", task)\n\n    def _start_thread(self, task: T.Literal[\"load\", \"save\"]) -> None:\n        \"\"\" Create the thread for the given task, add it it :attr:`self._threads` and start it.\n\n        Parameters\n        ----------\n        task: {\"load\", \"save\"}\n            The task that the thread is to be created for\n        \"\"\"\n        logger.debug(\"Starting thread: '%s'\", task)\n        args = self._completion_event if task == \"save\" else None\n        func = getattr(self, f\"_{task}\")\n        io_thread = MultiThread(func, args, thread_count=1)\n        io_thread.start()\n        self._threads[task] = io_thread\n        logger.debug(\"Started thread: '%s'\", task)\n\n    # Loading tasks\n    def _load(self, *args) -> None:  # pylint:disable=unused-argument\n        \"\"\" Load frames from disk.\n\n        In a background thread:\n            * Loads frames from disk.\n            * Discards or passes through cli selected skipped frames\n            * Pairs the frame with its :class:`~lib.align.DetectedFace` objects\n            * Performs any pre-processing actions\n            * Puts the frame and detected faces to the load queue\n        \"\"\"\n        logger.debug(\"Load Images: Start\")\n        idx = 0\n        for filename, image in self._images.load():\n            idx += 1\n            if self._queues[\"load\"].shutdown.is_set():\n                logger.debug(\"Load Queue: Stop signal received. Terminating\")\n                break\n            if image is None or (not image.any() and image.ndim not in (2, 3)):\n                # All black frames will return not numpy.any() so check dims too\n                logger.warning(\"Unable to open image. Skipping: '%s'\", filename)\n                continue\n            if self._check_skipframe(filename):\n                if self._args.keep_unchanged:\n                    logger.trace(\"Saving unchanged frame: %s\", filename)  # type:ignore\n                    out_file = os.path.join(self._args.output_dir, os.path.basename(filename))\n                    self._queues[\"save\"].put((out_file, image))\n                else:\n                    logger.trace(\"Discarding frame: '%s'\", filename)  # type:ignore\n                continue\n\n            detected_faces = self._get_detected_faces(filename, image)\n            item = ConvertItem(ExtractMedia(filename, image, detected_faces))\n            self._pre_process.do_actions(item.inbound)\n            self._queues[\"load\"].put(item)\n\n        logger.debug(\"Putting EOF\")\n        self._queues[\"load\"].put(\"EOF\")\n        logger.debug(\"Load Images: Complete\")\n\n    def _check_skipframe(self, filename: str) -> bool:\n        \"\"\" Check whether a frame is to be skipped.\n\n        Parameters\n        ----------\n        filename: str\n            The filename of the frame to check\n\n        Returns\n        -------\n        bool\n            ``True`` if the frame is to be skipped otherwise ``False``\n        \"\"\"\n        if not self._frame_ranges:\n            return False\n        indices = self._imageidxre.findall(filename)\n        if not indices:\n            logger.warning(\"Could not determine frame number. Frame will be converted: '%s'\",\n                           filename)\n            return False\n        idx = int(indices[0])\n        skipframe = not any(map(lambda b: b[0] <= idx <= b[1], self._frame_ranges))\n        logger.trace(\"idx: %s, skipframe: %s\", idx, skipframe)  # type: ignore[attr-defined]\n        return skipframe\n\n    def _get_detected_faces(self, filename: str, image: np.ndarray) -> list[DetectedFace]:\n        \"\"\" Return the detected faces for the given image.\n\n        If we have an alignments file, then the detected faces are created from that file. If\n        we're running On-The-Fly then they will be extracted from the extractor.\n\n        Parameters\n        ----------\n        filename: str\n            The filename to return the detected faces for\n        image: :class:`numpy.ndarray`\n            The frame that the detected faces exist in\n\n        Returns\n        -------\n        list\n            List of :class:`lib.align.DetectedFace` objects\n        \"\"\"\n        logger.trace(\"Getting faces for: '%s'\", filename)  # type:ignore\n        if not self._extractor:\n            detected_faces = self._alignments_faces(os.path.basename(filename), image)\n        else:\n            detected_faces = self._detect_faces(filename, image)\n        logger.trace(\"Got %s faces for: '%s'\", len(detected_faces), filename)  # type:ignore\n        return detected_faces\n\n    def _alignments_faces(self, frame_name: str, image: np.ndarray) -> list[DetectedFace]:\n        \"\"\" Return detected faces from an alignments file.\n\n        Parameters\n        ----------\n        frame_name: str\n            The name of the frame to return the detected faces for\n        image: :class:`numpy.ndarray`\n            The frame that the detected faces exist in\n\n        Returns\n        -------\n        list\n            List of :class:`lib.align.DetectedFace` objects\n        \"\"\"\n        if not self._check_alignments(frame_name):\n            return []\n\n        faces = self._alignments.get_faces_in_frame(frame_name)\n        detected_faces = []\n\n        for rawface in faces:\n            face = DetectedFace()\n            face.from_alignment(rawface, image=image)\n            detected_faces.append(face)\n        return detected_faces\n\n    def _check_alignments(self, frame_name: str) -> bool:\n        \"\"\" Ensure that we have alignments for the current frame.\n\n        If we have no alignments for this image, skip it and output a message.\n\n        Parameters\n        ----------\n        frame_name: str\n            The name of the frame to check that we have alignments for\n\n        Returns\n        -------\n        bool\n            ``True`` if we have alignments for this face, otherwise ``False``\n        \"\"\"\n        have_alignments = self._alignments.frame_exists(frame_name)\n        if not have_alignments:\n            tqdm.write(f\"No alignment found for {frame_name}, skipping\")\n        return have_alignments\n\n    def _detect_faces(self, filename: str, image: np.ndarray) -> list[DetectedFace]:\n        \"\"\" Extract the face from a frame for On-The-Fly conversion.\n\n        Pulls detected faces out of the Extraction pipeline.\n\n        Parameters\n        ----------\n        filename: str\n            The filename to return the detected faces for\n        image: :class:`numpy.ndarray`\n            The frame that the detected faces exist in\n\n        Returns\n        -------\n        list\n            List of :class:`lib.align.DetectedFace` objects\n         \"\"\"\n        assert self._extractor is not None\n        self._extractor.input_queue.put(ExtractMedia(filename, image))\n        faces = next(self._extractor.detected_faces())\n        return faces.detected_faces\n\n    # Saving tasks\n    def _save(self, completion_event: Event) -> None:\n        \"\"\" Save the converted images.\n\n        Puts the selected writer into a background thread and feeds it from the output of the\n        patch queue.\n\n        Parameters\n        ----------\n        completion_event: :class:`event.Event`\n            An even that this process triggers when it has finished saving\n        \"\"\"\n        logger.debug(\"Save Images: Start\")\n        write_preview = self._args.redirect_gui and self._writer.is_stream\n        preview_image = os.path.join(self._writer.output_folder, \".gui_preview.jpg\")\n        logger.debug(\"Write preview for gui: %s\", write_preview)\n        for idx in tqdm(range(self._total_count), desc=\"Converting\", file=sys.stdout):\n            if self._queues[\"save\"].shutdown.is_set():\n                logger.debug(\"Save Queue: Stop signal received. Terminating\")\n                break\n            item: tuple[str, np.ndarray | bytes] | T.Literal[\"EOF\"] = self._queues[\"save\"].get()\n            if item == \"EOF\":\n                logger.debug(\"EOF Received\")\n                break\n            filename, image = item\n            # Write out preview image for the GUI every 10 frames if writing to stream\n            if write_preview and idx % 10 == 0 and not os.path.exists(preview_image):\n                logger.debug(\"Writing GUI Preview image: '%s'\", preview_image)\n                assert isinstance(image, np.ndarray)\n                cv2.imwrite(preview_image, image)\n            self._writer.write(filename, image)\n        self._writer.close()\n        completion_event.set()\n        logger.debug(\"Save Faces: Complete\")\n\n\nclass Predict():\n    \"\"\" Obtains the output from the Faceswap model.\n\n    Parameters\n    ----------\n    queue_size: int\n        The maximum size of the input queue\n    arguments: :class:`argparse.Namespace`\n        The arguments that were passed to the convert process as generated from Faceswap's command\n        line arguments\n    \"\"\"\n    def __init__(self, queue_size: int, arguments: Namespace) -> None:\n        logger.debug(\"Initializing %s: (args: %s, queue_size: %s)\",\n                     self.__class__.__name__, arguments, queue_size)\n        self._args = arguments\n        self._in_queue: EventQueue | None = None\n        self._out_queue = queue_manager.get_queue(\"patch\")\n        self._serializer = get_serializer(\"json\")\n        self._faces_count = 0\n        self._verify_output = False\n\n        self._model = self._load_model()\n        self._batchsize = self._get_batchsize(queue_size)\n        self._sizes = self._get_io_sizes()\n        self._coverage_ratio = self._model.coverage_ratio\n        self._centering = self._model.config[\"centering\"]\n\n        self._thread: MultiThread | None = None\n        logger.debug(\"Initialized %s: (out_queue: %s)\", self.__class__.__name__, self._out_queue)\n\n    @property\n    def thread(self) -> MultiThread:\n        \"\"\" :class:`~lib.multithreading.MultiThread`: The thread that is running the prediction\n        function from the Faceswap model. \"\"\"\n        assert self._thread is not None\n        return self._thread\n\n    @property\n    def in_queue(self) -> EventQueue:\n        \"\"\" :class:`~lib.queue_manager.EventQueue`: The input queue to the predictor. \"\"\"\n        assert self._in_queue is not None\n        return self._in_queue\n\n    @property\n    def out_queue(self) -> EventQueue:\n        \"\"\" :class:`~lib.queue_manager.EventQueue`: The output queue from the predictor. \"\"\"\n        return self._out_queue\n\n    @property\n    def faces_count(self) -> int:\n        \"\"\" int: The total number of faces seen by the Predictor. \"\"\"\n        return self._faces_count\n\n    @property\n    def verify_output(self) -> bool:\n        \"\"\" bool: ``True`` if multiple faces have been found in frames, otherwise ``False``. \"\"\"\n        return self._verify_output\n\n    @property\n    def coverage_ratio(self) -> float:\n        \"\"\" float: The coverage ratio that the model was trained at. \"\"\"\n        return self._coverage_ratio\n\n    @property\n    def centering(self) -> CenteringType:\n        \"\"\" str: The centering that the model was trained on (`\"head\", \"face\"` or `\"legacy\"`) \"\"\"\n        return self._centering\n\n    @property\n    def has_predicted_mask(self) -> bool:\n        \"\"\" bool: ``True`` if the model was trained to learn a mask, otherwise ``False``. \"\"\"\n        return bool(self._model.config.get(\"learn_mask\", False))\n\n    @property\n    def output_size(self) -> int:\n        \"\"\" int: The size in pixels of the Faceswap model output. \"\"\"\n        return self._sizes[\"output\"]\n\n    def _get_io_sizes(self) -> dict[str, int]:\n        \"\"\" Obtain the input size and output size of the model.\n\n        Returns\n        -------\n        dict\n            input_size in pixels and output_size in pixels\n        \"\"\"\n        input_shape = self._model.model.input_shape\n        input_shape = [input_shape] if not isinstance(input_shape, list) else input_shape\n        output_shape = self._model.model.output_shape\n        output_shape = [output_shape] if not isinstance(output_shape, list) else output_shape\n        retval = {\"input\": input_shape[0][1], \"output\": output_shape[-1][1]}\n        logger.debug(retval)\n        return retval\n\n    def _load_model(self) -> ModelBase:\n        \"\"\" Load the Faceswap model.\n\n        Returns\n        -------\n        :mod:`plugins.train.model` plugin\n            The trained model in the specified model folder\n        \"\"\"\n        logger.debug(\"Loading Model\")\n        model_dir = get_folder(self._args.model_dir, make_folder=False)\n        if not model_dir:\n            raise FaceswapError(f\"{self._args.model_dir} does not exist.\")\n        trainer = self._get_model_name(model_dir)\n        model = PluginLoader.get_model(trainer)(model_dir, self._args, predict=True)\n        model.build()\n        logger.debug(\"Loaded Model\")\n        return model\n\n    def _get_batchsize(self, queue_size: int) -> int:\n        \"\"\" Get the batch size for feeding the model.\n\n        Sets the batch size to 1 if inference is being run on CPU, otherwise the minimum of the\n        input queue size and the model's `convert_batchsize` configuration option.\n\n        Parameters\n        ----------\n        queue_size: int\n            The queue size that is feeding the predictor\n\n        Returns\n        -------\n        int\n            The batch size that the model is to be fed at.\n        \"\"\"\n        logger.debug(\"Getting batchsize\")\n        is_cpu = GPUStats().device_count == 0\n        batchsize = 1 if is_cpu else self._model.config[\"convert_batchsize\"]\n        batchsize = min(queue_size, batchsize)\n        logger.debug(\"Got batchsize: %s\", batchsize)\n        return batchsize\n\n    def _get_model_name(self, model_dir: str) -> str:\n        \"\"\" Return the name of the Faceswap model used.\n\n        Retrieve the name of the model from the model's state file.\n\n        Parameters\n        ----------\n        model_dir: str\n            The folder that contains the trained Faceswap model\n\n        Returns\n        -------\n        str\n            The name of the Faceswap model being used.\n\n        \"\"\"\n        statefiles = [fname for fname in os.listdir(str(model_dir))\n                      if fname.endswith(\"_state.json\")]\n        if len(statefiles) != 1:\n            raise FaceswapError(\"There should be 1 state file in your model folder. \"\n                                f\"{len(statefiles)} were found.\")\n        statefile = os.path.join(str(model_dir), statefiles[0])\n\n        state = self._serializer.load(statefile)\n        trainer = state.get(\"name\", None)\n\n        if not trainer:\n            raise FaceswapError(\"Trainer name could not be read from state file.\")\n        logger.debug(\"Trainer from state file: '%s'\", trainer)\n        return trainer\n\n    def launch(self, load_queue: EventQueue) -> None:\n        \"\"\" Launch the prediction process in a background thread.\n\n        Starts the prediction thread and returns the thread.\n\n        Parameters\n        ----------\n        load_queue: :class:`~lib.queue_manager.EventQueue`\n            The queue that contains images and detected faces for feeding the model\n        \"\"\"\n        self._in_queue = load_queue\n        self._thread = MultiThread(self._predict_faces, thread_count=1)\n        self._thread.start()\n\n    def _predict_faces(self) -> None:\n        \"\"\" Run Prediction on the Faceswap model in a background thread.\n\n        Reads from the :attr:`self._in_queue`, prepares images for prediction\n        then puts the predictions back to the :attr:`self.out_queue`\n        \"\"\"\n        faces_seen = 0\n        consecutive_no_faces = 0\n        batch: list[ConvertItem] = []\n        assert self._in_queue is not None\n        while True:\n            item: T.Literal[\"EOF\"] | ConvertItem = self._in_queue.get()\n            if item == \"EOF\":\n                logger.debug(\"EOF Received\")\n                if batch:  # Process out any remaining items\n                    self._process_batch(batch, faces_seen)\n                break\n            logger.trace(\"Got from queue: '%s'\", item.inbound.filename)  # type:ignore\n            faces_count = len(item.inbound.detected_faces)\n\n            # Safety measure. If a large stream of frames appear that do not have faces,\n            # these will stack up into RAM. Keep a count of consecutive frames with no faces.\n            # If self._batchsize number of frames appear, force the current batch through\n            # to clear RAM.\n            consecutive_no_faces = consecutive_no_faces + 1 if faces_count == 0 else 0\n            self._faces_count += faces_count\n            if faces_count > 1:\n                self._verify_output = True\n                logger.verbose(\"Found more than one face in an image! '%s'\",  # type:ignore\n                               os.path.basename(item.inbound.filename))\n\n            self.load_aligned(item)\n            faces_seen += faces_count\n\n            batch.append(item)\n\n            if faces_seen < self._batchsize and consecutive_no_faces < self._batchsize:\n                logger.trace(\"Continuing. Current batchsize: %s, \"  # type:ignore\n                             \"consecutive_no_faces: %s\", faces_seen, consecutive_no_faces)\n                continue\n\n            self._process_batch(batch, faces_seen)\n\n            consecutive_no_faces = 0\n            faces_seen = 0\n            batch = []\n\n        logger.debug(\"Putting EOF\")\n        self._out_queue.put(\"EOF\")\n        logger.debug(\"Load queue complete\")\n\n    def _process_batch(self, batch: list[ConvertItem], faces_seen: int):\n        \"\"\" Predict faces on the given batch of images and queue out to patch thread\n\n        Parameters\n        ----------\n        batch: list\n            List of :class:`ConvertItem` objects for the current batch\n        faces_seen: int\n            The number of faces seen in the current batch\n\n        Returns\n        -------\n        :class:`np.narray`\n            The predicted faces for the current batch\n        \"\"\"\n        logger.trace(\"Batching to predictor. Frames: %s, Faces: %s\",  # type:ignore\n                     len(batch), faces_seen)\n        feed_batch = [feed_face for item in batch for feed_face in item.feed_faces]\n        if faces_seen != 0:\n            feed_faces = self._compile_feed_faces(feed_batch)\n            batch_size = None\n            predicted = self._predict(feed_faces, batch_size)\n        else:\n            predicted = np.array([])\n\n        self._queue_out_frames(batch, predicted)\n\n    def load_aligned(self, item: ConvertItem) -> None:\n        \"\"\" Load the model's feed faces and the reference output faces.\n\n        For each detected face in the incoming item, load the feed face and reference face\n        images, correctly sized for input and output respectively.\n\n        Parameters\n        ----------\n        item: :class:`ConvertMedia`\n            The convert media object, containing the ExctractMedia for the current image\n        \"\"\"\n        logger.trace(\"Loading aligned faces: '%s'\", item.inbound.filename)  # type:ignore\n        feed_faces = []\n        reference_faces = []\n        for detected_face in item.inbound.detected_faces:\n            feed_face = AlignedFace(detected_face.landmarks_xy,\n                                    image=item.inbound.image,\n                                    centering=self._centering,\n                                    size=self._sizes[\"input\"],\n                                    coverage_ratio=self._coverage_ratio,\n                                    dtype=\"float32\")\n            if self._sizes[\"input\"] == self._sizes[\"output\"]:\n                reference_faces.append(feed_face)\n            else:\n                reference_faces.append(AlignedFace(detected_face.landmarks_xy,\n                                                   image=item.inbound.image,\n                                                   centering=self._centering,\n                                                   size=self._sizes[\"output\"],\n                                                   coverage_ratio=self._coverage_ratio,\n                                                   dtype=\"float32\"))\n            feed_faces.append(feed_face)\n        item.feed_faces = feed_faces\n        item.reference_faces = reference_faces\n        logger.trace(\"Loaded aligned faces: '%s'\", item.inbound.filename)  # type:ignore\n\n    @staticmethod\n    def _compile_feed_faces(feed_faces: list[AlignedFace]) -> np.ndarray:\n        \"\"\" Compile a batch of faces for feeding into the Predictor.\n\n        Parameters\n        ----------\n        feed_faces: list\n            List of :class:`~lib.align.AlignedFace` objects sized for feeding into the model\n\n        Returns\n        -------\n        :class:`numpy.ndarray`\n            A batch of faces ready for feeding into the Faceswap model.\n        \"\"\"\n        logger.trace(\"Compiling feed face. Batchsize: %s\", len(feed_faces))  # type:ignore\n        retval = np.stack([T.cast(np.ndarray, feed_face.face)[..., :3]\n                           for feed_face in feed_faces]) / 255.0\n        logger.trace(\"Compiled Feed faces. Shape: %s\", retval.shape)  # type:ignore\n        return retval\n\n    def _predict(self, feed_faces: np.ndarray, batch_size: int | None = None) -> np.ndarray:\n        \"\"\" Run the Faceswap models' prediction function.\n\n        Parameters\n        ----------\n        feed_faces: :class:`numpy.ndarray`\n            The batch to be fed into the model\n        batch_size: int, optional\n            Used for plaidml only. Indicates to the model what batch size is being processed.\n            Default: ``None``\n\n        Returns\n        -------\n        :class:`numpy.ndarray`\n            The swapped faces for the given batch\n        \"\"\"\n        logger.trace(\"Predicting: Batchsize: %s\", len(feed_faces))  # type:ignore\n\n        if self._model.color_order.lower() == \"rgb\":\n            feed_faces = feed_faces[..., ::-1]\n\n        feed = [feed_faces]\n        logger.trace(\"Input shape(s): %s\", [item.shape for item in feed])  # type:ignore\n\n        inbound = self._model.model.predict(feed, verbose=0, batch_size=batch_size)\n        predicted: list[np.ndarray] = inbound if isinstance(inbound, list) else [inbound]\n\n        if self._model.color_order.lower() == \"rgb\":\n            predicted[0] = predicted[0][..., ::-1]\n\n        logger.trace(\"Output shape(s): %s\",  # type:ignore\n                     [predict.shape for predict in predicted])\n\n        # Only take last output(s)\n        if predicted[-1].shape[-1] == 1:  # Merge mask to alpha channel\n            retval = np.concatenate(predicted[-2:], axis=-1).astype(\"float32\")\n        else:\n            retval = predicted[-1].astype(\"float32\")\n\n        logger.trace(\"Final shape: %s\", retval.shape)  # type:ignore\n        return retval\n\n    def _queue_out_frames(self, batch: list[ConvertItem], swapped_faces: np.ndarray) -> None:\n        \"\"\" Compile the batch back to original frames and put to the Out Queue.\n\n        For batching, faces are split away from their frames. This compiles all detected faces\n        back to their parent frame before putting each frame to the out queue in batches.\n\n        Parameters\n        ----------\n        batch: dict\n            The batch that was used as the input for the model predict function\n        swapped_faces: :class:`numpy.ndarray`\n            The predictions returned from the model's predict function\n        \"\"\"\n        logger.trace(\"Queueing out batch. Batchsize: %s\", len(batch))  # type:ignore\n        pointer = 0\n        for item in batch:\n            num_faces = len(item.inbound.detected_faces)\n            if num_faces != 0:\n                item.swapped_faces = swapped_faces[pointer:pointer + num_faces]\n\n            logger.trace(\"Putting to queue. ('%s', detected_faces: %s, \"  # type:ignore\n                         \"reference_faces: %s, swapped_faces: %s)\", item.inbound.filename,\n                         len(item.inbound.detected_faces), len(item.reference_faces),\n                         item.swapped_faces.shape[0])\n            pointer += num_faces\n        self._out_queue.put(batch)\n        logger.trace(\"Queued out batch. Batchsize: %s\", len(batch))  # type:ignore\n\n\nclass OptionalActions():  # pylint:disable=too-few-public-methods\n    \"\"\" Process specific optional actions for Convert.\n\n    Currently only handles skip faces. This class should probably be (re)moved.\n\n    Parameters\n    ----------\n    arguments: :class:`argparse.Namespace`\n        The arguments that were passed to the convert process as generated from Faceswap's command\n        line arguments\n    input_images: list\n        List of input image files\n    alignments: :class:`lib.align.Alignments`\n        The alignments file for this conversion\n    \"\"\"\n    def __init__(self,\n                 arguments: Namespace,\n                 input_images: list[np.ndarray],\n                 alignments: Alignments) -> None:\n        logger.debug(\"Initializing %s\", self.__class__.__name__)\n        self._args = arguments\n        self._input_images = input_images\n        self._alignments = alignments\n\n        self._remove_skipped_faces()\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    # SKIP FACES #\n    def _remove_skipped_faces(self) -> None:\n        \"\"\" If the user has specified an input aligned directory, remove any non-matching faces\n        from the alignments file. \"\"\"\n        logger.debug(\"Filtering Faces\")\n        accept_dict = self._get_face_metadata()\n        if not accept_dict:\n            logger.debug(\"No aligned face data. Not skipping any faces\")\n            return\n        pre_face_count = self._alignments.faces_count\n        self._alignments.filter_faces(accept_dict, filter_out=False)\n        logger.info(\"Faces filtered out: %s\", pre_face_count - self._alignments.faces_count)\n\n    def _get_face_metadata(self) -> dict[str, list[int]]:\n        \"\"\" Check for the existence of an aligned directory for identifying which faces in the\n        target frames should be swapped. If it exists, scan the folder for face's metadata\n\n        Returns\n        -------\n        dict\n            Dictionary of source frame names with a list of associated face indices to be skipped\n        \"\"\"\n        retval: dict[str, list[int]] = {}\n        input_aligned_dir = self._args.input_aligned_dir\n\n        if input_aligned_dir is None:\n            logger.verbose(\"Aligned directory not specified. All faces listed in \"  # type:ignore\n                           \"the alignments file will be converted\")\n            return retval\n        if not os.path.isdir(input_aligned_dir):\n            logger.warning(\"Aligned directory not found. All faces listed in the \"\n                           \"alignments file will be converted\")\n            return retval\n\n        log_once = False\n        filelist = get_image_paths(input_aligned_dir)\n        for fullpath, metadata in tqdm(read_image_meta_batch(filelist),\n                                       total=len(filelist),\n                                       desc=\"Reading Face Data\",\n                                       leave=False):\n            if \"itxt\" not in metadata or \"source\" not in metadata[\"itxt\"]:\n                # UPDATE LEGACY FACES FROM ALIGNMENTS FILE\n                if not log_once:\n                    logger.warning(\"Legacy faces discovered in '%s'. These faces will be updated\",\n                                   input_aligned_dir)\n                    log_once = True\n                data = update_legacy_png_header(fullpath, self._alignments)\n                if not data:\n                    raise FaceswapError(\n                        f\"Some of the faces being passed in from '{input_aligned_dir}' could not \"\n                        f\"be matched to the alignments file '{self._alignments.file}'\\n\"\n                        \"Please double check your sources and try again.\")\n                meta = data[\"source\"]\n            else:\n                meta = metadata[\"itxt\"][\"source\"]\n            retval.setdefault(meta[\"source_filename\"], []).append(meta[\"face_index\"])\n\n        if not retval:\n            raise FaceswapError(\"Aligned directory is empty, no faces will be converted!\")\n        if len(retval) <= len(self._input_images) / 3:\n            logger.warning(\"Aligned directory contains far fewer images than the input \"\n                           \"directory, are you sure this is the right folder?\")\n        return retval\n", "scripts/__init__.py": "", "lib/sysinfo.py": "#!/usr/bin python3\n\"\"\" Obtain information about the running system, environment and GPU. \"\"\"\n\nimport json\nimport locale\nimport os\nimport platform\nimport sys\n\nfrom subprocess import PIPE, Popen\n\nimport psutil\n\nfrom lib.git import git\nfrom lib.gpu_stats import GPUStats, GPUInfo\nfrom lib.utils import get_backend\nfrom setup import CudaCheck\n\n\nclass _SysInfo():\n    \"\"\" Obtain information about the System, Python and GPU \"\"\"\n    def __init__(self) -> None:\n        self._state_file = _State().state_file\n        self._configs = _Configs().configs\n        self._system = {\"platform\": platform.platform(),\n                        \"system\": platform.system().lower(),\n                        \"machine\": platform.machine(),\n                        \"release\": platform.release(),\n                        \"processor\": platform.processor(),\n                        \"cpu_count\": os.cpu_count()}\n        self._python = {\"implementation\": platform.python_implementation(),\n                        \"version\": platform.python_version()}\n        self._gpu = self._get_gpu_info()\n        self._cuda_check = CudaCheck()\n\n    @property\n    def _encoding(self) -> str:\n        \"\"\" str: The system preferred encoding \"\"\"\n        return locale.getpreferredencoding()\n\n    @property\n    def _is_conda(self) -> bool:\n        \"\"\" bool: `True` if running in a Conda environment otherwise ``False``. \"\"\"\n        return (\"conda\" in sys.version.lower() or\n                os.path.exists(os.path.join(sys.prefix, 'conda-meta')))\n\n    @property\n    def _is_linux(self) -> bool:\n        \"\"\" bool: `True` if running on a Linux system otherwise ``False``. \"\"\"\n        return self._system[\"system\"] == \"linux\"\n\n    @property\n    def _is_macos(self) -> bool:\n        \"\"\" bool: `True` if running on a macOS system otherwise ``False``. \"\"\"\n        return self._system[\"system\"] == \"darwin\"\n\n    @property\n    def _is_windows(self) -> bool:\n        \"\"\" bool: `True` if running on a Windows system otherwise ``False``. \"\"\"\n        return self._system[\"system\"] == \"windows\"\n\n    @property\n    def _is_virtual_env(self) -> bool:\n        \"\"\" bool: `True` if running inside a virtual environment otherwise ``False``. \"\"\"\n        if not self._is_conda:\n            retval = (hasattr(sys, \"real_prefix\") or\n                      (hasattr(sys, \"base_prefix\") and sys.base_prefix != sys.prefix))\n        else:\n            prefix = os.path.dirname(sys.prefix)\n            retval = os.path.basename(prefix) == \"envs\"\n        return retval\n\n    @property\n    def _ram_free(self) -> int:\n        \"\"\" int: The amount of free RAM in bytes. \"\"\"\n        return psutil.virtual_memory().free\n\n    @property\n    def _ram_total(self) -> int:\n        \"\"\" int: The amount of total RAM in bytes. \"\"\"\n        return psutil.virtual_memory().total\n\n    @property\n    def _ram_available(self) -> int:\n        \"\"\" int: The amount of available RAM in bytes. \"\"\"\n        return psutil.virtual_memory().available\n\n    @property\n    def _ram_used(self) -> int:\n        \"\"\" int: The amount of used RAM in bytes. \"\"\"\n        return psutil.virtual_memory().used\n\n    @property\n    def _fs_command(self) -> str:\n        \"\"\" str: The command line command used to execute faceswap. \"\"\"\n        return \" \".join(sys.argv)\n\n    @property\n    def _installed_pip(self) -> str:\n        \"\"\" str: The list of installed pip packages within Faceswap's scope. \"\"\"\n        with Popen(f\"{sys.executable} -m pip freeze\", shell=True, stdout=PIPE) as pip:\n            installed = pip.communicate()[0].decode(self._encoding, errors=\"replace\").splitlines()\n        return \"\\n\".join(installed)\n\n    @property\n    def _installed_conda(self) -> str:\n        \"\"\" str: The list of installed Conda packages within Faceswap's scope. \"\"\"\n        if not self._is_conda:\n            return \"\"\n        with Popen(\"conda list\", shell=True, stdout=PIPE, stderr=PIPE) as conda:\n            stdout, stderr = conda.communicate()\n        if stderr:\n            return \"Could not get package list\"\n        installed = stdout.decode(self._encoding, errors=\"replace\").splitlines()\n        return \"\\n\".join(installed)\n\n    @property\n    def _conda_version(self) -> str:\n        \"\"\" str: The installed version of Conda, or `N/A` if Conda is not installed. \"\"\"\n        if not self._is_conda:\n            return \"N/A\"\n        with Popen(\"conda --version\", shell=True, stdout=PIPE, stderr=PIPE) as conda:\n            stdout, stderr = conda.communicate()\n        if stderr:\n            return \"Conda is used, but version not found\"\n        version = stdout.decode(self._encoding, errors=\"replace\").splitlines()\n        return \"\\n\".join(version)\n\n    @property\n    def _git_commits(self) -> str:\n        \"\"\" str: The last 5 git commits for the currently running Faceswap. \"\"\"\n        commits = git.get_commits(3)\n        if not commits:\n            return \"Not Found\"\n        return \" | \".join(commits)\n\n    @property\n    def _cuda_version(self) -> str:\n        \"\"\" str: The installed CUDA version. \"\"\"\n        # TODO Handle multiple CUDA installs\n        retval = self._cuda_check.cuda_version\n        if not retval:\n            retval = \"No global version found\"\n            if self._is_conda:\n                retval += \". Check Conda packages for Conda Cuda\"\n        return retval\n\n    @property\n    def _cudnn_version(self) -> str:\n        \"\"\" str: The installed cuDNN version. \"\"\"\n        retval = self._cuda_check.cudnn_version\n        if not retval:\n            retval = \"No global version found\"\n            if self._is_conda:\n                retval += \". Check Conda packages for Conda cuDNN\"\n        return retval\n\n    def _get_gpu_info(self) -> GPUInfo:\n        \"\"\" Obtain GPU Stats. If an error is raised, swallow the error, and add to GPUInfo output\n\n        Returns\n        -------\n        :class:`~lib.gpu_stats.GPUInfo`\n            The information on connected GPUs\n        \"\"\"\n        try:\n            retval = GPUStats(log=False).sys_info\n        except Exception as err:  # pylint:disable=broad-except\n            err_string = f\"{type(err)}: {err}\"\n            retval = GPUInfo(vram=[],\n                             vram_free=[],\n                             driver=\"N/A\",\n                             devices=[f\"Error obtaining GPU Stats: '{err_string}'\"],\n                             devices_active=[])\n        return retval\n\n    def full_info(self) -> str:\n        \"\"\" Obtain extensive system information stats, formatted into a human readable format.\n\n        Returns\n        -------\n        str\n            The system information for the currently running system, formatted for output to\n            console or a log file.\n        \"\"\"\n        retval = \"\\n============ System Information ============\\n\"\n        sys_info = {\"backend\": get_backend(),\n                    \"os_platform\": self._system[\"platform\"],\n                    \"os_machine\": self._system[\"machine\"],\n                    \"os_release\": self._system[\"release\"],\n                    \"py_conda_version\": self._conda_version,\n                    \"py_implementation\": self._python[\"implementation\"],\n                    \"py_version\": self._python[\"version\"],\n                    \"py_command\": self._fs_command,\n                    \"py_virtual_env\": self._is_virtual_env,\n                    \"sys_cores\": self._system[\"cpu_count\"],\n                    \"sys_processor\": self._system[\"processor\"],\n                    \"sys_ram\": self._format_ram(),\n                    \"encoding\": self._encoding,\n                    \"git_branch\": git.branch,\n                    \"git_commits\": self._git_commits,\n                    \"gpu_cuda\": self._cuda_version,\n                    \"gpu_cudnn\": self._cudnn_version,\n                    \"gpu_driver\": self._gpu.driver,\n                    \"gpu_devices\": \", \".join([f\"GPU_{idx}: {device}\"\n                                              for idx, device in enumerate(self._gpu.devices)]),\n                    \"gpu_vram\": \", \".join(\n                        f\"GPU_{idx}: {int(vram)}MB ({int(vram_free)}MB free)\"\n                        for idx, (vram, vram_free) in enumerate(zip(self._gpu.vram,\n                                                                    self._gpu.vram_free))),\n                    \"gpu_devices_active\": \", \".join([f\"GPU_{idx}\"\n                                                     for idx in self._gpu.devices_active])}\n        for key in sorted(sys_info.keys()):\n            retval += (f\"{key + ':':<20} {sys_info[key]}\\n\")\n        retval += \"\\n=============== Pip Packages ===============\\n\"\n        retval += self._installed_pip\n        if self._is_conda:\n            retval += \"\\n\\n============== Conda Packages ==============\\n\"\n            retval += self._installed_conda\n        retval += self._state_file\n        retval += \"\\n\\n================= Configs ==================\"\n        retval += self._configs\n        return retval\n\n    def _format_ram(self) -> str:\n        \"\"\" Format the RAM stats into Megabytes to make it more readable.\n\n        Returns\n        -------\n        str\n            The total, available, used and free RAM displayed in Megabytes\n        \"\"\"\n        retval = []\n        for name in (\"total\", \"available\", \"used\", \"free\"):\n            value = getattr(self, f\"_ram_{name}\")\n            value = int(value / (1024 * 1024))\n            retval.append(f\"{name.capitalize()}: {value}MB\")\n        return \", \".join(retval)\n\n\ndef get_sysinfo() -> str:\n    \"\"\" Obtain extensive system information stats, formatted into a human readable format.\n    If an error occurs obtaining the system information, then the error message is returned\n    instead.\n\n    Returns\n    -------\n    str\n        The system information for the currently running system, formatted for output to\n        console or a log file.\n    \"\"\"\n    try:\n        retval = _SysInfo().full_info()\n    except Exception as err:  # pylint:disable=broad-except\n        retval = f\"Exception occured trying to retrieve sysinfo: {str(err)}\"\n        raise\n    return retval\n\n\nclass _Configs():  # pylint:disable=too-few-public-methods\n    \"\"\" Parses the config files in /faceswap/config and outputs the information stored within them\n    in a human readable format. \"\"\"\n\n    def __init__(self) -> None:\n        self.config_dir = os.path.join(os.path.abspath(os.path.dirname(sys.argv[0])), \"config\")\n        self.configs = self._get_configs()\n\n    def _get_configs(self) -> str:\n        \"\"\" Obtain the formatted configurations from the config folder.\n\n        Returns\n        -------\n        str\n            The current configuration in the config files formatted in a human readable format\n        \"\"\"\n        try:\n            config_files = [os.path.join(self.config_dir, cfile)\n                            for cfile in os.listdir(self.config_dir)\n                            if os.path.basename(cfile) == \".faceswap\"\n                            or os.path.splitext(cfile)[1] == \".ini\"]\n            return self._parse_configs(config_files)\n        except FileNotFoundError:\n            return \"\"\n\n    def _parse_configs(self, config_files: list[str]) -> str:\n        \"\"\" Parse the given list of config files into a human readable format.\n\n        Parameters\n        ----------\n        config_files: list\n            A list of paths to the faceswap config files\n\n        Returns\n        -------\n        str\n            The current configuration in the config files formatted in a human readable format\n        \"\"\"\n        formatted = \"\"\n        for cfile in config_files:\n            fname = os.path.basename(cfile)\n            ext = os.path.splitext(cfile)[1]\n            formatted += f\"\\n--------- {fname} ---------\\n\"\n            if ext == \".ini\":\n                formatted += self._parse_ini(cfile)\n            elif fname == \".faceswap\":\n                formatted += self._parse_json(cfile)\n        return formatted\n\n    def _parse_ini(self, config_file: str) -> str:\n        \"\"\" Parse an ``.ini`` formatted config file into a human readable format.\n\n        Parameters\n        ----------\n        config_file: str\n            The path to the config.ini file\n\n        Returns\n        -------\n        str\n            The current configuration in the config file formatted in a human readable format\n        \"\"\"\n        formatted = \"\"\n        with open(config_file, \"r\", encoding=\"utf-8\", errors=\"replace\") as cfile:\n            for line in cfile.readlines():\n                line = line.strip()\n                if line.startswith(\"#\") or not line:\n                    continue\n                item = line.split(\"=\")\n                if len(item) == 1:\n                    formatted += f\"\\n{item[0].strip()}\\n\"\n                else:\n                    formatted += self._format_text(item[0], item[1])\n        return formatted\n\n    def _parse_json(self, config_file: str) -> str:\n        \"\"\" Parse an ``.json`` formatted config file into a formatted string.\n\n        Parameters\n        ----------\n        config_file: str\n            The path to the config.json file\n\n        Returns\n        -------\n        dict\n            The current configuration in the config file formatted as a python dictionary\n        \"\"\"\n        formatted: str = \"\"\n        with open(config_file, \"r\", encoding=\"utf-8\", errors=\"replace\") as cfile:\n            conf_dict = json.load(cfile)\n            for key in sorted(conf_dict.keys()):\n                formatted += self._format_text(key, conf_dict[key])\n        return formatted\n\n    @staticmethod\n    def _format_text(key: str, value: str) -> str:\n        \"\"\"Format a key value pair into a consistently spaced string output for display.\n\n        Parameters\n        ----------\n        key: str\n            The label for this display item\n        value: str\n            The value for this display item\n\n        Returns\n        -------\n        str\n            The formatted key value pair for display\n        \"\"\"\n        return f\"{key.strip() + ':':<25} {value.strip()}\\n\"\n\n\nclass _State():  # pylint:disable=too-few-public-methods\n    \"\"\" Parses the state file in the current model directory, if the model is training, and\n    formats the content into a human readable format. \"\"\"\n    def __init__(self) -> None:\n        self._model_dir = self._get_arg(\"-m\", \"--model-dir\")\n        self._trainer = self._get_arg(\"-t\", \"--trainer\")\n        self.state_file = self._get_state_file()\n\n    @property\n    def _is_training(self) -> bool:\n        \"\"\" bool: ``True`` if this function has been called during a training session\n        otherwise ``False``. \"\"\"\n        return len(sys.argv) > 1 and sys.argv[1].lower() == \"train\"\n\n    @staticmethod\n    def _get_arg(*args: str) -> str | None:\n        \"\"\" Obtain the value for a given command line option from sys.argv.\n\n        Returns\n        -------\n        str or ``None``\n            The value of the given command line option, if it exists, otherwise ``None``\n        \"\"\"\n        cmd = sys.argv\n        for opt in args:\n            if opt in cmd:\n                return cmd[cmd.index(opt) + 1]\n        return None\n\n    def _get_state_file(self) -> str:\n        \"\"\" Parses the model's state file and compiles the contents into a human readable string.\n\n        Returns\n        -------\n        str\n            The state file formatted into a human readable format\n        \"\"\"\n        if not self._is_training or self._model_dir is None or self._trainer is None:\n            return \"\"\n        fname = os.path.join(self._model_dir, f\"{self._trainer}_state.json\")\n        if not os.path.isfile(fname):\n            return \"\"\n\n        retval = \"\\n\\n=============== State File =================\\n\"\n        with open(fname, \"r\", encoding=\"utf-8\", errors=\"replace\") as sfile:\n            retval += sfile.read()\n        return retval\n\n\nsysinfo = get_sysinfo()  # pylint:disable=invalid-name\n", "lib/keypress.py": "#!/usr/bin/env python3\n\"\"\"\nSource: http://home.wlu.edu/~levys/software/kbhit.py\nA Python class implementing KBHIT, the standard keyboard-interrupt poller.\nWorks transparently on Windows and Posix (Linux, Mac OS X).  Doesn't work\nwith IDLE.\n\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the GNU Lesser General Public License as\npublished by the Free Software Foundation, either version 3 of the\nLicense, or (at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU General Public License for more details.\n\"\"\"\n\nimport os\nimport sys\n\n# Windows\nif os.name == \"nt\":\n    import msvcrt  # pylint:disable=import-error\n\n# Posix (Linux, OS X)\nelse:\n    import termios\n    import atexit\n    from select import select\n\n\nclass KBHit:\n    \"\"\" Creates a KBHit object that you can call to do various keyboard things. \"\"\"\n    def __init__(self, is_gui=False):\n        self.is_gui = is_gui\n        if os.name == \"nt\" or self.is_gui or not sys.stdout.isatty():\n            pass\n        else:\n            # Save the terminal settings\n            self.file_desc = sys.stdin.fileno()\n            self.new_term = termios.tcgetattr(self.file_desc)\n            self.old_term = termios.tcgetattr(self.file_desc)\n\n            # New terminal setting unbuffered\n            self.new_term[3] = self.new_term[3] & ~termios.ICANON & ~termios.ECHO\n            termios.tcsetattr(self.file_desc, termios.TCSAFLUSH, self.new_term)\n\n            # Support normal-terminal reset at exit\n            atexit.register(self.set_normal_term)\n\n    def set_normal_term(self):\n        \"\"\" Resets to normal terminal.  On Windows this is a no-op. \"\"\"\n        if os.name == \"nt\" or self.is_gui or not sys.stdout.isatty():\n            pass\n        else:\n            termios.tcsetattr(self.file_desc, termios.TCSAFLUSH, self.old_term)\n\n    def getch(self):\n        \"\"\" Returns a keyboard character after kbhit() has been called.\n            Should not be called in the same program as getarrow(). \"\"\"\n        if (self.is_gui or not sys.stdout.isatty()) and os.name != \"nt\":\n            return None\n        if os.name == \"nt\":\n            return msvcrt.getch().decode(\"utf-8\", errors=\"replace\")\n        return sys.stdin.read(1)\n\n    def getarrow(self):\n        \"\"\" Returns an arrow-key code after kbhit() has been called. Codes are\n        0 : up\n        1 : right\n        2 : down\n        3 : left\n        Should not be called in the same program as getch(). \"\"\"\n\n        if (self.is_gui or not sys.stdout.isatty()) and os.name != \"nt\":\n            return None\n        if os.name == \"nt\":\n            msvcrt.getch()  # skip 0xE0\n            char = msvcrt.getch()\n            vals = [72, 77, 80, 75]\n        else:\n            char = sys.stdin.read(3)[2]\n            vals = [65, 67, 66, 68]\n\n        return vals.index(ord(char.decode(\"utf-8\", errors=\"replace\")))\n\n    def kbhit(self):\n        \"\"\" Returns True if keyboard character was hit, False otherwise. \"\"\"\n        if (self.is_gui or not sys.stdout.isatty()) and os.name != \"nt\":\n            return None\n        if os.name == \"nt\":\n            return msvcrt.kbhit()\n        d_r, _, _ = select([sys.stdin], [], [], 0)\n        return d_r != []\n", "lib/config.py": "#!/usr/bin/env python3\n\"\"\" Default configurations for faceswap.\n    Extends out :class:`configparser.ConfigParser` functionality by checking for default\n    configuration updates and returning data in it's correct format \"\"\"\n\nimport gettext\nimport logging\nimport os\nimport sys\nimport textwrap\n\nfrom collections import OrderedDict\nfrom configparser import ConfigParser\nfrom dataclasses import dataclass\nfrom importlib import import_module\n\nfrom lib.utils import full_path_split\n\n# LOCALES\n_LANG = gettext.translation(\"lib.config\", localedir=\"locales\", fallback=True)\n_ = _LANG.gettext\n\nOrderedDictSectionType = OrderedDict[str, \"ConfigSection\"]\nOrderedDictItemType = OrderedDict[str, \"ConfigItem\"]\n\nlogger = logging.getLogger(__name__)\nConfigValueType = bool | int | float | list[str] | str | None\n\n\n@dataclass\nclass ConfigItem:\n    \"\"\" Dataclass for holding information about configuration items\n\n    Parameters\n    ----------\n    default: any\n        The default value for the configuration item\n    helptext: str\n        The helptext to be displayed for the configuration item\n    datatype: type\n        The type of the configuration item\n    rounding: int\n        The decimal places for floats or the step interval for ints for slider updates\n    min_max: tuple\n        The minumum and maximum value for the GUI slider for the configuration item\n    gui_radio: bool\n        ``True`` to display the configuration item in a Radio Box\n    fixed: bool\n        ``True`` if the item cannot be changed for existing models (training only)\n    group: str\n        The group that this configuration item belongs to in the GUI\n    \"\"\"\n    default: ConfigValueType\n    helptext: str\n    datatype: type\n    rounding: int\n    min_max: tuple[int, int] | tuple[float, float] | None\n    choices: str | list[str]\n    gui_radio: bool\n    fixed: bool\n    group: str | None\n\n\n@dataclass\nclass ConfigSection:\n    \"\"\" Dataclass for holding information about configuration sections\n\n    Parameters\n    ----------\n    helptext: str\n        The helptext to be displayed for the configuration section\n    items: :class:`collections.OrderedDict`\n        Dictionary of configuration items for the section\n    \"\"\"\n    helptext: str\n    items: OrderedDictItemType\n\n\nclass FaceswapConfig():\n    \"\"\" Config Items \"\"\"\n    def __init__(self, section: str | None, configfile: str | None = None) -> None:\n        \"\"\" Init Configuration\n\n        Parameters\n        ----------\n        section: str or ``None``\n            The configuration section. ``None`` for all sections\n        configfile: str, optional\n            Optional path to a config file. ``None`` for default location. Default: ``None``\n        \"\"\"\n        logger.debug(\"Initializing: %s\", self.__class__.__name__)\n        self.configfile = self._get_config_file(configfile)\n        self.config = ConfigParser(allow_no_value=True)\n        self.defaults: OrderedDictSectionType = OrderedDict()\n        self.config.optionxform = str  # type:ignore\n        self.section = section\n\n        self.set_defaults()\n        self._handle_config()\n        logger.debug(\"Initialized: %s\", self.__class__.__name__)\n\n    @property\n    def changeable_items(self) -> dict[str, ConfigValueType]:\n        \"\"\" Training only.\n            Return a dict of config items with their set values for items\n            that can be altered after the model has been created \"\"\"\n        retval: dict[str, ConfigValueType] = {}\n        sections = [sect for sect in self.config.sections() if sect.startswith(\"global\")]\n        all_sections = sections if self.section is None else sections + [self.section]\n        for sect in all_sections:\n            if sect not in self.defaults:\n                continue\n            for key, val in self.defaults[sect].items.items():\n                if val.fixed:\n                    continue\n                retval[key] = self.get(sect, key)\n        logger.debug(\"Alterable for existing models: %s\", retval)\n        return retval\n\n    def set_defaults(self) -> None:\n        \"\"\" Override for plugin specific config defaults\n\n            Should be a series of self.add_section() and self.add_item() calls\n\n            e.g:\n\n            section = \"sect_1\"\n            self.add_section(section,\n                             \"Section 1 Information\")\n\n            self.add_item(section=section,\n                          title=\"option_1\",\n                          datatype=bool,\n                          default=False,\n                          info=\"sect_1 option_1 information\")\n        \"\"\"\n        raise NotImplementedError\n\n    def _defaults_from_plugin(self, plugin_folder: str) -> None:\n        \"\"\" Scan the given plugins folder for config defaults.py files and update the\n        default configuration.\n\n        Parameters\n        ----------\n        plugin_folder: str\n            The folder to scan for plugins\n        \"\"\"\n        for dirpath, _, filenames in os.walk(plugin_folder):\n            default_files = [fname for fname in filenames if fname.endswith(\"_defaults.py\")]\n            if not default_files:\n                continue\n            base_path = os.path.dirname(os.path.realpath(sys.argv[0]))\n            # Can't use replace as there is a bug on some Windows installs that lowers some paths\n            import_path = \".\".join(full_path_split(dirpath[len(base_path):])[1:])\n            plugin_type = import_path.rsplit(\".\", maxsplit=1)[-1]\n            for filename in default_files:\n                self._load_defaults_from_module(filename, import_path, plugin_type)\n\n    def _load_defaults_from_module(self,\n                                   filename: str,\n                                   module_path: str,\n                                   plugin_type: str) -> None:\n        \"\"\" Load the plugin's defaults module, extract defaults and add to default configuration.\n\n        Parameters\n        ----------\n        filename: str\n            The filename to load the defaults from\n        module_path: str\n            The path to load the module from\n        plugin_type: str\n            The type of plugin that the defaults are being loaded for\n        \"\"\"\n        logger.debug(\"Adding defaults: (filename: %s, module_path: %s, plugin_type: %s\",\n                     filename, module_path, plugin_type)\n        module = os.path.splitext(filename)[0]\n        section = \".\".join((plugin_type, module.replace(\"_defaults\", \"\")))\n        logger.debug(\"Importing defaults module: %s.%s\", module_path, module)\n        mod = import_module(f\"{module_path}.{module}\")\n        self.add_section(section, mod._HELPTEXT)  # type:ignore[attr-defined]  # pylint:disable=protected-access  # noqa:E501\n        for key, val in mod._DEFAULTS.items():  # type:ignore[attr-defined]  # pylint:disable=protected-access  # noqa:E501\n            self.add_item(section=section, title=key, **val)\n        logger.debug(\"Added defaults: %s\", section)\n\n    @property\n    def config_dict(self) -> dict[str, ConfigValueType]:\n        \"\"\" dict: Collate global options and requested section into a dictionary with the correct\n        data types \"\"\"\n        conf: dict[str, ConfigValueType] = {}\n        sections = [sect for sect in self.config.sections() if sect.startswith(\"global\")]\n        if self.section is not None:\n            sections.append(self.section)\n        for sect in sections:\n            if sect not in self.config.sections():\n                continue\n            for key in self.config[sect]:\n                if key.startswith((\"#\", \"\\n\")):  # Skip comments\n                    continue\n                conf[key] = self.get(sect, key)\n        return conf\n\n    def get(self, section: str, option: str) -> ConfigValueType:\n        \"\"\" Return a config item in it's correct format.\n\n        Parameters\n        ----------\n        section: str\n            The configuration section currently being processed\n        option: str\n            The configuration option currently being processed\n\n        Returns\n        -------\n        varies\n            The selected configuration option in the correct data format\n        \"\"\"\n        logger.debug(\"Getting config item: (section: '%s', option: '%s')\", section, option)\n        datatype = self.defaults[section].items[option].datatype\n\n        retval: ConfigValueType\n        if datatype == bool:\n            retval = self.config.getboolean(section, option)\n        elif datatype == int:\n            retval = self.config.getint(section, option)\n        elif datatype == float:\n            retval = self.config.getfloat(section, option)\n        elif datatype == list:\n            retval = self._parse_list(section, option)\n        else:\n            retval = self.config.get(section, option)\n\n        if isinstance(retval, str) and retval.lower() == \"none\":\n            retval = None\n        logger.debug(\"Returning item: (type: %s, value: %s)\", datatype, retval)\n        return retval\n\n    def _parse_list(self, section: str, option: str) -> list[str]:\n        \"\"\" Parse options that are stored as lists in the config file. These can be space or\n        comma-separated items in the config file. They will be returned as a list of strings,\n        regardless of what the final data type should be, so conversion from strings to other\n        formats should be done explicitly within the retrieving code.\n\n        Parameters\n        ----------\n        section: str\n            The configuration section currently being processed\n        option: str\n            The configuration option currently being processed\n\n        Returns\n        -------\n        list\n            List of `str` selected items for the config choice.\n        \"\"\"\n        raw_option = self.config.get(section, option)\n        if not raw_option:\n            logger.debug(\"No options selected, returning empty list\")\n            return []\n        delimiter = \",\" if \",\" in raw_option else None\n        retval = [opt.strip().lower() for opt in raw_option.split(delimiter)]\n        logger.debug(\"Processed raw option '%s' to list %s for section '%s', option '%s'\",\n                     raw_option, retval, section, option)\n        return retval\n\n    def _get_config_file(self, configfile: str | None) -> str:\n        \"\"\" Return the config file from the calling folder or the provided file\n\n        Parameters\n        ----------\n        configfile: str or ``None``\n            Path to a config file. ``None`` for default location.\n\n        Returns\n        -------\n        str\n            The full path to the configuration file\n        \"\"\"\n        if configfile is not None:\n            if not os.path.isfile(configfile):\n                err = f\"Config file does not exist at: {configfile}\"\n                logger.error(err)\n                raise ValueError(err)\n            return configfile\n        filepath = sys.modules[self.__module__].__file__\n        assert filepath is not None\n        dirname = os.path.dirname(filepath)\n        folder, fname = os.path.split(dirname)\n        retval = os.path.join(os.path.dirname(folder), \"config\", f\"{fname}.ini\")\n        logger.debug(\"Config File location: '%s'\", retval)\n        return retval\n\n    def add_section(self, title: str, info: str) -> None:\n        \"\"\" Add a default section to config file\n\n        Parameters\n        ----------\n        title: str\n            The title for the section\n        info: str\n            The helptext for the section\n        \"\"\"\n        logger.debug(\"Add section: (title: '%s', info: '%s')\", title, info)\n        self.defaults[title] = ConfigSection(helptext=info, items=OrderedDict())\n\n    def add_item(self,\n                 section: str | None = None,\n                 title: str | None = None,\n                 datatype: type = str,\n                 default: ConfigValueType = None,\n                 info: str | None = None,\n                 rounding: int | None = None,\n                 min_max: tuple[int, int] | tuple[float, float] | None = None,\n                 choices: str | list[str] | None = None,\n                 gui_radio: bool = False,\n                 fixed: bool = True,\n                 group: str | None = None) -> None:\n        \"\"\" Add a default item to a config section\n\n            For int or float values, rounding and min_max must be set\n            This is for the slider in the GUI. The min/max values are not enforced:\n            rounding:   sets the decimal places for floats or the step interval for ints.\n            min_max:    tuple of min and max accepted values\n\n            For str values choices can be set to validate input and create a combo box\n            in the GUI\n\n            For list values, choices must be provided, and a multi-option select box will\n            be created\n\n            is_radio is to indicate to the GUI that it should display Radio Buttons rather than\n            combo boxes for multiple choice options.\n\n            The 'fixed' parameter is only for training configurations. Training configurations\n            are set when the model is created, and then reloaded from the state file.\n            Marking an item as fixed=False indicates that this value can be changed for\n            existing models, and will override the value saved in the state file with the\n            updated value in config.\n\n            The 'Group' parameter allows you to assign the config item to a group in the GUI\n\n        \"\"\"\n        logger.debug(\"Add item: (section: '%s', title: '%s', datatype: '%s', default: '%s', \"\n                     \"info: '%s', rounding: '%s', min_max: %s, choices: %s, gui_radio: %s, \"\n                     \"fixed: %s, group: %s)\", section, title, datatype, default, info, rounding,\n                     min_max, choices, gui_radio, fixed, group)\n\n        choices = [] if not choices else choices\n\n        assert (section is not None and\n                title is not None and\n                default is not None and\n                info is not None), (\"Default config items must have a section, title, defult and \"\n                                    \"information text\")\n        if not self.defaults.get(section, None):\n            raise ValueError(f\"Section does not exist: {section}\")\n        assert datatype in (str, bool, float, int, list), (\n            f\"'datatype' must be one of str, bool, float or int: {section} - {title}\")\n        if datatype in (float, int) and (rounding is None or min_max is None):\n            raise ValueError(\"'rounding' and 'min_max' must be set for numerical options\")\n        if isinstance(datatype, list) and not choices:\n            raise ValueError(\"'choices' must be defined for list based configuration items\")\n        if choices != \"colorchooser\" and not isinstance(choices, (list, tuple)):\n            raise ValueError(\"'choices' must be a list or tuple or 'colorchooser\")\n\n        info = self._expand_helptext(info, choices, default, datatype, min_max, fixed)\n        self.defaults[section].items[title] = ConfigItem(default=default,\n                                                         helptext=info,\n                                                         datatype=datatype,\n                                                         rounding=rounding or 0,\n                                                         min_max=min_max,\n                                                         choices=choices,\n                                                         gui_radio=gui_radio,\n                                                         fixed=fixed,\n                                                         group=group)\n\n    @classmethod\n    def _expand_helptext(cls,\n                         helptext: str,\n                         choices: str | list[str],\n                         default: ConfigValueType,\n                         datatype: type,\n                         min_max: tuple[int, int] | tuple[float, float] | None,\n                         fixed: bool) -> str:\n        \"\"\" Add extra helptext info from parameters \"\"\"\n        helptext += \"\\n\"\n        if not fixed:\n            helptext += _(\"\\nThis option can be updated for existing models.\\n\")\n        if datatype == list:\n            helptext += _(\"\\nIf selecting multiple options then each option should be separated \"\n                          \"by a space or a comma (e.g. item1, item2, item3)\\n\")\n        if choices and choices != \"colorchooser\":\n            helptext += _(\"\\nChoose from: {}\").format(choices)\n        elif datatype == bool:\n            helptext += _(\"\\nChoose from: True, False\")\n        elif datatype == int:\n            assert min_max is not None\n            cmin, cmax = min_max\n            helptext += _(\"\\nSelect an integer between {} and {}\").format(cmin, cmax)\n        elif datatype == float:\n            assert min_max is not None\n            cmin, cmax = min_max\n            helptext += _(\"\\nSelect a decimal number between {} and {}\").format(cmin, cmax)\n        helptext += _(\"\\n[Default: {}]\").format(default)\n        return helptext\n\n    def _check_exists(self) -> bool:\n        \"\"\" Check that a config file exists\n\n        Returns\n        -------\n        bool\n            ``True`` if the given configuration file exists\n        \"\"\"\n        if not os.path.isfile(self.configfile):\n            logger.debug(\"Config file does not exist: '%s'\", self.configfile)\n            return False\n        logger.debug(\"Config file exists: '%s'\", self.configfile)\n        return True\n\n    def _create_default(self) -> None:\n        \"\"\" Generate a default config if it does not exist \"\"\"\n        logger.debug(\"Creating default Config\")\n        for name, section in self.defaults.items():\n            logger.debug(\"Adding section: '%s')\", name)\n            self.insert_config_section(name, section.helptext)\n            for item, opt in section.items.items():\n                logger.debug(\"Adding option: (item: '%s', opt: '%s')\", item, opt)\n                self._insert_config_item(name, item, opt.default, opt)\n        self.save_config()\n\n    def insert_config_section(self,\n                              section: str,\n                              helptext: str,\n                              config: ConfigParser | None = None) -> None:\n        \"\"\" Insert a section into the config\n\n        Parameters\n        ----------\n        section: str\n            The section title to insert\n        helptext: str\n            The help text for the config section\n        config: :class:`configparser.ConfigParser`, optional\n            The config parser object to insert the section into. ``None`` to insert it into the\n            default config. Default: ``None``\n        \"\"\"\n        logger.debug(\"Inserting section: (section: '%s', helptext: '%s', config: '%s')\",\n                     section, helptext, config)\n        config = self.config if config is None else config\n        config.optionxform = str  # type:ignore\n        helptext = self.format_help(helptext, is_section=True)\n        config.add_section(section)\n        config.set(section, helptext)\n        logger.debug(\"Inserted section: '%s'\", section)\n\n    def _insert_config_item(self,\n                            section: str,\n                            item: str,\n                            default: ConfigValueType,\n                            option: ConfigItem,\n                            config: ConfigParser | None = None) -> None:\n        \"\"\" Insert an item into a config section\n\n        Parameters\n        ----------\n        section: str\n            The section to insert the item into\n        item: str\n            The name of the item to insert\n        default: ConfigValueType\n            The default value for the item\n        option: :class:`ConfigItem`\n            The configuration option to insert\n        config: :class:`configparser.ConfigParser`, optional\n            The config parser object to insert the section into. ``None`` to insert it into the\n            default config. Default: ``None``\n        \"\"\"\n        logger.debug(\"Inserting item: (section: '%s', item: '%s', default: '%s', helptext: '%s', \"\n                     \"config: '%s')\", section, item, default, option.helptext, config)\n        config = self.config if config is None else config\n        config.optionxform = str  # type:ignore\n        helptext = option.helptext\n        helptext = self.format_help(helptext, is_section=False)\n        config.set(section, helptext)\n        config.set(section, item, str(default))\n        logger.debug(\"Inserted item: '%s'\", item)\n\n    @classmethod\n    def format_help(cls, helptext: str, is_section: bool = False) -> str:\n        \"\"\" Format comments for default ini file\n\n        Parameters\n        ----------\n        helptext: str\n            The help text to be formatted\n        is_section: bool, optional\n            ``True`` if the help text pertains to a section. ``False`` if it pertains to an item.\n            Default: ``True``\n\n        Returns\n        -------\n        str\n            The formatted help text\n        \"\"\"\n        logger.debug(\"Formatting help: (helptext: '%s', is_section: '%s')\", helptext, is_section)\n        formatted = \"\"\n        for hlp in helptext.split(\"\\n\"):\n            subsequent_indent = \"\\t\\t\" if hlp.startswith(\"\\t\") else \"\"\n            hlp = f\"\\t- {hlp[1:].strip()}\" if hlp.startswith(\"\\t\") else hlp\n            formatted += textwrap.fill(hlp,\n                                       100,\n                                       tabsize=4,\n                                       subsequent_indent=subsequent_indent) + \"\\n\"\n        helptext = '# {}'.format(formatted[:-1].replace(\"\\n\", \"\\n# \"))  # Strip last newline\n        if is_section:\n            helptext = helptext.upper()\n        else:\n            helptext = f\"\\n{helptext}\"\n        logger.debug(\"formatted help: '%s'\", helptext)\n        return helptext\n\n    def _load_config(self) -> None:\n        \"\"\" Load values from config \"\"\"\n        logger.verbose(\"Loading config: '%s'\", self.configfile)  # type:ignore[attr-defined]\n        self.config.read(self.configfile, encoding=\"utf-8\")\n\n    def save_config(self) -> None:\n        \"\"\" Save a config file \"\"\"\n        logger.info(\"Updating config at: '%s'\", self.configfile)\n        with open(self.configfile, \"w\", encoding=\"utf-8\", errors=\"replace\") as f_cfgfile:\n            self.config.write(f_cfgfile)\n        logger.debug(\"Updated config at: '%s'\", self.configfile)\n\n    def _validate_config(self) -> None:\n        \"\"\" Check for options in default config against saved config\n            and add/remove as appropriate \"\"\"\n        logger.debug(\"Validating config\")\n        if self._check_config_change():\n            self._add_new_config_items()\n        self._check_config_choices()\n        logger.debug(\"Validated config\")\n\n    def _add_new_config_items(self) -> None:\n        \"\"\" Add new items to the config file \"\"\"\n        logger.debug(\"Updating config\")\n        new_config = ConfigParser(allow_no_value=True)\n        for section_name, section in self.defaults.items():\n            self.insert_config_section(section_name, section.helptext, new_config)\n            for item, opt in section.items.items():\n                if section_name not in self.config.sections():\n                    logger.debug(\"Adding new config section: '%s'\", section_name)\n                    opt_value = opt.default\n                else:\n                    opt_value = self.config[section_name].get(item, str(opt.default))\n                self._insert_config_item(section_name,\n                                         item,\n                                         opt_value,\n                                         opt,\n                                         new_config)\n        self.config = new_config\n        self.config.optionxform = str  # type:ignore\n        self.save_config()\n        logger.debug(\"Updated config\")\n\n    def _check_config_choices(self) -> None:\n        \"\"\" Check that config items are valid choices \"\"\"\n        logger.debug(\"Checking config choices\")\n        for section_name, section in self.defaults.items():\n            for item, opt in section.items.items():\n                if not opt.choices:\n                    continue\n                if opt.datatype == list:  # Multi-select items\n                    opt_values = self._parse_list(section_name, item)\n                    if not opt_values:  # No option selected\n                        continue\n                    if not all(val in opt.choices for val in opt_values):\n                        invalid = [val for val in opt_values if val not in opt.choices]\n                        valid = \", \".join(val for val in opt_values if val in opt.choices)\n                        logger.warning(\"The option(s) %s are not valid selections for '%s': '%s'. \"\n                                       \"setting to: '%s'\", invalid, section_name, item, valid)\n                        self.config.set(section_name, item, valid)\n                else:  # Single-select items\n                    if opt.choices == \"colorchooser\":\n                        continue\n                    opt_value = self.config.get(section_name, item)\n                    if opt_value.lower() == \"none\" and any(choice.lower() == \"none\"\n                                                           for choice in opt.choices):\n                        continue\n                    if opt_value not in opt.choices:\n                        default = str(opt.default)\n                        logger.warning(\"'%s' is not a valid config choice for '%s': '%s'. \"\n                                       \"Defaulting to: '%s'\",\n                                       opt_value, section_name, item, default)\n                        self.config.set(section_name, item, default)\n        logger.debug(\"Checked config choices\")\n\n    def _check_config_change(self) -> bool:\n        \"\"\" Check whether new default items have been added or removed from the config file\n        compared to saved version\n\n        Returns\n        -------\n        bool\n            ``True`` if a config option has been added or removed\n        \"\"\"\n        if set(self.config.sections()) != set(self.defaults.keys()):\n            logger.debug(\"Default config has new section(s)\")\n            return True\n\n        for section_name, section in self.defaults.items():\n            opts = list(section.items)\n            exists = [opt for opt in self.config[section_name].keys()\n                      if not opt.startswith((\"# \", \"\\n# \"))]\n            if set(exists) != set(opts):\n                logger.debug(\"Default config has new item(s)\")\n                return True\n        logger.debug(\"Default config has not changed\")\n        return False\n\n    def _handle_config(self) -> None:\n        \"\"\" Handle the config.\n\n        Checks whether a config file exists for this section. If not then a default is created.\n\n        Configuration choices are then loaded and validated\n        \"\"\"\n        logger.debug(\"Handling config: (section: %s, configfile: '%s')\",\n                     self.section, self.configfile)\n        if not self._check_exists():\n            self._create_default()\n        self._load_config()\n        self._validate_config()\n        logger.debug(\"Handled config\")\n\n\ndef generate_configs() -> None:\n    \"\"\" Generate config files if they don't exist.\n\n    This script is run prior to anything being set up, so don't use logging\n    Generates the default config files for plugins in the faceswap config folder\n    \"\"\"\n    base_path = os.path.realpath(os.path.dirname(sys.argv[0]))\n    plugins_path = os.path.join(base_path, \"plugins\")\n    configs_path = os.path.join(base_path, \"config\")\n    for dirpath, _, filenames in os.walk(plugins_path):\n        if \"_config.py\" in filenames:\n            section = os.path.split(dirpath)[-1]\n            config_file = os.path.join(configs_path, f\"{section}.ini\")\n            if not os.path.exists(config_file):\n                mod = import_module(f\"plugins.{section}._config\")\n                mod.Config(None)  # type:ignore[attr-defined]\n", "lib/multithreading.py": "#!/usr/bin/env python3\n\"\"\" Multithreading/processing utils for faceswap \"\"\"\nfrom __future__ import annotations\nimport logging\nimport typing as T\nfrom multiprocessing import cpu_count\n\nimport queue as Queue\nimport sys\nimport threading\nfrom types import TracebackType\n\nif T.TYPE_CHECKING:\n    from collections.abc import Callable, Generator\n\nlogger = logging.getLogger(__name__)\n_ErrorType: T.TypeAlias = tuple[type[BaseException],\n                                BaseException,\n                                TracebackType] | tuple[T.Any, T.Any, T.Any] | None\n_THREAD_NAMES: set[str] = set()\n\n\ndef total_cpus():\n    \"\"\" Return total number of cpus \"\"\"\n    return cpu_count()\n\n\ndef _get_name(name: str) -> str:\n    \"\"\" Obtain a unique name for a thread\n\n    Parameters\n    ----------\n    name: str\n        The requested name\n\n    Returns\n    -------\n    str\n        The request name with \"_#\" appended (# being an integer) making the name unique\n    \"\"\"\n    idx = 0\n    real_name = name\n    while True:\n        if real_name in _THREAD_NAMES:\n            real_name = f\"{name}_{idx}\"\n            idx += 1\n            continue\n        _THREAD_NAMES.add(real_name)\n        return real_name\n\n\nclass FSThread(threading.Thread):\n    \"\"\" Subclass of thread that passes errors back to parent\n\n    Parameters\n    ----------\n    target: callable object, Optional\n        The callable object to be invoked by the run() method. If ``None`` nothing is called.\n        Default: ``None``\n    name: str, optional\n        The thread name. if ``None`` a unique name is constructed of the form \"Thread-N\" where N\n        is a small decimal number. Default: ``None``\n    args: tuple\n        The argument tuple for the target invocation. Default: ().\n    kwargs: dict\n        keyword arguments for the target invocation. Default: {}.\n    \"\"\"\n    _target: Callable\n    _args: tuple\n    _kwargs: dict[str, T.Any]\n    _name: str\n\n    def __init__(self,\n                 target: Callable | None = None,\n                 name: str | None = None,\n                 args: tuple = (),\n                 kwargs: dict[str, T.Any] | None = None,\n                 *,\n                 daemon: bool | None = None) -> None:\n        super().__init__(target=target, name=name, args=args, kwargs=kwargs, daemon=daemon)\n        self.err: _ErrorType = None\n\n    def check_and_raise_error(self) -> None:\n        \"\"\" Checks for errors in thread and raises them in caller.\n\n        Raises\n        ------\n        Error\n            Re-raised error from within the thread\n        \"\"\"\n        if not self.err:\n            return\n        logger.debug(\"Thread error caught: %s\", self.err)\n        raise self.err[1].with_traceback(self.err[2])\n\n    def run(self) -> None:\n        \"\"\" Runs the target, reraising any errors from within the thread in the caller. \"\"\"\n        try:\n            if self._target is not None:\n                self._target(*self._args, **self._kwargs)\n        except Exception as err:  # pylint:disable=broad-except\n            self.err = sys.exc_info()\n            logger.debug(\"Error in thread (%s): %s\", self._name, str(err))\n        finally:\n            # Avoid a refcycle if the thread is running a function with\n            # an argument that has a member that points to the thread.\n            del self._target, self._args, self._kwargs\n\n\nclass MultiThread():\n    \"\"\" Threading for IO heavy ops. Catches errors in thread and rethrows to parent.\n\n    Parameters\n    ----------\n    target: callable object\n        The callable object to be invoked by the run() method.\n    args: tuple\n        The argument tuple for the target invocation. Default: ().\n    thread_count: int, optional\n        The number of threads to use. Default: 1\n    name: str, optional\n        The thread name. if ``None`` a unique name is constructed of the form {target.__name__}_N\n        where N is an incrementing integer. Default: ``None``\n    kwargs: dict\n        keyword arguments for the target invocation. Default: {}.\n    \"\"\"\n    def __init__(self,\n                 target: Callable,\n                 *args,\n                 thread_count: int = 1,\n                 name: str | None = None,\n                 **kwargs) -> None:\n        self._name = _get_name(name if name else target.__name__)\n        logger.debug(\"Initializing %s: (target: '%s', thread_count: %s)\",\n                     self.__class__.__name__, self._name, thread_count)\n        logger.trace(\"args: %s, kwargs: %s\", args, kwargs)  # type:ignore\n        self.daemon = True\n        self._thread_count = thread_count\n        self._threads: list[FSThread] = []\n        self._target = target\n        self._args = args\n        self._kwargs = kwargs\n        logger.debug(\"Initialized %s: '%s'\", self.__class__.__name__, self._name)\n\n    @property\n    def has_error(self) -> bool:\n        \"\"\" bool: ``True`` if a thread has errored, otherwise ``False`` \"\"\"\n        return any(thread.err for thread in self._threads)\n\n    @property\n    def errors(self) -> list[_ErrorType]:\n        \"\"\" list: List of thread error values \"\"\"\n        return [thread.err for thread in self._threads if thread.err]\n\n    @property\n    def name(self) -> str:\n        \"\"\" :str: The name of the thread \"\"\"\n        return self._name\n\n    def check_and_raise_error(self) -> None:\n        \"\"\" Checks for errors in thread and raises them in caller.\n\n        Raises\n        ------\n        Error\n            Re-raised error from within the thread\n        \"\"\"\n        if not self.has_error:\n            return\n        logger.debug(\"Thread error caught: %s\", self.errors)\n        error = self.errors[0]\n        assert error is not None\n        raise error[1].with_traceback(error[2])\n\n    def is_alive(self) -> bool:\n        \"\"\" Check if any threads are still alive\n\n        Returns\n        -------\n        bool\n            ``True`` if any threads are alive. ``False`` if no threads are alive\n        \"\"\"\n        return any(thread.is_alive() for thread in self._threads)\n\n    def start(self) -> None:\n        \"\"\" Start all the threads for the given method, args and kwargs \"\"\"\n        logger.debug(\"Starting thread(s): '%s'\", self._name)\n        for idx in range(self._thread_count):\n            name = self._name if self._thread_count == 1 else f\"{self._name}_{idx}\"\n            logger.debug(\"Starting thread %s of %s: '%s'\",\n                         idx + 1, self._thread_count, name)\n            thread = FSThread(name=name,\n                              target=self._target,\n                              args=self._args,\n                              kwargs=self._kwargs)\n            thread.daemon = self.daemon\n            thread.start()\n            self._threads.append(thread)\n        logger.debug(\"Started all threads '%s': %s\", self._name, len(self._threads))\n\n    def completed(self) -> bool:\n        \"\"\" Check if all threads have completed\n\n        Returns\n        -------\n        ``True`` if all threads have completed otherwise ``False``\n        \"\"\"\n        retval = all(not thread.is_alive() for thread in self._threads)\n        logger.debug(retval)\n        return retval\n\n    def join(self) -> None:\n        \"\"\" Join the running threads, catching and re-raising any errors\n\n        Clear the list of threads for class instance re-use\n        \"\"\"\n        logger.debug(\"Joining Threads: '%s'\", self._name)\n        for thread in self._threads:\n            logger.debug(\"Joining Thread: '%s'\", thread._name)  # pylint:disable=protected-access\n            thread.join()\n            if thread.err:\n                logger.error(\"Caught exception in thread: '%s'\",\n                             thread._name)  # pylint:disable=protected-access\n                raise thread.err[1].with_traceback(thread.err[2])\n        del self._threads\n        self._threads = []\n        logger.debug(\"Joined all Threads: '%s'\", self._name)\n\n\nclass BackgroundGenerator(MultiThread):\n    \"\"\" Run a task in the background background and queue data for consumption\n\n    Parameters\n    ----------\n    generator: iterable\n        The generator to run in the background\n    prefetch, int, optional\n        The number of items to pre-fetch from the generator before blocking (see Notes). Default: 1\n    name: str, optional\n        The thread name. if ``None`` a unique name is constructed of the form\n        {generator.__name__}_N where N is an incrementing integer. Default: ``None``\n    args: tuple, Optional\n        The argument tuple for generator invocation. Default: ``None``.\n    kwargs: dict, Optional\n        keyword arguments for the generator invocation. Default: ``None``.\n\n    Notes\n    -----\n    Putting to the internal queue only blocks if put is called while queue has already\n    reached max size. Therefore this means prefetch is actually 1 more than the parameter\n    supplied (N in the queue, one waiting for insertion)\n\n    References\n    ----------\n    https://stackoverflow.com/questions/7323664/\n    \"\"\"\n    def __init__(self,\n                 generator: Callable,\n                 prefetch: int = 1,\n                 name: str | None = None,\n                 args: tuple | None = None,\n                 kwargs: dict[str, T.Any] | None = None) -> None:\n        super().__init__(name=name, target=self._run)\n        self.queue: Queue.Queue = Queue.Queue(prefetch)\n        self.generator = generator\n        self._gen_args = args or tuple()\n        self._gen_kwargs = kwargs or {}\n        self.start()\n\n    def _run(self) -> None:\n        \"\"\" Run the :attr:`_generator` and put into the queue until until queue size is reached.\n\n        Raises\n        ------\n        Exception\n            If there is a failure to run the generator and put to the queue\n        \"\"\"\n        try:\n            for item in self.generator(*self._gen_args, **self._gen_kwargs):\n                self.queue.put(item)\n            self.queue.put(None)\n        except Exception:\n            self.queue.put(None)\n            raise\n\n    def iterator(self) -> Generator:\n        \"\"\" Iterate items out of the queue\n\n        Yields\n        ------\n        Any\n            The items from the generator\n        \"\"\"\n        while True:\n            next_item = self.queue.get()\n            self.check_and_raise_error()\n            if next_item is None or next_item == \"EOF\":\n                logger.debug(\"Got EOF OR NONE in BackgroundGenerator\")\n                break\n            yield next_item\n", "lib/utils.py": "#!/usr/bin python3\n\"\"\" Utilities available across all scripts \"\"\"\nfrom __future__ import annotations\nimport json\nimport logging\nimport os\nimport sys\nimport tkinter as tk\nimport typing as T\nimport warnings\nimport zipfile\n\nfrom multiprocessing import current_process\nfrom re import finditer\nfrom socket import timeout as socket_timeout, error as socket_error\nfrom threading import get_ident\nfrom time import time\nfrom urllib import request, error as urlliberror\n\nimport numpy as np\nfrom tqdm import tqdm\n\nif T.TYPE_CHECKING:\n    from argparse import Namespace\n    from http.client import HTTPResponse\n\n# Global variables\nIMAGE_EXTENSIONS = [\".bmp\", \".jpeg\", \".jpg\", \".png\", \".tif\", \".tiff\"]\nVIDEO_EXTENSIONS = [\".avi\", \".flv\", \".mkv\", \".mov\", \".mp4\", \".mpeg\", \".mpg\", \".webm\", \".wmv\",\n                    \".ts\", \".vob\"]\n_TF_VERS: tuple[int, int] | None = None\nValidBackends = T.Literal[\"nvidia\", \"cpu\", \"apple_silicon\", \"directml\", \"rocm\"]\n\n\nclass _Backend():  # pylint:disable=too-few-public-methods\n    \"\"\" Return the backend from config/.faceswap of from the `FACESWAP_BACKEND` Environment\n    Variable.\n\n    If file doesn't exist and a variable hasn't been set, create the config file. \"\"\"\n    def __init__(self) -> None:\n        self._backends: dict[str, ValidBackends] = {\"1\": \"cpu\",\n                                                    \"2\": \"directml\",\n                                                    \"3\": \"nvidia\",\n                                                    \"4\": \"apple_silicon\",\n                                                    \"5\": \"rocm\"}\n        self._valid_backends = list(self._backends.values())\n        self._config_file = self._get_config_file()\n        self.backend = self._get_backend()\n\n    @classmethod\n    def _get_config_file(cls) -> str:\n        \"\"\" Obtain the location of the main Faceswap configuration file.\n\n        Returns\n        -------\n        str\n            The path to the Faceswap configuration file\n        \"\"\"\n        pypath = os.path.dirname(os.path.realpath(sys.argv[0]))\n        config_file = os.path.join(pypath, \"config\", \".faceswap\")\n        return config_file\n\n    def _get_backend(self) -> ValidBackends:\n        \"\"\" Return the backend from either the `FACESWAP_BACKEND` Environment Variable or from\n        the :file:`config/.faceswap` configuration file. If neither of these exist, prompt the user\n        to select a backend.\n\n        Returns\n        -------\n        str\n            The backend configuration in use by Faceswap\n        \"\"\"\n        # Check if environment variable is set, if so use that\n        if \"FACESWAP_BACKEND\" in os.environ:\n            fs_backend = T.cast(ValidBackends, os.environ[\"FACESWAP_BACKEND\"].lower())\n            assert fs_backend in T.get_args(ValidBackends), (\n                f\"Faceswap backend must be one of {T.get_args(ValidBackends)}\")\n            print(f\"Setting Faceswap backend from environment variable to {fs_backend.upper()}\")\n            return fs_backend\n        # Intercept for sphinx docs build\n        if sys.argv[0].endswith(\"sphinx-build\"):\n            return \"nvidia\"\n        if not os.path.isfile(self._config_file):\n            self._configure_backend()\n        while True:\n            try:\n                with open(self._config_file, \"r\", encoding=\"utf8\") as cnf:\n                    config = json.load(cnf)\n                break\n            except json.decoder.JSONDecodeError:\n                self._configure_backend()\n                continue\n        fs_backend = config.get(\"backend\", \"\").lower()\n        if not fs_backend or fs_backend not in self._backends.values():\n            fs_backend = self._configure_backend()\n        if current_process().name == \"MainProcess\":\n            print(f\"Setting Faceswap backend to {fs_backend.upper()}\")\n        return fs_backend\n\n    def _configure_backend(self) -> ValidBackends:\n        \"\"\" Get user input to select the backend that Faceswap should use.\n\n        Returns\n        -------\n        str\n            The backend configuration in use by Faceswap\n        \"\"\"\n        print(\"First time configuration. Please select the required backend\")\n        while True:\n            txt = \", \".join([\": \".join([key, val.upper().replace(\"_\", \" \")])\n                             for key, val in self._backends.items()])\n            selection = input(f\"{txt}: \")\n            if selection not in self._backends:\n                print(f\"'{selection}' is not a valid selection. Please try again\")\n                continue\n            break\n        fs_backend = self._backends[selection]\n        config = {\"backend\": fs_backend}\n        with open(self._config_file, \"w\", encoding=\"utf8\") as cnf:\n            json.dump(config, cnf)\n        print(f\"Faceswap config written to: {self._config_file}\")\n        return fs_backend\n\n\n_FS_BACKEND: ValidBackends = _Backend().backend\n\n\ndef get_backend() -> ValidBackends:\n    \"\"\" Get the backend that Faceswap is currently configured to use.\n\n    Returns\n    -------\n    str\n        The backend configuration in use by Faceswap. One of  [\"cpu\", \"directml\", \"nvidia\", \"rocm\",\n        \"apple_silicon\"]\n\n    Example\n    -------\n    >>> from lib.utils import get_backend\n    >>> get_backend()\n    'nvidia'\n    \"\"\"\n    return _FS_BACKEND\n\n\ndef set_backend(backend: str) -> None:\n    \"\"\" Override the configured backend with the given backend.\n\n    Parameters\n    ----------\n    backend: [\"cpu\", \"directml\", \"nvidia\", \"rocm\", \"apple_silicon\"]\n        The backend to set faceswap to\n\n    Example\n    -------\n    >>> from lib.utils import set_backend\n    >>> set_backend(\"nvidia\")\n    \"\"\"\n    global _FS_BACKEND  # pylint:disable=global-statement\n    backend = T.cast(ValidBackends, backend.lower())\n    _FS_BACKEND = backend\n\n\ndef get_tf_version() -> tuple[int, int]:\n    \"\"\" Obtain the major. minor version of currently installed Tensorflow.\n\n    Returns\n    -------\n    tuple[int, int]\n        A tuple of the form (major, minor) representing the version of TensorFlow that is installed\n\n    Example\n    -------\n    >>> from lib.utils import get_tf_version\n    >>> get_tf_version()\n    (2, 10)\n    \"\"\"\n    global _TF_VERS  # pylint:disable=global-statement\n    if _TF_VERS is None:\n        import tensorflow as tf  # pylint:disable=import-outside-toplevel\n        split = tf.__version__.split(\".\")[:2]\n        _TF_VERS = (int(split[0]), int(split[1]))\n    return _TF_VERS\n\n\ndef get_folder(path: str, make_folder: bool = True) -> str:\n    \"\"\" Return a path to a folder, creating it if it doesn't exist\n\n    Parameters\n    ----------\n    path: str\n        The path to the folder to obtain\n    make_folder: bool, optional\n        ``True`` if the folder should be created if it does not already exist, ``False`` if the\n        folder should not be created\n\n    Returns\n    -------\n    str or `None`\n        The path to the requested folder. If `make_folder` is set to ``False`` and the requested\n        path does not exist, then ``None`` is returned\n\n    Example\n    -------\n    >>> from lib.utils import get_folder\n    >>> get_folder('/tmp/myfolder')\n    '/tmp/myfolder'\n\n    >>> get_folder('/tmp/myfolder', make_folder=False)\n    ''\n    \"\"\"\n    logger = logging.getLogger(__name__)\n    logger.debug(\"Requested path: '%s'\", path)\n    if not make_folder and not os.path.isdir(path):\n        logger.debug(\"%s does not exist\", path)\n        return \"\"\n    os.makedirs(path, exist_ok=True)\n    logger.debug(\"Returning: '%s'\", path)\n    return path\n\n\ndef get_image_paths(directory: str, extension: str | None = None) -> list[str]:\n    \"\"\" Gets the image paths from a given directory.\n\n    The function searches for files with the specified extension(s) in the given directory, and\n    returns a list of their paths. If no extension is provided, the function will search for files\n    with any of the following extensions: '.bmp', '.jpeg', '.jpg', '.png', '.tif', '.tiff'\n\n    Parameters\n    ----------\n    directory: str\n        The directory to search in\n    extension: str\n        The file extension to search for. If not provided, all image file types will be searched\n        for\n\n    Returns\n    -------\n    list[str]\n        The list of full paths to the images contained within the given folder\n\n    Example\n    -------\n    >>> from lib.utils import get_image_paths\n    >>> get_image_paths('/path/to/directory')\n    ['/path/to/directory/image1.jpg', '/path/to/directory/image2.png']\n    >>> get_image_paths('/path/to/directory', '.jpg')\n    ['/path/to/directory/image1.jpg']\n    \"\"\"\n    logger = logging.getLogger(__name__)\n    image_extensions = IMAGE_EXTENSIONS if extension is None else [extension]\n    dir_contents = []\n\n    if not os.path.exists(directory):\n        logger.debug(\"Creating folder: '%s'\", directory)\n        directory = get_folder(directory)\n\n    dir_scanned = sorted(os.scandir(directory), key=lambda x: x.name)\n    logger.debug(\"Scanned Folder contains %s files\", len(dir_scanned))\n    logger.trace(\"Scanned Folder Contents: %s\", dir_scanned)  # type:ignore[attr-defined]\n\n    for chkfile in dir_scanned:\n        if any(chkfile.name.lower().endswith(ext) for ext in image_extensions):\n            logger.trace(\"Adding '%s' to image list\", chkfile.path)  # type:ignore[attr-defined]\n            dir_contents.append(chkfile.path)\n\n    logger.debug(\"Returning %s images\", len(dir_contents))\n    return dir_contents\n\n\ndef get_dpi() -> float | None:\n    \"\"\" Gets the DPI (dots per inch) of the display screen.\n\n    Returns\n    -------\n    float or ``None``\n        The DPI of the display screen or ``None`` if the dpi couldn't be obtained (ie: if the\n        function is called on a headless system)\n\n    Example\n    -------\n    >>> from lib.utils import get_dpi\n    >>> get_dpi()\n    96.0\n    \"\"\"\n    logger = logging.getLogger(__name__)\n    try:\n        root = tk.Tk()\n        dpi = root.winfo_fpixels('1i')\n    except tk.TclError:\n        logger.warning(\"Display not detected. Could not obtain DPI\")\n        return None\n\n    return float(dpi)\n\n\ndef convert_to_secs(*args: int) -> int:\n    \"\"\"  Convert time in hours, minutes, and seconds to seconds.\n\n    Parameters\n    ----------\n    *args: int\n        1, 2 or 3 ints. If 2 ints are supplied, then (`minutes`, `seconds`) is implied. If 3 ints\n        are supplied then (`hours`, `minutes`, `seconds`) is implied.\n\n    Returns\n    -------\n    int\n        The given time converted to seconds\n\n    Example\n    -------\n    >>> from lib.utils import convert_to_secs\n    >>> convert_to_secs(1, 30, 0)\n    5400\n    >>> convert_to_secs(0, 15, 30)\n    930\n    >>> convert_to_secs(0, 0, 45)\n    45\n    \"\"\"\n    logger = logging.getLogger(__name__)\n    logger.debug(\"from time: %s\", args)\n    retval = 0.0\n    if len(args) == 1:\n        retval = float(args[0])\n    elif len(args) == 2:\n        retval = 60 * float(args[0]) + float(args[1])\n    elif len(args) == 3:\n        retval = 3600 * float(args[0]) + 60 * float(args[1]) + float(args[2])\n    retval = int(retval)\n    logger.debug(\"to secs: %s\", retval)\n    return retval\n\n\ndef full_path_split(path: str) -> list[str]:\n    \"\"\" Split a file path into all of its parts.\n\n    Parameters\n    ----------\n    path: str\n        The full path to be split\n\n    Returns\n    -------\n    list\n        The full path split into a separate item for each part\n\n    Example\n    -------\n    >>> from lib.utils import full_path_split\n    >>> full_path_split(\"/usr/local/bin/python\")\n    ['usr', 'local', 'bin', 'python']\n    >>> full_path_split(\"relative/path/to/file.txt\")\n    ['relative', 'path', 'to', 'file.txt']]\n    \"\"\"\n    logger = logging.getLogger(__name__)\n    allparts: list[str] = []\n    while True:\n        parts = os.path.split(path)\n        if parts[0] == path:   # sentinel for absolute paths\n            allparts.insert(0, parts[0])\n            break\n        if parts[1] == path:  # sentinel for relative paths\n            allparts.insert(0, parts[1])\n            break\n        path = parts[0]\n        allparts.insert(0, parts[1])\n    logger.trace(\"path: %s, allparts: %s\", path, allparts)  # type:ignore[attr-defined]\n    # Remove any empty strings which may have got inserted\n    allparts = [part for part in allparts if part]\n    return allparts\n\n\ndef set_system_verbosity(log_level: str):\n    \"\"\" Set the verbosity level of tensorflow and suppresses future and deprecation warnings from\n    any modules.\n\n    This function sets the `TF_CPP_MIN_LOG_LEVEL` environment variable to control the verbosity of\n    TensorFlow output, as well as filters certain warning types to be ignored. The log level is\n    determined based on the input string `log_level`.\n\n    Parameters\n    ----------\n    log_level: str\n        The requested Faceswap log level.\n\n    References\n    ----------\n    https://stackoverflow.com/questions/35911252/disable-tensorflow-debugging-information\n\n    Example\n    -------\n    >>> from lib.utils import set_system_verbosity\n    >>> set_system_verbosity('warning')\n    \"\"\"\n    logger = logging.getLogger(__name__)\n    from lib.logger import get_loglevel  # pylint:disable=import-outside-toplevel\n    numeric_level = get_loglevel(log_level)\n    log_level = \"3\" if numeric_level > 15 else \"0\"\n    logger.debug(\"System Verbosity level: %s\", log_level)\n    os.environ['TF_CPP_MIN_LOG_LEVEL'] = log_level\n    if log_level != '0':\n        for warncat in (FutureWarning, DeprecationWarning, UserWarning):\n            warnings.simplefilter(action='ignore', category=warncat)\n\n\ndef deprecation_warning(function: str, additional_info: str | None = None) -> None:\n    \"\"\" Log a deprecation warning message.\n\n    This function logs a warning message to indicate that the specified function has been\n    deprecated and will be removed in future. An optional additional message can also be included.\n\n    Parameters\n    ----------\n    function: str\n        The name of the function that will be deprecated.\n    additional_info: str, optional\n        Any additional information to display with the deprecation message. Default: ``None``\n\n    Example\n    -------\n    >>> from lib.utils import deprecation_warning\n    >>> deprecation_warning('old_function', 'Use new_function instead.')\n    \"\"\"\n    logger = logging.getLogger(__name__)\n    logger.debug(\"func_name: %s, additional_info: %s\", function, additional_info)\n    msg = f\"{function} has been deprecated and will be removed from a future update.\"\n    if additional_info is not None:\n        msg += f\" {additional_info}\"\n    logger.warning(msg)\n\n\ndef handle_deprecated_cliopts(arguments: Namespace) -> Namespace:\n    \"\"\" Handle deprecated command line arguments and update to correct argument.\n\n    Deprecated cli opts will be provided in the following format:\n    `\"depr_<option_key>_<deprecated_opt>_<new_opt>\"`\n\n    Parameters\n    ----------\n    arguments: :class:`argpares.Namespace`\n        The passed in faceswap cli arguments\n\n    Returns\n    -------\n    :class:`argpares.Namespace`\n        The cli arguments with deprecated values mapped to the correct entry\n    \"\"\"\n    logger = logging.getLogger(__name__)\n\n    for key, selected in vars(arguments).items():\n        if not key.startswith(\"depr_\") or key.startswith(\"depr_\") and selected is None:\n            continue  # Not a deprecated opt\n        if isinstance(selected, bool) and not selected:\n            continue  # store-true opt with default value\n\n        opt, old, new = key.replace(\"depr_\", \"\").rsplit(\"_\", maxsplit=2)\n        deprecation_warning(f\"Command line option '-{old}'\", f\"Use '-{new}, --{opt}' instead\")\n\n        exist = getattr(arguments, opt)\n        if exist == selected:\n            logger.debug(\"Keeping existing '%s' value of '%s'\", opt, exist)\n        else:\n            logger.debug(\"Updating arg '%s' from '%s' to '%s' from deprecated opt\",\n                         opt, exist, selected)\n\n    return arguments\n\n\ndef camel_case_split(identifier: str) -> list[str]:\n    \"\"\" Split a camelCase string into a list of its individual parts\n\n    Parameters\n    ----------\n    identifier: str\n        The camelCase text to be split\n\n    Returns\n    -------\n    list[str]\n        A list of the individual parts of the camelCase string.\n\n    References\n    ----------\n    https://stackoverflow.com/questions/29916065\n\n    Example\n    -------\n    >>> from lib.utils import camel_case_split\n    >>> camel_case_split('camelCaseExample')\n    ['camel', 'Case', 'Example']\n    \"\"\"\n    matches = finditer(\n        \".+?(?:(?<=[a-z])(?=[A-Z])|(?<=[A-Z])(?=[A-Z][a-z])|$)\",\n        identifier)\n    return [m.group(0) for m in matches]\n\n\ndef safe_shutdown(got_error: bool = False) -> None:\n    \"\"\" Safely shut down the system.\n\n    This function terminates the queue manager and exits the program in a clean and orderly manner.\n    An optional boolean parameter can be used to indicate whether an error occurred during the\n    program's execution.\n\n    Parameters\n    ----------\n    got_error: bool, optional\n        ``True`` if this function is being called as the result of raised error. Default: ``False``\n\n    Example\n    -------\n    >>> from lib.utils import safe_shutdown\n    >>> safe_shutdown()\n    >>> safe_shutdown(True)\n    \"\"\"\n    logger = logging.getLogger(__name__)\n    logger.debug(\"Safely shutting down\")\n    from lib.queue_manager import queue_manager  # pylint:disable=import-outside-toplevel\n    queue_manager.terminate_queues()\n    logger.debug(\"Cleanup complete. Shutting down queue manager and exiting\")\n    sys.exit(1 if got_error else 0)\n\n\nclass FaceswapError(Exception):\n    \"\"\" Faceswap Error for handling specific errors with useful information.\n\n    Raises\n    ------\n    FaceswapError\n        on a captured error\n\n    Example\n    -------\n    >>> from lib.utils import FaceswapError\n    >>> try:\n    ...     # Some code that may raise an error\n    ... except SomeError:\n    ...     raise FaceswapError(\"There was an error while running the code\")\n    FaceswapError: There was an error while running the code\n    \"\"\"\n    pass  # pylint:disable=unnecessary-pass\n\n\nclass GetModel():\n    \"\"\" Check for models in the cache path.\n\n    If available, return the path, if not available, get, unzip and install model\n\n    Parameters\n    ----------\n    model_filename: str or list\n        The name of the model to be loaded (see notes below)\n    git_model_id: int\n        The second digit in the github tag that identifies this model. See\n        https://github.com/deepfakes-models/faceswap-models for more information\n\n    Notes\n    ------\n    Models must have a certain naming convention: `<model_name>_v<version_number>.<extension>`\n    (eg: `s3fd_v1.pb`).\n\n    Multiple models can exist within the model_filename. They should be passed as a list and follow\n    the same naming convention as above. Any differences in filename should occur AFTER the version\n    number: `<model_name>_v<version_number><differentiating_information>.<extension>` (eg:\n    `[\"mtcnn_det_v1.1.py\", \"mtcnn_det_v1.2.py\", \"mtcnn_det_v1.3.py\"]`, `[\"resnet_ssd_v1.caffemodel\"\n    ,\"resnet_ssd_v1.prototext\"]`\n\n    Example\n    -------\n    >>> from lib.utils import GetModel\n    >>> model_downloader = GetModel(\"s3fd_keras_v2.h5\", 11)\n    \"\"\"\n\n    def __init__(self, model_filename: str | list[str], git_model_id: int) -> None:\n        self.logger = logging.getLogger(__name__)\n        if not isinstance(model_filename, list):\n            model_filename = [model_filename]\n        self._model_filename = model_filename\n        self._cache_dir = os.path.join(os.path.abspath(os.path.dirname(sys.argv[0])), \".fs_cache\")\n        self._git_model_id = git_model_id\n        self._url_base = \"https://github.com/deepfakes-models/faceswap-models/releases/download\"\n        self._chunk_size = 1024  # Chunk size for downloading and unzipping\n        self._retries = 6\n        self._get()\n\n    @property\n    def _model_full_name(self) -> str:\n        \"\"\" str: The full model name from the filename(s). \"\"\"\n        common_prefix = os.path.commonprefix(self._model_filename)\n        retval = os.path.splitext(common_prefix)[0]\n        self.logger.trace(retval)  # type:ignore[attr-defined]\n        return retval\n\n    @property\n    def _model_name(self) -> str:\n        \"\"\" str: The model name from the model's full name. \"\"\"\n        retval = self._model_full_name[:self._model_full_name.rfind(\"_\")]\n        self.logger.trace(retval)  # type:ignore[attr-defined]\n        return retval\n\n    @property\n    def _model_version(self) -> int:\n        \"\"\" int: The model's version number from the model full name. \"\"\"\n        retval = int(self._model_full_name[self._model_full_name.rfind(\"_\") + 2:])\n        self.logger.trace(retval)  # type:ignore[attr-defined]\n        return retval\n\n    @property\n    def model_path(self) -> str | list[str]:\n        \"\"\" str or list[str]: The model path(s) in the cache folder.\n\n        Example\n        -------\n        >>> from lib.utils import GetModel\n        >>> model_downloader = GetModel(\"s3fd_keras_v2.h5\", 11)\n        >>> model_downloader.model_path\n        '/path/to/s3fd_keras_v2.h5'\n        \"\"\"\n        paths = [os.path.join(self._cache_dir, fname) for fname in self._model_filename]\n        retval: str | list[str] = paths[0] if len(paths) == 1 else paths\n        self.logger.trace(retval)  # type:ignore[attr-defined]\n        return retval\n\n    @property\n    def _model_zip_path(self) -> str:\n        \"\"\" str: The full path to downloaded zip file. \"\"\"\n        retval = os.path.join(self._cache_dir, f\"{self._model_full_name}.zip\")\n        self.logger.trace(retval)  # type:ignore[attr-defined]\n        return retval\n\n    @property\n    def _model_exists(self) -> bool:\n        \"\"\" bool: ``True`` if the model exists in the cache folder otherwise ``False``. \"\"\"\n        if isinstance(self.model_path, list):\n            retval = all(os.path.exists(pth) for pth in self.model_path)\n        else:\n            retval = os.path.exists(self.model_path)\n        self.logger.trace(retval)  # type:ignore[attr-defined]\n        return retval\n\n    @property\n    def _url_download(self) -> str:\n        \"\"\" strL Base download URL for models. \"\"\"\n        tag = f\"v{self._git_model_id}.{self._model_version}\"\n        retval = f\"{self._url_base}/{tag}/{self._model_full_name}.zip\"\n        self.logger.trace(\"Download url: %s\", retval)  # type:ignore[attr-defined]\n        return retval\n\n    @property\n    def _url_partial_size(self) -> int:\n        \"\"\" int: How many bytes have already been downloaded. \"\"\"\n        zip_file = self._model_zip_path\n        retval = os.path.getsize(zip_file) if os.path.exists(zip_file) else 0\n        self.logger.trace(retval)  # type:ignore[attr-defined]\n        return retval\n\n    def _get(self) -> None:\n        \"\"\" Check the model exists, if not, download the model, unzip it and place it in the\n        model's cache folder. \"\"\"\n        if self._model_exists:\n            self.logger.debug(\"Model exists: %s\", self.model_path)\n            return\n        self._download_model()\n        self._unzip_model()\n        os.remove(self._model_zip_path)\n\n    def _download_model(self) -> None:\n        \"\"\" Download the model zip from github to the cache folder. \"\"\"\n        self.logger.info(\"Downloading model: '%s' from: %s\", self._model_name, self._url_download)\n        for attempt in range(self._retries):\n            try:\n                downloaded_size = self._url_partial_size\n                req = request.Request(self._url_download)\n                if downloaded_size != 0:\n                    req.add_header(\"Range\", f\"bytes={downloaded_size}-\")\n                with request.urlopen(req, timeout=10) as response:\n                    self.logger.debug(\"header info: {%s}\", response.info())\n                    self.logger.debug(\"Return Code: %s\", response.getcode())\n                    self._write_zipfile(response, downloaded_size)\n                break\n            except (socket_error, socket_timeout,\n                    urlliberror.HTTPError, urlliberror.URLError) as err:\n                if attempt + 1 < self._retries:\n                    self.logger.warning(\"Error downloading model (%s). Retrying %s of %s...\",\n                                        str(err), attempt + 2, self._retries)\n                else:\n                    self.logger.error(\"Failed to download model. Exiting. (Error: '%s', URL: \"\n                                      \"'%s')\", str(err), self._url_download)\n                    self.logger.info(\"You can try running again to resume the download.\")\n                    self.logger.info(\"Alternatively, you can manually download the model from: %s \"\n                                     \"and unzip the contents to: %s\",\n                                     self._url_download, self._cache_dir)\n                    sys.exit(1)\n\n    def _write_zipfile(self, response: HTTPResponse, downloaded_size: int) -> None:\n        \"\"\" Write the model zip file to disk.\n\n        Parameters\n        ----------\n        response: :class:`http.client.HTTPResponse`\n            The response from the model download task\n        downloaded_size: int\n            The amount of bytes downloaded so far\n        \"\"\"\n        content_length = response.getheader(\"content-length\")\n        content_length = \"0\" if content_length is None else content_length\n        length = int(content_length) + downloaded_size\n        if length == downloaded_size:\n            self.logger.info(\"Zip already exists. Skipping download\")\n            return\n        write_type = \"wb\" if downloaded_size == 0 else \"ab\"\n        with open(self._model_zip_path, write_type) as out_file:\n            pbar = tqdm(desc=\"Downloading\",\n                        unit=\"B\",\n                        total=length,\n                        unit_scale=True,\n                        unit_divisor=1024)\n            if downloaded_size != 0:\n                pbar.update(downloaded_size)\n            while True:\n                buffer = response.read(self._chunk_size)\n                if not buffer:\n                    break\n                pbar.update(len(buffer))\n                out_file.write(buffer)\n            pbar.close()\n\n    def _unzip_model(self) -> None:\n        \"\"\" Unzip the model file to the cache folder \"\"\"\n        self.logger.info(\"Extracting: '%s'\", self._model_name)\n        try:\n            with zipfile.ZipFile(self._model_zip_path, \"r\") as zip_file:\n                self._write_model(zip_file)\n        except Exception as err:  # pylint:disable=broad-except\n            self.logger.error(\"Unable to extract model file: %s\", str(err))\n            sys.exit(1)\n\n    def _write_model(self, zip_file: zipfile.ZipFile) -> None:\n        \"\"\" Extract files from zip file and write, with progress bar.\n\n        Parameters\n        ----------\n        zip_file: :class:`zipfile.ZipFile`\n            The downloaded model zip file\n        \"\"\"\n        length = sum(f.file_size for f in zip_file.infolist())\n        fnames = zip_file.namelist()\n        self.logger.debug(\"Zipfile: Filenames: %s, Total Size: %s\", fnames, length)\n        pbar = tqdm(desc=\"Decompressing\",\n                    unit=\"B\",\n                    total=length,\n                    unit_scale=True,\n                    unit_divisor=1024)\n        for fname in fnames:\n            out_fname = os.path.join(self._cache_dir, fname)\n            self.logger.debug(\"Extracting from: '%s' to '%s'\", self._model_zip_path, out_fname)\n            zipped = zip_file.open(fname)\n            with open(out_fname, \"wb\") as out_file:\n                while True:\n                    buffer = zipped.read(self._chunk_size)\n                    if not buffer:\n                        break\n                    pbar.update(len(buffer))\n                    out_file.write(buffer)\n        pbar.close()\n\n\nclass DebugTimes():\n    \"\"\" A simple tool to help debug timings.\n\n    Parameters\n    ----------\n    min: bool, Optional\n        Display minimum time taken in summary stats. Default: ``True``\n    mean: bool, Optional\n        Display mean time taken in summary stats. Default: ``True``\n    max: bool, Optional\n        Display maximum time taken in summary stats. Default: ``True``\n\n    Example\n    -------\n    >>> from lib.utils import DebugTimes\n    >>> debug_times = DebugTimes()\n    >>> debug_times.step_start(\"step 1\")\n    >>> # do something here\n    >>> debug_times.step_end(\"step 1\")\n    >>> debug_times.summary()\n    ----------------------------------\n    Step             Count   Min\n    ----------------------------------\n    step 1           1       0.000000\n    \"\"\"\n    def __init__(self,\n                 show_min: bool = True, show_mean: bool = True, show_max: bool = True) -> None:\n        self._times: dict[str, list[float]] = {}\n        self._steps: dict[str, float] = {}\n        self._interval = 1\n        self._display = {\"min\": show_min, \"mean\": show_mean, \"max\": show_max}\n\n    def step_start(self, name: str, record: bool = True) -> None:\n        \"\"\" Start the timer for the given step name.\n\n        Parameters\n        ----------\n        name: str\n            The name of the step to start the timer for\n        record: bool, optional\n            ``True`` to record the step time, ``False`` to not record it.\n            Used for when you have conditional code to time, but do not want to insert if/else\n            statements in the code. Default: `True`\n\n        Example\n        -------\n        >>> from lib.util import DebugTimes\n        >>> debug_times = DebugTimes()\n        >>> debug_times.step_start(\"Example Step\")\n        >>> # do something here\n        >>> debug_times.step_end(\"Example Step\")\n        \"\"\"\n        if not record:\n            return\n        storename = name + str(get_ident())\n        self._steps[storename] = time()\n\n    def step_end(self, name: str, record: bool = True) -> None:\n        \"\"\" Stop the timer and record elapsed time for the given step name.\n\n        Parameters\n        ----------\n        name: str\n            The name of the step to end the timer for\n        record: bool, optional\n            ``True`` to record the step time, ``False`` to not record it.\n            Used for when you have conditional code to time, but do not want to insert if/else\n            statements in the code. Default: `True`\n\n        Example\n        -------\n        >>> from lib.util import DebugTimes\n        >>> debug_times = DebugTimes()\n        >>> debug_times.step_start(\"Example Step\")\n        >>> # do something here\n        >>> debug_times.step_end(\"Example Step\")\n        \"\"\"\n        if not record:\n            return\n        storename = name + str(get_ident())\n        self._times.setdefault(name, []).append(time() - self._steps.pop(storename))\n\n    @classmethod\n    def _format_column(cls, text: str, width: int) -> str:\n        \"\"\" Pad the given text to be aligned to the given width.\n\n        Parameters\n        ----------\n        text: str\n            The text to be formatted\n        width: int\n            The size of the column to insert the text into\n\n        Returns\n        -------\n        str\n            The text with the correct amount of padding applied\n        \"\"\"\n        return f\"{text}{' ' * (width - len(text))}\"\n\n    def summary(self, decimal_places: int = 6, interval: int = 1) -> None:\n        \"\"\" Print a summary of step times.\n\n        Parameters\n        ----------\n        decimal_places: int, optional\n            The number of decimal places to display the summary elapsed times to. Default: 6\n        interval: int, optional\n            How many times summary must be called before printing to console. Default: 1\n\n        Example\n        -------\n        >>> from lib.utils import DebugTimes\n        >>> debug = DebugTimes()\n        >>> debug.step_start(\"test\")\n        >>> time.sleep(0.5)\n        >>> debug.step_end(\"test\")\n        >>> debug.summary()\n        ----------------------------------\n        Step             Count   Min\n        ----------------------------------\n        test             1       0.500000\n        \"\"\"\n        interval = max(1, interval)\n        if interval != self._interval:\n            self._interval += 1\n            return\n\n        name_col = max(len(key) for key in self._times) + 4\n        items_col = 8\n        time_col = (decimal_places + 4) * sum(1 for v in self._display.values() if v)\n        separator = \"-\" * (name_col + items_col + time_col)\n        print(\"\")\n        print(separator)\n        header = (f\"{self._format_column('Step', name_col)}\"\n                  f\"{self._format_column('Count', items_col)}\")\n        header += f\"{self._format_column('Min', time_col)}\" if self._display[\"min\"] else \"\"\n        header += f\"{self._format_column('Avg', time_col)}\" if self._display[\"mean\"] else \"\"\n        header += f\"{self._format_column('Max', time_col)}\" if self._display[\"max\"] else \"\"\n        print(header)\n        print(separator)\n        for key, val in self._times.items():\n            num = str(len(val))\n            contents = f\"{self._format_column(key, name_col)}{self._format_column(num, items_col)}\"\n            if self._display[\"min\"]:\n                _min = f\"{np.min(val):.{decimal_places}f}\"\n                contents += f\"{self._format_column(_min, time_col)}\"\n            if self._display[\"mean\"]:\n                avg = f\"{np.mean(val):.{decimal_places}f}\"\n                contents += f\"{self._format_column(avg, time_col)}\"\n            if self._display[\"max\"]:\n                _max = f\"{np.max(val):.{decimal_places}f}\"\n                contents += f\"{self._format_column(_max, time_col)}\"\n            print(contents)\n        self._interval = 1\n", "lib/keras_utils.py": "#!/usr/bin/env python3\n\"\"\" Common multi-backend Keras utilities \"\"\"\nfrom __future__ import annotations\nimport typing as T\n\nimport numpy as np\n\nimport tensorflow.keras.backend as K  # pylint:disable=import-error\n\nif T.TYPE_CHECKING:\n    from tensorflow import Tensor\n\n\ndef frobenius_norm(matrix: Tensor,\n                   axis: int = -1,\n                   keep_dims: bool = True,\n                   epsilon: float = 1e-15) -> Tensor:\n    \"\"\" Frobenius normalization for Keras Tensor\n\n    Parameters\n    ----------\n    matrix: Tensor\n        The matrix to normalize\n    axis: int, optional\n        The axis to normalize. Default: `-1`\n    keep_dims: bool, Optional\n        Whether to retain the original matrix shape or not. Default:``True``\n    epsilon: flot, optional\n        Epsilon to apply to the normalization to preven NaN errors on zero values\n\n    Returns\n    -------\n    Tensor\n        The normalized output\n    \"\"\"\n    return K.sqrt(K.sum(K.pow(matrix, 2), axis=axis, keepdims=keep_dims) + epsilon)\n\n\ndef replicate_pad(image: Tensor, padding: int) -> Tensor:\n    \"\"\" Apply replication padding to an input batch of images. Expects 4D tensor in BHWC format.\n\n    Notes\n    -----\n    At the time of writing Keras/Tensorflow does not have a native replication padding method.\n    The implementation here is probably not the most efficient, but it is a pure keras method\n    which should work on TF.\n\n    Parameters\n    ----------\n    image: Tensor\n        Image tensor to pad\n    pad: int\n        The amount of padding to apply to each side of the input image\n\n    Returns\n    -------\n    Tensor\n        The input image with replication padding applied\n    \"\"\"\n    top_pad = K.tile(image[:, :1, ...], (1, padding, 1, 1))\n    bottom_pad = K.tile(image[:, -1:, ...], (1, padding, 1, 1))\n    pad_top_bottom = K.concatenate([top_pad, image, bottom_pad], axis=1)\n    left_pad = K.tile(pad_top_bottom[..., :1, :], (1, 1, padding, 1))\n    right_pad = K.tile(pad_top_bottom[..., -1:, :],  (1, 1, padding, 1))\n    padded = K.concatenate([left_pad, pad_top_bottom, right_pad], axis=2)\n    return padded\n\n\nclass ColorSpaceConvert():\n    \"\"\" Transforms inputs between different color spaces on the GPU\n\n    Notes\n    -----\n    The following color space transformations are implemented:\n        - rgb to lab\n        - rgb to xyz\n        - srgb to _rgb\n        - srgb to ycxcz\n        - xyz to ycxcz\n        - xyz to lab\n        - xyz to rgb\n        - ycxcz to rgb\n        - ycxcz to xyz\n\n    Parameters\n    ----------\n    from_space: str\n        One of `\"srgb\"`, `\"rgb\"`, `\"xyz\"`\n    to_space: str\n        One of `\"lab\"`, `\"rgb\"`, `\"ycxcz\"`, `\"xyz\"`\n\n    Raises\n    ------\n    ValueError\n        If the requested color space conversion is not defined\n    \"\"\"\n    def __init__(self, from_space: str, to_space: str) -> None:\n        functions = {\"rgb_lab\": self._rgb_to_lab,\n                     \"rgb_xyz\": self._rgb_to_xyz,\n                     \"srgb_rgb\": self._srgb_to_rgb,\n                     \"srgb_ycxcz\": self._srgb_to_ycxcz,\n                     \"xyz_ycxcz\": self._xyz_to_ycxcz,\n                     \"xyz_lab\": self._xyz_to_lab,\n                     \"xyz_to_rgb\": self._xyz_to_rgb,\n                     \"ycxcz_rgb\": self._ycxcz_to_rgb,\n                     \"ycxcz_xyz\": self._ycxcz_to_xyz}\n        func_name = f\"{from_space.lower()}_{to_space.lower()}\"\n        if func_name not in functions:\n            raise ValueError(f\"The color transform {from_space} to {to_space} is not defined.\")\n\n        self._func = functions[func_name]\n        self._ref_illuminant = K.constant(np.array([[[0.950428545, 1.000000000, 1.088900371]]]),\n                                          dtype=\"float32\")\n        self._inv_ref_illuminant = 1. / self._ref_illuminant\n\n        self._rgb_xyz_map = self._get_rgb_xyz_map()\n        self._xyz_multipliers = K.constant([116, 500, 200], dtype=\"float32\")\n\n    @classmethod\n    def _get_rgb_xyz_map(cls) -> tuple[Tensor, Tensor]:\n        \"\"\" Obtain the mapping and inverse mapping for rgb to xyz color space conversion.\n\n        Returns\n        -------\n        tuple\n            The mapping and inverse Tensors for rgb to xyz color space conversion\n        \"\"\"\n        mapping = np.array([[10135552 / 24577794,  8788810 / 24577794, 4435075 / 24577794],\n                            [2613072 / 12288897, 8788810 / 12288897, 887015 / 12288897],\n                            [1425312 / 73733382, 8788810 / 73733382, 70074185 / 73733382]])\n        inverse = np.linalg.inv(mapping)\n        return (K.constant(mapping, dtype=\"float32\"), K.constant(inverse, dtype=\"float32\"))\n\n    def __call__(self, image: Tensor) -> Tensor:\n        \"\"\" Call the colorspace conversion function.\n\n        Parameters\n        ----------\n        image: Tensor\n            The image tensor in the colorspace defined by :param:`from_space`\n\n        Returns\n        -------\n        Tensor\n            The image tensor in the colorspace defined by :param:`to_space`\n        \"\"\"\n        return self._func(image)\n\n    def _rgb_to_lab(self, image: Tensor) -> Tensor:\n        \"\"\" RGB to LAB conversion.\n\n        Parameters\n        ----------\n        image: Tensor\n            The image tensor in RGB format\n\n        Returns\n        -------\n        Tensor\n            The image tensor in LAB format\n        \"\"\"\n        converted = self._rgb_to_xyz(image)\n        return self._xyz_to_lab(converted)\n\n    def _rgb_xyz_rgb(self, image: Tensor, mapping: Tensor) -> Tensor:\n        \"\"\" RGB to XYZ or XYZ to RGB conversion.\n\n        Notes\n        -----\n        The conversion in both directions is the same, but the mappping matrix for XYZ to RGB is\n        the inverse of RGB to XYZ.\n\n        References\n        ----------\n        https://www.image-engineering.de/library/technotes/958-how-to-convert-between-srgb-and-ciexyz\n\n        Parameters\n        ----------\n        mapping: Tensor\n            The mapping matrix to perform either the XYZ to RGB or RGB to XYZ color space\n            conversion\n\n        image: Tensor\n            The image tensor in RGB format\n\n        Returns\n        -------\n        Tensor\n            The image tensor in XYZ format\n        \"\"\"\n        dim = K.int_shape(image)\n        image = K.permute_dimensions(image, (0, 3, 1, 2))\n        image = K.reshape(image, (dim[0], dim[3], dim[1] * dim[2]))\n        converted = K.permute_dimensions(K.dot(mapping, image), (1, 2, 0))\n        return K.reshape(converted, dim)\n\n    def _rgb_to_xyz(self, image: Tensor) -> Tensor:\n        \"\"\" RGB to XYZ conversion.\n\n        Parameters\n        ----------\n        image: Tensor\n            The image tensor in RGB format\n\n        Returns\n        -------\n        Tensor\n            The image tensor in XYZ format\n        \"\"\"\n        return self._rgb_xyz_rgb(image, self._rgb_xyz_map[0])\n\n    @classmethod\n    def _srgb_to_rgb(cls, image: Tensor) -> Tensor:\n        \"\"\" SRGB to RGB conversion.\n\n        Notes\n        -----\n        RGB Image is clipped to a small epsilon to stabalize training\n\n        Parameters\n        ----------\n        image: Tensor\n            The image tensor in SRGB format\n\n        Returns\n        -------\n        Tensor\n            The image tensor in RGB format\n        \"\"\"\n        limit = 0.04045\n        return K.switch(image > limit,\n                        K.pow((K.clip(image, limit, None) + 0.055) / 1.055, 2.4),\n                        image / 12.92)\n\n    def _srgb_to_ycxcz(self, image: Tensor) -> Tensor:\n        \"\"\" SRGB to YcXcZ conversion.\n\n        Parameters\n        ----------\n        image: Tensor\n            The image tensor in SRGB format\n\n        Returns\n        -------\n        Tensor\n            The image tensor in YcXcZ format\n        \"\"\"\n        converted = self._srgb_to_rgb(image)\n        converted = self._rgb_to_xyz(converted)\n        return self._xyz_to_ycxcz(converted)\n\n    def _xyz_to_lab(self, image: Tensor) -> Tensor:\n        \"\"\" XYZ to LAB conversion.\n\n        Parameters\n        ----------\n        image: Tensor\n            The image tensor in XYZ format\n\n        Returns\n        -------\n        Tensor\n            The image tensor in LAB format\n        \"\"\"\n        image = image * self._inv_ref_illuminant\n        delta = 6 / 29\n        delta_cube = delta ** 3\n        factor = 1 / (3 * (delta ** 2))\n\n        clamped_term = K.pow(K.clip(image, delta_cube, None), 1.0 / 3.0)\n        div = factor * image + (4 / 29)\n\n        image = K.switch(image > delta_cube, clamped_term, div)\n        return K.concatenate([self._xyz_multipliers[0] * image[..., 1:2] - 16.,\n                              self._xyz_multipliers[1:] * (image[..., :2] - image[..., 1:3])],\n                             axis=-1)\n\n    def _xyz_to_rgb(self, image: Tensor) -> Tensor:\n        \"\"\" XYZ to YcXcZ conversion.\n\n        Parameters\n        ----------\n        image: Tensor\n            The image tensor in XYZ format\n\n        Returns\n        -------\n        Tensor\n            The image tensor in RGB format\n        \"\"\"\n        return self._rgb_xyz_rgb(image, self._rgb_xyz_map[1])\n\n    def _xyz_to_ycxcz(self, image: Tensor) -> Tensor:\n        \"\"\" XYZ to YcXcZ conversion.\n\n        Parameters\n        ----------\n        image: Tensor\n            The image tensor in XYZ format\n\n        Returns\n        -------\n        Tensor\n            The image tensor in YcXcZ format\n        \"\"\"\n        image = image * self._inv_ref_illuminant\n        return K.concatenate([self._xyz_multipliers[0] * image[..., 1:2] - 16.,\n                              self._xyz_multipliers[1:] * (image[..., :2] - image[..., 1:3])],\n                             axis=-1)\n\n    def _ycxcz_to_rgb(self, image: Tensor) -> Tensor:\n        \"\"\" YcXcZ to RGB conversion.\n\n        Parameters\n        ----------\n        image: Tensor\n            The image tensor in YcXcZ format\n\n        Returns\n        -------\n        Tensor\n            The image tensor in RGB format\n        \"\"\"\n        converted = self._ycxcz_to_xyz(image)\n        return self._xyz_to_rgb(converted)\n\n    def _ycxcz_to_xyz(self, image: Tensor) -> Tensor:\n        \"\"\" YcXcZ to XYZ conversion.\n\n        Parameters\n        ----------\n        image: Tensor\n            The image tensor in YcXcZ format\n\n        Returns\n        -------\n        Tensor\n            The image tensor in XYZ format\n        \"\"\"\n        ch_y = (image[..., 0:1] + 16.) / self._xyz_multipliers[0]\n        return K.concatenate([ch_y + (image[..., 1:2] / self._xyz_multipliers[1]),\n                              ch_y,\n                              ch_y - (image[..., 2:3] / self._xyz_multipliers[2])],\n                             axis=-1) * self._ref_illuminant\n", "lib/git.py": "#!/usr/bin python3\n\"\"\" Handles command line calls to git \"\"\"\nimport logging\nimport os\nimport sys\n\nfrom subprocess import PIPE, Popen\n\nlogger = logging.getLogger(__name__)\n\n\nclass Git():\n    \"\"\" Handles calls to github \"\"\"\n    def __init__(self) -> None:\n        logger.debug(\"Initializing: %s\", self.__class__.__name__)\n        self._working_dir = os.path.dirname(os.path.realpath(sys.argv[0]))\n        self._available = self._check_available()\n        logger.debug(\"Initialized: %s\", self.__class__.__name__)\n\n    def _from_git(self, command: str) -> tuple[bool, list[str]]:\n        \"\"\" Execute a git command\n\n        Parameters\n        ----------\n        command : str\n            The command to send to git\n\n        Returns\n        -------\n        success: bool\n            ``True`` if the command succesfully executed otherwise ``False``\n        list[str]\n            The output lines from stdout if there was no error, otherwise from stderr\n        \"\"\"\n        logger.debug(\"command: '%s'\", command)\n        cmd = f\"git {command}\"\n        with Popen(cmd, shell=True, stdout=PIPE, stderr=PIPE, cwd=self._working_dir) as proc:\n            stdout, stderr = proc.communicate()\n        retcode = proc.returncode\n        success = retcode == 0\n        lines = stdout.decode(\"utf-8\", errors=\"replace\").splitlines()\n        if not lines:\n            lines = stderr.decode(\"utf-8\", errors=\"replace\").splitlines()\n        logger.debug(\"command: '%s', returncode: %s, success: %s, lines: %s\",\n                     cmd, retcode, success, lines)\n        return success, lines\n\n    def _check_available(self) -> bool:\n        \"\"\" Check if git is available. Does a call to git status. If the process errors due to\n        folder ownership, attempts to add the folder to github safe folders list and tries\n        again\n\n        Returns\n        -------\n        bool\n            ``True`` if git is available otherwise ``False``\n\n        \"\"\"\n        success, msg = self._from_git(\"status\")\n        if success:\n            return True\n        config = next((line.strip() for line in msg if \"add safe.directory\" in line), None)\n        if not config:\n            return False\n        success, _ = self._from_git(config.split(\"git \", 1)[-1])\n        return True\n\n    @property\n    def status(self) -> list[str]:\n        \"\"\" Obtain the output of git status for tracked files only \"\"\"\n        if not self._available:\n            return []\n        success, status = self._from_git(\"status -uno\")\n        if not success or not status:\n            return []\n        return status\n\n    @property\n    def branch(self) -> str:\n        \"\"\" str: The git branch that is currently being used to execute Faceswap. \"\"\"\n        status = next((line.strip() for line in self.status if \"On branch\" in line), \"Not Found\")\n        return status.replace(\"On branch \", \"\")\n\n    @property\n    def branches(self) -> list[str]:\n        \"\"\" list[str]: List of all available branches. \"\"\"\n        if not self._available:\n            return []\n        success, branches = self._from_git(\"branch -a\")\n        if not success or not branches:\n            return []\n        return branches\n\n    def update_remote(self) -> bool:\n        \"\"\" Update all branches to track remote\n\n        Returns\n        -------\n        bool\n            ``True`` if update was succesful otherwise ``False``\n        \"\"\"\n        if not self._available:\n            return False\n        return self._from_git(\"remote update\")[0]\n\n    def pull(self) -> bool:\n        \"\"\" Pull the current branch\n\n        Returns\n        -------\n        bool\n            ``True`` if pull is successful otherwise ``False``\n        \"\"\"\n        if not self._available:\n            return False\n        return self._from_git(\"pull\")[0]\n\n    def checkout(self, branch: str) -> bool:\n        \"\"\" Checkout the requested branch\n\n        Parameters\n        ----------\n        branch : str\n            The branch to checkout\n\n        Returns\n        -------\n        bool\n            ``True`` if the branch was succesfully checkout out otherwise ``False``\n        \"\"\"\n        if not self._available:\n            return False\n        return self._from_git(f\"checkout {branch}\")[0]\n\n    def get_commits(self, count: int) -> list[str]:\n        \"\"\" Obtain the last commits to the repo\n\n        Parameters\n        ----------\n        count : int\n            The last number of commits to obtain\n\n        Returns\n        -------\n        list[str]\n            list of commits, or empty list if none found\n        \"\"\"\n        if not self._available:\n            return []\n        success, commits = self._from_git(f\"log --pretty=oneline --abbrev-commit -n {count}\")\n        if not success or not commits:\n            return []\n        return commits\n\n\ngit = Git()\n\"\"\" :class:`Git`: Handles calls to github \"\"\"\n", "lib/serializer.py": "#!/usr/bin/env python3\n\"\"\"\nLibrary for serializing python objects to and from various different serializer formats\n\"\"\"\n\nimport json\nimport logging\nimport os\nimport pickle\nimport zlib\n\nfrom io import BytesIO\n\nimport numpy as np\n\nfrom lib.utils import FaceswapError\n\ntry:\n    import yaml\n    _HAS_YAML = True\nexcept ImportError:\n    _HAS_YAML = False\n\nlogger = logging.getLogger(__name__)\n\n\nclass Serializer():\n    \"\"\" A convenience class for various serializers.\n\n    This class should not be called directly as it acts as the parent for various serializers.\n    All serializers should be called from :func:`get_serializer` or\n    :func:`get_serializer_from_filename`\n\n    Example\n    -------\n    >>> from lib.serializer import get_serializer\n    >>> serializer = get_serializer('json')\n    >>> json_file = '/path/to/json/file.json'\n    >>> data = serializer.load(json_file)\n    >>> serializer.save(json_file, data)\n\n    \"\"\"\n    def __init__(self):\n        self._file_extension = None\n        self._write_option = \"wb\"\n        self._read_option = \"rb\"\n\n    @property\n    def file_extension(self):\n        \"\"\" str: The file extension of the serializer \"\"\"\n        return self._file_extension\n\n    def save(self, filename, data):\n        \"\"\" Serialize data and save to a file\n\n        Parameters\n        ----------\n        filename: str\n            The path to where the serialized file should be saved\n        data: varies\n            The data that is to be serialized to file\n\n        Example\n        ------\n        >>> serializer = get_serializer('json')\n        >>> data ['foo', 'bar']\n        >>> json_file = '/path/to/json/file.json'\n        >>> serializer.save(json_file, data)\n        \"\"\"\n        logger.debug(\"filename: %s, data type: %s\", filename, type(data))\n        filename = self._check_extension(filename)\n        try:\n            with open(filename, self._write_option) as s_file:\n                s_file.write(self.marshal(data))\n        except IOError as err:\n            msg = f\"Error writing to '{filename}': {err.strerror}\"\n            raise FaceswapError(msg) from err\n\n    def _check_extension(self, filename):\n        \"\"\" Check the filename has an extension. If not add the correct one for the serializer \"\"\"\n        extension = os.path.splitext(filename)[1]\n        retval = filename if extension else f\"{filename}.{self.file_extension}\"\n        logger.debug(\"Original filename: '%s', final filename: '%s'\", filename, retval)\n        return retval\n\n    def load(self, filename):\n        \"\"\" Load data from an existing serialized file\n\n        Parameters\n        ----------\n        filename: str\n            The path to the serialized file\n\n        Returns\n        ----------\n        data: varies\n            The data in a python object format\n\n        Example\n        ------\n        >>> serializer = get_serializer('json')\n        >>> json_file = '/path/to/json/file.json'\n        >>> data = serializer.load(json_file)\n        \"\"\"\n        logger.debug(\"filename: %s\", filename)\n        try:\n            with open(filename, self._read_option) as s_file:\n                data = s_file.read()\n                logger.debug(\"stored data type: %s\", type(data))\n                retval = self.unmarshal(data)\n\n        except IOError as err:\n            msg = f\"Error reading from '{filename}': {err.strerror}\"\n            raise FaceswapError(msg) from err\n        logger.debug(\"data type: %s\", type(retval))\n        return retval\n\n    def marshal(self, data):\n        \"\"\" Serialize an object\n\n        Parameters\n        ----------\n        data: varies\n            The data that is to be serialized\n\n        Returns\n        -------\n        data: varies\n            The data in a the serialized data format\n\n        Example\n        ------\n        >>> serializer = get_serializer('json')\n        >>> data ['foo', 'bar']\n        >>> json_data = serializer.marshal(data)\n        \"\"\"\n        logger.debug(\"data type: %s\", type(data))\n        try:\n            retval = self._marshal(data)\n        except Exception as err:\n            msg = f\"Error serializing data for type {type(data)}: {str(err)}\"\n            raise FaceswapError(msg) from err\n        logger.debug(\"returned data type: %s\", type(retval))\n        return retval\n\n    def unmarshal(self, serialized_data):\n        \"\"\" Unserialize data to its original object type\n\n        Parameters\n        ----------\n        serialized_data: varies\n            Data in serializer format that is to be unmarshalled to its original object\n\n        Returns\n        -------\n        data: varies\n            The data in a python object format\n\n        Example\n        ------\n        >>> serializer = get_serializer('json')\n        >>> json_data = <json object>\n        >>> data = serializer.unmarshal(json_data)\n        \"\"\"\n        logger.debug(\"data type: %s\", type(serialized_data))\n        try:\n            retval = self._unmarshal(serialized_data)\n        except Exception as err:\n            msg = f\"Error unserializing data for type {type(serialized_data)}: {str(err)}\"\n            raise FaceswapError(msg) from err\n        logger.debug(\"returned data type: %s\", type(retval))\n        return retval\n\n    def _marshal(self, data):\n        \"\"\" Override for serializer specific marshalling \"\"\"\n        raise NotImplementedError()\n\n    def _unmarshal(self, data):\n        \"\"\" Override for serializer specific unmarshalling \"\"\"\n        raise NotImplementedError()\n\n\nclass _YAMLSerializer(Serializer):\n    \"\"\" YAML Serializer \"\"\"\n    def __init__(self):\n        super().__init__()\n        self._file_extension = \"yml\"\n\n    def _marshal(self, data):\n        return yaml.dump(data, default_flow_style=False).encode(\"utf-8\")\n\n    def _unmarshal(self, data):\n        return yaml.load(data.decode(\"utf-8\", errors=\"replace\"), Loader=yaml.FullLoader)\n\n\nclass _JSONSerializer(Serializer):\n    \"\"\" JSON Serializer \"\"\"\n    def __init__(self):\n        super().__init__()\n        self._file_extension = \"json\"\n\n    def _marshal(self, data):\n        return json.dumps(data, indent=2).encode(\"utf-8\")\n\n    def _unmarshal(self, data):\n        return json.loads(data.decode(\"utf-8\", errors=\"replace\"))\n\n\nclass _PickleSerializer(Serializer):\n    \"\"\" Pickle Serializer \"\"\"\n    def __init__(self):\n        super().__init__()\n        self._file_extension = \"pickle\"\n\n    def _marshal(self, data):\n        return pickle.dumps(data)\n\n    def _unmarshal(self, data):\n        return pickle.loads(data)\n\n\nclass _NPYSerializer(Serializer):\n    \"\"\" NPY Serializer \"\"\"\n    def __init__(self):\n        super().__init__()\n        self._file_extension = \"npy\"\n        self._bytes = BytesIO()\n\n    def _marshal(self, data):\n        \"\"\" NPY Marshal to bytesIO so standard bytes writer can write out \"\"\"\n        b_handler = BytesIO()\n        np.save(b_handler, data)\n        b_handler.seek(0)\n        return b_handler.read()\n\n    def _unmarshal(self, data):\n        \"\"\" NPY Unmarshal to bytesIO so we can use numpy loader \"\"\"\n        b_handler = BytesIO(data)\n        retval = np.load(b_handler)\n        del b_handler\n        if retval.dtype == \"object\":\n            retval = retval[()]\n        return retval\n\n\nclass _CompressedSerializer(Serializer):\n    \"\"\" A compressed pickle serializer for Faceswap \"\"\"\n    def __init__(self):\n        super().__init__()\n        self._file_extension = \"fsa\"\n        self._child = get_serializer(\"pickle\")\n\n    def _marshal(self, data):\n        \"\"\" Pickle and compress data \"\"\"\n        data = self._child._marshal(data)  # pylint:disable=protected-access\n        return zlib.compress(data)\n\n    def _unmarshal(self, data):\n        \"\"\" Decompress and unpicke data \"\"\"\n        data = zlib.decompress(data)\n        return self._child._unmarshal(data)  # pylint:disable=protected-access\n\n\ndef get_serializer(serializer):\n    \"\"\" Obtain a serializer object\n\n    Parameters\n    ----------\n    serializer: {'json', 'pickle', yaml', 'npy', 'compressed'}\n        The required serializer format\n\n    Returns\n    -------\n    serializer: :class:`Serializer`\n        A serializer object for handling the requested data format\n\n    Example\n    -------\n    >>> serializer = get_serializer('json')\n    \"\"\"\n    if serializer.lower() == \"npy\":\n        retval = _NPYSerializer()\n    elif serializer.lower() == \"compressed\":\n        retval = _CompressedSerializer()\n    elif serializer.lower() == \"json\":\n        retval = _JSONSerializer()\n    elif serializer.lower() == \"pickle\":\n        retval = _PickleSerializer()\n    elif serializer.lower() == \"yaml\" and _HAS_YAML:\n        retval = _YAMLSerializer()\n    elif serializer.lower() == \"yaml\":\n        logger.warning(\"You must have PyYAML installed to use YAML as the serializer.\"\n                       \"Switching to JSON as the serializer.\")\n        retval = _JSONSerializer\n    else:\n        logger.warning(\"Unrecognized serializer: '%s'. Returning json serializer\", serializer)\n    logger.debug(retval)\n    return retval\n\n\ndef get_serializer_from_filename(filename):\n    \"\"\" Obtain a serializer object from a filename\n\n    Parameters\n    ----------\n    filename: str\n        Filename to determine the serializer type from\n\n    Returns\n    -------\n    serializer: :class:`Serializer`\n        A serializer object for handling the requested data format\n\n    Example\n    -------\n    >>> filename = '/path/to/json/file.json'\n    >>> serializer = get_serializer_from_filename(filename)\n    \"\"\"\n    logger.debug(\"filename: '%s'\", filename)\n    extension = os.path.splitext(filename)[1].lower()\n    logger.debug(\"extension: '%s'\", extension)\n\n    if extension == \".json\":\n        retval = _JSONSerializer()\n    elif extension in (\".p\", \".pickle\"):\n        retval = _PickleSerializer()\n    elif extension == \".npy\":\n        retval = _NPYSerializer()\n    elif extension == \".fsa\":\n        retval = _CompressedSerializer()\n    elif extension in (\".yaml\", \".yml\") and _HAS_YAML:\n        retval = _YAMLSerializer()\n    elif extension in (\".yaml\", \".yml\"):\n        logger.warning(\"You must have PyYAML installed to use YAML as the serializer.\\n\"\n                       \"Switching to JSON as the serializer.\")\n        retval = _JSONSerializer()\n    else:\n        logger.warning(\"Unrecognized extension: '%s'. Returning json serializer\", extension)\n        retval = _JSONSerializer()\n    logger.debug(retval)\n    return retval\n", "lib/image.py": "#!/usr/bin python3\n\"\"\" Utilities for working with images and videos \"\"\"\nfrom __future__ import annotations\nimport json\nimport logging\nimport re\nimport subprocess\nimport os\nimport struct\nimport sys\nimport typing as T\n\nfrom ast import literal_eval\nfrom bisect import bisect\nfrom concurrent import futures\nfrom zlib import crc32\n\nimport cv2\nimport imageio\nimport imageio_ffmpeg as im_ffm\nimport numpy as np\nfrom tqdm import tqdm\n\nfrom lib.multithreading import MultiThread\nfrom lib.queue_manager import queue_manager, QueueEmpty\nfrom lib.utils import convert_to_secs, FaceswapError, VIDEO_EXTENSIONS, get_image_paths\n\nif T.TYPE_CHECKING:\n    from lib.align.alignments import PNGHeaderDict\n\nlogger = logging.getLogger(__name__)\n\n# ################### #\n# <<< IMAGE UTILS >>> #\n# ################### #\n\n\n# <<< IMAGE IO >>> #\n\nclass FfmpegReader(imageio.plugins.ffmpeg.FfmpegFormat.Reader):  # type:ignore\n    \"\"\" Monkey patch imageio ffmpeg to use keyframes whilst seeking \"\"\"\n    def __init__(self, format, request):\n        super().__init__(format, request)\n        self._frame_pts = None\n        self._keyframes = None\n        self.use_patch = False\n\n    def get_frame_info(self, frame_pts=None, keyframes=None):\n        \"\"\" Store the source video's keyframes in :attr:`_frame_info\" for the current video for use\n        in :func:`initialize`.\n\n        Parameters\n        ----------\n        frame_pts: list, optional\n            A list corresponding to the video frame count of the pts_time per frame. If this and\n            `keyframes` are provided, then analyzing the video is skipped and the values from the\n            given lists are used. Default: ``None``\n        keyframes: list, optional\n            A list containing the frame numbers of each key frame. if this and `frame_pts` are\n            provided, then analyzing the video is skipped and the values from the given lists are\n            used. Default: ``None``\n        \"\"\"\n        if frame_pts is not None and keyframes is not None:\n            logger.debug(\"Video meta information provided. Not analyzing video\")\n            self._frame_pts = frame_pts\n            self._keyframes = keyframes\n            return len(frame_pts), dict(pts_time=self._frame_pts, keyframes=self._keyframes)\n\n        assert isinstance(self._filename, str), \"Video path must be a string\"\n\n        # NB: The below video filter applies the detected frame rate prior to showinfo. This\n        # appears to help prevent an issue where the number of timestamp entries generated by\n        # showinfo does not correspond to the number of frames that the video file generates.\n        # This is because the demuxer will duplicate frames to meet the required frame rate.\n        # This **may** cause issues so be aware.\n\n        # Also, drop frame rates (i.e 23.98, 29.97 and 59.94) will introduce rounding errors which\n        # means sync will drift on generated pts. These **should** be the only 'drop-frame rates'\n        # that appear in video files, but this is video files, and nothing is guaranteed.\n        # (The actual values for these should be 24000/1001, 30000/1001 and 60000/1001\n        # respectively). The solutions to round these values is hacky at best, so:\n        # TODO find a more robust method for extracting/handling drop-frame rates.\n\n        fps = self._meta[\"fps\"]\n        rounded_fps = round(fps, 0)\n        if 0.01 < rounded_fps - fps < 0.10:  # 0.90 - 0.99\n            new_fps = f\"{int(rounded_fps * 1000)}/1001\"\n            logger.debug(\"Adjusting drop-frame fps: %s to %s\", fps, new_fps)\n            fps = new_fps\n\n        cmd = [im_ffm.get_ffmpeg_exe(),\n               \"-hide_banner\",\n               \"-copyts\",\n               \"-i\", self._filename,\n               \"-vf\", f\"fps=fps={fps},showinfo\",\n               \"-start_number\", \"0\",\n               \"-an\",\n               \"-f\", \"null\",\n               \"-\"]\n        logger.debug(\"FFMPEG Command: '%s'\", \" \".join(cmd))\n        process = subprocess.Popen(cmd,\n                                   stderr=subprocess.STDOUT,\n                                   stdout=subprocess.PIPE,\n                                   universal_newlines=True)\n        frame_pts = []\n        key_frames = []\n        last_update = 0\n        pbar = tqdm(desc=\"Analyzing Video\",\n                    leave=False,\n                    total=int(self._meta[\"duration\"]),\n                    unit=\"secs\")\n        while True:\n            output = process.stdout.readline().strip()\n            if output == \"\" and process.poll() is not None:\n                break\n            if \"iskey\" not in output:\n                continue\n            logger.trace(\"Keyframe line: %s\", output)\n            line = re.split(r\"\\s+|:\\s*\", output)\n            pts_time = float(line[line.index(\"pts_time\") + 1])\n            frame_no = int(line[line.index(\"n\") + 1])\n            frame_pts.append(pts_time)\n            if \"iskey:1\" in output:\n                key_frames.append(frame_no)\n\n            logger.trace(\"pts_time: %s, frame_no: %s\", pts_time, frame_no)\n            if int(pts_time) == last_update:\n                # Floating points make TQDM display poorly, so only update on full\n                # second increments\n                continue\n            pbar.update(int(pts_time) - last_update)\n            last_update = int(pts_time)\n        pbar.close()\n        return_code = process.poll()\n        frame_count = len(frame_pts)\n        logger.debug(\"Return code: %s, frame_pts: %s, keyframes: %s, frame_count: %s\",\n                     return_code, frame_pts, key_frames, frame_count)\n\n        self._frame_pts = frame_pts\n        self._keyframes = key_frames\n        return frame_count, dict(pts_time=self._frame_pts, keyframes=self._keyframes)\n\n    def _previous_keyframe_info(self, index=0):\n        \"\"\" Return the previous keyframe's pts_time and frame number \"\"\"\n        prev_keyframe_idx = bisect(self._keyframes, index) - 1\n        prev_keyframe = self._keyframes[prev_keyframe_idx]\n        prev_pts_time = self._frame_pts[prev_keyframe]\n        logger.trace(\"keyframe pts_time: %s, keyframe: %s\", prev_pts_time, prev_keyframe)\n        return prev_pts_time, prev_keyframe\n\n    def _initialize(self, index=0):  # noqa:C901\n        \"\"\" Replace ImageIO _initialize with a version that explictly uses keyframes.\n\n        Notes\n        -----\n        This introduces a minor change by seeking fast to the previous keyframe and then discarding\n        subsequent frames until the desired frame is reached. In testing, setting -ss flag either\n        prior to input, or both prior (fast) and after (slow) would not always bring back the\n        correct frame for all videos. Navigating to the previous keyframe then discarding frames\n        until the correct frame is reached appears to work well.\n        \"\"\"\n        # pylint:disable-all\n        if self._read_gen is not None:\n            self._read_gen.close()\n\n        iargs = []\n        oargs = []\n        skip_frames = 0\n\n        # Create input args\n        iargs += self._arg_input_params\n        if self.request._video:\n            iargs += [\"-f\", CAM_FORMAT]  # noqa\n            if self._arg_pixelformat:\n                iargs += [\"-pix_fmt\", self._arg_pixelformat]\n            if self._arg_size:\n                iargs += [\"-s\", self._arg_size]\n        elif index > 0:  # re-initialize  / seek\n            # Note: only works if we initialized earlier, and now have meta. Some info here:\n            # https://trac.ffmpeg.org/wiki/Seeking\n            # There are two ways to seek, one before -i (input_params) and after (output_params).\n            # The former is fast, because it uses keyframes, the latter is slow but accurate.\n            # According to the article above, the fast method should also be accurate from ffmpeg\n            # version 2.1, however in version 4.1 our tests start failing again. Not sure why, but\n            # we can solve this by combining slow and fast.\n            # Further note: The old method would go back 10 seconds and then seek slow. This was\n            # still somewhat unresponsive and did not always land on the correct frame. This monkey\n            # patched version goes to the previous keyframe then discards frames until the correct\n            # frame is landed on.\n            if self.use_patch and self._frame_pts is None:\n                self.get_frame_info()\n\n            if self.use_patch:\n                keyframe_pts, keyframe = self._previous_keyframe_info(index)\n                seek_fast = keyframe_pts\n                skip_frames = index - keyframe\n            else:\n                starttime = index / self._meta[\"fps\"]\n                seek_slow = min(10, starttime)\n                seek_fast = starttime - seek_slow\n\n            # We used to have this epsilon earlier, when we did not use\n            # the slow seek. I don't think we need it anymore.\n            # epsilon = -1 / self._meta[\"fps\"] * 0.1\n            iargs += [\"-ss\", \"%.06f\" % (seek_fast)]\n            if not self.use_patch:\n                oargs += [\"-ss\", \"%.06f\" % (seek_slow)]\n\n        # Output args, for writing to pipe\n        if self._arg_size:\n            oargs += [\"-s\", self._arg_size]\n        if self.request.kwargs.get(\"fps\", None):\n            fps = float(self.request.kwargs[\"fps\"])\n            oargs += [\"-r\", \"%.02f\" % fps]\n        oargs += self._arg_output_params\n\n        # Get pixelformat and bytes per pixel\n        pix_fmt = self._pix_fmt\n        bpp = self._depth * self._bytes_per_channel\n\n        # Create generator\n        rf = self._ffmpeg_api.read_frames\n        self._read_gen = rf(\n            self._filename, pix_fmt, bpp, input_params=iargs, output_params=oargs\n        )\n\n        # Read meta data. This start the generator (and ffmpeg subprocess)\n        if self.request._video:\n            # With cameras, catch error and turn into IndexError\n            try:\n                meta = self._read_gen.__next__()\n            except IOError as err:\n                err_text = str(err)\n                if \"darwin\" in sys.platform:\n                    if \"Unknown input format: 'avfoundation'\" in err_text:\n                        err_text += (\n                            \"Try installing FFMPEG using \"\n                            \"home brew to get a version with \"\n                            \"support for cameras.\"\n                        )\n                raise IndexError(\n                    \"No camera at {}.\\n\\n{}\".format(self.request._video, err_text)\n                )\n            else:\n                self._meta.update(meta)\n        elif index == 0:\n            self._meta.update(self._read_gen.__next__())\n        else:\n            if self.use_patch:\n                frames_skipped = 0\n                while skip_frames != frames_skipped:\n                    # Skip frames that are not the desired frame\n                    _ = self._read_gen.__next__()\n                    frames_skipped += 1\n            self._read_gen.__next__()  # we already have meta data\n\n\nimageio.plugins.ffmpeg.FfmpegFormat.Reader = FfmpegReader  # type: ignore\n\n\ndef read_image(filename, raise_error=False, with_metadata=False):\n    \"\"\" Read an image file from a file location.\n\n    Extends the functionality of :func:`cv2.imread()` by ensuring that an image was actually\n    loaded. Errors can be logged and ignored so that the process can continue on an image load\n    failure.\n\n    Parameters\n    ----------\n    filename: str\n        Full path to the image to be loaded.\n    raise_error: bool, optional\n        If ``True`` then any failures (including the returned image being ``None``) will be\n        raised. If ``False`` then an error message will be logged, but the error will not be\n        raised. Default: ``False``\n    with_metadata: bool, optional\n        Only returns a value if the images loaded are extracted Faceswap faces. If ``True`` then\n        returns the Faceswap metadata stored with in a Face images .png exif header.\n        Default: ``False``\n\n    Returns\n    -------\n    numpy.ndarray or tuple\n        If :attr:`with_metadata` is ``False`` then returns a `numpy.ndarray` of the image in `BGR`\n        channel order. If :attr:`with_metadata` is ``True`` then returns a `tuple` of\n        (`numpy.ndarray`\" of the image in `BGR`, `dict` of face's Faceswap metadata)\n    Example\n    -------\n    >>> image_file = \"/path/to/image.png\"\n    >>> try:\n    >>>    image = read_image(image_file, raise_error=True, with_metadata=False)\n    >>> except:\n    >>>     raise ValueError(\"There was an error\")\n    \"\"\"\n    logger.trace(\"Requested image: '%s'\", filename)\n    success = True\n    image = None\n    try:\n        with open(filename, \"rb\") as infile:\n            raw_file = infile.read()\n            image = cv2.imdecode(np.frombuffer(raw_file, dtype=\"uint8\"), cv2.IMREAD_COLOR)\n            if image is None:\n                raise ValueError(\"Image is None\")\n            if with_metadata:\n                metadata = png_read_meta(raw_file)\n                retval = (image, metadata)\n            else:\n                retval = image\n    except TypeError as err:\n        success = False\n        msg = \"Error while reading image (TypeError): '{}'\".format(filename)\n        msg += \". Original error message: {}\".format(str(err))\n        logger.error(msg)\n        if raise_error:\n            raise Exception(msg)\n    except ValueError as err:\n        success = False\n        msg = (\"Error while reading image. This can be caused by special characters in the \"\n               \"filename or a corrupt image file: '{}'\".format(filename))\n        msg += \". Original error message: {}\".format(str(err))\n        logger.error(msg)\n        if raise_error:\n            raise Exception(msg)\n    except Exception as err:  # pylint:disable=broad-except\n        success = False\n        msg = \"Failed to load image '{}'. Original Error: {}\".format(filename, str(err))\n        logger.error(msg)\n        if raise_error:\n            raise Exception(msg)\n    logger.trace(\"Loaded image: '%s'. Success: %s\", filename, success)\n    return retval\n\n\ndef read_image_batch(filenames, with_metadata=False):\n    \"\"\" Load a batch of images from the given file locations.\n\n    Leverages multi-threading to load multiple images from disk at the same time leading to vastly\n    reduced image read times.\n\n    Parameters\n    ----------\n    filenames: list\n        A list of ``str`` full paths to the images to be loaded.\n    with_metadata: bool, optional\n        Only returns a value if the images loaded are extracted Faceswap faces. If ``True`` then\n        returns the Faceswap metadata stored with in a Face images .png exif header.\n        Default: ``False``\n\n    Returns\n    -------\n    numpy.ndarray\n        The batch of images in `BGR` channel order returned in the order of :attr:`filenames`\n\n    Notes\n    -----\n    As the images are compiled into a batch, they must be all of the same dimensions.\n\n    Example\n    -------\n    >>> image_filenames = [\"/path/to/image_1.png\", \"/path/to/image_2.png\", \"/path/to/image_3.png\"]\n    >>> images = read_image_batch(image_filenames)\n    \"\"\"\n    logger.trace(\"Requested batch: '%s'\", filenames)\n    batch = [None for _ in range(len(filenames))]\n    if with_metadata:\n        meta = [None for _ in range(len(filenames))]\n\n    with futures.ThreadPoolExecutor() as executor:\n        images = {executor.submit(read_image, filename,\n                                  raise_error=True, with_metadata=with_metadata): idx\n                  for idx, filename in enumerate(filenames)}\n        for future in futures.as_completed(images):\n            ret_idx = images[future]\n            if with_metadata:\n                batch[ret_idx], meta[ret_idx] = future.result()\n            else:\n                batch[ret_idx] = future.result()\n\n    batch = np.array(batch)\n    retval = (batch, meta) if with_metadata else batch\n    logger.trace(\"Returning images: (filenames: %s, batch shape: %s, with_metadata: %s)\",\n                 filenames, batch.shape, with_metadata)\n    return retval\n\n\ndef read_image_meta(filename):\n    \"\"\" Read the Faceswap metadata stored in an extracted face's exif header.\n\n    Parameters\n    ----------\n    filename: str\n        Full path to the image to be retrieve the meta information for.\n\n    Returns\n    -------\n    dict\n        The output dictionary will contain the `width` and `height` of the png image as well as any\n        `itxt` information.\n    Example\n    -------\n    >>> image_file = \"/path/to/image.png\"\n    >>> metadata = read_image_meta(image_file)\n    >>> width = metadata[\"width]\n    >>> height = metadata[\"height\"]\n    >>> faceswap_info = metadata[\"itxt\"]\n    \"\"\"\n    retval = dict()\n    if os.path.splitext(filename)[-1].lower() != \".png\":\n        # Get the dimensions directly from the image for non-pngs\n        logger.trace(\"Non png found. Loading file for dimensions: '%s'\", filename)\n        img = cv2.imread(filename)\n        retval[\"height\"], retval[\"width\"] = img.shape[:2]\n        return retval\n    with open(filename, \"rb\") as infile:\n        try:\n            chunk = infile.read(8)\n        except PermissionError:\n            raise PermissionError(f\"PermissionError while reading: {filename}\")\n\n        if chunk != b\"\\x89PNG\\r\\n\\x1a\\n\":\n            raise ValueError(f\"Invalid header found in png: {filename}\")\n\n        while True:\n            chunk = infile.read(8)\n            length, field = struct.unpack(\">I4s\", chunk)\n            logger.trace(\"Read chunk: (chunk: %s, length: %s, field: %s\", chunk, length, field)\n            if not chunk or field == b\"IDAT\":\n                break\n            if field == b\"IHDR\":\n                # Get dimensions\n                chunk = infile.read(8)\n                retval[\"width\"], retval[\"height\"] = struct.unpack(\">II\", chunk)\n                length -= 8\n            elif field == b\"iTXt\":\n                keyword, value = infile.read(length).split(b\"\\0\", 1)\n                if keyword == b\"faceswap\":\n                    retval[\"itxt\"] = literal_eval(value[4:].decode(\"utf-8\", errors=\"replace\"))\n                    break\n                else:\n                    logger.trace(\"Skipping iTXt chunk: '%s'\", keyword.decode(\"latin-1\",\n                                                                             errors=\"ignore\"))\n                    length = 0  # Reset marker for next chunk\n            infile.seek(length + 4, 1)\n    logger.trace(\"filename: %s, metadata: %s\", filename, retval)\n    return retval\n\n\ndef read_image_meta_batch(filenames):\n    \"\"\" Read the Faceswap metadata stored in a batch extracted faces' exif headers.\n\n    Leverages multi-threading to load multiple images from disk at the same time\n    leading to vastly reduced image read times. Creates a generator to retrieve filenames\n    with their metadata as they are calculated.\n\n    Notes\n    -----\n    The order of returned values is non-deterministic so will most likely not be returned in the\n    same order as the filenames\n\n    Parameters\n    ----------\n    filenames: list\n        A list of ``str`` full paths to the images to be loaded.\n\n    Yields\n    -------\n    tuple\n        (**filename** (`str`), **metadata** (`dict`) )\n\n    Example\n    -------\n    >>> image_filenames = [\"/path/to/image_1.png\", \"/path/to/image_2.png\", \"/path/to/image_3.png\"]\n    >>> for filename, meta in read_image_meta_batch(image_filenames):\n    >>>         <do something>\n    \"\"\"\n    logger.trace(\"Requested batch: '%s'\", filenames)\n    executor = futures.ThreadPoolExecutor()\n    with executor:\n        logger.debug(\"Submitting %s items to executor\", len(filenames))\n        read_meta = {executor.submit(read_image_meta, filename): filename\n                     for filename in filenames}\n        logger.debug(\"Succesfully submitted %s items to executor\", len(filenames))\n        for future in futures.as_completed(read_meta):\n            retval = (read_meta[future], future.result())\n            logger.trace(\"Yielding: %s\", retval)\n            yield retval\n\n\ndef pack_to_itxt(metadata):\n    \"\"\" Pack the given metadata dictionary to a PNG iTXt header field.\n\n    Parameters\n    ----------\n    metadata: dict or bytes\n        The dictionary to write to the header. Can be pre-encoded as utf-8.\n\n    Returns\n    -------\n    bytes\n        A byte encoded PNG iTXt field, including chunk header and CRC\n    \"\"\"\n    if not isinstance(metadata, bytes):\n        metadata = str(metadata).encode(\"utf-8\", \"strict\")\n    key = \"faceswap\".encode(\"latin-1\", \"strict\")\n\n    chunk = key + b\"\\0\\0\\0\\0\\0\" + metadata\n    crc = struct.pack(\">I\", crc32(chunk, crc32(b\"iTXt\")) & 0xFFFFFFFF)\n    length = struct.pack(\">I\", len(chunk))\n    retval = length + b\"iTXt\" + chunk + crc\n    return retval\n\n\ndef update_existing_metadata(filename, metadata):\n    \"\"\" Update the png header metadata for an existing .png extracted face file on the filesystem.\n\n    Parameters\n    ----------\n    filename: str\n        The full path to the face to be updated\n    metadata: dict or bytes\n        The dictionary to write to the header. Can be pre-encoded as utf-8.\n    \"\"\"\n\n    tmp_filename = filename + \"~\"\n    with open(filename, \"rb\") as png, open(tmp_filename, \"wb\") as tmp:\n        chunk = png.read(8)\n        if chunk != b\"\\x89PNG\\r\\n\\x1a\\n\":\n            raise ValueError(f\"Invalid header found in png: {filename}\")\n        tmp.write(chunk)\n\n        while True:\n            chunk = png.read(8)\n            length, field = struct.unpack(\">I4s\", chunk)\n            logger.trace(\"Read chunk: (chunk: %s, length: %s, field: %s)\", chunk, length, field)\n\n            if field == b\"IDAT\":  # Write out all remaining data\n                logger.trace(\"Writing image data and closing png\")\n                tmp.write(chunk + png.read())\n                break\n\n            if field != b\"iTXt\":  # Write non iTXt chunk straight out\n                logger.trace(\"Copying existing chunk\")\n                tmp.write(chunk + png.read(length + 4))  # Header + CRC\n                continue\n\n            keyword, value = png.read(length).split(b\"\\0\", 1)\n            if keyword != b\"faceswap\":\n                # Write existing non fs-iTXt data + CRC\n                logger.trace(\"Copying non-faceswap iTXt chunk: %s\", keyword)\n                tmp.write(keyword + b\"\\0\" + value + png.read(4))\n                continue\n\n            logger.trace(\"Updating faceswap iTXt chunk\")\n            tmp.write(pack_to_itxt(metadata))\n            png.seek(4, 1)  # Skip old CRC\n\n    os.replace(tmp_filename, filename)\n\n\ndef encode_image(image: np.ndarray,\n                 extension: str,\n                 encoding_args: tuple[int, ...] | None = None,\n                 metadata: PNGHeaderDict | dict[str, T.Any] | bytes | None = None) -> bytes:\n    \"\"\" Encode an image.\n\n    Parameters\n    ----------\n    image: numpy.ndarray\n        The image to be encoded in `BGR` channel order.\n    extension: str\n        A compatible `cv2` image file extension that the final image is to be saved to.\n    encoding_args: tuple[int, ...], optional\n        Any encoding arguments to pass to cv2's imencode function\n    metadata: dict or bytes, optional\n        Metadata for the image. If provided, and the extension is png or tiff, this information\n        will be written to the PNG itxt header. Default:``None`` Can be provided as a python dict\n        or pre-encoded\n\n    Returns\n    -------\n    encoded_image: bytes\n        The image encoded into the correct file format as bytes\n\n    Example\n    -------\n    >>> image_file = \"/path/to/image.png\"\n    >>> image = read_image(image_file)\n    >>> encoded_image = encode_image(image, \".jpg\")\n    \"\"\"\n    if metadata and extension.lower() not in (\".png\", \".tif\"):\n        raise ValueError(\"Metadata is only supported for .png and .tif images\")\n    args = tuple() if encoding_args is None else encoding_args\n\n    retval = cv2.imencode(extension, image, args)[1].tobytes()\n    if metadata:\n        func = {\".png\": png_write_meta, \".tif\": tiff_write_meta}[extension]\n        retval = func(retval, metadata)\n    return retval\n\n\ndef png_write_meta(image: bytes, data: PNGHeaderDict | dict[str, T.Any] | bytes) -> bytes:\n    \"\"\" Write Faceswap information to a png's iTXt field.\n\n    Parameters\n    ----------\n    image: bytes\n        The bytes encoded png file to write header data to\n    data: dict or bytes\n        The dictionary to write to the header. Can be pre-encoded as utf-8.\n\n    Notes\n    -----\n    This is a fairly stripped down and non-robust header writer to fit a very specific task. OpenCV\n    will not write any iTXt headers to the PNG file, so we make the assumption that the only iTXt\n    header that exists is the one that we created for storing alignments.\n\n    References\n    ----------\n    PNG Specification: https://www.w3.org/TR/2003/REC-PNG-20031110/\n\n    \"\"\"\n    split = image.find(b\"IDAT\") - 4\n    retval = image[:split] + pack_to_itxt(data) + image[split:]\n    return retval\n\n\ndef tiff_write_meta(image: bytes, data: PNGHeaderDict | dict[str, T.Any] | bytes) -> bytes:\n    \"\"\" Write Faceswap information to a tiff's image_description field.\n\n    Parameters\n    ----------\n    png: bytes\n        The bytes encoded tiff file to write header data to\n    data: dict or bytes\n        The data to write to the image-description field. If provided as a dict, then it should be\n        a json serializable object, otherwise it should be data encoded as ascii bytes\n\n    Notes\n    -----\n    This handles a very specific task of adding, and populating, an ImageDescription field in a\n    Tiff file generated by OpenCV. For any other usecases it will likely fail\n    \"\"\"\n    if not isinstance(data, bytes):\n        data = json.dumps(data, ensure_ascii=True).encode(\"ascii\")\n\n    assert image[:2] == b\"II\", \"Not a supported TIFF file\"\n    assert struct.unpack(\"<H\", image[2:4])[0] == 42, \"Only version 42 Tiff files are supported\"\n    ptr = struct.unpack(\"<I\", image[4:8])[0]\n    rendered = image[:ptr]  # Pack up to IFD\n\n    num_tags = struct.unpack(\"<H\", image[ptr: ptr + 2])[0]\n    ptr += 2\n    rendered += struct.pack(\"<H\", num_tags + 1)  # Pack new IFD field count\n    remainder = image[ptr + num_tags * 12:]  # Hold the data from after the IFD\n    assert struct.unpack(\"<I\", remainder[:4])[0] == 0, \"Multi-page TIFF files not supported\"\n\n    dtypes = {2: \"1s\", 3: \"1H\", 4: \"1I\", 7: '1B'}\n\n    ifd = b\"\"\n    insert_idx = -1\n    for i in range(num_tags):\n        tag = image[ptr + i * 12:ptr + (1 + i) * 12]\n\n        tag_id = struct.unpack(\"<H\", tag[0:2])[0]\n        assert tag_id != 270, \"Not a supported TIFF file\"\n\n        tag_count = struct.unpack(\"<I\", tag[4:8])[0]\n        tag_type = dtypes[struct.unpack(\"<H\", tag[2:4])[0]]\n        size = tag_count * struct.calcsize(tag_type)\n\n        if insert_idx < 0 and tag_id > 270:\n            insert_idx = i  # Log insert location of image description\n\n        if size <= 4:  # value in offset column\n            ifd += tag\n            continue\n\n        ifd += tag[:8]\n        tag_offset = struct.unpack(\"<I\", tag[8:12])[0]\n        new_offset = struct.pack(\"<I\", tag_offset + 12)  # Increment by length of new ifd entry\n        ifd += new_offset\n\n    end = len(rendered) + len(ifd) + 12 + len(remainder)\n    desc = struct.pack(\"HH\", 270, 2)\n    desc += struct.pack(\"II\", len(data), end)\n    # TODO confirm no extra pages in end of IFD\n\n    rendered += ifd[:insert_idx * 12] + desc + ifd[insert_idx * 12:] + remainder + data\n    return rendered\n\n\ndef tiff_read_meta(image: bytes) -> dict[str, T.Any]:\n    \"\"\" Read information stored in a Tiff's Image Description field \"\"\"\n    assert image[:2] == b\"II\", \"Not a supported TIFF file\"\n    assert struct.unpack(\"<H\", image[2:4])[0] == 42, \"Only version 42 Tiff files are supported\"\n    ptr = struct.unpack(\"<I\", image[4:8])[0]\n\n    num_tags = struct.unpack(\"<H\", image[ptr: ptr + 2])[0]\n    ptr += 2\n    ifd_end = ptr + num_tags * 12\n    ifd = image[ptr: ifd_end]\n    next_ifd = struct.unpack(\"<I\", image[ifd_end:ifd_end + 4])[0]\n    assert next_ifd == 0, \"Multi-page TIFF files not supported\"\n\n    dtypes = {2: \"1s\", 3: \"1H\", 4: \"1I\", 7: '1B'}\n    data = None\n    for i in range(num_tags):\n        tag = ifd[i * 12:(1 + i) * 12]\n        tag_id = struct.unpack(\"<H\", tag[0:2])[0]\n        if tag_id != 270:\n            continue\n\n        tag_count = struct.unpack(\"<I\", tag[4:8])[0]\n        tag_type = dtypes[struct.unpack(\"<H\", tag[2:4])[0]]\n        size = tag_count * struct.calcsize(tag_type)\n        tag_offset = struct.unpack(\"<I\", tag[8:12])[0]\n\n        data = image[tag_offset: tag_offset + size]\n\n    assert data is not None, \"No Metadata found in Tiff File\"\n    retval = json.loads(data.decode(\"ascii\"))\n    return retval\n\n\ndef png_read_meta(image):\n    \"\"\" Read the Faceswap information stored in a png's iTXt field.\n\n    Parameters\n    ----------\n    image: bytes\n        The bytes encoded png file to read header data from\n\n    Returns\n    -------\n    dict\n        The Faceswap information stored in the PNG header\n\n    Notes\n    -----\n    This is a very stripped down, non-robust and non-secure header reader to fit a very specific\n    task. OpenCV will not write any iTXt headers to the PNG file, so we make the assumption that\n    the only iTXt header that exists is the one that Faceswap created for storing alignments.\n    \"\"\"\n    retval = None\n    pointer = 0\n    while True:\n        pointer = image.find(b\"iTXt\", pointer) - 4\n        if pointer < 0:\n            logger.trace(\"No metadata in png\")\n            break\n        length = struct.unpack(\">I\", image[pointer:pointer + 4])[0]\n        pointer += 8\n        keyword, value = image[pointer:pointer + length].split(b\"\\0\", 1)\n        if keyword == b\"faceswap\":\n            retval = literal_eval(value[4:].decode(\"utf-8\", errors=\"ignore\"))\n            break\n        logger.trace(\"Skipping iTXt chunk: '%s'\", keyword.decode(\"latin-1\", errors=\"ignore\"))\n        pointer += length + 4\n    return retval\n\n\ndef generate_thumbnail(image, size=96, quality=60):\n    \"\"\" Generate a jpg thumbnail for the given image.\n\n    Parameters\n    ----------\n    image: :class:`numpy.ndarray`\n        Three channel BGR image to convert to a jpg thumbnail\n    size: int\n        The width and height, in pixels, that the thumbnail should be generated at\n    quality: int\n        The jpg quality setting to use\n\n    Returns\n    -------\n    :class:`numpy.ndarray`\n        The given image encoded to a jpg at the given size and quality settings\n    \"\"\"\n    logger.trace(\"Input shape: %s, size: %s, quality: %s\", image.shape, size, quality)\n    orig_size = image.shape[0]\n    if orig_size != size:\n        interp = cv2.INTER_AREA if orig_size > size else cv2.INTER_CUBIC\n        image = cv2.resize(image, (size, size), interpolation=interp)\n    retval = cv2.imencode(\".jpg\", image, [cv2.IMWRITE_JPEG_QUALITY, quality])[1]\n    logger.trace(\"Output shape: %s\", retval.shape)\n    return retval\n\n\ndef batch_convert_color(batch, colorspace):\n    \"\"\" Convert a batch of images from one color space to another.\n\n    Converts a batch of images by reshaping the batch prior to conversion rather than iterating\n    over the images. This leads to a significant speed up in the convert process.\n\n    Parameters\n    ----------\n    batch: numpy.ndarray\n        A batch of images.\n    colorspace: str\n        The OpenCV Color Conversion Code suffix. For example for BGR to LAB this would be\n        ``'BGR2LAB'``.\n        See https://docs.opencv.org/4.1.1/d8/d01/group__imgproc__color__conversions.html for a full\n        list of color codes.\n\n    Returns\n    -------\n    numpy.ndarray\n        The batch converted to the requested color space.\n\n    Example\n    -------\n    >>> images_bgr = numpy.array([image1, image2, image3])\n    >>> images_lab = batch_convert_color(images_bgr, \"BGR2LAB\")\n\n    Notes\n    -----\n    This function is only compatible for color space conversions that have the same image shape\n    for source and destination color spaces.\n\n    If you use :func:`batch_convert_color` with 8-bit images, the conversion will have some\n    information lost. For many cases, this will not be noticeable but it is recommended\n    to use 32-bit images in cases that need the full range of colors or that convert an image\n    before an operation and then convert back.\n    \"\"\"\n    logger.trace(\"Batch converting: (batch shape: %s, colorspace: %s)\", batch.shape, colorspace)\n    original_shape = batch.shape\n    batch = batch.reshape((original_shape[0] * original_shape[1], *original_shape[2:]))\n    batch = cv2.cvtColor(batch, getattr(cv2, \"COLOR_{}\".format(colorspace)))\n    return batch.reshape(original_shape)\n\n\ndef hex_to_rgb(hexcode):\n    \"\"\" Convert a hex number to it's RGB counterpart.\n\n    Parameters\n    ----------\n    hexcode: str\n        The hex code to convert (e.g. `\"#0d25ac\"`)\n\n    Returns\n    -------\n    tuple\n        The hex code as a 3 integer (`R`, `G`, `B`) tuple\n    \"\"\"\n    value = hexcode.lstrip(\"#\")\n    chars = len(value)\n    return tuple(int(value[i:i + chars // 3], 16) for i in range(0, chars, chars // 3))\n\n\ndef rgb_to_hex(rgb):\n    \"\"\" Convert an RGB tuple to it's hex counterpart.\n\n    Parameters\n    ----------\n    rgb: tuple\n        The (`R`, `G`, `B`) integer values to convert (e.g. `(0, 255, 255)`)\n\n    Returns\n    -------\n    str:\n        The 6 digit hex code with leading `#` applied\n    \"\"\"\n    return \"#{:02x}{:02x}{:02x}\".format(*rgb)\n\n\n# ################### #\n# <<< VIDEO UTILS >>> #\n# ################### #\n\ndef count_frames(filename, fast=False):\n    \"\"\" Count the number of frames in a video file\n\n    There is no guaranteed accurate way to get a count of video frames without iterating through\n    a video and decoding every frame.\n\n    :func:`count_frames` can return an accurate count (albeit fairly slowly) or a possibly less\n    accurate count, depending on the :attr:`fast` parameter. A progress bar is displayed.\n\n    Parameters\n    ----------\n    filename: str\n        Full path to the video to return the frame count from.\n    fast: bool, optional\n        Whether to count the frames without decoding them. This is significantly faster but\n        accuracy is not guaranteed. Default: ``False``.\n\n    Returns\n    -------\n    int:\n        The number of frames in the given video file.\n\n    Example\n    -------\n    >>> filename = \"/path/to/video.mp4\"\n    >>> frame_count = count_frames(filename)\n    \"\"\"\n    logger.debug(\"filename: %s, fast: %s\", filename, fast)\n    assert isinstance(filename, str), \"Video path must be a string\"\n\n    cmd = [im_ffm.get_ffmpeg_exe(), \"-i\", filename, \"-map\", \"0:v:0\"]\n    if fast:\n        cmd.extend([\"-c\", \"copy\"])\n    cmd.extend([\"-f\", \"null\", \"-\"])\n\n    logger.debug(\"FFMPEG Command: '%s'\", \" \".join(cmd))\n    process = subprocess.Popen(cmd,\n                               stderr=subprocess.STDOUT,\n                               stdout=subprocess.PIPE,\n                               universal_newlines=True, encoding=\"utf8\")\n    pbar = None\n    duration = None\n    init_tqdm = False\n    update = 0\n    frames = 0\n    while True:\n        output = process.stdout.readline().strip()\n        if output == \"\" and process.poll() is not None:\n            break\n\n        if output.startswith(\"Duration:\"):\n            logger.debug(\"Duration line: %s\", output)\n            idx = output.find(\"Duration:\") + len(\"Duration:\")\n            duration = int(convert_to_secs(*output[idx:].split(\",\", 1)[0].strip().split(\":\")))\n            logger.debug(\"duration: %s\", duration)\n        if output.startswith(\"frame=\"):\n            logger.debug(\"frame line: %s\", output)\n            if not init_tqdm:\n                logger.debug(\"Initializing tqdm\")\n                pbar = tqdm(desc=\"Analyzing Video\", leave=False, total=duration, unit=\"secs\")\n                init_tqdm = True\n            time_idx = output.find(\"time=\") + len(\"time=\")\n            frame_idx = output.find(\"frame=\") + len(\"frame=\")\n            frames = int(output[frame_idx:].strip().split(\" \")[0].strip())\n            vid_time = int(convert_to_secs(*output[time_idx:].split(\" \")[0].strip().split(\":\")))\n            logger.debug(\"frames: %s, vid_time: %s\", frames, vid_time)\n            prev_update = update\n            update = vid_time\n            pbar.update(update - prev_update)\n    if pbar is not None:\n        pbar.close()\n    return_code = process.poll()\n    logger.debug(\"Return code: %s, frames: %s\", return_code, frames)\n    return frames\n\n\nclass ImageIO():\n    \"\"\" Perform disk IO for images or videos in a background thread.\n\n    This is the parent thread for :class:`ImagesLoader` and :class:`ImagesSaver` and should not\n    be called directly.\n\n    Parameters\n    ----------\n    path: str or list\n        The path to load or save images to/from. For loading this can be a folder which contains\n        images, video file or a list of image files. For saving this must be an existing folder.\n    queue_size: int\n        The amount of images to hold in the internal buffer.\n    args: tuple, optional\n        The arguments to be passed to the loader or saver thread. Default: ``None``\n\n    See Also\n    --------\n    lib.image.ImagesLoader : Background Image Loader inheriting from this class.\n    lib.image.ImagesSaver : Background Image Saver inheriting from this class.\n    \"\"\"\n\n    def __init__(self, path, queue_size, args=None):\n        logger.debug(\"Initializing %s: (path: %s, queue_size: %s, args: %s)\",\n                     self.__class__.__name__, path, queue_size, args)\n\n        self._args = tuple() if args is None else args\n\n        self._location = path\n        self._check_location_exists()\n\n        queue_name = queue_manager.add_queue(name=self.__class__.__name__,\n                                             maxsize=queue_size,\n                                             create_new=True)\n        self._queue = queue_manager.get_queue(queue_name)\n        self._thread = None\n\n    @property\n    def location(self):\n        \"\"\" str: The folder or video that was passed in as the :attr:`path` parameter. \"\"\"\n        return self._location\n\n    def _check_location_exists(self):\n        \"\"\" Check whether the input location exists.\n\n        Raises\n        ------\n        FaceswapError\n            If the given location does not exist\n        \"\"\"\n        if isinstance(self.location, str) and not os.path.exists(self.location):\n            raise FaceswapError(\"The location '{}' does not exist\".format(self.location))\n        if isinstance(self.location, (list, tuple)) and not all(os.path.exists(location)\n                                                                for location in self.location):\n            raise FaceswapError(\"Not all locations in the input list exist\")\n\n    def _set_thread(self):\n        \"\"\" Set the background thread for the load and save iterators and launch it. \"\"\"\n        logger.trace(\"Setting thread\")  # type:ignore[attr-defined]\n        if self._thread is not None and self._thread.is_alive():\n            logger.trace(\"Thread pre-exists and is alive: %s\",  # type:ignore[attr-defined]\n                         self._thread)\n            return\n        self._thread = MultiThread(self._process,\n                                   self._queue,\n                                   name=self.__class__.__name__,\n                                   thread_count=1)\n        logger.debug(\"Set thread: %s\", self._thread)\n        self._thread.start()\n\n    def _process(self, queue):\n        \"\"\" Image IO process to be run in a thread. Override for loader/saver process.\n\n        Parameters\n        ----------\n        queue: queue.Queue()\n            The ImageIO Queue\n        \"\"\"\n        raise NotImplementedError\n\n    def close(self):\n        \"\"\" Closes down and joins the internal threads \"\"\"\n        logger.debug(\"Received Close\")\n        if self._thread is not None:\n            self._thread.join()\n        del self._thread\n        self._thread = None\n        logger.debug(\"Closed\")\n\n\nclass ImagesLoader(ImageIO):\n    \"\"\" Perform image loading from a folder of images or a video.\n\n    Images will be loaded and returned in the order that they appear in the folder, or in the video\n    to ensure deterministic ordering. Loading occurs in a background thread, caching 8 images at a\n    time so that other processes do not need to wait on disk reads.\n\n    See also :class:`ImageIO` for additional attributes.\n\n    Parameters\n    ----------\n    path: str or list\n        The path to load images from. This can be a folder which contains images a video file or a\n        list of image files.\n    queue_size: int, optional\n        The amount of images to hold in the internal buffer. Default: 8.\n    fast_count: bool, optional\n        When loading from video, the video needs to be parsed frame by frame to get an accurate\n        count. This can be done quite quickly without guaranteed accuracy, or slower with\n        guaranteed accuracy. Set to ``True`` to count quickly, or ``False`` to count slower\n        but accurately. Default: ``True``.\n    skip_list: list, optional\n        Optional list of frame/image indices to not load. Any indices provided here will be skipped\n        when executing the :func:`load` function from the given location. Default: ``None``\n    count: int, optional\n        If the number of images that the loader will encounter is already known, it can be passed\n        in here to skip the image counting step, which can save time at launch. Set to ``None`` if\n        the count is not already known. Default: ``None``\n\n    Examples\n    --------\n    Loading from a video file:\n\n    >>> loader = ImagesLoader('/path/to/video.mp4')\n    >>> for filename, image in loader.load():\n    >>>     <do processing>\n    \"\"\"\n\n    def __init__(self,\n                 path,\n                 queue_size=8,\n                 fast_count=True,\n                 skip_list=None,\n                 count=None):\n        logger.debug(\"Initializing %s: (path: %s, queue_size: %s, fast_count: %s, skip_list: %s, \"\n                     \"count: %s)\", self.__class__.__name__, path, queue_size, fast_count,\n                     skip_list, count)\n\n        super().__init__(path, queue_size=queue_size)\n        self._skip_list = set() if skip_list is None else set(skip_list)\n        self._is_video = self._check_for_video()\n        self._fps = self._get_fps()\n\n        self._count = None\n        self._file_list = None\n        self._get_count_and_filelist(fast_count, count)\n\n    @property\n    def count(self):\n        \"\"\" int: The number of images or video frames in the source location. This count includes\n        any files that will ultimately be skipped if a :attr:`skip_list` has been provided. See\n        also: :attr:`process_count`\"\"\"\n        return self._count\n\n    @property\n    def process_count(self):\n        \"\"\" int: The number of images or video frames to be processed (IE the total count less\n        items that are to be skipped from the :attr:`skip_list`)\"\"\"\n        return self._count - len(self._skip_list)\n\n    @property\n    def is_video(self):\n        \"\"\" bool: ``True`` if the input is a video, ``False`` if it is not \"\"\"\n        return self._is_video\n\n    @property\n    def fps(self):\n        \"\"\" float: For an input folder of images, this will always return 25fps. If the input is a\n        video, then the fps of the video will be returned. \"\"\"\n        return self._fps\n\n    @property\n    def file_list(self):\n        \"\"\" list: A full list of files in the source location. This includes any files that will\n        ultimately be skipped if a :attr:`skip_list` has been provided. If the input is a video\n        then this is a list of dummy filenames as corresponding to an alignments file \"\"\"\n        return self._file_list\n\n    def add_skip_list(self, skip_list):\n        \"\"\" Add a skip list to this :class:`ImagesLoader`\n\n        Parameters\n        ----------\n        skip_list: list\n            A list of indices corresponding to the frame indices that should be skipped by the\n            :func:`load` function.\n        \"\"\"\n        logger.debug(skip_list)\n        self._skip_list = set(skip_list)\n\n    def _check_for_video(self):\n        \"\"\" Check whether the input is a video\n\n        Returns\n        -------\n        bool: 'True' if input is a video 'False' if it is a folder.\n\n        Raises\n        ------\n        FaceswapError\n            If the given location is a file and does not have a valid video extension.\n\n        \"\"\"\n        if not isinstance(self.location, str) or os.path.isdir(self.location):\n            retval = False\n        elif os.path.splitext(self.location)[1].lower() in VIDEO_EXTENSIONS:\n            retval = True\n        else:\n            raise FaceswapError(\"The input file '{}' is not a valid video\".format(self.location))\n        logger.debug(\"Input '%s' is_video: %s\", self.location, retval)\n        return retval\n\n    def _get_fps(self):\n        \"\"\" Get the Frames per Second.\n\n        If the input is a folder of images than 25.0 will be returned, as it is not possible to\n        calculate the fps just from frames alone. For video files the correct FPS will be returned.\n\n        Returns\n        -------\n        float: The Frames per Second of the input sources\n        \"\"\"\n        if self._is_video:\n            reader = imageio.get_reader(self.location, \"ffmpeg\")\n            retval = reader.get_meta_data()[\"fps\"]\n            reader.close()\n        else:\n            retval = 25.0\n        logger.debug(retval)\n        return retval\n\n    def _get_count_and_filelist(self, fast_count, count):\n        \"\"\" Set the count of images to be processed and set the file list\n\n            If the input is a video, a dummy file list is created for checking against an\n            alignments file, otherwise it will be a list of full filenames.\n\n        Parameters\n        ----------\n        fast_count: bool\n            When loading from video, the video needs to be parsed frame by frame to get an accurate\n            count. This can be done quite quickly without guaranteed accuracy, or slower with\n            guaranteed accuracy. Set to ``True`` to count quickly, or ``False`` to count slower\n            but accurately.\n        count: int\n            The number of images that the loader will encounter if already known, otherwise\n            ``None``\n        \"\"\"\n        if self._is_video:\n            self._count = int(count_frames(self.location,\n                                           fast=fast_count)) if count is None else count\n            self._file_list = [self._dummy_video_framename(i) for i in range(self.count)]\n        else:\n            if isinstance(self.location, (list, tuple)):\n                self._file_list = self.location\n            else:\n                self._file_list = get_image_paths(self.location)\n            self._count = len(self.file_list) if count is None else count\n\n        logger.debug(\"count: %s\", self.count)\n        logger.trace(\"filelist: %s\", self.file_list)\n\n    def _process(self, queue):\n        \"\"\" The load thread.\n\n        Loads from a folder of images or from a video and puts to a queue\n\n        Parameters\n        ----------\n        queue: queue.Queue()\n            The ImageIO Queue\n        \"\"\"\n        iterator = self._from_video if self._is_video else self._from_folder\n        logger.debug(\"Load iterator: %s\", iterator)\n        for retval in iterator():\n            filename, image = retval[:2]\n            if image is None or (not image.any() and image.ndim not in (2, 3)):\n                # All black frames will return not numpy.any() so check dims too\n                logger.warning(\"Unable to open image. Skipping: '%s'\", filename)\n                continue\n            logger.trace(\"Putting to queue: %s\", [v.shape if isinstance(v, np.ndarray) else v\n                                                  for v in retval])\n            queue.put(retval)\n        logger.trace(\"Putting EOF\")\n        queue.put(\"EOF\")\n\n    def _from_video(self):\n        \"\"\" Generator for loading frames from a video\n\n        Yields\n        ------\n        filename: str\n            The dummy filename of the loaded video frame.\n        image: numpy.ndarray\n            The loaded video frame.\n        \"\"\"\n        logger.debug(\"Loading frames from video: '%s'\", self.location)\n        reader = imageio.get_reader(self.location, \"ffmpeg\")\n        for idx, frame in enumerate(reader):\n            if idx in self._skip_list:\n                logger.trace(\"Skipping frame %s due to skip list\", idx)\n                continue\n            # Convert to BGR for cv2 compatibility\n            frame = frame[:, :, ::-1]\n            filename = self._dummy_video_framename(idx)\n            logger.trace(\"Loading video frame: '%s'\", filename)\n            yield filename, frame\n        reader.close()\n\n    def _dummy_video_framename(self, index):\n        \"\"\" Return a dummy filename for video files. The file name is made up of:\n        <video_filename>_<frame_number>.<video_extension>\n\n        Parameters\n        ----------\n        index: int\n            The index number for the frame in the video file\n\n        Notes\n        -----\n        Indexes start at 0, frame numbers start at 1, so index is incremented by 1\n        when creating the filename\n\n        Returns\n        -------\n        str: A dummied filename for a video frame \"\"\"\n        vidname, ext = os.path.splitext(os.path.basename(self.location))\n        return f\"{vidname}_{index + 1:06d}{ext}\"\n\n    def _from_folder(self):\n        \"\"\" Generator for loading images from a folder\n\n        Yields\n        ------\n        filename: str\n            The filename of the loaded image.\n        image: numpy.ndarray\n            The loaded image.\n        \"\"\"\n        logger.debug(\"Loading frames from folder: '%s'\", self.location)\n        for idx, filename in enumerate(self.file_list):\n            if idx in self._skip_list:\n                logger.trace(\"Skipping frame %s due to skip list\")\n                continue\n            image_read = read_image(filename, raise_error=False)\n            retval = filename, image_read\n            if retval[1] is None:\n                logger.warning(\"Frame not loaded: '%s'\", filename)\n                continue\n            yield retval\n\n    def load(self):\n        \"\"\" Generator for loading images from the given :attr:`location`\n\n        If :class:`FacesLoader` is in use then the Faceswap metadata of the image stored in the\n        image exif file is added as the final item in the output `tuple`.\n\n        Yields\n        ------\n        filename: str\n            The filename of the loaded image.\n        image: numpy.ndarray\n            The loaded image.\n        metadata: dict, (:class:`FacesLoader` only)\n            The Faceswap metadata associated with the loaded image.\n        \"\"\"\n        logger.debug(\"Initializing Load Generator\")\n        self._set_thread()\n        while True:\n            self._thread.check_and_raise_error()\n            try:\n                retval = self._queue.get(True, 1)\n            except QueueEmpty:\n                continue\n            if retval == \"EOF\":\n                logger.trace(\"Got EOF\")\n                break\n            logger.trace(\"Yielding: %s\", [v.shape if isinstance(v, np.ndarray) else v\n                                          for v in retval])\n            yield retval\n        logger.debug(\"Closing Load Generator\")\n        self.close()\n\n\nclass FacesLoader(ImagesLoader):\n    \"\"\" Loads faces from a faces folder along with the face's Faceswap metadata.\n\n    Examples\n    --------\n    Loading faces with their Faceswap metadata:\n\n    >>> loader = FacesLoader('/path/to/faces/folder')\n    >>> for filename, face, metadata in loader.load():\n    >>>     <do processing>\n    \"\"\"\n    def __init__(self, path, skip_list=None, count=None):\n        logger.debug(\"Initializing %s: (path: %s, count: %s)\", self.__class__.__name__,\n                     path, count)\n        super().__init__(path, queue_size=8, skip_list=skip_list, count=count)\n\n    def _get_count_and_filelist(self, fast_count, count):\n        \"\"\" Override default implementation to only return png files from the source folder\n\n        Parameters\n        ----------\n        fast_count: bool\n            Not used for faces loader\n        count: int\n            The number of images that the loader will encounter if already known, otherwise\n            ``None``\n        \"\"\"\n        if isinstance(self.location, (list, tuple)):\n            file_list = self.location\n        else:\n            file_list = get_image_paths(self.location)\n\n        self._file_list = [fname for fname in file_list\n                           if os.path.splitext(fname)[-1].lower() == \".png\"]\n        self._count = len(self.file_list) if count is None else count\n\n        logger.debug(\"count: %s\", self.count)\n        logger.trace(\"filelist: %s\", self.file_list)\n\n    def _from_folder(self):\n        \"\"\" Generator for loading images from a folder\n        Faces will only ever be loaded from a folder, so this is the only function requiring\n        an override\n\n        Yields\n        ------\n        filename: str\n            The filename of the loaded image.\n        image: numpy.ndarray\n            The loaded image.\n        metadata: dict\n            The Faceswap metadata associated with the loaded image.\n        \"\"\"\n        logger.debug(\"Loading images from folder: '%s'\", self.location)\n        for idx, filename in enumerate(self.file_list):\n            if idx in self._skip_list:\n                logger.trace(\"Skipping face %s due to skip list\")\n                continue\n            image_read = read_image(filename, raise_error=False, with_metadata=True)\n            retval = filename, *image_read\n            if retval[1] is None:\n                logger.warning(\"Face not loaded: '%s'\", filename)\n                continue\n            yield retval\n\n\nclass SingleFrameLoader(ImagesLoader):\n    \"\"\" Allows direct access to a frame by filename or frame index.\n\n    As we are interested in instant access to frames, there is no requirement to process in a\n    background thread, as either way we need to wait for the frame to load.\n\n    Parameters\n    ----------\n    video_meta_data: dict, optional\n        Existing video meta information containing the pts_time and iskey flags for the given\n        video. Used in conjunction with single_frame_reader for faster seeks. Providing this means\n        that the video does not need to be scanned again. Set to ``None`` if the video is to be\n        scanned. Default: ``None``\n     \"\"\"\n    def __init__(self, path, video_meta_data=None):\n        logger.debug(\"Initializing %s: (path: %s, video_meta_data: %s)\",\n                     self.__class__.__name__, path, video_meta_data)\n        self._video_meta_data = dict() if video_meta_data is None else video_meta_data\n        self._reader = None\n        super().__init__(path, queue_size=1, fast_count=False)\n\n    @property\n    def video_meta_data(self):\n        \"\"\" dict: For videos contains the keys `frame_pts` holding a list of time stamps for each\n        frame and `keyframes` holding the frame index of each key frame.\n\n        Notes\n        -----\n        Only populated if the input is a video and single frame reader is being used, otherwise\n        returns ``None``.\n        \"\"\"\n        return self._video_meta_data\n\n    def _get_count_and_filelist(self, fast_count, count):\n        if self._is_video:\n            self._reader = imageio.get_reader(self.location, \"ffmpeg\")\n            self._reader.use_patch = True\n            count, video_meta_data = self._reader.get_frame_info(\n                frame_pts=self._video_meta_data.get(\"pts_time\", None),\n                keyframes=self._video_meta_data.get(\"keyframes\", None))\n            self._video_meta_data = video_meta_data\n        super()._get_count_and_filelist(fast_count, count)\n\n    def image_from_index(self, index):\n        \"\"\" Return a single image from :attr:`file_list` for the given index.\n\n        Parameters\n        ----------\n        index: int\n            The index number (frame number) of the frame to retrieve. NB: The first frame is\n            index `0`\n\n        Returns\n        -------\n        filename: str\n            The filename of the returned image\n        image: :class:`numpy.ndarray`\n            The image for the given index\n\n        Notes\n        -----\n        Retrieving frames from video files can be slow as the whole video file needs to be\n        iterated to retrieve the requested frame. If a frame has already been retrieved, then\n        retrieving frames of a higher index will be quicker than retrieving frames of a lower\n        index, as iteration needs to start from the beginning again when navigating backwards.\n\n        We do not use a background thread for this task, as it is assumed that requesting an image\n        by index will be done when required.\n        \"\"\"\n        if self.is_video:\n            image = self._reader.get_data(index)[..., ::-1]\n            filename = self._dummy_video_framename(index)\n        else:\n            filename = self.file_list[index]\n            image = read_image(filename, raise_error=True)\n            filename = os.path.basename(filename)\n        logger.trace(\"index: %s, filename: %s image shape: %s\", index, filename, image.shape)\n        return filename, image\n\n\nclass ImagesSaver(ImageIO):\n    \"\"\" Perform image saving to a destination folder.\n\n    Images are saved in a background ThreadPoolExecutor to allow for concurrent saving.\n    See also :class:`ImageIO` for additional attributes.\n\n    Parameters\n    ----------\n    path: str\n        The folder to save images to. This must be an existing folder.\n    queue_size: int, optional\n        The amount of images to hold in the internal buffer. Default: 8.\n    as_bytes: bool, optional\n        ``True`` if the image is already encoded to bytes, ``False`` if the image is a\n        :class:`numpy.ndarray`. Default: ``False``.\n\n    Examples\n    --------\n\n    >>> saver = ImagesSaver('/path/to/save/folder')\n    >>> for filename, image in <image_iterator>:\n    >>>     saver.save(filename, image)\n    >>> saver.close()\n    \"\"\"\n\n    def __init__(self, path, queue_size=8, as_bytes=False):\n        logger.debug(\"Initializing %s: (path: %s, queue_size: %s, as_bytes: %s)\",\n                     self.__class__.__name__, path, queue_size, as_bytes)\n\n        super().__init__(path, queue_size=queue_size)\n        self._as_bytes = as_bytes\n\n    def _check_location_exists(self):\n        \"\"\" Check whether the output location exists and is a folder\n\n        Raises\n        ------\n        FaceswapError\n            If the given location does not exist or the location is not a folder\n        \"\"\"\n        if not isinstance(self.location, str):\n            raise FaceswapError(\"The output location must be a string not a \"\n                                \"{}\".format(type(self.location)))\n        super()._check_location_exists()\n        if not os.path.isdir(self.location):\n            raise FaceswapError(\"The output location '{}' is not a folder\".format(self.location))\n\n    def _process(self, queue):\n        \"\"\" Saves images from the save queue to the given :attr:`location` inside a thread.\n\n        Parameters\n        ----------\n        queue: queue.Queue()\n            The ImageIO Queue\n        \"\"\"\n        executor = futures.ThreadPoolExecutor(thread_name_prefix=self.__class__.__name__)\n        while True:\n            item = queue.get()\n            if item == \"EOF\":\n                logger.debug(\"EOF received\")\n                break\n            logger.trace(\"Submitting: '%s'\", item[0])\n            executor.submit(self._save, *item)\n        executor.shutdown()\n\n    def _save(self,\n              filename: str,\n              image: bytes | np.ndarray,\n              sub_folder: str | None) -> None:\n        \"\"\" Save a single image inside a ThreadPoolExecutor\n\n        Parameters\n        ----------\n        filename: str\n            The filename of the image to be saved. NB: Any folders passed in with the filename\n            will be stripped and replaced with :attr:`location`.\n        image: bytes or :class:`numpy.ndarray`\n            The encoded image or numpy array to be saved\n        subfolder: str or ``None``\n            If the file should be saved in a subfolder in the output location, the subfolder should\n            be provided here. ``None`` for no subfolder.\n        \"\"\"\n        location = os.path.join(self.location, sub_folder) if sub_folder else self._location\n        if sub_folder and not os.path.exists(location):\n            os.makedirs(location)\n\n        filename = os.path.join(location, os.path.basename(filename))\n        try:\n            if self._as_bytes:\n                assert isinstance(image, bytes)\n                with open(filename, \"wb\") as out_file:\n                    out_file.write(image)\n            else:\n                assert isinstance(image, np.ndarray)\n                cv2.imwrite(filename, image)\n            logger.trace(\"Saved image: '%s'\", filename)  # type:ignore\n        except Exception as err:  # pylint:disable=broad-except\n            logger.error(\"Failed to save image '%s'. Original Error: %s\", filename, str(err))\n        del image\n        del filename\n\n    def save(self,\n             filename: str,\n             image: bytes | np.ndarray,\n             sub_folder: str | None = None) -> None:\n        \"\"\" Save the given image in the background thread\n\n        Ensure that :func:`close` is called once all save operations are complete.\n\n        Parameters\n        ----------\n        filename: str\n            The filename of the image to be saved. NB: Any folders passed in with the filename\n            will be stripped and replaced with :attr:`location`.\n        image: bytes\n            The encoded image to be saved\n        subfolder: str, optional\n            If the file should be saved in a subfolder in the output location, the subfolder should\n            be provided here. ``None`` for no subfolder. Default: ``None``\n        \"\"\"\n        self._set_thread()\n        logger.trace(\"Putting to save queue: '%s'\", filename)  # type:ignore\n        self._queue.put((filename, image, sub_folder))\n\n    def close(self):\n        \"\"\" Signal to the Save Threads that they should be closed and cleanly shutdown\n        the saver \"\"\"\n        logger.debug(\"Putting EOF to save queue\")\n        self._queue.put(\"EOF\")\n        super().close()\n", "lib/vgg_face.py": "#!/usr/bin python3\n\"\"\" VGG_Face inference using OpenCV-DNN\nModel from: https://www.robots.ox.ac.uk/~vgg/software/vgg_face/\n\nLicensed under Creative Commons Attribution License.\nhttps://creativecommons.org/licenses/by-nc/4.0/\n\"\"\"\n\nimport logging\n\nimport cv2\nimport numpy as np\nfrom fastcluster import linkage\n\nfrom lib.utils import GetModel\n\nlogger = logging.getLogger(__name__)\n\n\nclass VGGFace():\n    \"\"\" VGG Face feature extraction.\n        Input images should be in BGR Order \"\"\"\n\n    def __init__(self, backend=\"CPU\"):\n        logger.debug(\"Initializing %s: (backend: %s)\", self.__class__.__name__, backend)\n        git_model_id = 7\n        model_filename = [\"vgg_face_v1.caffemodel\", \"vgg_face_v1.prototxt\"]\n        self.input_size = 224\n        # Average image provided in http://www.robots.ox.ac.uk/~vgg/software/vgg_face/\n        self.average_img = [129.1863, 104.7624, 93.5940]\n\n        self.model = self.get_model(git_model_id, model_filename, backend)\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    # <<< GET MODEL >>> #\n    def get_model(self, git_model_id, model_filename, backend):\n        \"\"\" Check if model is available, if not, download and unzip it \"\"\"\n        model = GetModel(model_filename, git_model_id).model_path\n        model = cv2.dnn.readNetFromCaffe(model[1], model[0])\n        model.setPreferableTarget(self.get_backend(backend))\n        return model\n\n    @staticmethod\n    def get_backend(backend):\n        \"\"\" Return the cv2 DNN backend \"\"\"\n        if backend == \"OPENCL\":\n            logger.info(\"Using OpenCL backend. If the process runs, you can safely ignore any of \"\n                        \"the failure messages.\")\n        retval = getattr(cv2.dnn, f\"DNN_TARGET_{backend}\")\n        return retval\n\n    def predict(self, face):\n        \"\"\" Return encodings for given image from vgg_face \"\"\"\n        if face.shape[0] != self.input_size:\n            face = self.resize_face(face)\n        blob = cv2.dnn.blobFromImage(face[..., :3],\n                                     1.0,\n                                     (self.input_size, self.input_size),\n                                     self.average_img,\n                                     False,\n                                     False)\n        self.model.setInput(blob)\n        preds = self.model.forward(\"fc7\")[0, :]\n        return preds\n\n    def resize_face(self, face):\n        \"\"\" Resize incoming face to model_input_size \"\"\"\n        sizes = (self.input_size, self.input_size)\n        interpolation = cv2.INTER_CUBIC if face.shape[0] < self.input_size else cv2.INTER_AREA\n        face = cv2.resize(face, dsize=sizes, interpolation=interpolation)\n        return face\n\n    @staticmethod\n    def find_cosine_similiarity(source_face, test_face):\n        \"\"\" Find the cosine similarity between a source face and a test face \"\"\"\n        var_a = np.matmul(np.transpose(source_face), test_face)\n        var_b = np.sum(np.multiply(source_face, source_face))\n        var_c = np.sum(np.multiply(test_face, test_face))\n        return 1 - (var_a / (np.sqrt(var_b) * np.sqrt(var_c)))\n\n    def sorted_similarity(self, predictions, method=\"ward\"):\n        \"\"\" Sort a matrix of predictions by similarity Adapted from:\n            https://gmarti.gitlab.io/ml/2017/09/07/how-to-sort-distance-matrix.html\n        input:\n            - predictions is a stacked matrix of vgg_face predictions shape: (x, 4096)\n            - method = [\"ward\",\"single\",\"average\",\"complete\"]\n        output:\n            - result_order is a list of indices with the order implied by the hierarhical tree\n\n        sorted_similarity transforms a distance matrix into a sorted distance matrix according to\n        the order implied by the hierarchical tree (dendrogram)\n        \"\"\"\n        logger.info(\"Sorting face distances. Depending on your dataset this may take some time...\")\n        num_predictions = predictions.shape[0]\n        result_linkage = linkage(predictions, method=method, preserve_input=False)\n        result_order = self.seriation(result_linkage,\n                                      num_predictions,\n                                      num_predictions + num_predictions - 2)\n\n        return result_order\n\n    def seriation(self, tree, points, current_index):\n        \"\"\" Seriation method for sorted similarity\n            input:\n                - tree is a hierarchical tree (dendrogram)\n                - points is the number of points given to the clustering process\n                - current_index is the position in the tree for the recursive traversal\n            output:\n                - order implied by the hierarchical tree\n\n            seriation computes the order implied by a hierarchical tree (dendrogram)\n        \"\"\"\n        if current_index < points:\n            return [current_index]\n        left = int(tree[current_index-points, 0])\n        right = int(tree[current_index-points, 1])\n        return self.seriation(tree, points, left) + self.seriation(tree, points, right)\n", "lib/convert.py": "#!/usr/bin/env python3\n\"\"\" Converter for Faceswap \"\"\"\nfrom __future__ import annotations\nimport logging\nimport typing as T\nfrom dataclasses import dataclass\n\nimport cv2\nimport numpy as np\n\nfrom plugins.plugin_loader import PluginLoader\n\nif T.TYPE_CHECKING:\n    from argparse import Namespace\n    from collections.abc import Callable\n    from lib.align.aligned_face import AlignedFace, CenteringType\n    from lib.align.detected_face import DetectedFace\n    from lib.config import FaceswapConfig\n    from lib.queue_manager import EventQueue\n    from scripts.convert import ConvertItem\n    from plugins.convert.color._base import Adjustment as ColorAdjust\n    from plugins.convert.color.seamless_clone import Color as SeamlessAdjust\n    from plugins.convert.mask.mask_blend import Mask as MaskAdjust\n    from plugins.convert.scaling._base import Adjustment as ScalingAdjust\n\nlogger = logging.getLogger(__name__)\n\n\n@dataclass\nclass Adjustments:\n    \"\"\" Dataclass to hold the optional processing plugins\n\n    Parameters\n    ----------\n    color: :class:`~plugins.color._base.Adjustment`, Optional\n        The selected color processing plugin. Default: `None`\n    mask: :class:`~plugins.mask_blend.Mask`, Optional\n        The selected mask processing plugin. Default: `None`\n    seamless: :class:`~plugins.color.seamless_clone.Color`, Optional\n        The selected mask processing plugin. Default: `None`\n    sharpening: :class:`~plugins.scaling._base.Adjustment`, Optional\n        The selected mask processing plugin. Default: `None`\n    \"\"\"\n    color: ColorAdjust | None = None\n    mask: MaskAdjust | None = None\n    seamless: SeamlessAdjust | None = None\n    sharpening: ScalingAdjust | None = None\n\n\nclass Converter():\n    \"\"\" The converter is responsible for swapping the original face(s) in a frame with the output\n    of a trained Faceswap model.\n\n    Parameters\n    ----------\n    output_size: int\n        The size of the face, in pixels, that is output from the Faceswap model\n    coverage_ratio: float\n        The ratio of the training image that was used for training the Faceswap model\n    centering: str\n        The extracted face centering that the model was trained on (`\"face\"` or \"`legacy`\")\n    draw_transparent: bool\n        Whether the final output should be drawn onto a transparent layer rather than the original\n        frame. Only available with certain writer plugins.\n    pre_encode: python function\n        Some writer plugins support the pre-encoding of images prior to saving out. As patching is\n        done in multiple threads, but writing is done in a single thread, it can speed up the\n        process to do any pre-encoding as part of the converter process.\n    arguments: :class:`argparse.Namespace`\n        The arguments that were passed to the convert process as generated from Faceswap's command\n        line arguments\n    configfile: str, optional\n        Optional location of custom configuration ``ini`` file. If ``None`` then use the default\n        config location. Default: ``None``\n    \"\"\"\n    def __init__(self,\n                 output_size: int,\n                 coverage_ratio: float,\n                 centering: CenteringType,\n                 draw_transparent: bool,\n                 pre_encode: Callable | None,\n                 arguments: Namespace,\n                 configfile: str | None = None) -> None:\n        logger.debug(\"Initializing %s: (output_size: %s,  coverage_ratio: %s, centering: %s, \"\n                     \"draw_transparent: %s, pre_encode: %s, arguments: %s, configfile: %s)\",\n                     self.__class__.__name__, output_size, coverage_ratio, centering,\n                     draw_transparent, pre_encode, arguments, configfile)\n        self._output_size = output_size\n        self._coverage_ratio = coverage_ratio\n        self._centering = centering\n        self._draw_transparent = draw_transparent\n        self._writer_pre_encode = pre_encode\n        self._args = arguments\n        self._configfile = configfile\n\n        self._scale = arguments.output_scale / 100\n        self._face_scale = 1.0 - arguments.face_scale / 100.\n        self._adjustments = Adjustments()\n        self._full_frame_output: bool = arguments.writer != \"patch\"\n\n        self._load_plugins()\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    @property\n    def cli_arguments(self) -> Namespace:\n        \"\"\":class:`argparse.Namespace`: The command line arguments passed to the convert\n        process \"\"\"\n        return self._args\n\n    def reinitialize(self, config: FaceswapConfig) -> None:\n        \"\"\" Reinitialize this :class:`Converter`.\n\n        Called as part of the :mod:`~tools.preview` tool. Resets all adjustments then loads the\n        plugins as specified in the given config.\n\n        Parameters\n        ----------\n        config: :class:`lib.config.FaceswapConfig`\n            Pre-loaded :class:`lib.config.FaceswapConfig`. used over any configuration on disk.\n        \"\"\"\n        logger.debug(\"Reinitializing converter\")\n        self._face_scale = 1.0 - self._args.face_scale / 100.\n        self._adjustments = Adjustments()\n        self._load_plugins(config=config, disable_logging=True)\n        logger.debug(\"Reinitialized converter\")\n\n    def _load_plugins(self,\n                      config: FaceswapConfig | None = None,\n                      disable_logging: bool = False) -> None:\n        \"\"\" Load the requested adjustment plugins.\n\n        Loads the :mod:`plugins.converter` plugins that have been requested for this conversion\n        session.\n\n        Parameters\n        ----------\n        config: :class:`lib.config.FaceswapConfig`, optional\n            Optional pre-loaded :class:`lib.config.FaceswapConfig`. If passed, then this will be\n            used over any configuration on disk. If ``None`` then it is ignored. Default: ``None``\n        disable_logging: bool, optional\n            Plugin loader outputs logging info every time a plugin is loaded. Set to ``True`` to\n            suppress these messages otherwise ``False``. Default: ``False``\n        \"\"\"\n        logger.debug(\"Loading plugins. config: %s\", config)\n        self._adjustments.mask = PluginLoader.get_converter(\"mask\",\n                                                            \"mask_blend\",\n                                                            disable_logging=disable_logging)(\n                                                                self._args.mask_type,\n                                                                self._output_size,\n                                                                self._coverage_ratio,\n                                                                configfile=self._configfile,\n                                                                config=config)\n\n        if self._args.color_adjustment != \"none\" and self._args.color_adjustment is not None:\n            self._adjustments.color = PluginLoader.get_converter(\"color\",\n                                                                 self._args.color_adjustment,\n                                                                 disable_logging=disable_logging)(\n                                                                    configfile=self._configfile,\n                                                                    config=config)\n\n        sharpening = PluginLoader.get_converter(\"scaling\",\n                                                \"sharpen\",\n                                                disable_logging=disable_logging)(\n                                                    configfile=self._configfile,\n                                                    config=config)\n        if sharpening.config.get(\"method\") is not None:\n            self._adjustments.sharpening = sharpening\n        logger.debug(\"Loaded plugins: %s\", self._adjustments)\n\n    def process(self, in_queue: EventQueue, out_queue: EventQueue):\n        \"\"\" Main convert process.\n\n        Takes items from the in queue, runs the relevant adjustments, patches faces to final frame\n        and outputs patched frame to the out queue.\n\n        Parameters\n        ----------\n        in_queue: :class:`~lib.queue_manager.EventQueue`\n            The output from :class:`scripts.convert.Predictor`. Contains detected faces from the\n            Faceswap model as well as the frame to be patched.\n        out_queue: :class:`~lib.queue_manager.EventQueue`\n            The queue to place patched frames into for writing by one of Faceswap's\n            :mod:`plugins.convert.writer` plugins.\n        \"\"\"\n        logger.debug(\"Starting convert process. (in_queue: %s, out_queue: %s)\",\n                     in_queue, out_queue)\n        logged = False\n        while True:\n            inbound: T.Literal[\"EOF\"] | ConvertItem | list[ConvertItem] = in_queue.get()\n            if inbound == \"EOF\":\n                logger.debug(\"EOF Received\")\n                logger.debug(\"Patch queue finished\")\n                # Signal EOF to other processes in pool\n                logger.debug(\"Putting EOF back to in_queue\")\n                in_queue.put(inbound)\n                break\n\n            items = inbound if isinstance(inbound, list) else [inbound]\n            for item in items:\n                logger.trace(\"Patch queue got: '%s'\",  # type: ignore[attr-defined]\n                             item.inbound.filename)\n                try:\n                    image = self._patch_image(item)\n                except Exception as err:  # pylint:disable=broad-except\n                    # Log error and output original frame\n                    logger.error(\"Failed to convert image: '%s'. Reason: %s\",\n                                 item.inbound.filename, str(err))\n                    image = item.inbound.image\n\n                    lvl = logger.trace if logged else logger.warning  # type: ignore[attr-defined]\n                    lvl(\"Convert error traceback:\", exc_info=True)\n                    logged = True\n                    # UNCOMMENT THIS CODE BLOCK TO PRINT TRACEBACK ERRORS\n                    # import sys; import traceback\n                    # exc_info = sys.exc_info(); traceback.print_exception(*exc_info)\n                logger.trace(\"Out queue put: %s\",  # type: ignore[attr-defined]\n                             item.inbound.filename)\n                out_queue.put((item.inbound.filename, image))\n        logger.debug(\"Completed convert process\")\n\n    def _get_warp_matrix(self, matrix: np.ndarray, size: int) -> np.ndarray:\n        \"\"\" Obtain the final scaled warp transformation matrix based on face scaling from the\n        original transformation matrix\n\n        Parameters\n        ----------\n        matrix: :class:`numpy.ndarray`\n            The transformation for patching the swapped face back onto the output frame\n        size: int\n            The size of the face patch, in pixels\n\n        Returns\n        -------\n        :class:`numpy.ndarray`\n            The final transformation matrix with any scaling applied\n        \"\"\"\n        if self._face_scale == 1.0:\n            mat = matrix\n        else:\n            mat = matrix * self._face_scale\n            patch_center = (size / 2, size / 2)\n            mat[..., 2] += (1 - self._face_scale) * np.array(patch_center)\n\n        return mat\n\n    def _patch_image(self, predicted: ConvertItem) -> np.ndarray | list[bytes]:\n        \"\"\" Patch a swapped face onto a frame.\n\n        Run selected adjustments and swap the faces in a frame.\n\n        Parameters\n        ----------\n        predicted: :class:`~scripts.convert.ConvertItem`\n            The output from :class:`scripts.convert.Predictor`.\n\n        Returns\n        -------\n        :class: `numpy.ndarray` or pre-encoded image output\n            The final frame ready for writing by a :mod:`plugins.convert.writer` plugin.\n            Frame is either an array, or the pre-encoded output from the writer's pre-encode\n            function (if it has one)\n\n        \"\"\"\n        logger.trace(\"Patching image: '%s'\",  # type: ignore[attr-defined]\n                     predicted.inbound.filename)\n        frame_size = (predicted.inbound.image.shape[1], predicted.inbound.image.shape[0])\n        new_image, background = self._get_new_image(predicted, frame_size)\n\n        if self._full_frame_output:\n            patched_face = self._post_warp_adjustments(background, new_image)\n            patched_face = self._scale_image(patched_face)\n            patched_face *= 255.0\n            patched_face = np.rint(patched_face,\n                                   out=np.empty(patched_face.shape, dtype=\"uint8\"),\n                                   casting='unsafe')\n        else:\n            patched_face = new_image\n\n        if self._writer_pre_encode is None:\n            retval: np.ndarray | list[bytes] = patched_face\n        else:\n            kwargs: dict[str, T.Any] = {}\n            if self.cli_arguments.writer == \"patch\":\n                kwargs[\"canvas_size\"] = (background.shape[1], background.shape[0])\n                kwargs[\"matrices\"] = np.array([self._get_warp_matrix(face.adjusted_matrix,\n                                                                     patched_face.shape[1])\n                                               for face in predicted.reference_faces],\n                                              dtype=\"float32\")\n            retval = self._writer_pre_encode(patched_face, **kwargs)\n        logger.trace(\"Patched image: '%s'\",  # type: ignore[attr-defined]\n                     predicted.inbound.filename)\n        return retval\n\n    def _warp_to_frame(self,\n                       reference: AlignedFace,\n                       face: np.ndarray,\n                       frame: np.ndarray,\n                       multiple_faces: bool) -> None:\n        \"\"\" Perform affine transformation to place a face patch onto the given frame.\n\n        Affine is done in place on the `frame` array, so this function does not return a value\n\n        Parameters\n        ----------\n        reference: :class:`lib.align.AlignedFace`\n            The object holding the original aligned face\n        face: :class:`numpy.ndarray`\n            The swapped face patch\n        frame: :class:`numpy.ndarray`\n            The frame to affine the face onto\n        multiple_faces: bool\n            Controls the border mode to use. Uses BORDER_CONSTANT if there is only 1 face in\n            the image, otherwise uses the inferior BORDER_TRANSPARENT\n        \"\"\"\n        # Warp face with the mask\n        mat = self._get_warp_matrix(reference.adjusted_matrix, face.shape[0])\n        border = cv2.BORDER_TRANSPARENT if multiple_faces else cv2.BORDER_CONSTANT\n        cv2.warpAffine(face,\n                       mat,\n                       (frame.shape[1], frame.shape[0]),\n                       frame,\n                       flags=cv2.WARP_INVERSE_MAP | reference.interpolators[1],\n                       borderMode=border)\n\n    def _get_new_image(self,\n                       predicted: ConvertItem,\n                       frame_size: tuple[int, int]) -> tuple[np.ndarray, np.ndarray]:\n        \"\"\" Get the new face from the predictor and apply pre-warp manipulations.\n\n        Applies any requested adjustments to the raw output of the Faceswap model\n        before transforming the image into the target frame.\n\n        Parameters\n        ----------\n        predicted: :class:`~scripts.convert.ConvertItem`\n            The output from :class:`scripts.convert.Predictor`.\n        frame_size: tuple\n            The (`width`, `height`) of the final frame in pixels\n\n        Returns\n        -------\n        placeholder:  :class: `numpy.ndarray`\n            The original frame with the swapped faces patched onto it\n        background:  :class: `numpy.ndarray`\n            The original frame\n        \"\"\"\n        logger.trace(\"Getting: (filename: '%s', faces: %s)\",  # type: ignore[attr-defined]\n                     predicted.inbound.filename, len(predicted.swapped_faces))\n\n        placeholder = np.zeros((frame_size[1], frame_size[0], 4), dtype=\"float32\")\n        if self._full_frame_output:\n            background = predicted.inbound.image / np.array(255.0, dtype=\"float32\")\n            placeholder[:, :, :3] = background\n        else:\n            faces = []  # Collect the faces into final array\n            background = placeholder  # Used for obtaining original frame dimensions\n\n        for new_face, detected_face, reference_face in zip(predicted.swapped_faces,\n                                                           predicted.inbound.detected_faces,\n                                                           predicted.reference_faces):\n            predicted_mask = new_face[:, :, -1] if new_face.shape[2] == 4 else None\n            new_face = new_face[:, :, :3]\n            new_face = self._pre_warp_adjustments(new_face,\n                                                  detected_face,\n                                                  reference_face,\n                                                  predicted_mask)\n\n            if self._full_frame_output:\n                self._warp_to_frame(reference_face,\n                                    new_face, placeholder,\n                                    len(predicted.swapped_faces) > 1)\n            else:\n                faces.append(new_face)\n\n        if not self._full_frame_output:\n            placeholder = np.array(faces, dtype=\"float32\")\n\n        logger.trace(\"Got filename: '%s'. (placeholders: %s)\",  # type: ignore[attr-defined]\n                     predicted.inbound.filename, placeholder.shape)\n\n        return placeholder, background\n\n    def _pre_warp_adjustments(self,\n                              new_face: np.ndarray,\n                              detected_face: DetectedFace,\n                              reference_face: AlignedFace,\n                              predicted_mask: np.ndarray | None) -> np.ndarray:\n        \"\"\" Run any requested adjustments that can be performed on the raw output from the Faceswap\n        model.\n\n        Any adjustments that can be performed before warping the face into the final frame are\n        performed here.\n\n        Parameters\n        ----------\n        new_face: :class:`numpy.ndarray`\n            The swapped face received from the faceswap model.\n        detected_face: :class:`~lib.align.DetectedFace`\n            The detected_face object as defined in :class:`scripts.convert.Predictor`\n        reference_face: :class:`~lib.align.AlignedFace`\n            The aligned face object sized to the model output of the original face for reference\n        predicted_mask: :class:`numpy.ndarray` or ``None``\n            The predicted mask output from the Faceswap model. ``None`` if the model\n            did not learn a mask\n\n        Returns\n        -------\n        :class:`numpy.ndarray`\n            The face output from the Faceswap Model with any requested pre-warp adjustments\n            performed.\n        \"\"\"\n        logger.trace(\"new_face shape: %s, predicted_mask shape: %s\",  # type: ignore[attr-defined]\n                     new_face.shape, predicted_mask.shape if predicted_mask is not None else None)\n        old_face = T.cast(np.ndarray, reference_face.face)[..., :3] / 255.0\n        new_face, raw_mask = self._get_image_mask(new_face,\n                                                  detected_face,\n                                                  predicted_mask,\n                                                  reference_face)\n        if self._adjustments.color is not None:\n            new_face = self._adjustments.color.run(old_face, new_face, raw_mask)\n        if self._adjustments.seamless is not None:\n            new_face = self._adjustments.seamless.run(old_face, new_face, raw_mask)\n        logger.trace(\"returning: new_face shape %s\", new_face.shape)  # type: ignore[attr-defined]\n        return new_face\n\n    def _get_image_mask(self,\n                        new_face: np.ndarray,\n                        detected_face: DetectedFace,\n                        predicted_mask: np.ndarray | None,\n                        reference_face: AlignedFace) -> tuple[np.ndarray, np.ndarray]:\n        \"\"\" Return any selected image mask\n\n        Places the requested mask into the new face's Alpha channel.\n\n        Parameters\n        ----------\n        new_face: :class:`numpy.ndarray`\n            The swapped face received from the faceswap model.\n        detected_face: :class:`~lib.DetectedFace`\n            The detected_face object as defined in :class:`scripts.convert.Predictor`\n        predicted_mask: :class:`numpy.ndarray` or ``None``\n            The predicted mask output from the Faceswap model. ``None`` if the model\n            did not learn a mask\n        reference_face: :class:`~lib.align.AlignedFace`\n            The aligned face object sized to the model output of the original face for reference\n\n        Returns\n        -------\n        :class:`numpy.ndarray`\n            The swapped face with the requested mask added to the Alpha channel\n        :class:`numpy.ndarray`\n            The raw mask with no erosion or blurring applied\n        \"\"\"\n        logger.trace(\"Getting mask. Image shape: %s\", new_face.shape)  # type: ignore[attr-defined]\n        if self._args.mask_type not in (\"none\", \"predicted\"):\n            mask_centering = detected_face.mask[self._args.mask_type].stored_centering\n        else:\n            mask_centering = \"face\"  # Unused but requires a valid value\n        assert self._adjustments.mask is not None\n        mask, raw_mask = self._adjustments.mask.run(detected_face,\n                                                    reference_face.pose.offset[mask_centering],\n                                                    reference_face.pose.offset[self._centering],\n                                                    self._centering,\n                                                    predicted_mask=predicted_mask)\n        logger.trace(\"Adding mask to alpha channel\")  # type: ignore[attr-defined]\n        new_face = np.concatenate((new_face, mask), -1)\n        logger.trace(\"Got mask. Image shape: %s\", new_face.shape)  # type: ignore[attr-defined]\n        return new_face, raw_mask\n\n    def _post_warp_adjustments(self, background: np.ndarray, new_image: np.ndarray) -> np.ndarray:\n        \"\"\" Perform any requested adjustments to the swapped faces after they have been transformed\n        into the final frame.\n\n        Parameters\n        ----------\n        background: :class:`numpy.ndarray`\n            The original frame\n        new_image: :class:`numpy.ndarray`\n            A blank frame of original frame size with the faces warped onto it\n\n        Returns\n        -------\n        :class:`numpy.ndarray`\n            The final merged and swapped frame with any requested post-warp adjustments applied\n        \"\"\"\n        if self._adjustments.sharpening is not None:\n            new_image = self._adjustments.sharpening.run(new_image)\n\n        if self._draw_transparent:\n            frame = new_image\n        else:\n            foreground, mask = np.split(new_image,  # pylint:disable=unbalanced-tuple-unpacking\n                                        (3, ),\n                                        axis=-1)\n            foreground *= mask\n            background *= (1.0 - mask)\n            background += foreground\n            frame = background\n        np.clip(frame, 0.0, 1.0, out=frame)\n        return frame\n\n    def _scale_image(self, frame: np.ndarray) -> np.ndarray:\n        \"\"\" Scale the final image if requested.\n\n        If output scale has been requested in command line arguments, scale the output\n        otherwise return the final frame.\n\n        Parameters\n        ----------\n        frame: :class:`numpy.ndarray`\n            The final frame with faces swapped\n\n        Returns\n        -------\n        :class:`numpy.ndarray`\n            The final frame scaled by the requested scaling factor\n        \"\"\"\n        if self._scale == 1:\n            return frame\n        logger.trace(\"source frame: %s\", frame.shape)  # type: ignore[attr-defined]\n        interp = cv2.INTER_CUBIC if self._scale > 1 else cv2.INTER_AREA\n        dims = (round((frame.shape[1] / 2 * self._scale) * 2),\n                round((frame.shape[0] / 2 * self._scale) * 2))\n        frame = cv2.resize(frame, dims, interpolation=interp)\n        logger.trace(\"resized frame: %s\", frame.shape)  # type: ignore[attr-defined]\n        np.clip(frame, 0.0, 1.0, out=frame)\n        return frame\n", "lib/queue_manager.py": "#!/usr/bin/env python3\n\"\"\" Queue Manager for faceswap\n\n    NB: Keep this in it's own module! If it gets loaded from\n    a multiprocess on a Windows System it will break Faceswap\"\"\"\n\nimport logging\nimport threading\n\nfrom queue import Queue, Empty as QueueEmpty  # pylint:disable=unused-import; # noqa\nfrom time import sleep\n\nlogger = logging.getLogger(__name__)\n\n\nclass EventQueue(Queue):\n    \"\"\" Standard Queue object with a separate global shutdown parameter indicating that the main\n    process, and by extension this queue, should be shut down.\n\n    Parameters\n    ----------\n    shutdown_event: :class:`threading.Event`\n        The global shutdown event common to all managed queues\n    maxsize: int, Optional\n        Upperbound limit on the number of items that can be placed in the queue. Default: `0`\n    \"\"\"\n    def __init__(self, shutdown_event: threading.Event, maxsize: int = 0) -> None:\n        super().__init__(maxsize=maxsize)\n        self._shutdown = shutdown_event\n\n    @property\n    def shutdown(self) -> threading.Event:\n        \"\"\" :class:`threading.Event`: The global shutdown event \"\"\"\n        return self._shutdown\n\n\nclass _QueueManager():\n    \"\"\" Manage :class:`EventQueue` objects for availabilty across processes.\n\n        Notes\n        -----\n        Don't import this class directly, instead import via :func:`queue_manager` \"\"\"\n    def __init__(self) -> None:\n        logger.debug(\"Initializing %s\", self.__class__.__name__)\n\n        self.shutdown = threading.Event()\n        self.queues: dict[str, EventQueue] = {}\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    def add_queue(self, name: str, maxsize: int = 0, create_new: bool = False) -> str:\n        \"\"\" Add a :class:`EventQueue` to the manager.\n\n        Parameters\n        ----------\n        name: str\n            The name of the queue to create\n        maxsize: int, optional\n            The maximum queue size. Set to `0` for unlimited. Default: `0`\n        create_new: bool, optional\n            If a queue of the given name exists, and this value is ``False``, then an error is\n            raised preventing the creation of duplicate queues. If this value is ``True`` and\n            the given name exists then an integer is appended to the end of the queue name and\n            incremented until the given name is unique. Default: ``False``\n\n        Returns\n        -------\n        str\n            The final generated name for the queue\n        \"\"\"\n        logger.debug(\"QueueManager adding: (name: '%s', maxsize: %s, create_new: %s)\",\n                     name, maxsize, create_new)\n        if not create_new and name in self.queues:\n            raise ValueError(f\"Queue '{name}' already exists.\")\n        if create_new and name in self.queues:\n            i = 0\n            while name in self.queues:\n                name = f\"{name}{i}\"\n            logger.debug(\"Duplicate queue name. Updated to: '%s'\", name)\n\n        self.queues[name] = EventQueue(self.shutdown, maxsize=maxsize)\n        logger.debug(\"QueueManager added: (name: '%s')\", name)\n        return name\n\n    def del_queue(self, name: str) -> None:\n        \"\"\" Remove a queue from the manager\n\n        Parameters\n        ----------\n        name: str\n            The name of the queue to be deleted. Must exist within the queue manager.\n        \"\"\"\n        logger.debug(\"QueueManager deleting: '%s'\", name)\n        del self.queues[name]\n        logger.debug(\"QueueManager deleted: '%s'\", name)\n\n    def get_queue(self, name: str, maxsize: int = 0) -> EventQueue:\n        \"\"\" Return a :class:`EventQueue` from the manager. If it doesn't exist, create it.\n\n        Parameters\n        ----------\n        name: str\n            The name of the queue to obtain\n        maxsize: int, Optional\n            The maximum queue size. Set to `0` for unlimited. Only used if the requested queue\n            does not already exist. Default: `0`\n         \"\"\"\n        logger.debug(\"QueueManager getting: '%s'\", name)\n        queue = self.queues.get(name)\n        if not queue:\n            self.add_queue(name, maxsize)\n            queue = self.queues[name]\n        logger.debug(\"QueueManager got: '%s'\", name)\n        return queue\n\n    def terminate_queues(self) -> None:\n        \"\"\" Terminates all managed queues.\n\n        Sets the global shutdown event, clears and send EOF to all queues.  To be called if there\n        is an error \"\"\"\n        logger.debug(\"QueueManager terminating all queues\")\n        self.shutdown.set()\n        self._flush_queues()\n        for q_name, queue in self.queues.items():\n            logger.debug(\"QueueManager terminating: '%s'\", q_name)\n            queue.put(\"EOF\")\n        logger.debug(\"QueueManager terminated all queues\")\n\n    def _flush_queues(self):\n        \"\"\" Empty out the contents of every managed queue. \"\"\"\n        for q_name in self.queues:\n            self.flush_queue(q_name)\n        logger.debug(\"QueueManager flushed all queues\")\n\n    def flush_queue(self, name: str) -> None:\n        \"\"\" Flush the contents from a managed queue.\n\n        Parameters\n        ----------\n        name: str\n            The name of the managed :class:`EventQueue` to flush\n        \"\"\"\n        logger.debug(\"QueueManager flushing: '%s'\", name)\n        queue = self.queues[name]\n        while not queue.empty():\n            queue.get(True, 1)\n\n    def debug_monitor(self, update_interval: int = 2) -> None:\n        \"\"\" A debug tool for monitoring managed :class:`EventQueues`.\n\n        Prints queue sizes to the console for all managed queues.\n\n        Parameters\n        ----------\n        update_interval: int, Optional\n            The number of seconds between printing information to the console. Default: 2\n        \"\"\"\n        thread = threading.Thread(target=self._debug_queue_sizes,\n                                  args=(update_interval, ))\n        thread.daemon = True\n        thread.start()\n\n    def _debug_queue_sizes(self, update_interval) -> None:\n        \"\"\" Print the queue size for each managed queue to console.\n\n        Parameters\n        ----------\n        update_interval: int\n            The number of seconds between printing information to the console\n        \"\"\"\n        while True:\n            logger.info(\"====================================================\")\n            for name in sorted(self.queues.keys()):\n                logger.info(\"%s: %s\", name, self.queues[name].qsize())\n            sleep(update_interval)\n\n\nqueue_manager = _QueueManager()  # pylint:disable=invalid-name\n", "lib/__init__.py": "#!/usr/bin/env python3\n\"\"\" Initialization for faceswap's lib section \"\"\"\n# Import logger here so our custom loglevels are set for when executing code outside of FS\nfrom . import logger\n", "lib/logger.py": "#!/usr/bin/python\n\"\"\" Logging Functions for Faceswap. \"\"\"\n# NOTE: Don't import non stdlib packages. This module is accessed by setup.py\nimport collections\nimport logging\nfrom logging.handlers import RotatingFileHandler\nimport os\nimport platform\nimport re\nimport sys\nimport typing as T\nimport time\nimport traceback\n\nfrom datetime import datetime\n\n\n# TODO - Remove this monkey patch when TF autograph fixed to handle newer logging lib\ndef _patched_format(self, record):\n    \"\"\" Autograph tf-2.10 has a bug with the 3.10 version of logging.PercentStyle._format(). It is\n    non-critical but spits out warnings. This is the Python 3.9 version of the function and should\n    be removed once fixed \"\"\"\n    return self._fmt % record.__dict__  # pylint:disable=protected-access\n\n\nsetattr(logging.PercentStyle, \"_format\", _patched_format)\n\n\nclass FaceswapLogger(logging.Logger):\n    \"\"\" A standard :class:`logging.logger` with additional \"verbose\" and \"trace\" levels added. \"\"\"\n    def __init__(self, name: str) -> None:\n        for new_level in ((\"VERBOSE\", 15), (\"TRACE\", 5)):\n            level_name, level_num = new_level\n            if hasattr(logging, level_name):\n                continue\n            logging.addLevelName(level_num, level_name)\n            setattr(logging, level_name, level_num)\n        super().__init__(name)\n\n    def verbose(self, msg: str, *args, **kwargs) -> None:\n        # pylint:disable=wrong-spelling-in-docstring\n        \"\"\" Create a log message at severity level 15.\n\n        Parameters\n        ----------\n        msg: str\n            The log message to be recorded at Verbose level\n        args: tuple\n            Standard logging arguments\n        kwargs: dict\n            Standard logging key word arguments\n        \"\"\"\n        if self.isEnabledFor(15):\n            self._log(15, msg, args, **kwargs)\n\n    def trace(self, msg: str, *args, **kwargs) -> None:\n        # pylint:disable=wrong-spelling-in-docstring\n        \"\"\" Create a log message at severity level 5.\n\n        Parameters\n        ----------\n        msg: str\n            The log message to be recorded at Trace level\n        args: tuple\n            Standard logging arguments\n        kwargs: dict\n            Standard logging key word arguments\n        \"\"\"\n        if self.isEnabledFor(5):\n            self._log(5, msg, args, **kwargs)\n\n\nclass ColoredFormatter(logging.Formatter):\n    \"\"\" Overrides the stand :class:`logging.Formatter` to enable colored labels for message level\n    labels on supported platforms\n\n    Parameters\n    ----------\n    fmt: str\n        The format string for the message as a whole\n    pad_newlines: bool, Optional\n        If ``True`` new lines will be padded to appear in line with the log message, if ``False``\n        they will be left aligned\n\n    kwargs: dict\n        Standard :class:`logging.Formatter` keyword arguments\n    \"\"\"\n    def __init__(self, fmt: str, pad_newlines: bool = False, **kwargs) -> None:\n        super().__init__(fmt, **kwargs)\n        self._use_color = self._get_color_compatibility()\n        self._level_colors = {\"CRITICAL\": \"\\033[31m\",  # red\n                              \"ERROR\": \"\\033[31m\",  # red\n                              \"WARNING\": \"\\033[33m\",  # yellow\n                              \"INFO\": \"\\033[32m\",  # green\n                              \"VERBOSE\": \"\\033[34m\"}  # blue\n        self._default_color = \"\\033[0m\"\n        self._newline_padding = self._get_newline_padding(pad_newlines, fmt)\n\n    @classmethod\n    def _get_color_compatibility(cls) -> bool:\n        \"\"\" Return whether the system supports color ansi codes. Most OSes do other than Windows\n        below Windows 10 version 1511.\n\n        Returns\n        -------\n        bool\n            ``True`` if the system supports color ansi codes otherwise ``False``\n        \"\"\"\n        if platform.system().lower() != \"windows\":\n            return True\n        try:\n            win = sys.getwindowsversion()  # type:ignore # pylint:disable=no-member\n            if win.major >= 10 and win.build >= 10586:\n                return True\n        except Exception:  # pylint:disable=broad-except\n            return False\n        return False\n\n    def _get_newline_padding(self, pad_newlines: bool, fmt: str) -> int:\n        \"\"\" Parses the format string to obtain padding for newlines if requested\n\n        Parameters\n        ----------\n        fmt: str\n            The format string for the message as a whole\n        pad_newlines: bool, Optional\n            If ``True`` new lines will be padded to appear in line with the log message, if\n            ``False`` they will be left aligned\n\n        Returns\n        -------\n        int\n            The amount of padding to apply to the front of newlines\n        \"\"\"\n        if not pad_newlines:\n            return 0\n        msg_idx = fmt.find(\"%(message)\") + 1\n        filtered = fmt[:msg_idx - 1]\n        spaces = filtered.count(\" \")\n        pads = [int(pad.replace(\"s\", \"\")) for pad in re.findall(r\"\\ds\", filtered)]\n        if \"asctime\" in filtered:\n            pads.append(self._get_sample_time_string())\n        return sum(pads) + spaces\n\n    def _get_sample_time_string(self) -> int:\n        \"\"\" Obtain a sample time string and calculate correct padding.\n\n        This may be inaccurate when ticking over an integer from single to double digits, but that\n        shouldn't be a huge issue.\n\n        Returns\n        -------\n        int\n            The length of the formatted date-time string\n        \"\"\"\n        sample_time = time.time()\n        date_format = self.datefmt if self.datefmt else self.default_time_format\n        datestring = time.strftime(date_format, logging.Formatter.converter(sample_time))\n        if not self.datefmt and self.default_msec_format:\n            msecs = (sample_time - int(sample_time)) * 1000\n            datestring = self.default_msec_format % (datestring, msecs)\n        return len(datestring)\n\n    def format(self, record: logging.LogRecord) -> str:\n        \"\"\" Color the log message level if supported otherwise return the standard log message.\n\n        Parameters\n        ----------\n        record: :class:`logging.LogRecord`\n            The incoming log record to be formatted for entry into the logger.\n\n        Returns\n        -------\n        str\n            The formatted log message\n        \"\"\"\n        formatted = super().format(record)\n        levelname = record.levelname\n        if self._use_color and levelname in self._level_colors:\n            formatted = re.sub(levelname,\n                               f\"{self._level_colors[levelname]}{levelname}{self._default_color}\",\n                               formatted,\n                               1)\n        if self._newline_padding:\n            formatted = formatted.replace(\"\\n\", f\"\\n{' ' * self._newline_padding}\")\n        return formatted\n\n\nclass FaceswapFormatter(logging.Formatter):\n    \"\"\" Overrides the standard :class:`logging.Formatter`.\n\n    Strip newlines from incoming log messages.\n\n    Rewrites some upstream warning messages to debug level to avoid spamming the console.\n    \"\"\"\n\n    def format(self, record: logging.LogRecord) -> str:\n        \"\"\" Strip new lines from log records and rewrite certain warning messages to debug level.\n\n        Parameters\n        ----------\n        record : :class:`logging.LogRecord`\n            The incoming log record to be formatted for entry into the logger.\n\n        Returns\n        -------\n        str\n            The formatted log message\n        \"\"\"\n        record.message = record.getMessage()\n        record = self._rewrite_warnings(record)\n        record = self._lower_external(record)\n        # strip newlines\n        if record.levelno < 30 and (\"\\n\" in record.message or \"\\r\" in record.message):\n            record.message = record.message.replace(\"\\n\", \"\\\\n\").replace(\"\\r\", \"\\\\r\")\n\n        if self.usesTime():\n            record.asctime = self.formatTime(record, self.datefmt)\n        msg = self.formatMessage(record)\n        if record.exc_info:\n            # Cache the traceback text to avoid converting it multiple times\n            # (it's constant anyway)\n            if not record.exc_text:\n                record.exc_text = self.formatException(record.exc_info)\n        if record.exc_text:\n            if msg[-1:] != \"\\n\":\n                msg = msg + \"\\n\"\n            msg = msg + record.exc_text\n        if record.stack_info:\n            if msg[-1:] != \"\\n\":\n                msg = msg + \"\\n\"\n            msg = msg + self.formatStack(record.stack_info)\n        return msg\n\n    @classmethod\n    def _rewrite_warnings(cls, record: logging.LogRecord) -> logging.LogRecord:\n        \"\"\" Change certain warning messages from WARNING to DEBUG to avoid passing non-important\n        information to output.\n\n        Parameters\n        ----------\n        record: :class:`logging.LogRecord`\n            The log record to check for rewriting\n\n        Returns\n        -------\n        :class:`logging.LogRecord`\n            The log rewritten or untouched record\n\n        \"\"\"\n        if record.levelno == 30 and record.funcName == \"warn\" and record.module == \"ag_logging\":\n            # TF 2.3 in Conda is imported with the wrong gast(0.4 when 0.3.3 should be used). This\n            # causes warnings in autograph. They don't appear to impact performance so de-elevate\n            # warning to debug\n            record.levelno = 10\n            record.levelname = \"DEBUG\"\n\n        if record.levelno == 30 and (record.funcName == \"_tfmw_add_deprecation_warning\" or\n                                     record.module in (\"deprecation\", \"deprecation_wrapper\")):\n            # Keras Deprecations.\n            record.levelno = 10\n            record.levelname = \"DEBUG\"\n\n        return record\n\n    @classmethod\n    def _lower_external(cls, record: logging.LogRecord) -> logging.LogRecord:\n        \"\"\" Some external libs log at a higher level than we would really like, so lower their\n        log level.\n\n        Specifically: Matplotlib font properties\n\n        Parameters\n        ----------\n        record: :class:`logging.LogRecord`\n            The log record to check for rewriting\n\n        Returns\n        ----------\n        :class:`logging.LogRecord`\n            The log rewritten or untouched record\n        \"\"\"\n        if (record.levelno == 20 and record.funcName == \"__init__\"\n                and record.module == \"font_manager\"):\n            # Matplotlib font manager\n            record.levelno = 10\n            record.levelname = \"DEBUG\"\n\n        return record\n\n\nclass RollingBuffer(collections.deque):\n    \"\"\"File-like that keeps a certain number of lines of text in memory for writing out to the\n    crash log. \"\"\"\n\n    def write(self, buffer: str) -> None:\n        \"\"\" Splits lines from the incoming buffer and writes them out to the rolling buffer.\n\n        Parameters\n        ----------\n        buffer: str\n            The log messages to write to the rolling buffer\n        \"\"\"\n        for line in buffer.rstrip().splitlines():\n            self.append(f\"{line}\\n\")\n\n\nclass TqdmHandler(logging.StreamHandler):\n    \"\"\" Overrides :class:`logging.StreamHandler` to use :func:`tqdm.tqdm.write` rather than writing\n    to :func:`sys.stderr` so that log messages do not mess up tqdm progress bars. \"\"\"\n\n    def emit(self, record: logging.LogRecord) -> None:\n        \"\"\" Format the incoming message and pass to :func:`tqdm.tqdm.write`.\n\n        Parameters\n        ----------\n        record : :class:`logging.LogRecord`\n            The incoming log record to be formatted for entry into the logger.\n        \"\"\"\n        # tqdm is imported here as it won't be installed when setup.py is running\n        from tqdm import tqdm  # pylint:disable=import-outside-toplevel\n        msg = self.format(record)\n        tqdm.write(msg)\n\n\ndef _set_root_logger(loglevel: int = logging.INFO) -> logging.Logger:\n    \"\"\" Setup the root logger.\n\n    Parameters\n    ----------\n    loglevel: int, optional\n        The log level to set the root logger to. Default :attr:`logging.INFO`\n\n    Returns\n    -------\n    :class:`logging.Logger`\n        The root logger for Faceswap\n    \"\"\"\n    rootlogger = logging.getLogger()\n    rootlogger.setLevel(loglevel)\n    return rootlogger\n\n\ndef log_setup(loglevel, log_file: str, command: str, is_gui: bool = False) -> None:\n    \"\"\" Set up logging for Faceswap.\n\n    Sets up the root logger, the formatting for the crash logger and the file logger, and sets up\n    the crash, file and stream log handlers.\n\n    Parameters\n    ----------\n    loglevel: str\n        The requested log level that Faceswap should be run at.\n    log_file: str\n        The location of the log file to write Faceswap's log to\n    command: str\n        The Faceswap command that is being run. Used to dictate whether the log file should\n        have \"_gui\" appended to the filename or not.\n    is_gui: bool, optional\n        Whether Faceswap is running in the GUI or not. Dictates where the stream handler should\n        output messages to. Default: ``False``\n     \"\"\"\n    numeric_loglevel = get_loglevel(loglevel)\n    root_loglevel = min(logging.DEBUG, numeric_loglevel)\n    rootlogger = _set_root_logger(loglevel=root_loglevel)\n\n    if command == \"setup\":\n        log_format = FaceswapFormatter(\"%(asctime)s %(module)-16s %(funcName)-30s %(levelname)-8s \"\n                                       \"%(message)s\", datefmt=\"%m/%d/%Y %H:%M:%S\")\n        s_handler = _stream_setup_handler(numeric_loglevel)\n        f_handler = _file_handler(root_loglevel, log_file, log_format, command)\n    else:\n        log_format = FaceswapFormatter(\"%(asctime)s %(processName)-15s %(threadName)-30s \"\n                                       \"%(module)-15s %(funcName)-30s %(levelname)-8s %(message)s\",\n                                       datefmt=\"%m/%d/%Y %H:%M:%S\")\n        s_handler = _stream_handler(numeric_loglevel, is_gui)\n        f_handler = _file_handler(numeric_loglevel, log_file, log_format, command)\n\n    rootlogger.addHandler(f_handler)\n    rootlogger.addHandler(s_handler)\n\n    if command != \"setup\":\n        c_handler = _crash_handler(log_format)\n        rootlogger.addHandler(c_handler)\n        logging.info(\"Log level set to: %s\", loglevel.upper())\n\n\ndef _file_handler(loglevel,\n                  log_file: str,\n                  log_format: FaceswapFormatter,\n                  command: str) -> RotatingFileHandler:\n    \"\"\" Add a rotating file handler for the current Faceswap session. 1 backup is always kept.\n\n    Parameters\n    ----------\n    loglevel: str\n        The requested log level that messages should be logged at.\n    log_file: str\n        The location of the log file to write Faceswap's log to\n    log_format: :class:`FaceswapFormatter:\n        The formatting to store log messages as\n    command: str\n        The Faceswap command that is being run. Used to dictate whether the log file should\n        have \"_gui\" appended to the filename or not.\n\n    Returns\n    -------\n    :class:`logging.RotatingFileHandler`\n        The logging file handler\n    \"\"\"\n    if log_file:\n        filename = log_file\n    else:\n        filename = os.path.join(os.path.dirname(os.path.realpath(sys.argv[0])), \"faceswap\")\n        # Windows has issues sharing the log file with sub-processes, so log GUI separately\n        filename += \"_gui.log\" if command == \"gui\" else \".log\"\n\n    should_rotate = os.path.isfile(filename)\n    handler = RotatingFileHandler(filename, backupCount=1, encoding=\"utf-8\")\n    if should_rotate:\n        handler.doRollover()\n    handler.setFormatter(log_format)\n    handler.setLevel(loglevel)\n    return handler\n\n\ndef _stream_handler(loglevel: int, is_gui: bool) -> logging.StreamHandler | TqdmHandler:\n    \"\"\" Add a stream handler for the current Faceswap session. The stream handler will only ever\n    output at a maximum of VERBOSE level to avoid spamming the console.\n\n    Parameters\n    ----------\n    loglevel: int\n        The requested log level that messages should be logged at.\n    is_gui: bool, optional\n        Whether Faceswap is running in the GUI or not. Dictates where the stream handler should\n        output messages to.\n\n    Returns\n    -------\n    :class:`TqdmHandler` or :class:`logging.StreamHandler`\n        The stream handler to use\n    \"\"\"\n    # Don't set stdout to lower than verbose\n    loglevel = max(loglevel, 15)\n    log_format = FaceswapFormatter(\"%(asctime)s %(levelname)-8s %(message)s\",\n                                   datefmt=\"%m/%d/%Y %H:%M:%S\")\n\n    if is_gui:\n        # tqdm.write inserts extra lines in the GUI, so use standard output as\n        # it is not needed there.\n        log_console = logging.StreamHandler(sys.stdout)\n    else:\n        log_console = TqdmHandler(sys.stdout)\n    log_console.setFormatter(log_format)\n    log_console.setLevel(loglevel)\n    return log_console\n\n\ndef _stream_setup_handler(loglevel: int) -> logging.StreamHandler:\n    \"\"\" Add a stream handler for faceswap's setup.py script\n    This stream handler outputs a limited set of easy to use information using colored labels\n    if available. It will only ever output at a minimum of INFO level\n\n    Parameters\n    ----------\n    loglevel: int\n        The requested log level that messages should be logged at.\n\n    Returns\n    -------\n    :class:`logging.StreamHandler`\n        The stream handler to use\n    \"\"\"\n    loglevel = max(loglevel, 15)\n    log_format = ColoredFormatter(\"%(levelname)-8s %(message)s\", pad_newlines=True)\n    handler = logging.StreamHandler(sys.stdout)\n    handler.setFormatter(log_format)\n    handler.setLevel(loglevel)\n    return handler\n\n\ndef _crash_handler(log_format: FaceswapFormatter) -> logging.StreamHandler:\n    \"\"\" Add a handler that stores the last 100 debug lines to :attr:'_DEBUG_BUFFER' for use in\n    crash reports.\n\n    Parameters\n    ----------\n    log_format: :class:`FaceswapFormatter:\n        The formatting to store log messages as\n\n    Returns\n    -------\n    :class:`logging.StreamHandler`\n        The crash log handler\n    \"\"\"\n    log_crash = logging.StreamHandler(_DEBUG_BUFFER)\n    log_crash.setFormatter(log_format)\n    log_crash.setLevel(logging.DEBUG)\n    return log_crash\n\n\ndef get_loglevel(loglevel: str) -> int:\n    \"\"\" Check whether a valid log level has been supplied, and return the numeric log level that\n    corresponds to the given string level.\n\n    Parameters\n    ----------\n    loglevel: str\n        The loglevel that has been requested\n\n    Returns\n    -------\n    int\n        The numeric representation of the given loglevel\n    \"\"\"\n    numeric_level = getattr(logging, loglevel.upper(), None)\n    if not isinstance(numeric_level, int):\n        raise ValueError(f\"Invalid log level: {loglevel}\")\n    return numeric_level\n\n\ndef crash_log() -> str:\n    \"\"\" On a crash, write out the contents of :func:`_DEBUG_BUFFER` containing the last 100 lines\n    of debug messages to a crash report in the root Faceswap folder.\n\n    Returns\n    -------\n    str\n        The filename of the file that contains the crash report\n    \"\"\"\n    original_traceback = traceback.format_exc().encode(\"utf-8\")\n    path = os.path.dirname(os.path.realpath(sys.argv[0]))\n    filename = os.path.join(path, datetime.now().strftime(\"crash_report.%Y.%m.%d.%H%M%S%f.log\"))\n    freeze_log = [line.encode(\"utf-8\") for line in _DEBUG_BUFFER]\n    try:\n        from lib.sysinfo import sysinfo  # pylint:disable=import-outside-toplevel\n    except Exception:  # pylint:disable=broad-except\n        sysinfo = (\"\\n\\nThere was an error importing System Information from lib.sysinfo. This is \"\n                   f\"probably a bug which should be fixed:\\n{traceback.format_exc()}\")\n    with open(filename, \"wb\") as outfile:\n        outfile.writelines(freeze_log)\n        outfile.write(original_traceback)\n        outfile.write(sysinfo.encode(\"utf-8\"))\n    return filename\n\n\ndef _process_value(value: T.Any) -> T.Any:\n    \"\"\" Process the values from a local dict and return in a loggable format\n\n    Parameters\n    ----------\n    value: Any\n        The dictionary value\n\n    Returns\n    -------\n    Any\n        The original or ammended value\n    \"\"\"\n    if isinstance(value, str):\n        return f'\"{value}\"'\n    if isinstance(value, (list, tuple, set)) and len(value) > 10:\n        return f'[type: \"{type(value).__name__}\" len: {len(value)}'\n\n    try:\n        import numpy as np  # pylint:disable=import-outside-toplevel\n    except ImportError:\n        return value\n\n    if isinstance(value, np.ndarray) and np.prod(value.shape) > 10:\n        return f'[type: \"{type(value).__name__}\" shape: {value.shape}, dtype: \"{value.dtype}\"]'\n    return value\n\n\ndef parse_class_init(locals_dict: dict[str, T.Any]) -> str:\n    \"\"\" Parse a locals dict from a class and return in a format suitable for logging\n    Parameters\n    ----------\n    locals_dict: dict[str, T.Any]\n        A locals() dictionary from a newly initialized class\n    Returns\n    -------\n    str\n        The locals information suitable for logging\n    \"\"\"\n    delimit = {k: _process_value(v)\n               for k, v in locals_dict.items() if k != \"self\"}\n    dsp = \", \".join(f\"{k}: {v}\" for k, v in delimit.items())\n    dsp = f\" ({dsp})\" if dsp else \"\"\n    return f\"Initializing {locals_dict['self'].__class__.__name__}{dsp}\"\n\n\n_OLD_FACTORY = logging.getLogRecordFactory()\n\n\ndef _faceswap_logrecord(*args, **kwargs) -> logging.LogRecord:\n    \"\"\" Add a flag to :class:`logging.LogRecord` to not strip formatting from particular\n    records. \"\"\"\n    record = _OLD_FACTORY(*args, **kwargs)\n    record.strip_spaces = True  # type:ignore\n    return record\n\n\nlogging.setLogRecordFactory(_faceswap_logrecord)\n\n# Set logger class to custom logger\nlogging.setLoggerClass(FaceswapLogger)\n\n# Stores the last 100 debug messages\n_DEBUG_BUFFER = RollingBuffer(maxlen=100)\n", "lib/align/aligned_face.py": "#!/usr/bin/env python3\n\"\"\" Aligner for faceswap.py \"\"\"\nfrom __future__ import annotations\n\nfrom dataclasses import dataclass, field\nimport logging\nimport typing as T\n\nfrom threading import Lock\n\nimport cv2\nimport numpy as np\n\nfrom lib.logger import parse_class_init\n\nfrom .constants import CenteringType, EXTRACT_RATIOS, LandmarkType, _MEAN_FACE\nfrom .pose import PoseEstimate\n\nlogger = logging.getLogger(__name__)\n\n\ndef get_matrix_scaling(matrix: np.ndarray) -> tuple[int, int]:\n    \"\"\" Given a matrix, return the cv2 Interpolation method and inverse interpolation method for\n    applying the matrix on an image.\n\n    Parameters\n    ----------\n    matrix: :class:`numpy.ndarray`\n        The transform matrix to return the interpolator for\n\n    Returns\n    -------\n    tuple\n        The interpolator and inverse interpolator for the given matrix. This will be (Cubic, Area)\n        for an upscale matrix and (Area, Cubic) for a downscale matrix\n    \"\"\"\n    x_scale = np.sqrt(matrix[0, 0] * matrix[0, 0] + matrix[0, 1] * matrix[0, 1])\n    y_scale = (matrix[0, 0] * matrix[1, 1] - matrix[0, 1] * matrix[1, 0]) / x_scale\n    avg_scale = (x_scale + y_scale) * 0.5\n    if avg_scale >= 1.:\n        interpolators = cv2.INTER_CUBIC, cv2.INTER_AREA\n    else:\n        interpolators = cv2.INTER_AREA, cv2.INTER_CUBIC\n    logger.trace(\"interpolator: %s, inverse interpolator: %s\",  # type:ignore[attr-defined]\n                 interpolators[0], interpolators[1])\n    return interpolators\n\n\ndef transform_image(image: np.ndarray,\n                    matrix: np.ndarray,\n                    size: int,\n                    padding: int = 0) -> np.ndarray:\n    \"\"\" Perform transformation on an image, applying the given size and padding to the matrix.\n\n    Parameters\n    ----------\n    image: :class:`numpy.ndarray`\n        The image to transform\n    matrix: :class:`numpy.ndarray`\n        The transformation matrix to apply to the image\n    size: int\n        The final size of the transformed image\n    padding: int, optional\n        The amount of padding to apply to the final image. Default: `0`\n\n    Returns\n    -------\n    :class:`numpy.ndarray`\n        The transformed image\n    \"\"\"\n    logger.trace(\"image shape: %s, matrix: %s, size: %s. padding: %s\",  # type:ignore[attr-defined]\n                 image.shape, matrix, size, padding)\n    # transform the matrix for size and padding\n    mat = matrix * (size - 2 * padding)\n    mat[:, 2] += padding\n\n    # transform image\n    interpolators = get_matrix_scaling(mat)\n    retval = cv2.warpAffine(image, mat, (size, size), flags=interpolators[0])\n    logger.trace(\"transformed matrix: %s, final image shape: %s\",  # type:ignore[attr-defined]\n                 mat, image.shape)\n    return retval\n\n\ndef get_adjusted_center(image_size: int,\n                        source_offset: np.ndarray,\n                        target_offset: np.ndarray,\n                        source_centering: CenteringType) -> np.ndarray:\n    \"\"\" Obtain the correct center of a face extracted image to translate between two different\n    extract centerings.\n\n    Parameters\n    ----------\n    image_size: int\n        The size of the image at the given :attr:`source_centering`\n    source_offset: :class:`numpy.ndarray`\n        The pose offset to translate a base extracted face to source centering\n    target_offset: :class:`numpy.ndarray`\n        The pose offset to translate a base extracted face to target centering\n    source_centering: [\"face\", \"head\", \"legacy\"]\n        The centering of the source image\n\n    Returns\n    -------\n    :class:`numpy.ndarray`\n        The center point of the image at the given size for the target centering\n    \"\"\"\n    source_size = image_size - (image_size * EXTRACT_RATIOS[source_centering])\n    offset = target_offset - source_offset\n    offset *= source_size\n    center = np.rint(offset + image_size / 2).astype(\"int32\")\n    logger.trace(  # type:ignore[attr-defined]\n        \"image_size: %s, source_offset: %s, target_offset: %s, source_centering: '%s', \"\n        \"adjusted_offset: %s, center: %s\",\n        image_size, source_offset, target_offset, source_centering, offset, center)\n    return center\n\n\ndef get_centered_size(source_centering: CenteringType,\n                      target_centering: CenteringType,\n                      size: int,\n                      coverage_ratio: float = 1.0) -> int:\n    \"\"\" Obtain the size of a cropped face from an aligned image.\n\n    Given an image of a certain dimensions, returns the dimensions of the sub-crop within that\n    image for the requested centering at the requested coverage ratio\n\n    Notes\n    -----\n    `\"legacy\"` places the nose in the center of the image (the original method for aligning).\n    `\"face\"` aligns for the nose to be in the center of the face (top to bottom) but the center\n    of the skull for left to right. `\"head\"` places the center in the middle of the skull in 3D\n    space.\n\n    The ROI in relation to the source image is calculated by rounding the padding of one side\n    to the nearest integer then applying this padding to the center of the crop, to ensure that\n    any dimensions always have an even number of pixels.\n\n    Parameters\n    ----------\n    source_centering: [\"head\", \"face\", \"legacy\"]\n        The centering that the original image is aligned at\n    target_centering: [\"head\", \"face\", \"legacy\"]\n        The centering that the sub-crop size should be obtained for\n    size: int\n        The size of the source image to obtain the cropped size for\n    coverage_ratio: float, optional\n        The coverage ratio to be applied to the target image. Default: `1.0`\n\n    Returns\n    -------\n    int\n        The pixel size of a sub-crop image from a full head aligned image with the given coverage\n        ratio\n    \"\"\"\n    if source_centering == target_centering and coverage_ratio == 1.0:\n        retval = size\n    else:\n        src_size = size - (size * EXTRACT_RATIOS[source_centering])\n        retval = 2 * int(np.rint((src_size / (1 - EXTRACT_RATIOS[target_centering])\n                                 * coverage_ratio) / 2))\n    logger.trace(  # type:ignore[attr-defined]\n        \"source_centering: %s, target_centering: %s, size: %s, coverage_ratio: %s, \"\n        \"source_size: %s, crop_size: %s\",\n        source_centering, target_centering, size, coverage_ratio, src_size, retval)\n    return retval\n\n\n@dataclass\nclass _FaceCache:  # pylint:disable=too-many-instance-attributes\n    \"\"\" Cache for storing items related to a single aligned face.\n\n    Items are cached so that they are only created the first time they are called.\n    Each item includes a threading lock to make cache creation thread safe.\n\n    Parameters\n    ----------\n    pose: :class:`lib.align.PoseEstimate`, optional\n        The estimated pose in 3D space. Default: ``None``\n    original_roi: :class:`numpy.ndarray`, optional\n        The location of the extracted face box within the original frame. Default: ``None``\n    landmarks: :class:`numpy.ndarray`, optional\n        The 68 point facial landmarks aligned to the extracted face box. Default: ``None``\n    landmarks_normalized: :class:`numpy.ndarray`:\n        The 68 point facial landmarks normalized to 0.0 - 1.0 as aligned by Umeyama.\n        Default: ``None``\n    average_distance: float, optional\n        The average distance of the core landmarks (18-67) from the mean face that was used for\n        aligning the image.  Default: `0.0`\n    relative_eye_mouth_position: float, optional\n        A float value representing the relative position of the lowest eye/eye-brow point to the\n        highest mouth point. Positive values indicate that eyes/eyebrows are aligned above the\n        mouth, negative values indicate that eyes/eyebrows are misaligned below the mouth.\n        Default: `0.0`\n    adjusted_matrix: :class:`numpy.ndarray`, optional\n        The 3x2 transformation matrix for extracting and aligning the core face area out of the\n        original frame with padding and sizing applied. Default: ``None``\n    interpolators: tuple, optional\n        (`interpolator` and `reverse interpolator`) for the :attr:`adjusted matrix`.\n        Default: `(0, 0)`\n    cropped_roi, dict, optional\n        The (`left`, `top`, `right`, `bottom` location of the region of interest within an\n            aligned face centered for each centering. Default: `{}`\n    cropped_slices: dict, optional\n        The slices for an input full head image and output cropped image. Default: `{}`\n    \"\"\"\n    pose: PoseEstimate | None = None\n    original_roi: np.ndarray | None = None\n    landmarks: np.ndarray | None = None\n    landmarks_normalized: np.ndarray | None = None\n    average_distance: float = 0.0\n    relative_eye_mouth_position: float = 0.0\n    adjusted_matrix: np.ndarray | None = None\n    interpolators: tuple[int, int] = (0, 0)\n    cropped_roi: dict[CenteringType, np.ndarray] = field(default_factory=dict)\n    cropped_slices: dict[CenteringType, dict[T.Literal[\"in\", \"out\"],\n                                             tuple[slice, slice]]] = field(default_factory=dict)\n\n    _locks: dict[str, Lock] = field(default_factory=dict)\n\n    def __post_init__(self):\n        \"\"\" Initialize the locks for the class parameters \"\"\"\n        self._locks = {name: Lock() for name in self.__dict__}\n\n    def lock(self, name: str) -> Lock:\n        \"\"\" Obtain the lock for the given property\n\n        Parameters\n        ----------\n        name: str\n            The name of a parameter within the cache\n\n        Returns\n        -------\n        :class:`threading.Lock`\n            The lock associated with the requested parameter\n        \"\"\"\n        return self._locks[name]\n\n\nclass AlignedFace():\n    \"\"\" Class to align a face.\n\n    Holds the aligned landmarks and face image, as well as associated matrices and information\n    about an aligned face.\n\n    Parameters\n    ----------\n    landmarks: :class:`numpy.ndarray`\n        The original 68 point landmarks that pertain to the given image for this face\n    image: :class:`numpy.ndarray`, optional\n        The original frame that contains the face that is to be aligned. Pass `None` if the aligned\n        face is not to be generated, and just the co-ordinates should be calculated.\n    centering: [\"legacy\", \"face\", \"head\"], optional\n        The type of extracted face that should be loaded. \"legacy\" places the nose in the center of\n        the image (the original method for aligning). \"face\" aligns for the nose to be in the\n        center of the face (top to bottom) but the center of the skull for left to right. \"head\"\n        aligns for the center of the skull (in 3D space) being the center of the extracted image,\n        with the crop holding the full head. Default: `\"face\"`\n    size: int, optional\n        The size in pixels, of each edge of the final aligned face. Default: `64`\n    coverage_ratio: float, optional\n        The amount of the aligned image to return. A ratio of 1.0 will return the full contents of\n        the aligned image. A ratio of 0.5 will return an image of the given size, but will crop to\n        the central 50%% of the image.\n    dtype: str, optional\n        Set a data type for the final face to be returned as. Passing ``None`` will return a face\n        with the same data type as the original :attr:`image`. Default: ``None``\n    is_aligned_face: bool, optional\n        Indicates that the :attr:`image` is an aligned face rather than a frame.\n        Default: ``False``\n    is_legacy: bool, optional\n        Only used if `is_aligned` is ``True``. ``True`` indicates that the aligned image being\n        loaded is a legacy extracted face rather than a current head extracted face\n    \"\"\"\n    def __init__(self,\n                 landmarks: np.ndarray,\n                 image: np.ndarray | None = None,\n                 centering: CenteringType = \"face\",\n                 size: int = 64,\n                 coverage_ratio: float = 1.0,\n                 dtype: str | None = None,\n                 is_aligned: bool = False,\n                 is_legacy: bool = False) -> None:\n        logger.trace(parse_class_init(locals()))  # type:ignore[attr-defined]\n        self._frame_landmarks = landmarks\n        self._landmark_type = LandmarkType.from_shape(landmarks.shape)\n        self._centering = centering\n        self._size = size\n        self._coverage_ratio = coverage_ratio\n        self._dtype = dtype\n        self._is_aligned = is_aligned\n        self._source_centering: CenteringType = \"legacy\" if is_legacy and is_aligned else \"head\"\n        self._padding = self._padding_from_coverage(size, coverage_ratio)\n\n        lookup = self._landmark_type\n        self._mean_lookup = LandmarkType.LM_2D_51 if lookup == LandmarkType.LM_2D_68 else lookup\n\n        self._cache = _FaceCache()\n        self._matrices: dict[CenteringType, np.ndarray] = {\"legacy\": self._get_default_matrix()}\n\n        self._face = self.extract_face(image)\n        logger.trace(\"Initialized: %s (padding: %s, face shape: %s)\",  # type:ignore[attr-defined]\n                     self.__class__.__name__, self._padding,\n                     self._face if self._face is None else self._face.shape)\n\n    @property\n    def centering(self) -> T.Literal[\"legacy\", \"head\", \"face\"]:\n        \"\"\" str: The centering of the Aligned Face. One of `\"legacy\"`, `\"head\"`, `\"face\"`. \"\"\"\n        return self._centering\n\n    @property\n    def size(self) -> int:\n        \"\"\" int: The size (in pixels) of one side of the square extracted face image. \"\"\"\n        return self._size\n\n    @property\n    def padding(self) -> int:\n        \"\"\" int: The amount of padding (in pixels) that is applied to each side of the\n        extracted face image for the selected extract type. \"\"\"\n        return self._padding[self._centering]\n\n    @property\n    def matrix(self) -> np.ndarray:\n        \"\"\" :class:`numpy.ndarray`: The 3x2 transformation matrix for extracting and aligning the\n        core face area out of the original frame, with no padding or sizing applied. The returned\n        matrix is offset for the given :attr:`centering`. \"\"\"\n        if self._centering not in self._matrices:\n            matrix = self._matrices[\"legacy\"].copy()\n            matrix[:, 2] -= self.pose.offset[self._centering]\n            self._matrices[self._centering] = matrix\n            logger.trace(\"original matrix: %s, new matrix: %s\",  # type:ignore[attr-defined]\n                         self._matrices[\"legacy\"], matrix)\n        return self._matrices[self._centering]\n\n    @property\n    def pose(self) -> PoseEstimate:\n        \"\"\" :class:`lib.align.PoseEstimate`: The estimated pose in 3D space. \"\"\"\n        with self._cache.lock(\"pose\"):\n            if self._cache.pose is None:\n                lms = np.nan_to_num(cv2.transform(np.expand_dims(self._frame_landmarks, axis=1),\n                                    self._matrices[\"legacy\"]).squeeze())\n                self._cache.pose = PoseEstimate(lms, self._landmark_type)\n        return self._cache.pose\n\n    @property\n    def adjusted_matrix(self) -> np.ndarray:\n        \"\"\" :class:`numpy.ndarray`: The 3x2 transformation matrix for extracting and aligning the\n        core face area out of the original frame with padding and sizing applied. \"\"\"\n        with self._cache.lock(\"adjusted_matrix\"):\n            if self._cache.adjusted_matrix is None:\n                matrix = self.matrix.copy()\n                mat = matrix * (self._size - 2 * self.padding)\n                mat[:, 2] += self.padding\n                logger.trace(\"adjusted_matrix: %s\", mat)  # type:ignore[attr-defined]\n                self._cache.adjusted_matrix = mat\n        return self._cache.adjusted_matrix\n\n    @property\n    def face(self) -> np.ndarray | None:\n        \"\"\" :class:`numpy.ndarray`: The aligned face at the given :attr:`size` at the specified\n        :attr:`coverage` in the given :attr:`dtype`. If an :attr:`image` has not been provided\n        then an the attribute will return ``None``. \"\"\"\n        return self._face\n\n    @property\n    def original_roi(self) -> np.ndarray:\n        \"\"\" :class:`numpy.ndarray`: The location of the extracted face box within the original\n        frame. \"\"\"\n        with self._cache.lock(\"original_roi\"):\n            if self._cache.original_roi is None:\n                roi = np.array([[0, 0],\n                                [0, self._size - 1],\n                                [self._size - 1, self._size - 1],\n                                [self._size - 1, 0]])\n                roi = np.rint(self.transform_points(roi, invert=True)).astype(\"int32\")\n                logger.trace(\"original roi: %s\", roi)  # type:ignore[attr-defined]\n                self._cache.original_roi = roi\n        return self._cache.original_roi\n\n    @property\n    def landmarks(self) -> np.ndarray:\n        \"\"\" :class:`numpy.ndarray`: The 68 point facial landmarks aligned to the extracted face\n        box. \"\"\"\n        with self._cache.lock(\"landmarks\"):\n            if self._cache.landmarks is None:\n                lms = self.transform_points(self._frame_landmarks)\n                logger.trace(\"aligned landmarks: %s\", lms)  # type:ignore[attr-defined]\n                self._cache.landmarks = lms\n            return self._cache.landmarks\n\n    @property\n    def landmark_type(self) -> LandmarkType:\n        \"\"\":class:`~LandmarkType`: The type of landmarks that generated this aligned face \"\"\"\n        return self._landmark_type\n\n    @property\n    def normalized_landmarks(self) -> np.ndarray:\n        \"\"\" :class:`numpy.ndarray`: The 68 point facial landmarks normalized to 0.0 - 1.0 as\n        aligned by Umeyama. \"\"\"\n        with self._cache.lock(\"landmarks_normalized\"):\n            if self._cache.landmarks_normalized is None:\n                lms = np.expand_dims(self._frame_landmarks, axis=1)\n                lms = cv2.transform(lms, self._matrices[\"legacy\"]).squeeze()\n                logger.trace(\"normalized landmarks: %s\", lms)  # type:ignore[attr-defined]\n                self._cache.landmarks_normalized = lms\n        return self._cache.landmarks_normalized\n\n    @property\n    def interpolators(self) -> tuple[int, int]:\n        \"\"\" tuple: (`interpolator` and `reverse interpolator`) for the :attr:`adjusted matrix`. \"\"\"\n        with self._cache.lock(\"interpolators\"):\n            if not any(self._cache.interpolators):\n                interpolators = get_matrix_scaling(self.adjusted_matrix)\n                logger.trace(\"interpolators: %s\", interpolators)  # type:ignore[attr-defined]\n                self._cache.interpolators = interpolators\n        return self._cache.interpolators\n\n    @property\n    def average_distance(self) -> float:\n        \"\"\" float: The average distance of the core landmarks (18-67) from the mean face that was\n        used for aligning the image. \"\"\"\n        with self._cache.lock(\"average_distance\"):\n            if not self._cache.average_distance:\n                mean_face = _MEAN_FACE[self._mean_lookup]\n                lms = self.normalized_landmarks\n                if self._landmark_type == LandmarkType.LM_2D_68:\n                    lms = lms[17:]  # 68 point landmarks only use core face items\n                average_distance = np.mean(np.abs(lms - mean_face))\n                logger.trace(\"average_distance: %s\", average_distance)  # type:ignore[attr-defined]\n                self._cache.average_distance = average_distance\n        return self._cache.average_distance\n\n    @property\n    def relative_eye_mouth_position(self) -> float:\n        \"\"\" float: Value representing the relative position of the lowest eye/eye-brow point to the\n        highest mouth point. Positive values indicate that eyes/eyebrows are aligned above the\n        mouth, negative values indicate that eyes/eyebrows are misaligned below the mouth. \"\"\"\n        with self._cache.lock(\"relative_eye_mouth_position\"):\n            if not self._cache.relative_eye_mouth_position:\n                if self._landmark_type != LandmarkType.LM_2D_68:\n                    position = 1.0  # arbitrary positive value\n                else:\n                    lowest_eyes = np.max(self.normalized_landmarks[np.r_[17:27, 36:48], 1])\n                    highest_mouth = np.min(self.normalized_landmarks[48:68, 1])\n                    position = highest_mouth - lowest_eyes\n                logger.trace(\"lowest_eyes: %s, highest_mouth: %s, \"  # type:ignore[attr-defined]\n                             \"relative_eye_mouth_position: %s\", lowest_eyes, highest_mouth,\n                             position)\n                self._cache.relative_eye_mouth_position = position\n        return self._cache.relative_eye_mouth_position\n\n    @classmethod\n    def _padding_from_coverage(cls, size: int, coverage_ratio: float) -> dict[CenteringType, int]:\n        \"\"\" Return the image padding for a face from coverage_ratio set against a\n            pre-padded training image.\n\n        Parameters\n        ----------\n        size: int\n            The final size of the aligned image in pixels\n        coverage_ratio: float\n            The ratio of the final image to pad to\n\n        Returns\n        -------\n        dict\n            The padding required, in pixels for 'head', 'face' and 'legacy' face types\n        \"\"\"\n        retval = {_type: round((size * (coverage_ratio - (1 - EXTRACT_RATIOS[_type]))) / 2)\n                  for _type in T.get_args(T.Literal[\"legacy\", \"face\", \"head\"])}\n        logger.trace(retval)  # type:ignore[attr-defined]\n        return retval\n\n    def _get_default_matrix(self) -> np.ndarray:\n        \"\"\" Get the default (legacy) matrix. All subsequent matrices are calculated from this\n\n        Returns\n        -------\n        :class:`numpy.ndarray`\n            The default 'legacy' matrix\n        \"\"\"\n        lms = self._frame_landmarks\n        if self._landmark_type == LandmarkType.LM_2D_68:\n            lms = lms[17:]  # 68 point landmarks only use core face items\n        retval = _umeyama(lms, _MEAN_FACE[self._mean_lookup], True)[0:2]\n        logger.trace(\"Default matrix: %s\", retval)  # type:ignore[attr-defined]\n        return retval\n\n    def transform_points(self, points: np.ndarray, invert: bool = False) -> np.ndarray:\n        \"\"\" Perform transformation on a series of (x, y) co-ordinates in world space into\n        aligned face space.\n\n        Parameters\n        ----------\n        points: :class:`numpy.ndarray`\n            The points to transform\n        invert: bool, optional\n            ``True`` to reverse the transformation (i.e. transform the points into world space from\n            aligned face space). Default: ``False``\n\n        Returns\n        -------\n        :class:`numpy.ndarray`\n            The transformed points\n        \"\"\"\n        retval = np.expand_dims(points, axis=1)\n        mat = cv2.invertAffineTransform(self.adjusted_matrix) if invert else self.adjusted_matrix\n        retval = cv2.transform(retval, mat).squeeze()\n        logger.trace(  # type:ignore[attr-defined]\n            \"invert: %s, Original points: %s, transformed points: %s\", invert, points, retval)\n        return retval\n\n    def extract_face(self, image: np.ndarray | None) -> np.ndarray | None:\n        \"\"\" Extract the face from a source image and populate :attr:`face`. If an image is not\n        provided then ``None`` is returned.\n\n        Parameters\n        ----------\n        image: :class:`numpy.ndarray` or ``None``\n            The original frame to extract the face from. ``None`` if the face should not be\n            extracted\n\n        Returns\n        -------\n        :class:`numpy.ndarray` or ``None``\n            The extracted face at the given size, with the given coverage of the given dtype or\n            ``None`` if no image has been provided.\n        \"\"\"\n        if image is None:\n            logger.trace(\"_extract_face called without a loaded \"  # type:ignore[attr-defined]\n                         \"image. Returning empty face.\")\n            return None\n\n        if self._is_aligned and (self._centering != self._source_centering or\n                                 self._coverage_ratio != 1.0):\n            # Crop out the sub face from full head\n            image = self._convert_centering(image)\n\n        if self._is_aligned and image.shape[0] != self._size:  # Resize the given aligned face\n            interp = cv2.INTER_CUBIC if image.shape[0] < self._size else cv2.INTER_AREA\n            retval = cv2.resize(image, (self._size, self._size), interpolation=interp)\n        elif self._is_aligned:\n            retval = image\n        else:\n            retval = transform_image(image, self.matrix, self._size, self.padding)\n        retval = retval if self._dtype is None else retval.astype(self._dtype)\n        return retval\n\n    def _convert_centering(self, image: np.ndarray) -> np.ndarray:\n        \"\"\" When the face being loaded is pre-aligned, the loaded image will have 'head' centering\n        so it needs to be cropped out to the appropriate centering.\n\n        This function temporarily converts this object to a full head aligned face, extracts the\n        sub-cropped face to the correct centering, reverse the sub crop and returns the cropped\n        face at the selected coverage ratio.\n\n        Parameters\n        ----------\n        image: :class:`numpy.ndarray`\n            The original head-centered aligned image\n\n        Returns\n        -------\n        :class:`numpy.ndarray`\n            The aligned image with the correct centering, scaled to image input size\n        \"\"\"\n        logger.trace(  # type:ignore[attr-defined]\n            \"image_size: %s, target_size: %s, coverage_ratio: %s\",\n            image.shape[0], self.size, self._coverage_ratio)\n\n        img_size = image.shape[0]\n        target_size = get_centered_size(self._source_centering,\n                                        self._centering,\n                                        img_size,\n                                        self._coverage_ratio)\n        out = np.zeros((target_size, target_size, image.shape[-1]), dtype=image.dtype)\n\n        slices = self._get_cropped_slices(img_size, target_size)\n        out[slices[\"out\"][0], slices[\"out\"][1], :] = image[slices[\"in\"][0], slices[\"in\"][1], :]\n        logger.trace(  # type:ignore[attr-defined]\n            \"Cropped from aligned extract: (centering: %s, in shape: %s, out shape: %s)\",\n            self._centering, image.shape, out.shape)\n        return out\n\n    def _get_cropped_slices(self,\n                            image_size: int,\n                            target_size: int,\n                            ) -> dict[T.Literal[\"in\", \"out\"], tuple[slice, slice]]:\n        \"\"\" Obtain the slices to turn a full head extract into an alternatively centered extract.\n\n        Parameters\n        ----------\n        image_size: int\n            The size of the full head extracted image loaded from disk\n        target_size: int\n            The size of the target centered face with coverage ratio applied in relation to the\n            original image size\n\n        Returns\n        -------\n        dict\n            The slices for an input full head image and output cropped image\n        \"\"\"\n        with self._cache.lock(\"cropped_slices\"):\n            if not self._cache.cropped_slices.get(self._centering):\n                roi = self.get_cropped_roi(image_size, target_size, self._centering)\n                slice_in = (slice(max(roi[1], 0), max(roi[3], 0)),\n                            slice(max(roi[0], 0), max(roi[2], 0)))\n                slice_out = (slice(max(roi[1] * -1, 0),\n                                   target_size - min(target_size, max(0, roi[3] - image_size))),\n                             slice(max(roi[0] * -1, 0),\n                                   target_size - min(target_size, max(0, roi[2] - image_size))))\n                self._cache.cropped_slices[self._centering] = {\"in\": slice_in, \"out\": slice_out}\n                logger.trace(\"centering: %s, cropped_slices: %s\",  # type:ignore[attr-defined]\n                             self._centering, self._cache.cropped_slices[self._centering])\n        return self._cache.cropped_slices[self._centering]\n\n    def get_cropped_roi(self,\n                        image_size: int,\n                        target_size: int,\n                        centering: CenteringType) -> np.ndarray:\n        \"\"\" Obtain the region of interest within an aligned face set to centered coverage for\n        an alternative centering\n\n        Parameters\n        ----------\n        image_size: int\n            The size of the full head extracted image loaded from disk\n        target_size: int\n            The size of the target centered face with coverage ratio applied in relation to the\n            original image size\n\n        centering: [\"legacy\", \"face\"]\n            The type of centering to obtain the region of interest for. \"legacy\" places the nose\n            in the center of the image (the original method for aligning). \"face\" aligns for the\n            nose to be in the center of the face (top to bottom) but the center of the skull for\n            left to right.\n\n        Returns\n        -------\n        :class:`numpy.ndarray`\n            The (`left`, `top`, `right`, `bottom` location of the region of interest within an\n            aligned face centered on the head for the given centering\n        \"\"\"\n        with self._cache.lock(\"cropped_roi\"):\n            if centering not in self._cache.cropped_roi:\n                center = get_adjusted_center(image_size,\n                                             self.pose.offset[self._source_centering],\n                                             self.pose.offset[centering],\n                                             self._source_centering)\n                padding = target_size // 2\n                roi = np.array([center - padding, center + padding]).ravel()\n                logger.trace(  # type:ignore[attr-defined]\n                    \"centering: '%s', center: %s, padding: %s, sub roi: %s\",\n                    centering, center, padding, roi)\n                self._cache.cropped_roi[centering] = roi\n        return self._cache.cropped_roi[centering]\n\n    def split_mask(self) -> np.ndarray:\n        \"\"\" Remove the mask from the alpha channel of :attr:`face` and return the mask\n\n        Returns\n        -------\n        :class:`numpy.ndarray`\n            The mask that was stored in the :attr:`face`'s alpha channel\n\n        Raises\n        ------\n        AssertionError\n            If :attr:`face` does not contain a mask in the alpha channel\n        \"\"\"\n        assert self._face is not None\n        assert self._face.shape[-1] == 4, \"No mask stored in the alpha channel\"\n        mask = self._face[..., 3]\n        self._face = self._face[..., :3]\n        return mask\n\n\ndef _umeyama(source: np.ndarray, destination: np.ndarray, estimate_scale: bool) -> np.ndarray:\n    \"\"\"Estimate N-D similarity transformation with or without scaling.\n\n    Imported, and slightly adapted, directly from:\n    https://github.com/scikit-image/scikit-image/blob/master/skimage/transform/_geometric.py\n\n\n    Parameters\n    ----------\n    source: :class:`numpy.ndarray`\n        (M, N) array source coordinates.\n    destination: :class:`numpy.ndarray`\n        (M, N) array destination coordinates.\n    estimate_scale: bool\n        Whether to estimate scaling factor.\n\n    Returns\n    -------\n    :class:`numpy.ndarray`\n        (N + 1, N + 1) The homogeneous similarity transformation matrix. The matrix contains\n        NaN values only if the problem is not well-conditioned.\n\n    References\n    ----------\n    .. [1] \"Least-squares estimation of transformation parameters between two\n            point patterns\", Shinji Umeyama, PAMI 1991, :DOI:`10.1109/34.88573`\n    \"\"\"\n    # pylint:disable=invalid-name,too-many-locals\n    num = source.shape[0]\n    dim = source.shape[1]\n\n    # Compute mean of source and destination.\n    src_mean = source.mean(axis=0)\n    dst_mean = destination.mean(axis=0)\n\n    # Subtract mean from source and destination.\n    src_demean = source - src_mean\n    dst_demean = destination - dst_mean\n\n    # Eq. (38).\n    A = dst_demean.T @ src_demean / num\n\n    # Eq. (39).\n    d = np.ones((dim,), dtype=np.double)\n    if np.linalg.det(A) < 0:\n        d[dim - 1] = -1\n\n    retval = np.eye(dim + 1, dtype=np.double)\n\n    U, S, V = np.linalg.svd(A)\n\n    # Eq. (40) and (43).\n    rank = np.linalg.matrix_rank(A)\n    if rank == 0:\n        return np.nan * retval\n    if rank == dim - 1:\n        if np.linalg.det(U) * np.linalg.det(V) > 0:\n            retval[:dim, :dim] = U @ V\n        else:\n            s = d[dim - 1]\n            d[dim - 1] = -1\n            retval[:dim, :dim] = U @ np.diag(d) @ V\n            d[dim - 1] = s\n    else:\n        retval[:dim, :dim] = U @ np.diag(d) @ V\n\n    if estimate_scale:\n        # Eq. (41) and (42).\n        scale = 1.0 / src_demean.var(axis=0).sum() * (S @ d)\n    else:\n        scale = 1.0\n\n    retval[:dim, dim] = dst_mean - scale * (retval[:dim, :dim] @ src_mean.T)\n    retval[:dim, :dim] *= scale\n\n    return retval\n", "lib/align/pose.py": "#!/usr/bin/env python3\n\"\"\" Holds estimated pose information for a faceswap aligned face \"\"\"\nfrom __future__ import annotations\n\nimport logging\nimport typing as T\n\nimport cv2\nimport numpy as np\n\nfrom lib.logger import parse_class_init\n\nfrom .constants import _MEAN_FACE, LandmarkType\n\nlogger = logging.getLogger(__name__)\n\nif T.TYPE_CHECKING:\n    from .constants import CenteringType\n\n\nclass PoseEstimate():\n    \"\"\" Estimates pose from a generic 3D head model for the given 2D face landmarks.\n\n    Parameters\n    ----------\n    landmarks: :class:`numpy.ndarry`\n        The original 68 point landmarks aligned to 0.0 - 1.0 range\n    landmarks_type: :class:`~LandmarksType`\n        The type of landmarks that are generating this face\n\n    References\n    ----------\n    Head Pose Estimation using OpenCV and Dlib - https://www.learnopencv.com/tag/solvepnp/\n    3D Model points - http://aifi.isr.uc.pt/Downloads/OpenGL/glAnthropometric3DModel.cpp\n    \"\"\"\n    _logged_once = False\n\n    def __init__(self, landmarks: np.ndarray, landmarks_type: LandmarkType) -> None:\n        logger.trace(parse_class_init(locals()))  # type:ignore[attr-defined]\n        self._distortion_coefficients = np.zeros((4, 1))  # Assuming no lens distortion\n        self._xyz_2d: np.ndarray | None = None\n\n        if landmarks_type != LandmarkType.LM_2D_68:\n            self._log_once(\"Pose estimation is not available for non-68 point landmarks. Pose and \"\n                           \"offset data will all be returned as the incorrect value of '0'\")\n        self._landmarks_type = landmarks_type\n        self._camera_matrix = self._get_camera_matrix()\n        self._rotation, self._translation = self._solve_pnp(landmarks)\n        self._offset = self._get_offset()\n        self._pitch_yaw_roll: tuple[float, float, float] = (0, 0, 0)\n        logger.trace(\"Initialized %s\", self.__class__.__name__)  # type:ignore[attr-defined]\n\n    @property\n    def xyz_2d(self) -> np.ndarray:\n        \"\"\" :class:`numpy.ndarray` projected (x, y) coordinates for each x, y, z point at a\n        constant distance from adjusted center of the skull (0.5, 0.5) in the 2D space. \"\"\"\n        if self._xyz_2d is None:\n            xyz = cv2.projectPoints(np.array([[6., 0., -2.3],\n                                              [0., 6., -2.3],\n                                              [0., 0., 3.7]]).astype(\"float32\"),\n                                    self._rotation,\n                                    self._translation,\n                                    self._camera_matrix,\n                                    self._distortion_coefficients)[0].squeeze()\n            self._xyz_2d = xyz - self._offset[\"head\"]\n        return self._xyz_2d\n\n    @property\n    def offset(self) -> dict[CenteringType, np.ndarray]:\n        \"\"\" dict: The amount to offset a standard 0.0 - 1.0 umeyama transformation matrix for a\n        from the center of the face (between the eyes) or center of the head (middle of skull)\n        rather than the nose area. \"\"\"\n        return self._offset\n\n    @property\n    def pitch(self) -> float:\n        \"\"\" float: The pitch of the aligned face in eular angles \"\"\"\n        if not any(self._pitch_yaw_roll):\n            self._get_pitch_yaw_roll()\n        return self._pitch_yaw_roll[0]\n\n    @property\n    def yaw(self) -> float:\n        \"\"\" float: The yaw of the aligned face in eular angles \"\"\"\n        if not any(self._pitch_yaw_roll):\n            self._get_pitch_yaw_roll()\n        return self._pitch_yaw_roll[1]\n\n    @property\n    def roll(self) -> float:\n        \"\"\" float: The roll of the aligned face in eular angles \"\"\"\n        if not any(self._pitch_yaw_roll):\n            self._get_pitch_yaw_roll()\n        return self._pitch_yaw_roll[2]\n\n    @classmethod\n    def _log_once(cls, message: str) -> None:\n        \"\"\" Log a warning about unsupported landmarks if a message has not already been logged \"\"\"\n        if cls._logged_once:\n            return\n        logger.warning(message)\n        cls._logged_once = True\n\n    def _get_pitch_yaw_roll(self) -> None:\n        \"\"\" Obtain the yaw, roll and pitch from the :attr:`_rotation` in eular angles. \"\"\"\n        proj_matrix = np.zeros((3, 4), dtype=\"float32\")\n        proj_matrix[:3, :3] = cv2.Rodrigues(self._rotation)[0]\n        euler = cv2.decomposeProjectionMatrix(proj_matrix)[-1]\n        self._pitch_yaw_roll = T.cast(tuple[float, float, float], tuple(euler.squeeze()))\n        logger.trace(\"yaw_pitch: %s\", self._pitch_yaw_roll)  # type:ignore[attr-defined]\n\n    @classmethod\n    def _get_camera_matrix(cls) -> np.ndarray:\n        \"\"\" Obtain an estimate of the camera matrix based off the original frame dimensions.\n\n        Returns\n        -------\n        :class:`numpy.ndarray`\n            An estimated camera matrix\n        \"\"\"\n        focal_length = 4\n        camera_matrix = np.array([[focal_length, 0, 0.5],\n                                  [0, focal_length, 0.5],\n                                  [0, 0, 1]], dtype=\"double\")\n        logger.trace(\"camera_matrix: %s\", camera_matrix)  # type:ignore[attr-defined]\n        return camera_matrix\n\n    def _solve_pnp(self, landmarks: np.ndarray) -> tuple[np.ndarray, np.ndarray]:\n        \"\"\" Solve the Perspective-n-Point for the given landmarks.\n\n        Takes 2D landmarks in world space and estimates the rotation and translation vectors\n        in 3D space.\n\n        Parameters\n        ----------\n        landmarks: :class:`numpy.ndarry`\n            The original 68 point landmark co-ordinates relating to the original frame\n\n        Returns\n        -------\n        rotation: :class:`numpy.ndarray`\n            The solved rotation vector\n        translation: :class:`numpy.ndarray`\n            The solved translation vector\n        \"\"\"\n        if self._landmarks_type != LandmarkType.LM_2D_68:\n            points: np.ndarray = np.empty([])\n            rotation = np.array([[0.0], [0.0], [0.0]])\n            translation = rotation.copy()\n        else:\n            points = landmarks[[6, 7, 8, 9, 10, 17, 21, 22, 26, 31, 32, 33, 34,\n                                35, 36, 39, 42, 45, 48, 50, 51, 52, 54, 56, 57, 58]]\n            _, rotation, translation = cv2.solvePnP(_MEAN_FACE[LandmarkType.LM_3D_26],\n                                                    points,\n                                                    self._camera_matrix,\n                                                    self._distortion_coefficients,\n                                                    flags=cv2.SOLVEPNP_ITERATIVE)\n        logger.trace(\"points: %s, rotation: %s, translation: %s\",  # type:ignore[attr-defined]\n                     points, rotation, translation)\n        return rotation, translation\n\n    def _get_offset(self) -> dict[CenteringType, np.ndarray]:\n        \"\"\" Obtain the offset between the original center of the extracted face to the new center\n        of the head in 2D space.\n\n        Returns\n        -------\n        :class:`numpy.ndarray`\n            The x, y offset of the new center from the old center.\n        \"\"\"\n        offset: dict[CenteringType, np.ndarray] = {\"legacy\": np.array([0.0, 0.0])}\n        if self._landmarks_type != LandmarkType.LM_2D_68:\n            offset[\"face\"] = np.array([0.0, 0.0])\n            offset[\"head\"] = np.array([0.0, 0.0])\n        else:\n            points: dict[T.Literal[\"face\", \"head\"], tuple[float, ...]] = {\"head\": (0.0, 0.0, -2.3),\n                                                                          \"face\": (0.0, -1.5, 4.2)}\n            for key, pnts in points.items():\n                center = cv2.projectPoints(np.array([pnts]).astype(\"float32\"),\n                                           self._rotation,\n                                           self._translation,\n                                           self._camera_matrix,\n                                           self._distortion_coefficients)[0].squeeze()\n                logger.trace(\"center %s: %s\", key, center)  # type:ignore[attr-defined]\n                offset[key] = center - np.array([0.5, 0.5])\n        logger.trace(\"offset: %s\", offset)  # type:ignore[attr-defined]\n        return offset\n", "lib/align/detected_face.py": "#!/usr/bin python3\n\"\"\" Face and landmarks detection for faceswap.py \"\"\"\nfrom __future__ import annotations\nimport logging\nimport os\nimport typing as T\n\nfrom hashlib import sha1\nfrom zlib import compress, decompress\n\nimport numpy as np\n\nfrom lib.image import encode_image, read_image\nfrom lib.logger import parse_class_init\nfrom lib.utils import FaceswapError\nfrom .alignments import (Alignments, AlignmentFileDict, PNGHeaderAlignmentsDict,\n                         PNGHeaderDict, PNGHeaderSourceDict)\nfrom .aligned_face import AlignedFace\nfrom .aligned_mask import LandmarksMask, Mask\nfrom .constants import LANDMARK_PARTS\n\nif T.TYPE_CHECKING:\n    from .aligned_face import CenteringType\n\nlogger = logging.getLogger(__name__)\n\n\nclass DetectedFace():\n    \"\"\" Detected face and landmark information\n\n    Holds information about a detected face, it's location in a source image\n    and the face's 68 point landmarks.\n\n    Methods for aligning a face are also callable from here.\n\n    Parameters\n    ----------\n    image: numpy.ndarray, optional\n        Original frame that holds this face. Optional (not required if just storing coordinates)\n    left: int\n        The left most point (in pixels) of the face's bounding box as discovered in\n        :mod:`plugins.extract.detect`\n    width: int\n        The width (in pixels) of the face's bounding box as discovered in\n        :mod:`plugins.extract.detect`\n    top: int\n        The top most point (in pixels) of the face's bounding box as discovered in\n        :mod:`plugins.extract.detect`\n    height: int\n        The height (in pixels) of the face's bounding box as discovered in\n        :mod:`plugins.extract.detect`\n    landmarks_xy: list\n        The 68 point landmarks as discovered in :mod:`plugins.extract.align`. Should be a ``list``\n        of 68 `(x, y)` ``tuples`` with each of the landmark co-ordinates.\n    mask: dict\n        The generated mask(s) for the face as generated in :mod:`plugins.extract.mask`. Must be a\n        dict of {**name** (`str`): :class:`~lib.align.aligned_mask.Mask`}.\n\n    Attributes\n    ----------\n    image: numpy.ndarray, optional\n        This is a generic image placeholder that should not be relied on to be holding a particular\n        image. It may hold the source frame that holds the face, a cropped face or a scaled image\n        depending on the method using this object.\n    left: int\n        The left most point (in pixels) of the face's bounding box as discovered in\n        :mod:`plugins.extract.detect`\n    width: int\n        The width (in pixels) of the face's bounding box as discovered in\n        :mod:`plugins.extract.detect`\n    top: int\n        The top most point (in pixels) of the face's bounding box as discovered in\n        :mod:`plugins.extract.detect`\n    height: int\n        The height (in pixels) of the face's bounding box as discovered in\n        :mod:`plugins.extract.detect`\n    landmarks_xy: list\n        The 68 point landmarks as discovered in :mod:`plugins.extract.align`.\n    mask: dict\n        The generated mask(s) for the face as generated in :mod:`plugins.extract.mask`. Is a\n        dict of {**name** (`str`): :class:`~lib.align.aligned_mask.Mask`}.\n    \"\"\"\n    def __init__(self,\n                 image: np.ndarray | None = None,\n                 left: int | None = None,\n                 width: int | None = None,\n                 top: int | None = None,\n                 height: int | None = None,\n                 landmarks_xy: np.ndarray | None = None,\n                 mask: dict[str, Mask] | None = None) -> None:\n        logger.trace(parse_class_init(locals()))  # type:ignore[attr-defined]\n        self.image = image\n        self.left = left\n        self.width = width\n        self.top = top\n        self.height = height\n        self._landmarks_xy = landmarks_xy\n        self._identity: dict[str, np.ndarray] = {}\n        self.thumbnail: np.ndarray | None = None\n        self.mask = {} if mask is None else mask\n        self._training_masks: tuple[bytes, tuple[int, int, int]] | None = None\n\n        self._aligned: AlignedFace | None = None\n        logger.trace(\"Initialized %s\", self.__class__.__name__)  # type:ignore[attr-defined]\n\n    @property\n    def aligned(self) -> AlignedFace:\n        \"\"\" The aligned face connected to this detected face. \"\"\"\n        assert self._aligned is not None\n        return self._aligned\n\n    @property\n    def landmarks_xy(self) -> np.ndarray:\n        \"\"\" The aligned face connected to this detected face. \"\"\"\n        assert self._landmarks_xy is not None\n        return self._landmarks_xy\n\n    @property\n    def right(self) -> int:\n        \"\"\"int: Right point (in pixels) of face detection bounding box within the parent image \"\"\"\n        assert self.left is not None and self.width is not None\n        return self.left + self.width\n\n    @property\n    def bottom(self) -> int:\n        \"\"\"int: Bottom point (in pixels) of face detection bounding box within the parent image \"\"\"\n        assert self.top is not None and self.height is not None\n        return self.top + self.height\n\n    @property\n    def identity(self) -> dict[str, np.ndarray]:\n        \"\"\" dict: Identity mechanism as key, identity embedding as value. \"\"\"\n        return self._identity\n\n    def add_mask(self,\n                 name: str,\n                 mask: np.ndarray,\n                 affine_matrix: np.ndarray,\n                 interpolator: int,\n                 storage_size: int = 128,\n                 storage_centering: CenteringType = \"face\") -> None:\n        \"\"\" Add a :class:`~lib.align.aligned_mask.Mask` to this detected face\n\n        The mask should be the original output from  :mod:`plugins.extract.mask`\n        If a mask with this name already exists it will be overwritten by the given\n        mask.\n\n        Parameters\n        ----------\n        name: str\n            The name of the mask as defined by the :attr:`plugins.extract.mask._base.name`\n            parameter.\n        mask: numpy.ndarray\n            The mask that is to be added as output from :mod:`plugins.extract.mask`\n            It should be in the range 0.0 - 1.0 ideally with a ``dtype`` of ``float32``\n        affine_matrix: numpy.ndarray\n            The transformation matrix required to transform the mask to the original frame.\n        interpolator, int:\n            The CV2 interpolator required to transform this mask to it's original frame.\n        storage_size, int (optional):\n            The size the mask is to be stored at. Default: 128\n        storage_centering, str (optional):\n            The centering to store the mask at. One of `\"legacy\"`, `\"face\"`, `\"head\"`.\n            Default: `\"face\"`\n        \"\"\"\n        logger.trace(\"name: '%s', mask shape: %s, affine_matrix: %s, \"  # type:ignore[attr-defined]\n                     \"interpolator: %s, storage_size: %s, storage_centering: %s)\", name,\n                     mask.shape, affine_matrix, interpolator, storage_size, storage_centering)\n        fsmask = Mask(storage_size=storage_size, storage_centering=storage_centering)\n        fsmask.add(mask, affine_matrix, interpolator)\n        self.mask[name] = fsmask\n\n    def add_landmarks_xy(self, landmarks: np.ndarray) -> None:\n        \"\"\" Add landmarks to the detected face object. If landmarks alread exist, they will be\n        overwritten.\n\n        Parameters\n        ----------\n        landmarks: :class:`numpy.ndarray`\n            The 68 point face landmarks to add for the face\n        \"\"\"\n        logger.trace(\"landmarks shape: '%s'\", landmarks.shape)  # type:ignore[attr-defined]\n        self._landmarks_xy = landmarks\n\n    def add_identity(self, name: str, embedding: np.ndarray, ) -> None:\n        \"\"\" Add an identity embedding to this detected face. If an identity already exists for the\n        given :attr:`name` it will be overwritten\n\n        Parameters\n        ----------\n        name: str\n            The name of the mechanism that calculated the identity\n        embedding: numpy.ndarray\n            The identity embedding\n        \"\"\"\n        logger.trace(\"name: '%s', embedding shape: %s\",  # type:ignore[attr-defined]\n                     name, embedding.shape)\n        assert name == \"vggface2\"\n        assert embedding.shape[0] == 512\n        self._identity[name] = embedding\n\n    def clear_all_identities(self) -> None:\n        \"\"\" Remove all stored identity embeddings \"\"\"\n        self._identity = {}\n\n    def get_landmark_mask(self,\n                          area: T.Literal[\"eye\", \"face\", \"mouth\"],\n                          blur_kernel: int,\n                          dilation: float) -> np.ndarray:\n        \"\"\" Add a :class:`L~lib.align.aligned_mask.LandmarksMask` to this detected face\n\n        Landmark based masks are generated from face Aligned Face landmark points. An aligned\n        face must be loaded. As the data is coming from the already aligned face, no further mask\n        cropping is required.\n\n        Parameters\n        ----------\n        area: [\"face\", \"mouth\", \"eye\"]\n            The type of mask to obtain. `face` is a full face mask the others are masks for those\n            specific areas\n        blur_kernel: int\n            The size of the kernel for blurring the mask edges\n        dilation: float\n            The amount of dilation to apply to the mask. as a percentage of the mask size\n\n        Returns\n        -------\n        :class:`numpy.ndarray`\n            The generated landmarks mask for the selected area\n\n        Raises\n        ------\n        FaceSwapError\n            If the aligned face does not contain the correct landmarks to generate a landmark mask\n        \"\"\"\n        # TODO Face mask generation from landmarks\n        logger.trace(\"area: %s, dilation: %s\", area, dilation)  # type:ignore[attr-defined]\n\n        lm_type = self.aligned.landmark_type\n        if lm_type not in LANDMARK_PARTS:\n            raise FaceswapError(f\"Landmark based masks cannot be created for {lm_type.name}\")\n\n        lm_parts = LANDMARK_PARTS[self.aligned.landmark_type]\n        mapped = {\"mouth\": [\"mouth_outer\"], \"eye\": [\"right_eye\", \"left_eye\"]}\n        if not all(part in lm_parts for parts in mapped.values() for part in parts):\n            raise FaceswapError(f\"Landmark based masks cannot be created for {lm_type.name}\")\n\n        areas = {key: [slice(*lm_parts[v][:2]) for v in val]for key, val in mapped.items()}\n        points = [self.aligned.landmarks[zone] for zone in areas[area]]\n\n        lmmask = LandmarksMask(points,\n                               storage_size=self.aligned.size,\n                               storage_centering=self.aligned.centering,\n                               dilation=dilation)\n        lmmask.set_blur_and_threshold(blur_kernel=blur_kernel)\n        lmmask.generate_mask(\n            self.aligned.adjusted_matrix,\n            self.aligned.interpolators[1])\n        return lmmask.mask\n\n    def store_training_masks(self,\n                             masks: list[np.ndarray | None],\n                             delete_masks: bool = False) -> None:\n        \"\"\" Concatenate and compress the given training masks and store for retrieval.\n\n        Parameters\n        ----------\n        masks: list\n            A list of training mask. Must be all be uint-8 3D arrays of the same size in\n            0-255 range\n        delete_masks: bool, optional\n            ``True`` to delete any of the :class:`~lib.align.aligned_mask.Mask` objects owned by\n            this detected face. Use to free up unrequired memory usage. Default: ``False``\n        \"\"\"\n        if delete_masks:\n            del self.mask\n            self.mask = {}\n\n        valid = [msk for msk in masks if msk is not None]\n        if not valid:\n            return\n        combined = np.concatenate(valid, axis=-1)\n        self._training_masks = (compress(combined), combined.shape)\n\n    def get_training_masks(self) -> np.ndarray | None:\n        \"\"\" Obtain the decompressed combined training masks.\n\n        Returns\n        -------\n        :class:`numpy.ndarray`\n            A 3D array containing the decompressed training masks as uint8 in 0-255 range if\n            training masks are present otherwise ``None``\n        \"\"\"\n        if not self._training_masks:\n            return None\n        return np.frombuffer(decompress(self._training_masks[0]),\n                             dtype=\"uint8\").reshape(self._training_masks[1])\n\n    def to_alignment(self) -> AlignmentFileDict:\n        \"\"\"  Return the detected face formatted for an alignments file\n\n        returns\n        -------\n        alignment: dict\n            The alignment dict will be returned with the keys ``x``, ``w``, ``y``, ``h``,\n            ``landmarks_xy``, ``mask``. The additional key ``thumb`` will be provided if the\n            detected face object contains a thumbnail.\n        \"\"\"\n        if (self.left is None or self.width is None or self.top is None or self.height is None):\n            raise AssertionError(\"Some detected face variables have not been initialized\")\n        alignment = AlignmentFileDict(x=self.left,\n                                      w=self.width,\n                                      y=self.top,\n                                      h=self.height,\n                                      landmarks_xy=self.landmarks_xy,\n                                      mask={name: mask.to_dict()\n                                            for name, mask in self.mask.items()},\n                                      identity={k: v.tolist() for k, v in self._identity.items()},\n                                      thumb=self.thumbnail)\n        logger.trace(\"Returning: %s\", alignment)  # type:ignore[attr-defined]\n        return alignment\n\n    def from_alignment(self, alignment: AlignmentFileDict,\n                       image: np.ndarray | None = None, with_thumb: bool = False) -> None:\n        \"\"\" Set the attributes of this class from an alignments file and optionally load the face\n        into the ``image`` attribute.\n\n        Parameters\n        ----------\n        alignment: dict\n            A dictionary entry for a face from an alignments file containing the keys\n            ``x``, ``w``, ``y``, ``h``, ``landmarks_xy``.\n            Optionally the key ``thumb`` will be provided. This is for use in the manual tool and\n            contains the compressed jpg thumbnail of the face to be allocated to :attr:`thumbnail.\n            Optionally the key ``mask`` will be provided, but legacy alignments will not have\n            this key.\n        image: numpy.ndarray, optional\n            If an image is passed in, then the ``image`` attribute will\n            be set to the cropped face based on the passed in bounding box co-ordinates\n        with_thumb: bool, optional\n            Whether to load the jpg thumbnail into the detected face object, if provided.\n            Default: ``False``\n        \"\"\"\n\n        logger.trace(\"Creating from alignment: (alignment: %s,\"  # type:ignore[attr-defined]\n                     \" has_image: %s)\", alignment, bool(image is not None))\n        self.left = alignment[\"x\"]\n        self.width = alignment[\"w\"]\n        self.top = alignment[\"y\"]\n        self.height = alignment[\"h\"]\n        landmarks = alignment[\"landmarks_xy\"]\n        if not isinstance(landmarks, np.ndarray):\n            landmarks = np.array(landmarks, dtype=\"float32\")\n        self._identity = {T.cast(T.Literal[\"vggface2\"], k): np.array(v, dtype=\"float32\")\n                          for k, v in alignment.get(\"identity\", {}).items()}\n        self._landmarks_xy = landmarks.copy()\n\n        if with_thumb:\n            # Thumbnails currently only used for manual tool. Default to None\n            self.thumbnail = alignment.get(\"thumb\")\n        # Manual tool and legacy alignments will not have a mask\n        self._aligned = None\n\n        if alignment.get(\"mask\", None) is not None:\n            self.mask = {}\n            for name, mask_dict in alignment[\"mask\"].items():\n                self.mask[name] = Mask()\n                self.mask[name].from_dict(mask_dict)\n        if image is not None and image.any():\n            self._image_to_face(image)\n        logger.trace(\"Created from alignment: (left: %s, width: %s, \"  # type:ignore[attr-defined]\n                     \"top: %s, height: %s, landmarks: %s, mask: %s)\",\n                     self.left, self.width, self.top, self.height, self.landmarks_xy, self.mask)\n\n    def to_png_meta(self) -> PNGHeaderAlignmentsDict:\n        \"\"\" Return the detected face formatted for insertion into a png itxt header.\n\n        returns: dict\n            The alignments dict will be returned with the keys ``x``, ``w``, ``y``, ``h``,\n            ``landmarks_xy`` and ``mask``\n        \"\"\"\n        if (self.left is None or self.width is None or self.top is None or self.height is None):\n            raise AssertionError(\"Some detected face variables have not been initialized\")\n        alignment = PNGHeaderAlignmentsDict(\n            x=self.left,\n            w=self.width,\n            y=self.top,\n            h=self.height,\n            landmarks_xy=self.landmarks_xy.tolist(),\n            mask={name: mask.to_png_meta() for name, mask in self.mask.items()},\n            identity={k: v.tolist() for k, v in self._identity.items()})\n        return alignment\n\n    def from_png_meta(self, alignment: PNGHeaderAlignmentsDict) -> None:\n        \"\"\" Set the attributes of this class from alignments stored in a png exif header.\n\n        Parameters\n        ----------\n        alignment: dict\n            A dictionary entry for a face from alignments stored in a png exif header containing\n            the keys ``x``, ``w``, ``y``, ``h``, ``landmarks_xy`` and ``mask``\n        \"\"\"\n        self.left = alignment[\"x\"]\n        self.width = alignment[\"w\"]\n        self.top = alignment[\"y\"]\n        self.height = alignment[\"h\"]\n        self._landmarks_xy = np.array(alignment[\"landmarks_xy\"], dtype=\"float32\")\n        self.mask = {}\n        for name, mask_dict in alignment[\"mask\"].items():\n            self.mask[name] = Mask()\n            self.mask[name].from_dict(mask_dict)\n        self._identity = {}\n        for key, val in alignment.get(\"identity\", {}).items():\n            assert key in [\"vggface2\"]\n            self._identity[T.cast(T.Literal[\"vggface2\"], key)] = np.array(val, dtype=\"float32\")\n        logger.trace(\"Created from png exif header: (left: %s, \"  # type:ignore[attr-defined]\n                     \"width: %s, top: %s  height: %s, landmarks: %s, mask: %s, identity: %s)\",\n                     self.left, self.width, self.top, self.height, self.landmarks_xy, self.mask,\n                     {k: v.shape for k, v in self._identity.items()})\n\n    def _image_to_face(self, image: np.ndarray) -> None:\n        \"\"\" set self.image to be the cropped face from detected bounding box \"\"\"\n        logger.trace(\"Cropping face from image\")  # type:ignore[attr-defined]\n        self.image = image[self.top: self.bottom,\n                           self.left: self.right]\n\n    # <<< Aligned Face methods and properties >>> #\n    def load_aligned(self,\n                     image: np.ndarray | None,\n                     size: int = 256,\n                     dtype: str | None = None,\n                     centering: CenteringType = \"head\",\n                     coverage_ratio: float = 1.0,\n                     force: bool = False,\n                     is_aligned: bool = False,\n                     is_legacy: bool = False) -> None:\n        \"\"\" Align a face from a given image.\n\n        Aligning a face is a relatively expensive task and is not required for all uses of\n        the :class:`~lib.align.DetectedFace` object, so call this function explicitly to\n        load an aligned face.\n\n        This method plugs into :mod:`lib.align.AlignedFace` to perform face alignment based on this\n        face's ``landmarks_xy``. If the face has already been aligned, then this function will\n        return having performed no action.\n\n        Parameters\n        ----------\n        image: numpy.ndarray\n            The image that contains the face to be aligned\n        size: int\n            The size of the output face in pixels\n        dtype: str, optional\n            Optionally set a ``dtype`` for the final face to be formatted in. Default: ``None``\n        centering: [\"legacy\", \"face\", \"head\"], optional\n            The type of extracted face that should be loaded. \"legacy\" places the nose in the\n            center of the image (the original method for aligning). \"face\" aligns for the nose to\n            be in the center of the face (top to bottom) but the center of the skull for left to\n            right. \"head\" aligns for the center of the skull (in 3D space) being the center of the\n            extracted image, with the crop holding the full head.\n            Default: `\"head\"`\n        coverage_ratio: float, optional\n            The amount of the aligned image to return. A ratio of 1.0 will return the full contents\n            of the aligned image. A ratio of 0.5 will return an image of the given size, but will\n            crop to the central 50%% of the image. Default: `1.0`\n        force: bool, optional\n            Force an update of the aligned face, even if it is already loaded. Default: ``False``\n        is_aligned: bool, optional\n            Indicates that the :attr:`image` is an aligned face rather than a frame.\n            Default: ``False``\n        is_legacy: bool, optional\n            Only used if `is_aligned` is ``True``. ``True`` indicates that the aligned image being\n            loaded is a legacy extracted face rather than a current head extracted face\n        Notes\n        -----\n        This method must be executed to get access to the following an :class:`AlignedFace` object\n        \"\"\"\n        if self._aligned and not force:\n            # Don't reload an already aligned face\n            logger.trace(\"Skipping alignment calculation for already \"  # type:ignore[attr-defined]\n                         \"aligned face\")\n        else:\n            logger.trace(\"Loading aligned face: (size: %s, \"  # type:ignore[attr-defined]\n                         \"dtype: %s)\", size, dtype)\n            self._aligned = AlignedFace(self.landmarks_xy,\n                                        image=image,\n                                        centering=centering,\n                                        size=size,\n                                        coverage_ratio=coverage_ratio,\n                                        dtype=dtype,\n                                        is_aligned=is_aligned,\n                                        is_legacy=is_aligned and is_legacy)\n\n\n_HASHES_SEEN: dict[str, dict[str, int]] = {}\n\n\ndef update_legacy_png_header(filename: str, alignments: Alignments\n                             ) -> PNGHeaderDict | None:\n    \"\"\" Update a legacy extracted face from pre v2.1 alignments by placing the alignment data for\n    the face in the png exif header for the given filename with the given alignment data.\n\n    If the given file is not a .png then a png is created and the original file is removed\n\n    Parameters\n    ----------\n    filename: str\n        The image file to update\n    alignments: :class:`lib.align.alignments.Alignments`\n        The alignments data the contains the information to store in the image header. This must be\n        a v2.0 or less alignments file as later versions no longer store the face hash (not\n        required)\n\n    Returns\n    -------\n    dict\n        The metadata that has been applied to the given image\n    \"\"\"\n    if alignments.version > 2.0:\n        raise FaceswapError(\"The faces being passed in do not correspond to the given Alignments \"\n                            \"file. Please double check your sources and try again.\")\n    # Track hashes for multiple files with the same hash. Not the most robust but should be\n    # effective enough\n    folder = os.path.dirname(filename)\n    if folder not in _HASHES_SEEN:\n        _HASHES_SEEN[folder] = {}\n    hashes_seen = _HASHES_SEEN[folder]\n\n    in_image = read_image(filename, raise_error=True)\n    in_hash = sha1(in_image).hexdigest()\n    hashes_seen[in_hash] = hashes_seen.get(in_hash, -1) + 1\n\n    alignment = alignments.hashes_to_alignment.get(in_hash)\n    if not alignment:\n        logger.debug(\"Alignments not found for image: '%s'\", filename)\n        return None\n\n    detected_face = DetectedFace()\n    detected_face.from_alignment(alignment)\n    # For dupe hash handling, make sure we get a different filename for repeat hashes\n    src_fname, face_idx = list(alignments.hashes_to_frame[in_hash].items())[hashes_seen[in_hash]]\n    orig_filename = f\"{os.path.splitext(src_fname)[0]}_{face_idx}.png\"\n    meta = PNGHeaderDict(alignments=detected_face.to_png_meta(),\n                         source=PNGHeaderSourceDict(\n                            alignments_version=alignments.version,\n                            original_filename=orig_filename,\n                            face_index=face_idx,\n                            source_filename=src_fname,\n                            source_is_video=False,  # Can't check so set false\n                            source_frame_dims=None))\n\n    out_filename = f\"{os.path.splitext(filename)[0]}.png\"  # Make sure saved file is png\n    out_image = encode_image(in_image, \".png\", metadata=meta)\n\n    with open(out_filename, \"wb\") as out_file:\n        out_file.write(out_image)\n\n    if filename != out_filename:  # Remove the old non-png:\n        logger.debug(\"Removing replaced face with deprecated extension: '%s'\", filename)\n        os.remove(filename)\n\n    return meta\n", "lib/align/alignments.py": "#!/usr/bin/env python3\n\"\"\" Alignments file functions for reading, writing and manipulating the data stored in a\nserialized alignments file. \"\"\"\nfrom __future__ import annotations\nimport logging\nimport os\nimport typing as T\nfrom datetime import datetime\n\nimport numpy as np\n\nfrom lib.serializer import get_serializer, get_serializer_from_filename\nfrom lib.utils import FaceswapError\n\nfrom .thumbnails import Thumbnails\nfrom .updater import (FileStructure, IdentityAndVideoMeta, LandmarkRename, Legacy, ListToNumpy,\n                      MaskCentering, VideoExtension)\n\nif T.TYPE_CHECKING:\n    from collections.abc import Generator\n    from .aligned_face import CenteringType\n\nlogger = logging.getLogger(__name__)\n_VERSION = 2.4\n# VERSION TRACKING\n# 1.0 - Never really existed. Basically any alignments file prior to version 2.0\n# 2.0 - Implementation of full head extract. Any alignments version below this will have used\n#       legacy extract\n# 2.1 - Alignments data to extracted face PNG header. SHA1 hashes of faces no longer calculated\n#       or stored in alignments file\n# 2.2 - Add support for differently centered masks (i.e. not all masks stored as face centering)\n# 2.3 - Add 'identity' key to alignments file. May or may not be populated, to contain vggface2\n#       embeddings. Make 'video_meta' key a standard key. Can be unpopulated\n# 2.4 - Update video file alignment keys to end in the video extension rather than '.png'\n\n\n# TODO Convert these to Dataclasses\nclass MaskAlignmentsFileDict(T.TypedDict):\n    \"\"\" Typed Dictionary for storing Masks. \"\"\"\n    mask: bytes\n    affine_matrix: list[float] | np.ndarray\n    interpolator: int\n    stored_size: int\n    stored_centering: CenteringType\n\n\nclass PNGHeaderAlignmentsDict(T.TypedDict):\n    \"\"\" Base Dictionary for storing a single faces' Alignment Information in Alignments files and\n    PNG Headers. \"\"\"\n    x: int\n    y: int\n    w: int\n    h: int\n    landmarks_xy: list[float] | np.ndarray\n    mask: dict[str, MaskAlignmentsFileDict]\n    identity: dict[str, list[float]]\n\n\nclass AlignmentFileDict(PNGHeaderAlignmentsDict):\n    \"\"\" Typed Dictionary for storing a single faces' Alignment Information in alignments files. \"\"\"\n    thumb: np.ndarray | None\n\n\nclass PNGHeaderSourceDict(T.TypedDict):\n    \"\"\" Dictionary for storing additional meta information in PNG headers \"\"\"\n    alignments_version: float\n    original_filename: str\n    face_index: int\n    source_filename: str\n    source_is_video: bool\n    source_frame_dims: tuple[int, int] | None\n\n\nclass AlignmentDict(T.TypedDict):\n    \"\"\" Dictionary for holding all of the alignment information within a single alignment file \"\"\"\n    faces: list[AlignmentFileDict]\n    video_meta: dict[str, float | int]\n\n\nclass PNGHeaderDict(T.TypedDict):\n    \"\"\" Dictionary for storing all alignment and meta information in PNG Headers \"\"\"\n    alignments: PNGHeaderAlignmentsDict\n    source: PNGHeaderSourceDict\n\n\nclass Alignments():\n    \"\"\" The alignments file is a custom serialized ``.fsa`` file that holds information for each\n    frame for a video or series of images.\n\n    Specifically, it holds a list of faces that appear in each frame. Each face contains\n    information detailing their detected bounding box location within the frame, the 68 point\n    facial landmarks and any masks that have been extracted.\n\n    Additionally it can also hold video meta information (timestamp and whether a frame is a\n    key frame.)\n\n    Parameters\n    ----------\n    folder: str\n        The folder that contains the alignments ``.fsa`` file\n    filename: str, optional\n        The filename of the ``.fsa`` alignments file. If not provided then the given folder will be\n        checked for a default alignments file filename. Default: \"alignments\"\n    \"\"\"\n    def __init__(self, folder: str, filename: str = \"alignments\") -> None:\n        logger.debug(\"Initializing %s: (folder: '%s', filename: '%s')\",\n                     self.__class__.__name__, folder, filename)\n        self._io = _IO(self, folder, filename)\n        self._data = self._load()\n        self._io.update_legacy()\n\n        self._legacy = Legacy(self)\n        self._thumbnails = Thumbnails(self)\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    # << PROPERTIES >> #\n\n    @property\n    def frames_count(self) -> int:\n        \"\"\" int: The number of frames that appear in the alignments :attr:`data`. \"\"\"\n        retval = len(self._data)\n        logger.trace(retval)  # type:ignore[attr-defined]\n        return retval\n\n    @property\n    def faces_count(self) -> int:\n        \"\"\" int: The total number of faces that appear in the alignments :attr:`data`. \"\"\"\n        retval = sum(len(val[\"faces\"]) for val in self._data.values())\n        logger.trace(retval)  # type:ignore[attr-defined]\n        return retval\n\n    @property\n    def file(self) -> str:\n        \"\"\" str: The full path to the currently loaded alignments file. \"\"\"\n        return self._io.file\n\n    @property\n    def data(self) -> dict[str, AlignmentDict]:\n        \"\"\" dict: The loaded alignments :attr:`file` in dictionary form. \"\"\"\n        return self._data\n\n    @property\n    def have_alignments_file(self) -> bool:\n        \"\"\" bool: ``True`` if an alignments file exists at location :attr:`file` otherwise\n        ``False``. \"\"\"\n        return self._io.have_alignments_file\n\n    @property\n    def hashes_to_frame(self) -> dict[str, dict[str, int]]:\n        \"\"\" dict: The SHA1 hash of the face mapped to the frame(s) and face index within the frame\n        that the hash corresponds to.\n\n        Notes\n        -----\n        This method is depractated and exists purely for updating legacy hash based alignments\n        to new png header storage in :class:`lib.align.update_legacy_png_header`.\n        \"\"\"\n        return self._legacy.hashes_to_frame\n\n    @property\n    def hashes_to_alignment(self) -> dict[str, AlignmentFileDict]:\n        \"\"\" dict: The SHA1 hash of the face mapped to the alignment for the face that the hash\n        corresponds to. The structure of the dictionary is:\n\n        Notes\n        -----\n        This method is depractated and exists purely for updating legacy hash based alignments\n        to new png header storage in :class:`lib.align.update_legacy_png_header`.\n        \"\"\"\n        return self._legacy.hashes_to_alignment\n\n    @property\n    def mask_summary(self) -> dict[str, int]:\n        \"\"\" dict: The mask type names stored in the alignments :attr:`data` as key with the number\n        of faces which possess the mask type as value. \"\"\"\n        masks: dict[str, int] = {}\n        for val in self._data.values():\n            for face in val[\"faces\"]:\n                if face.get(\"mask\", None) is None:\n                    masks[\"none\"] = masks.get(\"none\", 0) + 1\n                for key in face.get(\"mask\", {}):\n                    masks[key] = masks.get(key, 0) + 1\n        return masks\n\n    @property\n    def video_meta_data(self) -> dict[str, list[int] | list[float] | None]:\n        \"\"\" dict: The frame meta data stored in the alignments file. If data does not exist in the\n        alignments file then ``None`` is returned for each Key \"\"\"\n        retval: dict[str, list[int] | list[float] | None] = {\"pts_time\": None, \"keyframes\": None}\n        pts_time: list[float] = []\n        keyframes: list[int] = []\n        for idx, key in enumerate(sorted(self.data)):\n            if not self.data[key].get(\"video_meta\", {}):\n                return retval\n            meta = self.data[key][\"video_meta\"]\n            pts_time.append(T.cast(float, meta[\"pts_time\"]))\n            if meta[\"keyframe\"]:\n                keyframes.append(idx)\n        retval = {\"pts_time\": pts_time, \"keyframes\": keyframes}\n        return retval\n\n    @property\n    def thumbnails(self) -> Thumbnails:\n        \"\"\" :class:`~lib.align.thumbnails.Thumbnails`: The low resolution thumbnail images that\n        exist within the alignments file \"\"\"\n        return self._thumbnails\n\n    @property\n    def version(self) -> float:\n        \"\"\" float: The alignments file version number. \"\"\"\n        return self._io.version\n\n    def _load(self) -> dict[str, AlignmentDict]:\n        \"\"\" Load the alignments data from the serialized alignments :attr:`file`.\n\n        Populates :attr:`_version` with the alignment file's loaded version as well as returning\n        the serialized data.\n\n        Returns\n        -------\n        dict:\n            The loaded alignments data\n        \"\"\"\n        return self._io.load()\n\n    def save(self) -> None:\n        \"\"\" Write the contents of :attr:`data` and :attr:`_meta` to a serialized ``.fsa`` file at\n        the location :attr:`file`. \"\"\"\n        return self._io.save()\n\n    def backup(self) -> None:\n        \"\"\" Create a backup copy of the alignments :attr:`file`.\n\n        Creates a copy of the serialized alignments :attr:`file` appending a\n        timestamp onto the end of the file name and storing in the same folder as\n        the original :attr:`file`.\n        \"\"\"\n        return self._io.backup()\n\n    def save_video_meta_data(self, pts_time: list[float], keyframes: list[int]) -> None:\n        \"\"\" Save video meta data to the alignments file.\n\n        If the alignments file does not have an entry for every frame (e.g. if Extract Every N\n        was used) then the frame is added to the alignments file with no faces, so that they video\n        meta data can be stored.\n\n        Parameters\n        ----------\n        pts_time: list\n            A list of presentation timestamps (`float`) in frame index order for every frame in\n            the input video\n        keyframes: list\n            A list of frame indices corresponding to the key frames in the input video\n        \"\"\"\n        if pts_time[0] != 0:\n            pts_time, keyframes = self._pad_leading_frames(pts_time, keyframes)\n\n        sample_filename = next(fname for fname in self.data)\n        basename = sample_filename[:sample_filename.rfind(\"_\")]\n        ext = os.path.splitext(sample_filename)[-1]\n        logger.debug(\"sample filename: '%s', base filename: '%s' extension: '%s'\",\n                     sample_filename, basename, ext)\n        logger.info(\"Saving video meta information to Alignments file\")\n\n        for idx, pts in enumerate(pts_time):\n            meta: dict[str, float | int] = {\"pts_time\": pts, \"keyframe\": idx in keyframes}\n            key = f\"{basename}_{idx + 1:06d}{ext}\"\n            if key not in self.data:\n                self.data[key] = {\"video_meta\": meta, \"faces\": []}\n            else:\n                self.data[key][\"video_meta\"] = meta\n\n        logger.debug(\"Alignments count: %s, timestamp count: %s\", len(self.data), len(pts_time))\n        if len(self.data) != len(pts_time):\n            raise FaceswapError(\n                \"There is a mismatch between the number of frames found in the video file \"\n                f\"({len(pts_time)}) and the number of frames found in the alignments file \"\n                f\"({len(self.data)}).\\nThis can be caused by a number of issues:\"\n                \"\\n  - The video has a Variable Frame Rate and FFMPEG is having a hard time \"\n                \"calculating the correct number of frames.\"\n                \"\\n  - You are working with a Merged Alignments file. This is not supported for \"\n                \"your current use case.\"\n                \"\\nYou should either extract the video to individual frames, re-encode the \"\n                \"video at a constant frame rate and re-run extraction or work with a dedicated \"\n                \"alignments file for your requested video.\")\n        self._io.save()\n\n    @classmethod\n    def _pad_leading_frames(cls, pts_time: list[float], keyframes: list[int]) -> tuple[list[float],\n                                                                                       list[int]]:\n        \"\"\" Calculate the number of frames to pad the video by when the first frame is not\n        a key frame.\n\n        A somewhat crude method by obtaining the gaps between existing frames and calculating\n        how many frames should be inserted at the beginning based on the first presentation\n        timestamp.\n\n        Parameters\n        ----------\n        pts_time: list\n            A list of presentation timestamps (`float`) in frame index order for every frame in\n            the input video\n        keyframes: list\n            A list of keyframes (`int`) for the input video\n\n        Returns\n        -------\n        tuple\n            The presentation time stamps with extra frames padded to the beginning and the\n            keyframes adjusted to include the new frames\n        \"\"\"\n        start_pts = pts_time[0]\n        logger.debug(\"Video not cut on keyframe. Start pts: %s\", start_pts)\n        gaps: list[float] = []\n        prev_time = None\n        for item in pts_time:\n            if prev_time is not None:\n                gaps.append(item - prev_time)\n            prev_time = item\n        data_points = len(gaps)\n        avg_gap = sum(gaps) / data_points\n        frame_count = int(round(start_pts / avg_gap))\n        pad_pts = [avg_gap * i for i in range(frame_count)]\n        logger.debug(\"data_points: %s, avg_gap: %s, frame_count: %s, pad_pts: %s\",\n                     data_points, avg_gap, frame_count, pad_pts)\n        pts_time = pad_pts + pts_time\n        keyframes = [i + frame_count for i in keyframes]\n        return pts_time, keyframes\n\n    # << VALIDATION >> #\n    def frame_exists(self, frame_name: str) -> bool:\n        \"\"\" Check whether a given frame_name exists within the alignments :attr:`data`.\n\n        Parameters\n        ----------\n        frame_name: str\n            The frame name to check. This should be the base name of the frame, not the full path\n\n        Returns\n        -------\n        bool\n            ``True`` if the given frame_name exists within the alignments :attr:`data`\n            otherwise ``False``\n        \"\"\"\n        retval = frame_name in self._data.keys()\n        logger.trace(\"'%s': %s\", frame_name, retval)  # type:ignore[attr-defined]\n        return retval\n\n    def frame_has_faces(self, frame_name: str) -> bool:\n        \"\"\" Check whether a given frame_name exists within the alignments :attr:`data` and contains\n        at least 1 face.\n\n        Parameters\n        ----------\n        frame_name: str\n            The frame name to check. This should be the base name of the frame, not the full path\n\n        Returns\n        -------\n        bool\n            ``True`` if the given frame_name exists within the alignments :attr:`data` and has at\n            least 1 face associated with it, otherwise ``False``\n        \"\"\"\n        frame_data = self._data.get(frame_name, T.cast(AlignmentDict, {}))\n        retval = bool(frame_data.get(\"faces\", []))\n        logger.trace(\"'%s': %s\", frame_name, retval)  # type:ignore[attr-defined]\n        return retval\n\n    def frame_has_multiple_faces(self, frame_name: str) -> bool:\n        \"\"\" Check whether a given frame_name exists within the alignments :attr:`data` and contains\n        more than 1 face.\n\n        Parameters\n        ----------\n        frame_name: str\n            The frame_name name to check. This should be the base name of the frame, not the full\n            path\n\n        Returns\n        -------\n        bool\n            ``True`` if the given frame_name exists within the alignments :attr:`data` and has more\n            than 1 face associated with it, otherwise ``False``\n        \"\"\"\n        if not frame_name:\n            retval = False\n        else:\n            frame_data = self._data.get(frame_name, T.cast(AlignmentDict, {}))\n            retval = bool(len(frame_data.get(\"faces\", [])) > 1)\n        logger.trace(\"'%s': %s\", frame_name, retval)  # type:ignore[attr-defined]\n        return retval\n\n    def mask_is_valid(self, mask_type: str) -> bool:\n        \"\"\" Ensure the given ``mask_type`` is valid for the alignments :attr:`data`.\n\n        Every face in the alignments :attr:`data` must have the given mask type to successfully\n        pass the test.\n\n        Parameters\n        ----------\n        mask_type: str\n            The mask type to check against the current alignments :attr:`data`\n\n        Returns\n        -------\n        bool:\n            ``True`` if all faces in the current alignments possess the given ``mask_type``\n            otherwise ``False``\n        \"\"\"\n        retval = any((face.get(\"mask\", None) is not None and\n                      face[\"mask\"].get(mask_type, None) is not None)\n                     for val in self._data.values()\n                     for face in val[\"faces\"])\n        logger.debug(retval)\n        return retval\n\n    # << DATA >> #\n    def get_faces_in_frame(self, frame_name: str) -> list[AlignmentFileDict]:\n        \"\"\" Obtain the faces from :attr:`data` associated with a given frame_name.\n\n        Parameters\n        ----------\n        frame_name: str\n            The frame name to return faces for. This should be the base name of the frame, not the\n            full path\n\n        Returns\n        -------\n        list\n            The list of face dictionaries that appear within the requested frame_name\n        \"\"\"\n        logger.trace(\"Getting faces for frame_name: '%s'\", frame_name)  # type:ignore[attr-defined]\n        frame_data = self._data.get(frame_name, T.cast(AlignmentDict, {}))\n        return frame_data.get(\"faces\", T.cast(list[AlignmentFileDict], []))\n\n    def count_faces_in_frame(self, frame_name: str) -> int:\n        \"\"\" Return number of faces that appear within :attr:`data` for the given frame_name.\n\n        Parameters\n        ----------\n        frame_name: str\n            The frame name to return the count for. This should be the base name of the frame, not\n            the full path\n\n        Returns\n        -------\n        int\n            The number of faces that appear in the given frame_name\n        \"\"\"\n        frame_data = self._data.get(frame_name, T.cast(AlignmentDict, {}))\n        retval = len(frame_data.get(\"faces\", []))\n        logger.trace(retval)  # type:ignore[attr-defined]\n        return retval\n\n    # << MANIPULATION >> #\n    def delete_face_at_index(self, frame_name: str, face_index: int) -> bool:\n        \"\"\" Delete the face for the given frame_name at the given face index from :attr:`data`.\n\n        Parameters\n        ----------\n        frame_name: str\n            The frame name to remove the face from. This should be the base name of the frame, not\n            the full path\n        face_index: int\n            The index number of the face within the given frame_name to remove\n\n        Returns\n        -------\n        bool\n            ``True`` if a face was successfully deleted otherwise ``False``\n        \"\"\"\n        logger.debug(\"Deleting face %s for frame_name '%s'\", face_index, frame_name)\n        face_index = int(face_index)\n        if face_index + 1 > self.count_faces_in_frame(frame_name):\n            logger.debug(\"No face to delete: (frame_name: '%s', face_index %s)\",\n                         frame_name, face_index)\n            return False\n        del self._data[frame_name][\"faces\"][face_index]\n        logger.debug(\"Deleted face: (frame_name: '%s', face_index %s)\", frame_name, face_index)\n        return True\n\n    def add_face(self, frame_name: str, face: AlignmentFileDict) -> int:\n        \"\"\" Add a new face for the given frame_name in :attr:`data` and return it's index.\n\n        Parameters\n        ----------\n        frame_name: str\n            The frame name to add the face to. This should be the base name of the frame, not the\n            full path\n        face: dict\n            The face information to add to the given frame_name, correctly formatted for storing in\n            :attr:`data`\n\n        Returns\n        -------\n        int\n            The index of the newly added face within :attr:`data` for the given frame_name\n        \"\"\"\n        logger.debug(\"Adding face to frame_name: '%s'\", frame_name)\n        if frame_name not in self._data:\n            self._data[frame_name] = {\"faces\": [], \"video_meta\": {}}\n        self._data[frame_name][\"faces\"].append(face)\n        retval = self.count_faces_in_frame(frame_name) - 1\n        logger.debug(\"Returning new face index: %s\", retval)\n        return retval\n\n    def update_face(self, frame_name: str, face_index: int, face: AlignmentFileDict) -> None:\n        \"\"\" Update the face for the given frame_name at the given face index in :attr:`data`.\n\n        Parameters\n        ----------\n        frame_name: str\n            The frame name to update the face for. This should be the base name of the frame, not\n            the full path\n        face_index: int\n            The index number of the face within the given frame_name to update\n        face: dict\n            The face information to update to the given frame_name at the given face_index,\n            correctly formatted for storing in :attr:`data`\n        \"\"\"\n        logger.debug(\"Updating face %s for frame_name '%s'\", face_index, frame_name)\n        self._data[frame_name][\"faces\"][face_index] = face\n\n    def filter_faces(self, filter_dict: dict[str, list[int]], filter_out: bool = False) -> None:\n        \"\"\" Remove faces from :attr:`data` based on a given filter list.\n\n        Parameters\n        ----------\n        filter_dict: dict\n            Dictionary of source filenames as key with a list of face indices to filter as value.\n        filter_out: bool, optional\n            ``True`` if faces should be removed from :attr:`data` when there is a corresponding\n            match in the given filter_dict. ``False`` if faces should be kept in :attr:`data` when\n            there is a corresponding match in the given filter_dict, but removed if there is no\n            match. Default: ``False``\n        \"\"\"\n        logger.debug(\"filter_dict: %s, filter_out: %s\", filter_dict, filter_out)\n        for source_frame, frame_data in self._data.items():\n            face_indices = filter_dict.get(source_frame, [])\n            if filter_out:\n                filter_list = face_indices\n            else:\n                filter_list = [idx for idx in range(len(frame_data[\"faces\"]))\n                               if idx not in face_indices]\n            logger.trace(\"frame: '%s', filter_list: %s\",  # type:ignore[attr-defined]\n                         source_frame, filter_list)\n\n            for face_idx in reversed(sorted(filter_list)):\n                logger.verbose(  # type:ignore[attr-defined]\n                    \"Filtering out face: (filename: %s, index: %s)\", source_frame, face_idx)\n                del frame_data[\"faces\"][face_idx]\n\n    def update_from_dict(self, data: dict[str, AlignmentDict]) -> None:\n        \"\"\" Replace all alignments with the contents of the given dictionary\n\n        Parameters\n        ----------\n        data: dict[str, AlignmentDict]\n            The alignments, in correctly formatted dictionary form, to be populated into this\n            :class:`Alignments`\n        \"\"\"\n        logger.debug(\"Populating alignments with %s entries\", len(data))\n        self._data = data\n\n    # << GENERATORS >> #\n    def yield_faces(self) -> Generator[tuple[str, list[AlignmentFileDict], int, str], None, None]:\n        \"\"\" Generator to obtain all faces with meta information from :attr:`data`. The results\n        are yielded by frame.\n\n        Notes\n        -----\n        The yielded order is non-deterministic.\n\n        Yields\n        ------\n        frame_name: str\n            The frame name that the face belongs to. This is the base name of the frame, as it\n            appears in :attr:`data`, not the full path\n        faces: list\n            The list of face `dict` objects that exist for this frame\n        face_count: int\n            The number of faces that exist within :attr:`data` for this frame\n        frame_fullname: str\n            The full path (folder and filename) for the yielded frame\n        \"\"\"\n        for frame_fullname, val in self._data.items():\n            frame_name = os.path.splitext(frame_fullname)[0]\n            face_count = len(val[\"faces\"])\n            logger.trace(  # type:ignore[attr-defined]\n                \"Yielding: (frame: '%s', faces: %s, frame_fullname: '%s')\",\n                frame_name, face_count, frame_fullname)\n            yield frame_name, val[\"faces\"], face_count, frame_fullname\n\n    def update_legacy_has_source(self, filename: str) -> None:\n        \"\"\" Update legacy alignments files when we have the source filename available.\n\n        Updates here can only be performed when we have the source filename\n\n        Parameters\n        ----------\n        filename: str:\n            The filename/folder of the original source images/video for the current alignments\n        \"\"\"\n        updates = [updater.is_updated for updater in (VideoExtension(self, filename), )]\n        if any(updates):\n            self._io.update_version()\n            self.save()\n\n\nclass _IO():\n    \"\"\" Class to handle the saving/loading of an alignments file.\n\n    Parameters\n    ----------\n    alignments: :class:'~Alignments`\n        The parent alignments class that these IO operations belong to\n    folder: str\n        The folder that contains the alignments ``.fsa`` file\n    filename: str\n        The filename of the ``.fsa`` alignments file.\n    \"\"\"\n    def __init__(self, alignments: Alignments, folder: str, filename: str) -> None:\n        logger.debug(\"Initializing %s: (alignments: %s)\", self.__class__.__name__, alignments)\n        self._alignments = alignments\n        self._serializer = get_serializer(\"compressed\")\n        self._file = self._get_location(folder, filename)\n        self._version: float = _VERSION\n\n    @property\n    def file(self) -> str:\n        \"\"\" str: The full path to the currently loaded alignments file. \"\"\"\n        return self._file\n\n    @property\n    def version(self) -> float:\n        \"\"\" float: The alignments file version number. \"\"\"\n        return self._version\n\n    @property\n    def have_alignments_file(self) -> bool:\n        \"\"\" bool: ``True`` if an alignments file exists at location :attr:`file` otherwise\n        ``False``. \"\"\"\n        retval = os.path.exists(self._file)\n        logger.trace(retval)  # type:ignore[attr-defined]\n        return retval\n\n    def _update_file_format(self, folder: str, filename: str) -> str:\n        \"\"\" Convert old style serialized alignments to new ``.fsa`` format.\n\n        Parameters\n        ----------\n        folder: str\n            The folder that the legacy alignments exist in\n        filename: str\n            The file name of the legacy alignments\n\n        Returns\n        -------\n        str\n            The full path to the newly created ``.fsa`` alignments file\n        \"\"\"\n        logger.info(\"Reformatting legacy alignments file...\")\n        old_location = os.path.join(str(folder), filename)\n        new_location = f\"{os.path.splitext(old_location)[0]}.{self._serializer.file_extension}\"\n        if os.path.exists(old_location):\n            if os.path.exists(new_location):\n                logger.info(\"Using existing updated alignments file found at '%s'. If you do not \"\n                            \"wish to use this existing file then you should delete or rename it.\",\n                            new_location)\n            else:\n                logger.info(\"Old location: '%s', New location: '%s'\", old_location, new_location)\n                load_serializer = get_serializer_from_filename(old_location)\n                data = load_serializer.load(old_location)\n                self._serializer.save(new_location, data)\n        return os.path.basename(new_location)\n\n    def _test_for_legacy(self, location: str) -> None:\n        \"\"\" For alignments filenames passed in without an extension, test for legacy\n        serialization formats and update to current ``.fsa`` format if any are found.\n\n        Parameters\n        ----------\n        location: str\n            The folder location to check for legacy alignments\n        \"\"\"\n        logger.debug(\"Checking for legacy alignments file formats: '%s'\", location)\n        filename = os.path.splitext(location)[0]\n        for ext in (\".json\", \".p\", \".pickle\", \".yaml\"):\n            legacy_filename = f\"{filename}{ext}\"\n            if os.path.exists(legacy_filename):\n                logger.debug(\"Legacy alignments file exists: '%s'\", legacy_filename)\n                _ = self._update_file_format(*os.path.split(legacy_filename))\n                break\n            logger.debug(\"Legacy alignments file does not exist: '%s'\", legacy_filename)\n\n    def _get_location(self, folder: str, filename: str) -> str:\n        \"\"\" Obtains the location of an alignments file.\n\n        If a legacy alignments file is provided/discovered, then the alignments file will be\n        updated to the custom ``.fsa`` format and saved.\n\n        Parameters\n        ----------\n        folder: str\n            The folder that the alignments file is located in\n        filename: str\n            The filename of the alignments file\n\n        Returns\n        -------\n        str\n            The full path to the alignments file\n        \"\"\"\n        logger.debug(\"Getting location: (folder: '%s', filename: '%s')\", folder, filename)\n        noext_name, extension = os.path.splitext(filename)\n        if extension in (\".json\", \".p\", \".pickle\", \".yaml\", \".yml\"):\n            # Reformat legacy alignments file\n            filename = self._update_file_format(folder, filename)\n            logger.debug(\"Updated legacy alignments. New filename: '%s'\", filename)\n        if extension[1:] == self._serializer.file_extension:\n            logger.debug(\"Valid Alignments filename provided: '%s'\", filename)\n        else:\n            filename = f\"{noext_name}.{self._serializer.file_extension}\"\n            logger.debug(\"File extension set from serializer: '%s'\",\n                         self._serializer.file_extension)\n        location = os.path.join(str(folder), filename)\n        if not os.path.exists(location):\n            # Test for old format alignments files and reformat if they exist. This will be\n            # executed if an alignments file has not been explicitly provided therefore it will not\n            # have been picked up in the extension test\n            self._test_for_legacy(location)\n        logger.verbose(\"Alignments filepath: '%s'\", location)  # type:ignore[attr-defined]\n        return location\n\n    def update_legacy(self) -> None:\n        \"\"\" Check whether the alignments are legacy, and if so update them to current alignments\n        format. \"\"\"\n        updates = [updater.is_updated for updater in (FileStructure(self._alignments),\n                                                      LandmarkRename(self._alignments),\n                                                      ListToNumpy(self._alignments),\n                                                      MaskCentering(self._alignments),\n                                                      IdentityAndVideoMeta(self._alignments))]\n        if any(updates):\n            self.update_version()\n            self.save()\n\n    def update_version(self) -> None:\n        \"\"\" Update the version of the alignments file to the latest version \"\"\"\n        self._version = _VERSION\n        logger.info(\"Updating alignments file to version %s\", self._version)\n\n    def load(self) -> dict[str, AlignmentDict]:\n        \"\"\" Load the alignments data from the serialized alignments :attr:`file`.\n\n        Populates :attr:`_version` with the alignment file's loaded version as well as returning\n        the serialized data.\n\n        Returns\n        -------\n        dict:\n            The loaded alignments data\n        \"\"\"\n        logger.debug(\"Loading alignments\")\n        if not self.have_alignments_file:\n            raise FaceswapError(f\"Error: Alignments file not found at {self._file}\")\n\n        logger.info(\"Reading alignments from: '%s'\", self._file)\n        data = self._serializer.load(self._file)\n        meta = data.get(\"__meta__\", {\"version\": 1.0})\n        self._version = meta[\"version\"]\n        data = data.get(\"__data__\", data)\n        logger.debug(\"Loaded alignments\")\n        return data\n\n    def save(self) -> None:\n        \"\"\" Write the contents of :attr:`data` and :attr:`_meta` to a serialized ``.fsa`` file at\n        the location :attr:`file`. \"\"\"\n        logger.debug(\"Saving alignments\")\n        logger.info(\"Writing alignments to: '%s'\", self._file)\n        data = {\"__meta__\": {\"version\": self._version},\n                \"__data__\": self._alignments.data}\n        self._serializer.save(self._file, data)\n        logger.debug(\"Saved alignments\")\n\n    def backup(self) -> None:\n        \"\"\" Create a backup copy of the alignments :attr:`file`.\n\n        Creates a copy of the serialized alignments :attr:`file` appending a\n        timestamp onto the end of the file name and storing in the same folder as\n        the original :attr:`file`.\n        \"\"\"\n        logger.debug(\"Backing up alignments\")\n        if not os.path.isfile(self._file):\n            logger.debug(\"No alignments to back up\")\n            return\n        now = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        src = self._file\n        split = os.path.splitext(src)\n        dst = f\"{split[0]}_{now}{split[1]}\"\n        idx = 1\n        while True:\n            if not os.path.exists(dst):\n                break\n            logger.debug(\"Backup file %s exists. Incrementing\", dst)\n            dst = f\"{split[0]}_{now}({idx}){split[1]}\"\n            idx += 1\n\n        logger.info(\"Backing up original alignments to '%s'\", dst)\n        os.rename(src, dst)\n        logger.debug(\"Backed up alignments\")\n", "lib/align/updater.py": "#!/usr/bin/env python3\n\"\"\" Handles updating of an alignments file from an older version to the current version. \"\"\"\nfrom __future__ import annotations\n\nimport logging\nimport os\nimport typing as T\n\nimport numpy as np\n\nfrom lib.logger import parse_class_init\nfrom lib.utils import VIDEO_EXTENSIONS\n\nlogger = logging.getLogger(__name__)\n\nif T.TYPE_CHECKING:\n    from .alignments import Alignments, AlignmentFileDict\n\n\nclass _Updater():\n    \"\"\" Base class for inheriting to test for and update of an alignments file property\n\n    Parameters\n    ----------\n    alignments: :class:`~Alignments`\n        The alignments object that is being tested and updated\n    \"\"\"\n    def __init__(self, alignments: Alignments) -> None:\n        logger.debug(parse_class_init(locals()))\n        self._alignments = alignments\n        self._needs_update = self._test()\n        if self._needs_update:\n            self._update()\n        logger.debug(\"Initialized: %s\", self.__class__.__name__)\n\n    @property\n    def is_updated(self) -> bool:\n        \"\"\" bool. ``True`` if this updater has been run otherwise ``False`` \"\"\"\n        return self._needs_update\n\n    def _test(self) -> bool:\n        \"\"\" Calls the child's :func:`test` method and logs output\n\n        Returns\n        -------\n        bool\n            ``True`` if the test condition is met otherwise ``False``\n        \"\"\"\n        logger.debug(\"checking %s\", self.__class__.__name__)\n        retval = self.test()\n        logger.debug(\"legacy %s: %s\", self.__class__.__name__, retval)\n        return retval\n\n    def test(self) -> bool:\n        \"\"\" Override to set the condition to test for.\n\n        Returns\n        -------\n        bool\n            ``True`` if the test condition is met otherwise ``False``\n        \"\"\"\n        raise NotImplementedError()\n\n    def _update(self) -> int:\n        \"\"\" Calls the child's :func:`update` method, logs output and sets the\n        :attr:`is_updated` flag\n\n        Returns\n        -------\n        int\n            The number of items that were updated\n        \"\"\"\n        retval = self.update()\n        logger.debug(\"Updated %s: %s\", self.__class__.__name__, retval)\n        return retval\n\n    def update(self) -> int:\n        \"\"\" Override to set the action to perform on the alignments object if the test has\n        passed\n\n        Returns\n        -------\n        int\n            The number of items that were updated\n        \"\"\"\n        raise NotImplementedError()\n\n\nclass VideoExtension(_Updater):\n    \"\"\" Alignments files from video files used to have a dummy '.png' extension for each of the\n    keys. This has been changed to be file extension of the original input video (for better)\n    identification of alignments files generated from video files\n\n    Parameters\n    ----------\n    alignments: :class:`~Alignments`\n        The alignments object that is being tested and updated\n    video_filename: str\n        The video filename that holds these alignments\n    \"\"\"\n    def __init__(self, alignments: Alignments, video_filename: str) -> None:\n        self._video_name, self._extension = os.path.splitext(video_filename)\n        super().__init__(alignments)\n\n    def test(self) -> bool:\n        \"\"\" Requires update if the extension of the key in the alignment file is not the same\n        as for the input video file\n\n        Returns\n        -------\n        bool\n            ``True`` if the key extensions need updating otherwise ``False``\n        \"\"\"\n        # Note: Don't check on alignments file version. It's possible that the file gets updated to\n        # a newer version before this check is run\n        if self._extension.lower() not in VIDEO_EXTENSIONS:\n            return False\n\n        exts = set(os.path.splitext(k)[-1] for k in self._alignments.data)\n        if len(exts) != 1:\n            logger.debug(\"Alignments file has multiple key extensions. Skipping\")\n            return False\n\n        if self._extension in exts:\n            logger.debug(\"Alignments file contains correct key extensions. Skipping\")\n            return False\n\n        logger.debug(\"Needs update for video extension (version: %s, extension: %s)\",\n                     self._alignments.version, self._extension)\n        return True\n\n    def update(self) -> int:\n        \"\"\" Update alignments files that have been extracted from videos to have the key end in the\n        video file extension rather than ',png' (the old way)\n\n        Parameters\n        ----------\n        video_filename: str\n            The filename of the video file that created these alignments\n        \"\"\"\n        updated = 0\n        for key in list(self._alignments.data):\n            fname = os.path.splitext(key)[0]\n            if fname.rsplit(\"_\", maxsplit=1)[0] != self._video_name:\n                continue  # Key is from a different source\n\n            val = self._alignments.data[key]\n            new_key = f\"{fname}{self._extension}\"\n\n            del self._alignments.data[key]\n            self._alignments.data[new_key] = val\n\n            updated += 1\n\n        logger.debug(\"Updated alignment keys for video extension: %s\", updated)\n        return updated\n\n\nclass FileStructure(_Updater):\n    \"\"\" Alignments were structured: {frame_name: <list of faces>}. We need to be able to store\n    information at the frame level, so new structure is:  {frame_name: {faces: <list of faces>}}\n    \"\"\"\n    def test(self) -> bool:\n        \"\"\" Test whether the alignments file is laid out in the old structure of\n        `{frame_name: [faces]}`\n\n        Returns\n        -------\n        bool\n            ``True`` if the file has legacy structure otherwise ``False``\n        \"\"\"\n        return any(isinstance(val, list) for val in self._alignments.data.values())\n\n    def update(self) -> int:\n        \"\"\" Update legacy alignments files from the format `{frame_name: [faces}` to the\n        format `{frame_name: {faces: [faces]}`.\n\n        Returns\n        -------\n        int\n            The number of items that were updated\n        \"\"\"\n        updated = 0\n        for key, val in self._alignments.data.items():\n            if not isinstance(val, list):\n                continue\n            self._alignments.data[key] = {\"faces\": val}\n            updated += 1\n        return updated\n\n\nclass LandmarkRename(_Updater):\n    \"\"\" Landmarks renamed from landmarksXY to landmarks_xy for PEP compliance \"\"\"\n    def test(self) -> bool:\n        \"\"\" check for legacy landmarksXY keys.\n\n        Returns\n        -------\n        bool\n            ``True`` if the alignments file contains legacy `landmarksXY` keys otherwise ``False``\n        \"\"\"\n        return (any(key == \"landmarksXY\"\n                    for val in self._alignments.data.values()\n                    for alignment in val[\"faces\"]\n                    for key in alignment))\n\n    def update(self) -> int:\n        \"\"\" Update legacy `landmarksXY` keys to PEP compliant `landmarks_xy` keys.\n\n        Returns\n        -------\n        int\n            The number of landmarks keys that were changed\n        \"\"\"\n        update_count = 0\n        for val in self._alignments.data.values():\n            for alignment in val[\"faces\"]:\n                if \"landmarksXY\" in alignment:\n                    alignment[\"landmarks_xy\"] = alignment.pop(\"landmarksXY\")  # type:ignore\n                    update_count += 1\n        return update_count\n\n\nclass ListToNumpy(_Updater):\n    \"\"\" Landmarks stored as list instead of numpy array \"\"\"\n    def test(self) -> bool:\n        \"\"\" check for legacy landmarks stored as `list` rather than :class:`numpy.ndarray`.\n\n        Returns\n        -------\n        bool\n            ``True`` if not all landmarks are :class:`numpy.ndarray` otherwise ``False``\n        \"\"\"\n        return not all(isinstance(face[\"landmarks_xy\"], np.ndarray)\n                       for val in self._alignments.data.values()\n                       for face in val[\"faces\"])\n\n    def update(self) -> int:\n        \"\"\" Update landmarks stored as `list` to :class:`numpy.ndarray`.\n\n        Returns\n        -------\n        int\n            The number of landmarks keys that were changed\n        \"\"\"\n        update_count = 0\n        for val in self._alignments.data.values():\n            for alignment in val[\"faces\"]:\n                test = alignment[\"landmarks_xy\"]\n                if not isinstance(test, np.ndarray):\n                    alignment[\"landmarks_xy\"] = np.array(test, dtype=\"float32\")\n                    update_count += 1\n        return update_count\n\n\nclass MaskCentering(_Updater):\n    \"\"\" Masks not containing the stored_centering parameters. Prior to this implementation all\n    masks were stored with face centering \"\"\"\n\n    def test(self) -> bool:\n        \"\"\" Mask centering was introduced in alignments version 2.2\n\n        Returns\n        -------\n        bool\n            ``True`` mask centering requires updating otherwise ``False``\n        \"\"\"\n        return self._alignments.version < 2.2\n\n    def update(self) -> int:\n        \"\"\" Add the mask key to the alignment file and update the centering of existing masks\n\n        Returns\n        -------\n        int\n            The number of masks that were updated\n        \"\"\"\n        update_count = 0\n        for val in self._alignments.data.values():\n            for alignment in val[\"faces\"]:\n                if \"mask\" not in alignment:\n                    alignment[\"mask\"] = {}\n                for mask in alignment[\"mask\"].values():\n                    mask[\"stored_centering\"] = \"face\"\n                    update_count += 1\n        return update_count\n\n\nclass IdentityAndVideoMeta(_Updater):\n    \"\"\" Prior to version 2.3 the identity key did not exist and the video_meta key was not\n    compulsory. These should now both always appear, but do not need to be populated. \"\"\"\n\n    def test(self) -> bool:\n        \"\"\" Identity Key was introduced in alignments version 2.3\n\n        Returns\n        -------\n        bool\n            ``True`` identity key needs inserting otherwise ``False``\n        \"\"\"\n        return self._alignments.version < 2.3\n\n    # Identity information was not previously stored in the alignments file.\n    def update(self) -> int:\n        \"\"\" Add the video_meta and identity keys to the alignment file and leave empty\n\n        Returns\n        -------\n        int\n            The number of keys inserted\n        \"\"\"\n        update_count = 0\n        for val in self._alignments.data.values():\n            this_update = 0\n            if \"video_meta\" not in val:\n                val[\"video_meta\"] = {}\n                this_update = 1\n            for alignment in val[\"faces\"]:\n                if \"identity\" not in alignment:\n                    alignment[\"identity\"] = {}\n                    this_update = 1\n                update_count += this_update\n        return update_count\n\n\nclass Legacy():\n    \"\"\" Legacy alignments properties that are no longer used, but are still required for backwards\n    compatibility/upgrading reasons.\n\n    Parameters\n    ----------\n    alignments: :class:`~Alignments`\n        The alignments object that requires these legacy properties\n    \"\"\"\n    def __init__(self, alignments: Alignments) -> None:\n        self._alignments = alignments\n        self._hashes_to_frame: dict[str, dict[str, int]] = {}\n        self._hashes_to_alignment: dict[str, AlignmentFileDict] = {}\n\n    @property\n    def hashes_to_frame(self) -> dict[str, dict[str, int]]:\n        \"\"\" dict: The SHA1 hash of the face mapped to the frame(s) and face index within the frame\n        that the hash corresponds to. The structure of the dictionary is:\n\n        {**SHA1_hash** (`str`): {**filename** (`str`): **face_index** (`int`)}}.\n\n        Notes\n        -----\n        This method is deprecated and exists purely for updating legacy hash based alignments\n        to new png header storage in :class:`lib.align.update_legacy_png_header`.\n\n        The first time this property is referenced, the dictionary will be created and cached.\n        Subsequent references will be made to this cached dictionary.\n        \"\"\"\n        if not self._hashes_to_frame:\n            logger.debug(\"Generating hashes to frame\")\n            for frame_name, val in self._alignments.data.items():\n                for idx, face in enumerate(val[\"faces\"]):\n                    self._hashes_to_frame.setdefault(\n                        face[\"hash\"], {})[frame_name] = idx  # type:ignore\n        return self._hashes_to_frame\n\n    @property\n    def hashes_to_alignment(self) -> dict[str, AlignmentFileDict]:\n        \"\"\" dict: The SHA1 hash of the face mapped to the alignment for the face that the hash\n        corresponds to. The structure of the dictionary is:\n\n        Notes\n        -----\n        This method is deprecated and exists purely for updating legacy hash based alignments\n        to new png header storage in :class:`lib.align.update_legacy_png_header`.\n\n        The first time this property is referenced, the dictionary will be created and cached.\n        Subsequent references will be made to this cached dictionary.\n        \"\"\"\n        if not self._hashes_to_alignment:\n            logger.debug(\"Generating hashes to alignment\")\n            self._hashes_to_alignment = {face[\"hash\"]: face  # type:ignore\n                                         for val in self._alignments.data.values()\n                                         for face in val[\"faces\"]}\n        return self._hashes_to_alignment\n", "lib/align/aligned_mask.py": "#!/usr/bin python3\n\"\"\" Handles retrieval and storage of Faceswap aligned masks \"\"\"\n\nfrom __future__ import annotations\nimport logging\nimport typing as T\n\nfrom zlib import compress, decompress\n\nimport cv2\nimport numpy as np\n\nfrom lib.logger import parse_class_init\n\nfrom .alignments import MaskAlignmentsFileDict\nfrom . import get_adjusted_center, get_centered_size\n\nif T.TYPE_CHECKING:\n    from collections.abc import Callable\n    from .aligned_face import CenteringType\n\nlogger = logging.getLogger(__name__)\n\n\nclass Mask():\n    \"\"\" Face Mask information and convenience methods\n\n    Holds a Faceswap mask as generated from :mod:`plugins.extract.mask` and the information\n    required to transform it to its original frame.\n\n    Holds convenience methods to handle the warping, storing and retrieval of the mask.\n\n    Parameters\n    ----------\n    storage_size: int, optional\n        The size (in pixels) that the mask should be stored at. Default: 128.\n    storage_centering, str (optional):\n        The centering to store the mask at. One of `\"legacy\"`, `\"face\"`, `\"head\"`.\n        Default: `\"face\"`\n\n    Attributes\n    ----------\n    stored_size: int\n        The size, in pixels, of the stored mask across its height and width.\n    stored_centering: str\n        The centering that the mask is stored at. One of `\"legacy\"`, `\"face\"`, `\"head\"`\n    \"\"\"\n    def __init__(self,\n                 storage_size: int = 128,\n                 storage_centering: CenteringType = \"face\") -> None:\n        logger.trace(parse_class_init(locals()))  # type:ignore[attr-defined]\n        self.stored_size = storage_size\n        self.stored_centering = storage_centering\n\n        self._mask: bytes | None = None\n        self._affine_matrix: np.ndarray | None = None\n        self._interpolator: int | None = None\n\n        self._blur_type: T.Literal[\"gaussian\", \"normalized\"] | None = None\n        self._blur_passes: int = 0\n        self._blur_kernel: float | int = 0\n        self._threshold = 0.0\n        self._dilation: tuple[T.Literal[\"erode\", \"dilate\"], np.ndarray | None] = (\"erode\", None)\n        self._sub_crop_size = 0\n        self._sub_crop_slices: dict[T.Literal[\"in\", \"out\"], list[slice]] = {}\n\n        self.set_blur_and_threshold()\n        logger.trace(\"Initialized: %s\", self.__class__.__name__)  # type:ignore[attr-defined]\n\n    @property\n    def mask(self) -> np.ndarray:\n        \"\"\" :class:`numpy.ndarray`: The mask at the size of :attr:`stored_size` with any requested\n        blurring, threshold amount and centering applied.\"\"\"\n        mask = self.stored_mask\n        if self._dilation[-1] is not None or self._threshold != 0.0 or self._blur_kernel != 0:\n            mask = mask.copy()\n        self._dilate_mask(mask)\n        if self._threshold != 0.0:\n            mask[mask < self._threshold] = 0.0\n            mask[mask > 255.0 - self._threshold] = 255.0\n        if self._blur_kernel != 0 and self._blur_type is not None:\n            mask = BlurMask(self._blur_type,\n                            mask,\n                            self._blur_kernel,\n                            passes=self._blur_passes).blurred\n        if self._sub_crop_size:  # Crop the mask to the given centering\n            out = np.zeros((self._sub_crop_size, self._sub_crop_size, 1), dtype=mask.dtype)\n            slice_in, slice_out = self._sub_crop_slices[\"in\"], self._sub_crop_slices[\"out\"]\n            out[slice_out[0], slice_out[1], :] = mask[slice_in[0], slice_in[1], :]\n            mask = out\n        logger.trace(\"mask shape: %s\", mask.shape)  # type:ignore[attr-defined]\n        return mask\n\n    @property\n    def stored_mask(self) -> np.ndarray:\n        \"\"\" :class:`numpy.ndarray`: The mask at the size of :attr:`stored_size` as it is stored\n        (i.e. with no blurring/centering applied). \"\"\"\n        assert self._mask is not None\n        dims = (self.stored_size, self.stored_size, 1)\n        mask = np.frombuffer(decompress(self._mask), dtype=\"uint8\").reshape(dims)\n        logger.trace(\"stored mask shape: %s\", mask.shape)  # type:ignore[attr-defined]\n        return mask\n\n    @property\n    def original_roi(self) -> np.ndarray:\n        \"\"\" :class: `numpy.ndarray`: The original region of interest of the mask in the\n        source frame. \"\"\"\n        points = np.array([[0, 0],\n                           [0, self.stored_size - 1],\n                           [self.stored_size - 1, self.stored_size - 1],\n                           [self.stored_size - 1, 0]], np.int32).reshape((-1, 1, 2))\n        matrix = cv2.invertAffineTransform(self.affine_matrix)\n        roi = cv2.transform(points, matrix).reshape((4, 2))\n        logger.trace(\"Returning: %s\", roi)  # type:ignore[attr-defined]\n        return roi\n\n    @property\n    def affine_matrix(self) -> np.ndarray:\n        \"\"\" :class: `numpy.ndarray`: The affine matrix to transpose the mask to a full frame. \"\"\"\n        assert self._affine_matrix is not None\n        return self._affine_matrix\n\n    @property\n    def interpolator(self) -> int:\n        \"\"\" int: The cv2 interpolator required to transpose the mask to a full frame. \"\"\"\n        assert self._interpolator is not None\n        return self._interpolator\n\n    def _dilate_mask(self, mask: np.ndarray) -> None:\n        \"\"\" Erode/Dilate the mask. The action is performed in-place on the given mask.\n\n        No action is performed if a dilation amount has not been set\n\n        Parameters\n        ----------\n        mask: :class:`numpy.ndarray`\n            The mask to be eroded/dilated\n        \"\"\"\n        if self._dilation[-1] is None:\n            return\n\n        func = cv2.erode if self._dilation[0] == \"erode\" else cv2.dilate\n        func(mask, self._dilation[-1], dst=mask, iterations=1)\n\n    def get_full_frame_mask(self, width: int, height: int) -> np.ndarray:\n        \"\"\" Return the stored mask in a full size frame of the given dimensions\n\n        Parameters\n        ----------\n        width: int\n            The width of the original frame that the mask was extracted from\n        height: int\n            The height of the original frame that the mask was extracted from\n\n        Returns\n        -------\n        :class:`numpy.ndarray`: The mask affined to the original full frame of the given dimensions\n        \"\"\"\n        frame = np.zeros((width, height, 1), dtype=\"uint8\")\n        mask = cv2.warpAffine(self.mask,\n                              self.affine_matrix,\n                              (width, height),\n                              frame,\n                              flags=cv2.WARP_INVERSE_MAP | self.interpolator,\n                              borderMode=cv2.BORDER_CONSTANT)\n        logger.trace(\"mask shape: %s, mask dtype: %s, mask min: %s, \"  # type:ignore[attr-defined]\n                     \"mask max: %s\", mask.shape, mask.dtype, mask.min(), mask.max())\n        return mask\n\n    def add(self, mask: np.ndarray, affine_matrix: np.ndarray, interpolator: int) -> None:\n        \"\"\" Add a Faceswap mask to this :class:`Mask`.\n\n        The mask should be the original output from  :mod:`plugins.extract.mask`\n\n        Parameters\n        ----------\n        mask: :class:`numpy.ndarray`\n            The mask that is to be added as output from :mod:`plugins.extract.mask`\n            It should be in the range 0.0 - 1.0 ideally with a ``dtype`` of ``float32``\n        affine_matrix: :class:`numpy.ndarray`\n            The transformation matrix required to transform the mask to the original frame.\n        interpolator, int:\n            The CV2 interpolator required to transform this mask to it's original frame\n        \"\"\"\n        logger.trace(\"mask shape: %s, mask dtype: %s, mask min: %s, \"  # type:ignore[attr-defined]\n                     \"mask max: %s, affine_matrix: %s, interpolator: %s)\",\n                     mask.shape, mask.dtype, mask.min(), affine_matrix, mask.max(), interpolator)\n        self._affine_matrix = self._adjust_affine_matrix(mask.shape[0], affine_matrix)\n        self._interpolator = interpolator\n        self.replace_mask(mask)\n\n    def replace_mask(self, mask: np.ndarray) -> None:\n        \"\"\" Replace the existing :attr:`_mask` with the given mask.\n\n        Parameters\n        ----------\n        mask: :class:`numpy.ndarray`\n            The mask that is to be added as output from :mod:`plugins.extract.mask`.\n            It should be in the range 0.0 - 1.0 ideally with a ``dtype`` of ``float32``\n        \"\"\"\n        mask = (cv2.resize(mask * 255.0,\n                           (self.stored_size, self.stored_size),\n                           interpolation=cv2.INTER_AREA)).astype(\"uint8\")\n        self._mask = compress(mask.tobytes())\n\n    def set_dilation(self, amount: float) -> None:\n        \"\"\" Set the internal dilation object for returned masks\n\n        Parameters\n        ----------\n        amount: float\n            The amount of erosion/dilation to apply as a percentage of the total mask size.\n            Negative values erode the mask. Positive values dilate the mask\n        \"\"\"\n        if amount == 0:\n            self._dilation = (\"erode\", None)\n            return\n\n        action: T.Literal[\"erode\", \"dilate\"] = \"erode\" if amount < 0 else \"dilate\"\n        kernel = int(round(self.stored_size * abs(amount / 100.), 0))\n        self._dilation = (action, cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (kernel, kernel)))\n\n        logger.trace(\"action: '%s', amount: %s, kernel: %s, \",  # type:ignore[attr-defined]\n                     action, amount, kernel)\n\n    def set_blur_and_threshold(self,\n                               blur_kernel: int = 0,\n                               blur_type: T.Literal[\"gaussian\", \"normalized\"] | None = \"gaussian\",\n                               blur_passes: int = 1,\n                               threshold: int = 0) -> None:\n        \"\"\" Set the internal blur kernel and threshold amount for returned masks\n\n        Parameters\n        ----------\n        blur_kernel: int, optional\n            The kernel size, in pixels to apply gaussian blurring to the mask. Set to 0 for no\n            blurring. Should be odd, if an even number is passed in (outside of 0) then it is\n            rounded up to the next odd number. Default: 0\n        blur_type: [\"gaussian\", \"normalized\"], optional\n            The blur type to use. ``gaussian`` or ``normalized`` box filter. Default: ``gaussian``\n        blur_passes: int, optional\n            The number of passed to perform when blurring. Default: 1\n        threshold: int, optional\n            The threshold amount to minimize/maximize mask values to 0 and 100. Percentage value.\n            Default: 0\n        \"\"\"\n        logger.trace(\"blur_kernel: %s, blur_type: %s, \"  # type:ignore[attr-defined]\n                     \"blur_passes: %s, threshold: %s\",\n                     blur_kernel, blur_type, blur_passes, threshold)\n        if blur_type is not None:\n            blur_kernel += 0 if blur_kernel == 0 or blur_kernel % 2 == 1 else 1\n            self._blur_kernel = blur_kernel\n            self._blur_type = blur_type\n            self._blur_passes = blur_passes\n        self._threshold = (threshold / 100.0) * 255.0\n\n    def set_sub_crop(self,\n                     source_offset: np.ndarray,\n                     target_offset: np.ndarray,\n                     centering: CenteringType,\n                     coverage_ratio: float = 1.0) -> None:\n        \"\"\" Set the internal crop area of the mask to be returned.\n\n        This impacts the returned mask from :attr:`mask` if the requested mask is required for\n        different face centering than what has been stored.\n\n        Parameters\n        ----------\n        source_offset: :class:`numpy.ndarray`\n            The (x, y) offset for the mask at its stored centering\n        target_offset: :class:`numpy.ndarray`\n            The (x, y) offset for the mask at the requested target centering\n        centering: str\n            The centering to set the sub crop area for. One of `\"legacy\"`, `\"face\"`. `\"head\"`\n        coverage_ratio: float, optional\n            The coverage ratio to be applied to the target image. ``None`` for default (1.0).\n            Default: ``None``\n        \"\"\"\n        if centering == self.stored_centering and coverage_ratio == 1.0:\n            return\n\n        center = get_adjusted_center(self.stored_size,\n                                     source_offset,\n                                     target_offset,\n                                     self.stored_centering)\n        crop_size = get_centered_size(self.stored_centering,\n                                      centering,\n                                      self.stored_size,\n                                      coverage_ratio=coverage_ratio)\n        roi = np.array([center - crop_size // 2, center + crop_size // 2]).ravel()\n\n        self._sub_crop_size = crop_size\n        self._sub_crop_slices[\"in\"] = [slice(max(roi[1], 0), max(roi[3], 0)),\n                                       slice(max(roi[0], 0), max(roi[2], 0))]\n        self._sub_crop_slices[\"out\"] = [\n            slice(max(roi[1] * -1, 0),\n                  crop_size - min(crop_size, max(0, roi[3] - self.stored_size))),\n            slice(max(roi[0] * -1, 0),\n                  crop_size - min(crop_size, max(0, roi[2] - self.stored_size)))]\n\n        logger.trace(\"src_size: %s, coverage_ratio: %s, \"  # type:ignore[attr-defined]\n                     \"sub_crop_size: %s, sub_crop_slices: %s\",\n                     roi, coverage_ratio, self._sub_crop_size, self._sub_crop_slices)\n\n    def _adjust_affine_matrix(self, mask_size: int, affine_matrix: np.ndarray) -> np.ndarray:\n        \"\"\" Adjust the affine matrix for the mask's storage size\n\n        Parameters\n        ----------\n        mask_size: int\n            The original size of the mask.\n        affine_matrix: :class:`numpy.ndarray`\n            The affine matrix to transform the mask at original size to the parent frame.\n\n        Returns\n        -------\n        affine_matrix: :class:`numpy,ndarray`\n            The affine matrix adjusted for the mask at its stored dimensions.\n        \"\"\"\n        zoom = self.stored_size / mask_size\n        zoom_mat = np.array([[zoom, 0, 0.], [0, zoom, 0.]])\n        adjust_mat = np.dot(zoom_mat, np.concatenate((affine_matrix, np.array([[0., 0., 1.]]))))\n        logger.trace(\"storage_size: %s, mask_size: %s, zoom: %s, \"  # type:ignore[attr-defined]\n                     \"original matrix: %s, adjusted_matrix: %s\", self.stored_size, mask_size, zoom,\n                     affine_matrix.shape, adjust_mat.shape)\n        return adjust_mat\n\n    def to_dict(self, is_png=False) -> MaskAlignmentsFileDict:\n        \"\"\" Convert the mask to a dictionary for saving to an alignments file\n\n        Parameters\n        ----------\n        is_png: bool\n            ``True`` if the dictionary is being created for storage in a png header otherwise\n            ``False``. Default: ``False``\n\n        Returns\n        -------\n        dict:\n            The :class:`Mask` for saving to an alignments file. Contains the keys ``mask``,\n            ``affine_matrix``, ``interpolator``, ``stored_size``, ``stored_centering``\n        \"\"\"\n        assert self._mask is not None\n        affine_matrix = self.affine_matrix.tolist() if is_png else self.affine_matrix\n        retval = MaskAlignmentsFileDict(mask=self._mask,\n                                        affine_matrix=affine_matrix,\n                                        interpolator=self.interpolator,\n                                        stored_size=self.stored_size,\n                                        stored_centering=self.stored_centering)\n        logger.trace({k: v if k != \"mask\" else type(v)  # type:ignore[attr-defined]\n                      for k, v in retval.items()})\n        return retval\n\n    def to_png_meta(self) -> MaskAlignmentsFileDict:\n        \"\"\" Convert the mask to a dictionary supported by png itxt headers.\n\n        Returns\n        -------\n        dict:\n            The :class:`Mask` for saving to an alignments file. Contains the keys ``mask``,\n            ``affine_matrix``, ``interpolator``, ``stored_size``, ``stored_centering``\n        \"\"\"\n        return self.to_dict(is_png=True)\n\n    def from_dict(self, mask_dict: MaskAlignmentsFileDict) -> None:\n        \"\"\" Populates the :class:`Mask` from a dictionary loaded from an alignments file.\n\n        Parameters\n        ----------\n        mask_dict: dict\n            A dictionary stored in an alignments file containing the keys ``mask``,\n            ``affine_matrix``, ``interpolator``, ``stored_size``, ``stored_centering``\n        \"\"\"\n        self._mask = mask_dict[\"mask\"]\n        affine_matrix = mask_dict[\"affine_matrix\"]\n        self._affine_matrix = (affine_matrix if isinstance(affine_matrix, np.ndarray)\n                               else np.array(affine_matrix, dtype=\"float64\"))\n        self._interpolator = mask_dict[\"interpolator\"]\n        self.stored_size = mask_dict[\"stored_size\"]\n        centering = mask_dict.get(\"stored_centering\")\n        self.stored_centering = \"face\" if centering is None else centering\n        logger.trace({k: v if k != \"mask\" else type(v)  # type:ignore[attr-defined]\n                      for k, v in mask_dict.items()})\n\n\nclass LandmarksMask(Mask):\n    \"\"\" Create a single channel mask from aligned landmark points.\n\n    Landmarks masks are created on the fly, so the stored centering and size should be the same as\n    the aligned face that the mask will be applied to. As the masks are created on the fly, blur +\n    dilation is applied to the mask at creation (prior to compression) rather than after\n    decompression when requested.\n\n    Note\n    ----\n    Threshold is not used for Landmarks mask as the mask is binary\n\n    Parameters\n    ----------\n    points: list\n        A list of landmark points that correspond to the given storage_size to create\n        the mask. Each item in the list should be a :class:`numpy.ndarray` that a filled\n        convex polygon will be created from\n    storage_size: int, optional\n        The size (in pixels) that the compressed mask should be stored at. Default: 128.\n    storage_centering, str (optional):\n        The centering to store the mask at. One of `\"legacy\"`, `\"face\"`, `\"head\"`.\n        Default: `\"face\"`\n    dilation: float, optional\n        The amount of dilation to apply to the mask. as a percentage of the mask size. Default: 0.0\n    \"\"\"\n    def __init__(self,\n                 points: list[np.ndarray],\n                 storage_size: int = 128,\n                 storage_centering: CenteringType = \"face\",\n                 dilation: float = 0.0) -> None:\n        super().__init__(storage_size=storage_size, storage_centering=storage_centering)\n        self._points = points\n        self.set_dilation(dilation)\n\n    @property\n    def mask(self) -> np.ndarray:\n        \"\"\" :class:`numpy.ndarray`: Overrides the default mask property, creating the processed\n        mask at first call and compressing it. The decompressed mask is returned from this\n        property. \"\"\"\n        return self.stored_mask\n\n    def generate_mask(self, affine_matrix: np.ndarray, interpolator: int) -> None:\n        \"\"\" Generate the mask.\n\n        Creates the mask applying any requested dilation and blurring and assigns compressed mask\n        to :attr:`_mask`\n\n        Parameters\n        ----------\n        affine_matrix: :class:`numpy.ndarray`\n            The transformation matrix required to transform the mask to the original frame.\n        interpolator, int:\n            The CV2 interpolator required to transform this mask to it's original frame\n        \"\"\"\n        mask = np.zeros((self.stored_size, self.stored_size, 1), dtype=\"float32\")\n        for landmarks in self._points:\n            lms = np.rint(landmarks).astype(\"int\")\n            cv2.fillConvexPoly(mask, cv2.convexHull(lms), [1.0], lineType=cv2.LINE_AA)\n        if self._dilation[-1] is not None:\n            self._dilate_mask(mask)\n        if self._blur_kernel != 0 and self._blur_type is not None:\n            mask = BlurMask(self._blur_type,\n                            mask,\n                            self._blur_kernel,\n                            passes=self._blur_passes).blurred\n        logger.trace(\"mask: (shape: %s, dtype: %s)\",  # type:ignore[attr-defined]\n                     mask.shape, mask.dtype)\n        self.add(mask, affine_matrix, interpolator)\n\n\nclass BlurMask():\n    \"\"\" Factory class to return the correct blur object for requested blur type.\n\n    Works for square images only. Currently supports Gaussian and Normalized Box Filters.\n\n    Parameters\n    ----------\n    blur_type: [\"gaussian\", \"normalized\"]\n        The type of blur to use\n    mask: :class:`numpy.ndarray`\n        The mask to apply the blur to\n    kernel: int or float\n        Either the kernel size (in pixels) or the size of the kernel as a ratio of mask size\n    is_ratio: bool, optional\n        Whether the given :attr:`kernel` parameter is a ratio or not. If ``True`` then the\n        actual kernel size will be calculated from the given ratio and the mask size. If\n        ``False`` then the kernel size will be set directly from the :attr:`kernel` parameter.\n        Default: ``False``\n    passes: int, optional\n        The number of passes to perform when blurring. Default: ``1``\n\n    Example\n    -------\n    >>> print(mask.shape)\n    (128, 128, 1)\n    >>> new_mask = BlurMask(\"gaussian\", mask, 3, is_ratio=False, passes=1).blurred\n    >>> print(new_mask.shape)\n    (128, 128, 1)\n    \"\"\"\n    def __init__(self,\n                 blur_type: T.Literal[\"gaussian\", \"normalized\"],\n                 mask: np.ndarray,\n                 kernel: int | float,\n                 is_ratio: bool = False,\n                 passes: int = 1) -> None:\n        logger.trace(parse_class_init(locals()))  # type:ignore[attr-defined]\n        self._blur_type = blur_type\n        self._mask = mask\n        self._passes = passes\n        kernel_size = self._get_kernel_size(kernel, is_ratio)\n        self._kernel_size = self._get_kernel_tuple(kernel_size)\n        logger.trace(\"Initialized %s\", self.__class__.__name__)  # type:ignore[attr-defined]\n\n    @property\n    def blurred(self) -> np.ndarray:\n        \"\"\" :class:`numpy.ndarray`: The final mask with blurring applied. \"\"\"\n        func = self._func_mapping[self._blur_type]\n        kwargs = self._get_kwargs()\n        blurred = self._mask\n        for i in range(self._passes):\n            assert isinstance(kwargs[\"ksize\"], tuple)\n            ksize = int(kwargs[\"ksize\"][0])\n            logger.trace(\"Pass: %s, kernel_size: %s\",  # type:ignore[attr-defined]\n                         i + 1, (ksize, ksize))\n            blurred = func(blurred, **kwargs)\n            ksize = int(round(ksize * self._multipass_factor))\n            kwargs[\"ksize\"] = self._get_kernel_tuple(ksize)\n        blurred = blurred[..., None]\n        logger.trace(\"Returning blurred mask. Shape: %s\",  # type:ignore[attr-defined]\n                     blurred.shape)\n        return blurred\n\n    @property\n    def _multipass_factor(self) -> float:\n        \"\"\" For multiple passes the kernel must be scaled down. This value is\n            different for box filter and gaussian \"\"\"\n        factor = {\"gaussian\": 0.8, \"normalized\": 0.5}\n        return factor[self._blur_type]\n\n    @property\n    def _sigma(self) -> T.Literal[0]:\n        \"\"\" int: The Sigma for Gaussian Blur. Returns 0 to force calculation from kernel size. \"\"\"\n        return 0\n\n    @property\n    def _func_mapping(self) -> dict[T.Literal[\"gaussian\", \"normalized\"], Callable]:\n        \"\"\" dict: :attr:`_blur_type` mapped to cv2 Function name. \"\"\"\n        return {\"gaussian\": cv2.GaussianBlur, \"normalized\": cv2.blur}\n\n    @property\n    def _kwarg_requirements(self) -> dict[T.Literal[\"gaussian\", \"normalized\"], list[str]]:\n        \"\"\" dict: :attr:`_blur_type` mapped to cv2 Function required keyword arguments. \"\"\"\n        return {\"gaussian\": ['ksize', 'sigmaX'], \"normalized\": ['ksize']}\n\n    @property\n    def _kwarg_mapping(self) -> dict[str, int | tuple[int, int]]:\n        \"\"\" dict: cv2 function keyword arguments mapped to their parameters. \"\"\"\n        return {\"ksize\": self._kernel_size, \"sigmaX\": self._sigma}\n\n    def _get_kernel_size(self, kernel: int | float, is_ratio: bool) -> int:\n        \"\"\" Set the kernel size to absolute value.\n\n        If :attr:`is_ratio` is ``True`` then the kernel size is calculated from the given ratio and\n        the :attr:`_mask` size, otherwise the given kernel size is just returned.\n\n        Parameters\n        ----------\n        kernel: int or float\n            Either the kernel size (in pixels) or the size of the kernel as a ratio of mask size\n        is_ratio: bool, optional\n            Whether the given :attr:`kernel` parameter is a ratio or not. If ``True`` then the\n            actual kernel size will be calculated from the given ratio and the mask size. If\n            ``False`` then the kernel size will be set directly from the :attr:`kernel` parameter.\n\n        Returns\n        -------\n        int\n            The size (in pixels) of the blur kernel\n        \"\"\"\n        if not is_ratio:\n            return int(kernel)\n\n        mask_diameter = np.sqrt(np.sum(self._mask))\n        radius = round(max(1., mask_diameter * kernel / 100.))\n        kernel_size = int(radius * 2 + 1)\n        logger.trace(\"kernel_size: %s\", kernel_size)  # type:ignore[attr-defined]\n        return kernel_size\n\n    @staticmethod\n    def _get_kernel_tuple(kernel_size: int) -> tuple[int, int]:\n        \"\"\" Make sure kernel_size is odd and return it as a tuple.\n\n        Parameters\n        ----------\n        kernel_size: int\n            The size in pixels of the blur kernel\n\n        Returns\n        -------\n        tuple\n            The kernel size as a tuple of ('int', 'int')\n        \"\"\"\n        kernel_size += 1 if kernel_size % 2 == 0 else 0\n        retval = (kernel_size, kernel_size)\n        logger.trace(retval)  # type:ignore[attr-defined]\n        return retval\n\n    def _get_kwargs(self) -> dict[str, int | tuple[int, int]]:\n        \"\"\" dict: the valid keyword arguments for the requested :attr:`_blur_type` \"\"\"\n        retval = {kword: self._kwarg_mapping[kword]\n                  for kword in self._kwarg_requirements[self._blur_type]}\n        logger.trace(\"BlurMask kwargs: %s\", retval)  # type:ignore[attr-defined]\n        return retval\n", "lib/align/constants.py": "#!/usr/bin/env python3\n\"\"\" Constants that are required across faceswap's lib.align package \"\"\"\nfrom __future__ import annotations\n\nimport typing as T\nfrom enum import Enum\n\nimport numpy as np\n\nCenteringType = T.Literal[\"face\", \"head\", \"legacy\"]\n\nEXTRACT_RATIOS: dict[CenteringType, float] = {\"legacy\": 0.375, \"face\": 0.5, \"head\": 0.625}\n\"\"\"dict[Literal[\"legacy\", \"face\", head\"] float]: The amount of padding applied to each\ncentering type when generating aligned faces \"\"\"\n\n\nclass LandmarkType(Enum):\n    \"\"\" Enumeration for the landmark types that Faceswap supports \"\"\"\n    LM_2D_4 = 1\n    LM_2D_51 = 2\n    LM_2D_68 = 3\n    LM_3D_26 = 4\n\n    @classmethod\n    def from_shape(cls, shape: tuple[int, ...]) -> LandmarkType:\n        \"\"\" The landmark type for a given shape\n\n        Parameters\n        ----------\n        shape: tuple[int, ...]\n            The shape to get the landmark type for\n\n        Returns\n        -------\n        Type[LandmarkType]\n            The enum for the given shape\n\n        Raises\n        ------\n        ValueError\n            If the requested shape is not valid\n        \"\"\"\n        shapes: dict[tuple[int, ...], LandmarkType] = {(4, 2): cls.LM_2D_4,\n                                                       (51, 2): cls.LM_2D_51,\n                                                       (68, 2): cls.LM_2D_68,\n                                                       (26, 3): cls.LM_3D_26}\n        if shape not in shapes:\n            raise ValueError(f\"The given shape {shape} is not valid. Valid shapes: {list(shapes)}\")\n        return shapes[shape]\n\n\n_MEAN_FACE: dict[LandmarkType, np.ndarray] = {\n    LandmarkType.LM_2D_4: np.array(\n        [[0.0, 0.0], [1.0, 0.0], [1.0, 1.0], [0.0, 1.0]]),  # Clockwise from TL\n    LandmarkType.LM_2D_51: np.array([\n        [0.010086, 0.106454], [0.085135, 0.038915], [0.191003, 0.018748], [0.300643, 0.034489],\n        [0.403270, 0.077391], [0.596729, 0.077391], [0.699356, 0.034489], [0.808997, 0.018748],\n        [0.914864, 0.038915], [0.989913, 0.106454], [0.500000, 0.203352], [0.500000, 0.307009],\n        [0.500000, 0.409805], [0.500000, 0.515625], [0.376753, 0.587326], [0.435909, 0.609345],\n        [0.500000, 0.628106], [0.564090, 0.609345], [0.623246, 0.587326], [0.131610, 0.216423],\n        [0.196995, 0.178758], [0.275698, 0.179852], [0.344479, 0.231733], [0.270791, 0.245099],\n        [0.192616, 0.244077], [0.655520, 0.231733], [0.724301, 0.179852], [0.803005, 0.178758],\n        [0.868389, 0.216423], [0.807383, 0.244077], [0.729208, 0.245099], [0.264022, 0.780233],\n        [0.350858, 0.745405], [0.438731, 0.727388], [0.500000, 0.742578], [0.561268, 0.727388],\n        [0.649141, 0.745405], [0.735977, 0.780233], [0.652032, 0.864805], [0.566594, 0.902192],\n        [0.500000, 0.909281], [0.433405, 0.902192], [0.347967, 0.864805], [0.300252, 0.784792],\n        [0.437969, 0.778746], [0.500000, 0.785343], [0.562030, 0.778746], [0.699747, 0.784792],\n        [0.563237, 0.824182], [0.500000, 0.831803], [0.436763, 0.824182]]),\n    LandmarkType.LM_3D_26: np.array([\n        [4.056931, -11.432347, 1.636229],   # 8 chin LL\n        [1.833492, -12.542305, 4.061275],   # 7 chin L\n        [0.0, -12.901019, 4.070434],        # 6 chin C\n        [-1.833492, -12.542305, 4.061275],  # 5 chin R\n        [-4.056931, -11.432347, 1.636229],  # 4 chin RR\n        [6.825897, 1.275284, 4.402142],     # 33 L eyebrow L\n        [1.330353, 1.636816, 6.903745],     # 29 L eyebrow R\n        [-1.330353, 1.636816, 6.903745],    # 34 R eyebrow L\n        [-6.825897, 1.275284, 4.402142],    # 38 R eyebrow R\n        [1.930245, -5.060977, 5.914376],    # 54 nose LL\n        [0.746313, -5.136947, 6.263227],    # 53 nose L\n        [0.0, -5.485328, 6.76343],          # 52 nose C\n        [-0.746313, -5.136947, 6.263227],   # 51 nose R\n        [-1.930245, -5.060977, 5.914376],   # 50 nose RR\n        [5.311432, 0.0, 3.987654],          # 13 L eye L\n        [1.78993, -0.091703, 4.413414],     # 17 L eye R\n        [-1.78993, -0.091703, 4.413414],    # 25 R eye L\n        [-5.311432, 0.0, 3.987654],         # 21 R eye R\n        [2.774015, -7.566103, 5.048531],    # 43 mouth L\n        [0.509714, -7.056507, 6.566167],    # 42 mouth top L\n        [0.0, -7.131772, 6.704956],         # 41 mouth top C\n        [-0.509714, -7.056507, 6.566167],   # 40 mouth top R\n        [-2.774015, -7.566103, 5.048531],   # 39 mouth R\n        [-0.589441, -8.443925, 6.109526],   # 46 mouth bottom R\n        [0.0, -8.601736, 6.097667],         # 45 mouth bottom C\n        [0.589441, -8.443925, 6.109526]])}   # 44 mouth bottom L\n\"\"\"dict[:class:`~LandmarkType, np.ndarray]: 'Mean' landmark points for various landmark types. Used\nfor aligning faces \"\"\"\n\nLANDMARK_PARTS: dict[LandmarkType, dict[str, tuple[int, int, bool]]] = {\n            LandmarkType.LM_2D_68: {\"mouth_outer\": (48, 60, True),\n                                    \"mouth_inner\": (60, 68, True),\n                                    \"right_eyebrow\": (17, 22, False),\n                                    \"left_eyebrow\": (22, 27, False),\n                                    \"right_eye\": (36, 42, True),\n                                    \"left_eye\": (42, 48, True),\n                                    \"nose\": (27, 36, False),\n                                    \"jaw\": (0, 17, False),\n                                    \"chin\": (8, 11, False)},\n            LandmarkType.LM_2D_4: {\"face\": (0, 4, True)}}\n\"\"\"dict[:class:`LandmarkType`, dict[str, tuple[int, int, bool]]: For each landmark type, stores\nthe (start index, end index, is polygon) information about each part of the face. \"\"\"\n", "lib/align/thumbnails.py": "#!/usr/bin/env python3\n\"\"\" Handles the generation of thumbnail jpgs for storing inside an alignments file/png header \"\"\"\nfrom __future__ import annotations\n\nimport logging\nimport typing as T\n\nimport numpy as np\n\nfrom lib.logger import parse_class_init\n\nif T.TYPE_CHECKING:\n    from .alignments import Alignments\n\nlogger = logging.getLogger(__name__)\n\n\nclass Thumbnails():\n    \"\"\" Thumbnail images stored in the alignments file.\n\n    The thumbnails are stored as low resolution (64px), low quality jpg in the alignments file\n    and are used for the Manual Alignments tool.\n\n    Parameters\n    ----------\n    alignments: :class:'~lib.align.alignments.Alignments`\n        The parent alignments class that these thumbs belong to\n    \"\"\"\n    def __init__(self, alignments: Alignments) -> None:\n        logger.debug(parse_class_init(locals()))\n        self._alignments_dict = alignments.data\n        self._frame_list = list(sorted(self._alignments_dict))\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    @property\n    def has_thumbnails(self) -> bool:\n        \"\"\" bool: ``True`` if all faces in the alignments file contain thumbnail images\n        otherwise ``False``. \"\"\"\n        retval = all(np.any(T.cast(np.ndarray, face.get(\"thumb\")))\n                     for frame in self._alignments_dict.values()\n                     for face in frame[\"faces\"])\n        logger.trace(retval)  # type:ignore[attr-defined]\n        return retval\n\n    def get_thumbnail_by_index(self, frame_index: int, face_index: int) -> np.ndarray:\n        \"\"\" Obtain a jpg thumbnail from the given frame index for the given face index\n\n        Parameters\n        ----------\n        frame_index: int\n            The frame index that contains the thumbnail\n        face_index: int\n            The face index within the frame to retrieve the thumbnail for\n\n        Returns\n        -------\n        :class:`numpy.ndarray`\n            The encoded jpg thumbnail\n        \"\"\"\n        retval = self._alignments_dict[self._frame_list[frame_index]][\"faces\"][face_index][\"thumb\"]\n        assert retval is not None\n        logger.trace(  # type:ignore[attr-defined]\n            \"frame index: %s, face_index: %s, thumb shape: %s\",\n            frame_index, face_index, retval.shape)\n        return retval\n\n    def add_thumbnail(self, frame: str, face_index: int, thumb: np.ndarray) -> None:\n        \"\"\" Add a thumbnail for the given face index for the given frame.\n\n        Parameters\n        ----------\n        frame: str\n            The name of the frame to add the thumbnail for\n        face_index: int\n            The face index within the given frame to add the thumbnail for\n        thumb: :class:`numpy.ndarray`\n            The encoded jpg thumbnail at 64px to add to the alignments file\n        \"\"\"\n        logger.debug(\"frame: %s, face_index: %s, thumb shape: %s thumb dtype: %s\",\n                     frame, face_index, thumb.shape, thumb.dtype)\n        self._alignments_dict[frame][\"faces\"][face_index][\"thumb\"] = thumb\n", "lib/align/__init__.py": "#!/usr/bin/env python3\n\"\"\" Package for handling alignments files, detected faces and aligned faces along with their\nassociated objects. \"\"\"\nfrom .aligned_face import (AlignedFace, get_adjusted_center, get_matrix_scaling,\n                           get_centered_size, transform_image)\nfrom .aligned_mask import BlurMask, LandmarksMask, Mask\nfrom .alignments import Alignments\nfrom .constants import CenteringType, EXTRACT_RATIOS, LANDMARK_PARTS, LandmarkType\nfrom .detected_face import DetectedFace,  update_legacy_png_header\n", "lib/gpu_stats/cpu.py": "#!/usr/bin/env python3\n\"\"\" Dummy functions for running faceswap on CPU. \"\"\"\nfrom ._base import _GPUStats\n\n\nclass CPUStats(_GPUStats):\n    \"\"\" Holds information and statistics about the CPU on the currently running system.\n\n    Notes\n    -----\n    The information held here is not  useful, but _GPUStats is dynamically imported depending on\n    the backend used, so we need to make sure this class is available for Faceswap run on the CPU\n    Backend.\n\n    The base :class:`_GPUStats` handles the dummying in of information when no GPU is detected.\n\n    Parameters\n    ----------\n    log: bool, optional\n        Whether the class should output information to the logger. There may be occasions where the\n        logger has not yet been set up when this class is queried. Attempting to log in these\n        instances will raise an error. If GPU stats are being queried prior to the logger being\n        available then this parameter should be set to ``False``. Otherwise set to ``True``.\n        Default: ``True``\n    \"\"\"\n\n    def _get_device_count(self) -> int:\n        \"\"\" Detect the number of GPUs attached to the system. Always returns zero for CPU\n        backends.\n\n        Returns\n        -------\n        int\n            The total number of GPUs connected to the PC\n        \"\"\"\n        retval = 0\n        self._log(\"debug\", f\"GPU Device count: {retval}\")\n        return retval\n\n    def _get_handles(self) -> list:\n        \"\"\" Obtain the device handles for all connected GPUs.\n\n        Returns\n        -------\n        list\n            An empty list for CPU Backends\n        \"\"\"\n        handles: list = []\n        self._log(\"debug\", f\"GPU Handles found: {len(handles)}\")\n        return handles\n\n    def _get_driver(self) -> str:\n        \"\"\" Obtain the driver version currently in use.\n\n        Returns\n        -------\n        str\n            An empty string for CPU backends\n        \"\"\"\n        driver = \"\"\n        self._log(\"debug\", f\"GPU Driver: {driver}\")\n        return driver\n\n    def _get_device_names(self) -> list[str]:\n        \"\"\" Obtain the list of names of connected GPUs as identified in :attr:`_handles`.\n\n        Returns\n        -------\n        list\n            An empty list for CPU backends\n        \"\"\"\n        names: list[str] = []\n        self._log(\"debug\", f\"GPU Devices: {names}\")\n        return names\n\n    def _get_vram(self) -> list[int]:\n        \"\"\" Obtain the RAM in Megabytes for the running system.\n\n        Returns\n        -------\n        list\n            An empty list for CPU backends\n        \"\"\"\n        vram: list[int] = []\n        self._log(\"debug\", f\"GPU VRAM: {vram}\")\n        return vram\n\n    def _get_free_vram(self) -> list[int]:\n        \"\"\" Obtain the amount of RAM that is available, in Megabytes, for the running system.\n\n        Returns\n        -------\n        list\n             An empty list for CPU backends\n        \"\"\"\n        vram: list[int] = []\n        self._log(\"debug\", f\"GPU VRAM free: {vram}\")\n        return vram\n", "lib/gpu_stats/rocm.py": "#!/usr/bin/env python3\n\"\"\" Collects and returns Information about connected AMD GPUs for ROCm using sysfs and from\nmodinfo\n\nAs no ROCm compatible hardware was available for testing, this just returns information on all AMD\nGPUs discovered on the system regardless of ROCm compatibility.\n\nIt is a good starting point but may need to be refined over time\n\"\"\"\nimport os\nimport re\nfrom subprocess import run\n\nfrom ._base import _GPUStats\n\n_DEVICE_LOOKUP = {  # ref: https://gist.github.com/roalercon/51f13a387f3754615cce\n    int(\"0x130F\", 0): \"AMD Radeon(TM) R7 Graphics\",\n    int(\"0x1313\", 0): \"AMD Radeon(TM) R7 Graphics\",\n    int(\"0x1316\", 0): \"AMD Radeon(TM) R5 Graphics\",\n    int(\"0x6600\", 0): \"AMD Radeon HD 8600/8700M\",\n    int(\"0x6601\", 0): \"AMD Radeon (TM) HD 8500M/8700M\",\n    int(\"0x6604\", 0): \"AMD Radeon R7 M265 Series\",\n    int(\"0x6605\", 0): \"AMD Radeon R7 M260 Series\",\n    int(\"0x6606\", 0): \"AMD Radeon HD 8790M\",\n    int(\"0x6607\", 0): \"AMD Radeon (TM) HD8530M\",\n    int(\"0x6610\", 0): \"AMD Radeon HD 8670 Graphics\",\n    int(\"0x6611\", 0): \"AMD Radeon HD 8570 Graphics\",\n    int(\"0x6613\", 0): \"AMD Radeon R7 200 Series\",\n    int(\"0x6640\", 0): \"AMD Radeon HD 8950\",\n    int(\"0x6658\", 0): \"AMD Radeon R7 200 Series\",\n    int(\"0x665C\", 0): \"AMD Radeon HD 7700 Series\",\n    int(\"0x665D\", 0): \"AMD Radeon R7 200 Series\",\n    int(\"0x6660\", 0): \"AMD Radeon HD 8600M Series\",\n    int(\"0x6663\", 0): \"AMD Radeon HD 8500M Series\",\n    int(\"0x6664\", 0): \"AMD Radeon R5 M200 Series\",\n    int(\"0x6665\", 0): \"AMD Radeon R5 M230 Series\",\n    int(\"0x6667\", 0): \"AMD Radeon R5 M200 Series\",\n    int(\"0x666F\", 0): \"AMD Radeon HD 8500M\",\n    int(\"0x6704\", 0): \"AMD FirePro V7900 (FireGL V)\",\n    int(\"0x6707\", 0): \"AMD FirePro V5900 (FireGL V)\",\n    int(\"0x6718\", 0): \"AMD Radeon HD 6900 Series\",\n    int(\"0x6719\", 0): \"AMD Radeon HD 6900 Series\",\n    int(\"0x671D\", 0): \"AMD Radeon HD 6900 Series\",\n    int(\"0x671F\", 0): \"AMD Radeon HD 6900 Series\",\n    int(\"0x6720\", 0): \"AMD Radeon HD 6900M Series\",\n    int(\"0x6738\", 0): \"AMD Radeon HD 6800 Series\",\n    int(\"0x6739\", 0): \"AMD Radeon HD 6800 Series\",\n    int(\"0x673E\", 0): \"AMD Radeon HD 6700 Series\",\n    int(\"0x6740\", 0): \"AMD Radeon HD 6700M Series\",\n    int(\"0x6741\", 0): \"AMD Radeon 6600M and 6700M Series\",\n    int(\"0x6742\", 0): \"AMD Radeon HD 5570\",\n    int(\"0x6743\", 0): \"AMD Radeon E6760\",\n    int(\"0x6749\", 0): \"AMD FirePro V4900 (FireGL V)\",\n    int(\"0x674A\", 0): \"AMD FirePro V3900 (ATI FireGL)\",\n    int(\"0x6750\", 0): \"AMD Radeon HD 6500 series\",\n    int(\"0x6751\", 0): \"AMD Radeon HD 7600A Series\",\n    int(\"0x6758\", 0): \"AMD Radeon HD 6670\",\n    int(\"0x6759\", 0): \"AMD Radeon HD 6570 Graphics\",\n    int(\"0x675B\", 0): \"AMD Radeon HD 7600 Series\",\n    int(\"0x675D\", 0): \"AMD Radeon HD 7500 Series\",\n    int(\"0x675F\", 0): \"AMD Radeon HD 5500 Series\",\n    int(\"0x6760\", 0): \"AMD Radeon HD 6400M Series\",\n    int(\"0x6761\", 0): \"AMD Radeon HD 6430M\",\n    int(\"0x6763\", 0): \"AMD Radeon E6460\",\n    int(\"0x6770\", 0): \"AMD Radeon HD 6400 Series\",\n    int(\"0x6771\", 0): \"AMD Radeon R5 235X\",\n    int(\"0x6772\", 0): \"AMD Radeon HD 7400A Series\",\n    int(\"0x6778\", 0): \"AMD Radeon HD 7000 series\",\n    int(\"0x6779\", 0): \"AMD Radeon HD 6450\",\n    int(\"0x677B\", 0): \"AMD Radeon HD 7400 Series\",\n    int(\"0x6780\", 0): \"AMD FirePro W9000 (FireGL V)\",\n    int(\"0x678A\", 0): \"AMD FirePro S10000 (FireGL V)\",\n    int(\"0x6798\", 0): \"AMD Radeon HD 7900 Series\",\n    int(\"0x679A\", 0): \"AMD Radeon HD 7900 Series\",\n    int(\"0x679B\", 0): \"AMD Radeon HD 7900 Series\",\n    int(\"0x679E\", 0): \"AMD Radeon HD 7800 Series\",\n    int(\"0x67B0\", 0): \"AMD Radeon R9 200 Series\",\n    int(\"0x67B1\", 0): \"AMD Radeon R9 200 Series\",\n    int(\"0x6800\", 0): \"AMD Radeon HD 7970M\",\n    int(\"0x6801\", 0): \"AMD Radeon(TM) HD8970M\",\n    int(\"0x6808\", 0): \"AMD FirePro S7000 (FireGL V)\",\n    int(\"0x6809\", 0): \"AMD FirePro R5000 (FireGL V)\",\n    int(\"0x6810\", 0): \"AMD Radeon R9 200 Series\",\n    int(\"0x6811\", 0): \"AMD Radeon R9 200 Series\",\n    int(\"0x6818\", 0): \"AMD Radeon HD 7800 Series\",\n    int(\"0x6819\", 0): \"AMD Radeon HD 7800 Series\",\n    int(\"0x6820\", 0): \"AMD Radeon HD 8800M Series\",\n    int(\"0x6821\", 0): \"AMD Radeon HD 8800M Series\",\n    int(\"0x6822\", 0): \"AMD Radeon E8860\",\n    int(\"0x6823\", 0): \"AMD Radeon HD 8800M Series\",\n    int(\"0x6825\", 0): \"AMD Radeon HD 7800M Series\",\n    int(\"0x6827\", 0): \"AMD Radeon HD 7800M Series\",\n    int(\"0x6828\", 0): \"AMD FirePro W600\",\n    int(\"0x682B\", 0): \"AMD Radeon HD 8800M Series\",\n    int(\"0x682D\", 0): \"AMD Radeon HD 7700M Series\",\n    int(\"0x682F\", 0): \"AMD Radeon HD 7700M Series\",\n    int(\"0x6835\", 0): \"AMD Radeon R7 Series / HD 9000 Series\",\n    int(\"0x6837\", 0): \"AMD Radeon HD 6570\",\n    int(\"0x683D\", 0): \"AMD Radeon HD 7700 Series\",\n    int(\"0x683F\", 0): \"AMD Radeon HD 7700 Series\",\n    int(\"0x6840\", 0): \"AMD Radeon HD 7600M Series\",\n    int(\"0x6841\", 0): \"AMD Radeon HD 7500M/7600M Series\",\n    int(\"0x6842\", 0): \"AMD Radeon HD 7000M Series\",\n    int(\"0x6843\", 0): \"AMD Radeon HD 7670M\",\n    int(\"0x6858\", 0): \"AMD Radeon HD 7400 Series\",\n    int(\"0x6859\", 0): \"AMD Radeon HD 7400 Series\",\n    int(\"0x6888\", 0): \"ATI FirePro V8800 (FireGL V)\",\n    int(\"0x6889\", 0): \"ATI FirePro V7800 (FireGL V)\",\n    int(\"0x688A\", 0): \"ATI FirePro V9800 (FireGL V)\",\n    int(\"0x688C\", 0): \"AMD FireStream 9370\",\n    int(\"0x688D\", 0): \"AMD FireStream 9350\",\n    int(\"0x6898\", 0): \"AMD Radeon HD 5800 Series\",\n    int(\"0x6899\", 0): \"AMD Radeon HD 5800 Series\",\n    int(\"0x689B\", 0): \"AMD Radeon HD 6800 Series\",\n    int(\"0x689C\", 0): \"AMD Radeon HD 5900 Series\",\n    int(\"0x689E\", 0): \"AMD Radeon HD 5800 Series\",\n    int(\"0x68A0\", 0): \"AMD Mobility Radeon HD 5800 Series\",\n    int(\"0x68A1\", 0): \"AMD Mobility Radeon HD 5800 Series\",\n    int(\"0x68A8\", 0): \"AMD Radeon HD 6800M Series\",\n    int(\"0x68A9\", 0): \"ATI FirePro V5800 (FireGL V)\",\n    int(\"0x68B8\", 0): \"AMD Radeon HD 5700 Series\",\n    int(\"0x68B9\", 0): \"AMD Radeon HD 5600/5700\",\n    int(\"0x68BA\", 0): \"AMD Radeon HD 6700 Series\",\n    int(\"0x68BE\", 0): \"AMD Radeon HD 5700 Series\",\n    int(\"0x68BF\", 0): \"AMD Radeon HD 6700 Green Edition\",\n    int(\"0x68C0\", 0): \"AMD Mobility Radeon HD 5000\",\n    int(\"0x68C1\", 0): \"AMD Mobility Radeon HD 5000 Series\",\n    int(\"0x68C7\", 0): \"AMD Mobility Radeon HD 5570\",\n    int(\"0x68C8\", 0): \"ATI FirePro V4800 (FireGL V)\",\n    int(\"0x68C9\", 0): \"ATI FirePro 3800 (FireGL) Graphics Adapter\",\n    int(\"0x68D8\", 0): \"AMD Radeon HD 5670\",\n    int(\"0x68D9\", 0): \"AMD Radeon HD 5570\",\n    int(\"0x68DA\", 0): \"AMD Radeon HD 5500 Series\",\n    int(\"0x68E0\", 0): \"AMD Mobility Radeon HD 5000 Series\",\n    int(\"0x68E1\", 0): \"AMD Mobility Radeon HD 5000 Series\",\n    int(\"0x68E4\", 0): \"AMD Radeon HD 5450\",\n    int(\"0x68E5\", 0): \"AMD Radeon HD 6300M Series\",\n    int(\"0x68F1\", 0): \"AMD FirePro 2460\",\n    int(\"0x68F2\", 0): \"AMD FirePro 2270 (ATI FireGL)\",\n    int(\"0x68F9\", 0): \"AMD Radeon HD 5450\",\n    int(\"0x68FA\", 0): \"AMD Radeon HD 7300 Series\",\n    int(\"0x9640\", 0): \"AMD Radeon HD 6550D\",\n    int(\"0x9641\", 0): \"AMD Radeon HD 6620G\",\n    int(\"0x9642\", 0): \"AMD Radeon HD 6370D\",\n    int(\"0x9643\", 0): \"AMD Radeon HD 6380G\",\n    int(\"0x9644\", 0): \"AMD Radeon HD 6410D\",\n    int(\"0x9645\", 0): \"AMD Radeon HD 6410D\",\n    int(\"0x9647\", 0): \"AMD Radeon HD 6520G\",\n    int(\"0x9648\", 0): \"AMD Radeon HD 6480G\",\n    int(\"0x9649\", 0): \"AMD Radeon(TM) HD 6480G\",\n    int(\"0x964A\", 0): \"AMD Radeon HD 6530D\",\n    int(\"0x9802\", 0): \"AMD Radeon HD 6310 Graphics\",\n    int(\"0x9803\", 0): \"AMD Radeon HD 6250 Graphics\",\n    int(\"0x9804\", 0): \"AMD Radeon HD 6250 Graphics\",\n    int(\"0x9805\", 0): \"AMD Radeon HD 6250 Graphics\",\n    int(\"0x9806\", 0): \"AMD Radeon HD 6320 Graphics\",\n    int(\"0x9807\", 0): \"AMD Radeon HD 6290 Graphics\",\n    int(\"0x9808\", 0): \"AMD Radeon HD 7340 Graphics\",\n    int(\"0x9809\", 0): \"AMD Radeon HD 7310 Graphics\",\n    int(\"0x980A\", 0): \"AMD Radeon HD 7290 Graphics\",\n    int(\"0x9830\", 0): \"AMD Radeon HD 8400\",\n    int(\"0x9831\", 0): \"AMD Radeon(TM) HD 8400E\",\n    int(\"0x9832\", 0): \"AMD Radeon HD 8330\",\n    int(\"0x9833\", 0): \"AMD Radeon(TM) HD 8330E\",\n    int(\"0x9834\", 0): \"AMD Radeon HD 8210\",\n    int(\"0x9835\", 0): \"AMD Radeon(TM) HD 8210E\",\n    int(\"0x9836\", 0): \"AMD Radeon HD 8280\",\n    int(\"0x9837\", 0): \"AMD Radeon(TM) HD 8280E\",\n    int(\"0x9838\", 0): \"AMD Radeon HD 8240\",\n    int(\"0x9839\", 0): \"AMD Radeon HD 8180\",\n    int(\"0x983D\", 0): \"AMD Radeon HD 8250\",\n    int(\"0x9900\", 0): \"AMD Radeon HD 7660G\",\n    int(\"0x9901\", 0): \"AMD Radeon HD 7660D\",\n    int(\"0x9903\", 0): \"AMD Radeon HD 7640G\",\n    int(\"0x9904\", 0): \"AMD Radeon HD 7560D\",\n    int(\"0x9906\", 0): \"AMD FirePro A300 Series (FireGL V) Graphics Adapter\",\n    int(\"0x9907\", 0): \"AMD Radeon HD 7620G\",\n    int(\"0x9908\", 0): \"AMD Radeon HD 7600G\",\n    int(\"0x990A\", 0): \"AMD Radeon HD 7500G\",\n    int(\"0x990B\", 0): \"AMD Radeon HD 8650G\",\n    int(\"0x990C\", 0): \"AMD Radeon HD 8670D\",\n    int(\"0x990D\", 0): \"AMD Radeon HD 8550G\",\n    int(\"0x990E\", 0): \"AMD Radeon HD 8570D\",\n    int(\"0x990F\", 0): \"AMD Radeon HD 8610G\",\n    int(\"0x9910\", 0): \"AMD Radeon HD 7660G\",\n    int(\"0x9913\", 0): \"AMD Radeon HD 7640G\",\n    int(\"0x9917\", 0): \"AMD Radeon HD 7620G\",\n    int(\"0x9918\", 0): \"AMD Radeon HD 7600G\",\n    int(\"0x9919\", 0): \"AMD Radeon HD 7500G\",\n    int(\"0x9990\", 0): \"AMD Radeon HD 7520G\",\n    int(\"0x9991\", 0): \"AMD Radeon HD 7540D\",\n    int(\"0x9992\", 0): \"AMD Radeon HD 7420G\",\n    int(\"0x9993\", 0): \"AMD Radeon HD 7480D\",\n    int(\"0x9994\", 0): \"AMD Radeon HD 7400G\",\n    int(\"0x9995\", 0): \"AMD Radeon HD 8450G\",\n    int(\"0x9996\", 0): \"AMD Radeon HD 8470D\",\n    int(\"0x9997\", 0): \"AMD Radeon HD 8350G\",\n    int(\"0x9998\", 0): \"AMD Radeon HD 8370D\",\n    int(\"0x9999\", 0): \"AMD Radeon HD 8510G\",\n    int(\"0x999A\", 0): \"AMD Radeon HD 8410G\",\n    int(\"0x999B\", 0): \"AMD Radeon HD 8310G\",\n    int(\"0x999C\", 0): \"AMD Radeon HD 8650D\",\n    int(\"0x999D\", 0): \"AMD Radeon HD 8550D\",\n    int(\"0x99A0\", 0): \"AMD Radeon HD 7520G\",\n    int(\"0x99A2\", 0): \"AMD Radeon HD 7420G\",\n    int(\"0x99A4\", 0): \"AMD Radeon HD 7400G\"}\n\n\nclass ROCm(_GPUStats):\n    \"\"\" Holds information and statistics about GPUs connected using sysfs\n\n    Parameters\n    ----------\n    log: bool, optional\n        Whether the class should output information to the logger. There may be occasions where the\n        logger has not yet been set up when this class is queried. Attempting to log in these\n        instances will raise an error. If GPU stats are being queried prior to the logger being\n        available then this parameter should be set to ``False``. Otherwise set to ``True``.\n        Default: ``True``\n    \"\"\"\n    def __init__(self, log: bool = True) -> None:\n        self._vendor_id = \"0x1002\"  # AMD VendorID\n        self._sysfs_paths: list[str] = []\n        super().__init__(log=log)\n\n    def _from_sysfs_file(self, path: str) -> str:\n        \"\"\" Obtain the value from a sysfs file. On permission error or file doesn't exist, log and\n        return empty value\n\n        Parameters\n        ----------\n        path: str\n            The path to a sysfs file to obtain the value from\n\n        Returns\n        -------\n        str\n            The obtained value from the given path\n        \"\"\"\n        if not os.path.isfile(path):\n            self._log(\"debug\", f\"File '{path}' does not exist. Returning empty string\")\n            return \"\"\n        try:\n            with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as sysfile:\n                val = sysfile.read().strip()\n        except PermissionError:\n            self._log(\"debug\", f\"Permission error accessing file '{path}'. Returning empty string\")\n            val = \"\"\n        return val\n\n    def _get_sysfs_paths(self) -> list[str]:\n        \"\"\" Obtain a list of sysfs paths to AMD branded GPUs connected to the system\n\n        Returns\n        -------\n        list[str]\n            List of full paths to the sysfs entries for connected AMD GPUs\n        \"\"\"\n        base_dir = \"/sys/class/drm/\"\n\n        retval: list[str] = []\n        if not os.path.exists(base_dir):\n            self._log(\"warning\", f\"sysfs not found at '{base_dir}'\")\n            return retval\n\n        for folder in sorted(os.listdir(base_dir)):\n            folder_path = os.path.join(base_dir, folder, \"device\")\n            vendor_path = os.path.join(folder_path, \"vendor\")\n            if not os.path.isdir(vendor_path) and not re.match(r\"^card\\d+$\", folder):\n                self._log(\"debug\", f\"skipping path '{folder_path}'\")\n                continue\n\n            vendor_id = self._from_sysfs_file(vendor_path)\n            if vendor_id != self._vendor_id:\n                self._log(\"debug\", f\"Skipping non AMD Vendor '{vendor_id}' for device: '{folder}'\")\n                continue\n\n            retval.append(folder_path)\n\n        self._log(\"debug\", f\"sysfs AMD devices: {retval}\")\n        return retval\n\n    def _initialize(self) -> None:\n        \"\"\" Initialize sysfs for ROCm backend.\n\n        If :attr:`_is_initialized` is ``True`` then this function just returns performing no\n        action.\n\n        if ``False`` then the location of AMD cards within sysfs is collected\n        \"\"\"\n        if self._is_initialized:\n            return\n        self._log(\"debug\", \"Initializing sysfs for AMDGPU (ROCm).\")\n        self._sysfs_paths = self._get_sysfs_paths()\n        super()._initialize()\n\n    def _get_device_count(self) -> int:\n        \"\"\" The number of AMD cards found in sysfs\n\n        Returns\n        -------\n        int\n            The total number of GPUs available\n        \"\"\"\n        retval = len(self._sysfs_paths)\n        self._log(\"debug\", f\"GPU Device count: {retval}\")\n        return retval\n\n    def _get_handles(self) -> list:\n        \"\"\" The sysfs doesn't use device handles, so we just return the list of the sysfs locations\n        per card\n\n        Returns\n        -------\n        list\n            The list of all discovered GPUs\n        \"\"\"\n        handles = self._sysfs_paths\n        self._log(\"debug\", f\"sysfs GPU Handles found: {handles}\")\n        return handles\n\n    def _get_driver(self) -> str:\n        \"\"\" Obtain the driver versions currently in use from modinfo\n\n        Returns\n        -------\n        str\n            The current AMDGPU driver versions\n        \"\"\"\n        retval = \"\"\n        cmd = [\"modinfo\", \"amdgpu\"]\n        try:\n            proc = run(cmd,\n                       check=True,\n                       timeout=5,\n                       capture_output=True,\n                       encoding=\"utf-8\",\n                       errors=\"ignore\")\n            for line in proc.stdout.split(\"\\n\"):\n                if line.startswith(\"version:\"):\n                    retval = line.split()[-1]\n                    break\n        except Exception as err:  # pylint:disable=broad-except\n            self._log(\"debug\", f\"Error reading modinfo: '{str(err)}'\")\n\n        self._log(\"debug\", f\"GPU Drivers: {retval}\")\n        return retval\n\n    def _get_device_names(self) -> list[str]:\n        \"\"\" Obtain the list of names of connected GPUs as identified in :attr:`_handles`.\n\n        Returns\n        -------\n        list\n            The list of connected AMD GPU names\n        \"\"\"\n        retval = []\n        for device in self._sysfs_paths:\n            name = self._from_sysfs_file(os.path.join(device, \"product_name\"))\n            number = self._from_sysfs_file(os.path.join(device, \"product_number\"))\n            if name or number:  # product_name or product_number populated\n                self._log(\"debug\", f\"Got name from product_name: '{name}', product_number: \"\n                                   f\"'{number}'\")\n                retval.append(f\"{name + ' ' if name else ''}{number}\")\n                continue\n\n            device_id = self._from_sysfs_file(os.path.join(device, \"device\"))\n            self._log(\"debug\", f\"Got device_id: '{device_id}'\")\n\n            if not device_id:  # Can't get device name\n                retval.append(\"Not found\")\n                continue\n            try:\n                lookup = int(device_id, 0)\n            except ValueError:\n                retval.append(device_id)\n                continue\n\n            device_name = _DEVICE_LOOKUP.get(lookup, device_id)\n            retval.append(device_name)\n\n        self._log(\"debug\", f\"Device names: {retval}\")\n        return retval\n\n    def _get_active_devices(self) -> list[int]:\n        \"\"\" Obtain the indices of active GPUs (those that have not been explicitly excluded by\n        HIP_VISIBLE_DEVICES environment variable or explicitly excluded in the command line\n        arguments).\n\n        Returns\n        -------\n        list\n            The list of device indices that are available for Faceswap to use\n        \"\"\"\n        devices = super()._get_active_devices()\n        env_devices = os.environ.get(\"HIP_VISIBLE_DEVICES \")\n        if env_devices:\n            new_devices = [int(i) for i in env_devices.split(\",\")]\n            devices = [idx for idx in devices if idx in new_devices]\n        self._log(\"debug\", f\"Active GPU Devices: {devices}\")\n        return devices\n\n    def _get_vram(self) -> list[int]:\n        \"\"\" Obtain the VRAM in Megabytes for each connected AMD GPU as identified in\n        :attr:`_handles`.\n\n        Returns\n        -------\n        list\n            The VRAM in Megabytes for each connected Nvidia GPU\n        \"\"\"\n        retval = []\n        for device in self._sysfs_paths:\n            query = self._from_sysfs_file(os.path.join(device, \"mem_info_vram_total\"))\n            try:\n                vram = int(query)\n            except ValueError:\n                self._log(\"debug\", f\"Couldn't extract VRAM from string: '{query}'\", )\n                vram = 0\n            retval.append(int(vram / (1024 * 1024)))\n\n        self._log(\"debug\", f\"GPU VRAM: {retval}\")\n        return retval\n\n    def _get_free_vram(self) -> list[int]:\n        \"\"\" Obtain the amount of VRAM that is available, in Megabytes, for each connected AMD\n        GPU.\n\n        Returns\n        -------\n        list\n             List of `float`s containing the amount of VRAM available, in Megabytes, for each\n             connected GPU as corresponding to the values in :attr:`_handles\n        \"\"\"\n        retval = []\n        total_vram = self._get_vram()\n        for device, vram in zip(self._sysfs_paths, total_vram):\n            if not vram:\n                retval.append(0)\n                continue\n            query = self._from_sysfs_file(os.path.join(device, \"mem_info_vram_used\"))\n            try:\n                used = int(query)\n            except ValueError:\n                self._log(\"debug\", f\"Couldn't extract used VRAM from string: '{query}'\")\n                used = 0\n\n            retval.append(vram - int(used / (1024 * 1024)))\n        self._log(\"debug\", f\"GPU VRAM free: {retval}\")\n        return retval\n", "lib/gpu_stats/apple_silicon.py": "#!/usr/bin/env python3\n\"\"\" Collects and returns Information on available Apple Silicon SoCs in Apple Macs. \"\"\"\nimport typing as T\n\nimport os\nimport psutil\nimport tensorflow as tf\n\nfrom lib.utils import FaceswapError\n\nfrom ._base import _GPUStats\n\n\n_METAL_INITIALIZED: bool = False\n\n\nclass AppleSiliconStats(_GPUStats):\n    \"\"\" Holds information and statistics about Apple Silicon SoC(s) available on the currently\n    running Apple system.\n\n    Notes\n    -----\n    Apple Silicon is a bit different from other backends, as it does not have a dedicated GPU with\n    it's own dedicated VRAM, rather the RAM is shared with the CPU and GPU. A combination of psutil\n    and Tensorflow are used to pull as much useful information as possible.\n\n    Parameters\n    ----------\n    log: bool, optional\n        Whether the class should output information to the logger. There may be occasions where the\n        logger has not yet been set up when this class is queried. Attempting to log in these\n        instances will raise an error. If GPU stats are being queried prior to the logger being\n        available then this parameter should be set to ``False``. Otherwise set to ``True``.\n        Default: ``True``\n    \"\"\"\n    def __init__(self, log: bool = True) -> None:\n        # Following attribute set in :func:``_initialize``\n        self._tf_devices: list[T.Any] = []\n\n        super().__init__(log=log)\n\n    def _initialize(self) -> None:\n        \"\"\" Initialize Metal for Apple Silicon SoC(s).\n\n        If :attr:`_is_initialized` is ``True`` then this function just returns performing no\n        action. Otherwise :attr:`is_initialized` is set to ``True`` after successfully\n        initializing Metal.\n        \"\"\"\n        if self._is_initialized:\n            return\n        self._log(\"debug\", \"Initializing Metal for Apple Silicon SoC.\")\n        self._initialize_metal()\n\n        self._tf_devices = tf.config.list_physical_devices(device_type=\"GPU\")\n\n        super()._initialize()\n\n    def _initialize_metal(self) -> None:\n        \"\"\" Initialize Metal on first call to this class and set global\n        :attr:``_METAL_INITIALIZED`` to ``True``. If Metal has already been initialized then return\n        performing no action.\n        \"\"\"\n        global _METAL_INITIALIZED  # pylint:disable=global-statement\n\n        if _METAL_INITIALIZED:\n            return\n\n        self._log(\"debug\", \"Performing first time Apple SoC setup.\")\n\n        os.environ[\"DISPLAY\"] = \":0\"\n\n        try:\n            os.system(\"open -a XQuartz\")\n        except Exception as err:  # pylint:disable=broad-except\n            self._log(\"debug\", f\"Swallowing error opening XQuartz: {str(err)}\")\n\n        self._test_tensorflow()\n\n        _METAL_INITIALIZED = True\n\n    def _test_tensorflow(self) -> None:\n        \"\"\" Test that tensorflow can execute correctly.\n\n        Raises\n        ------\n        FaceswapError\n            If the Tensorflow library could not be successfully initialized\n        \"\"\"\n        try:\n            meminfo = tf.config.experimental.get_memory_info('GPU:0')\n            devices = tf.config.list_logical_devices()\n            self._log(\"debug\",\n                      f\"Tensorflow initialization test: (mem_info: {meminfo}, devices: {devices}\")\n        except RuntimeError as err:\n            msg = (\"An unhandled exception occured initializing the device via Tensorflow \"\n                   f\"Library. Original error: {str(err)}\")\n            raise FaceswapError(msg) from err\n\n    def _get_device_count(self) -> int:\n        \"\"\" Detect the number of SoCs attached to the system.\n\n        Returns\n        -------\n        int\n            The total number of SoCs available\n        \"\"\"\n        retval = len(self._tf_devices)\n        self._log(\"debug\", f\"GPU Device count: {retval}\")\n        return retval\n\n    def _get_handles(self) -> list:\n        \"\"\" Obtain the device handles for all available Apple Silicon SoCs.\n\n        Notes\n        -----\n        Apple SoC does not use handles, so return a list of indices corresponding to found\n        GPU devices\n\n        Returns\n        -------\n        list\n            The list of indices for available Apple Silicon SoCs\n        \"\"\"\n        handles = list(range(self._device_count))\n        self._log(\"debug\", f\"GPU Handles found: {handles}\")\n        return handles\n\n    def _get_driver(self) -> str:\n        \"\"\" Obtain the Apple Silicon driver version currently in use.\n\n        Notes\n        -----\n        As the SoC is not a discreet GPU it does not technically have a driver version, so just\n        return `'Not Applicable'` as a string\n\n        Returns\n        -------\n        str\n            The current SoC driver version\n        \"\"\"\n        driver = \"Not Applicable\"\n        self._log(\"debug\", f\"GPU Driver: {driver}\")\n        return driver\n\n    def _get_device_names(self) -> list[str]:\n        \"\"\" Obtain the list of names of available Apple Silicon SoC(s) as identified in\n        :attr:`_handles`.\n\n        Returns\n        -------\n        list\n            The list of available Apple Silicon SoC names\n        \"\"\"\n        names = [d.name for d in self._tf_devices]\n        self._log(\"debug\", f\"GPU Devices: {names}\")\n        return names\n\n    def _get_vram(self) -> list[int]:\n        \"\"\" Obtain the VRAM in Megabytes for each available Apple Silicon SoC(s) as identified in\n        :attr:`_handles`.\n\n        Notes\n        -----\n        `tf.config.experimental.get_memory_info('GPU:0')` does not work, so uses psutil instead.\n        The total memory on the system is returned as it is shared between the CPU and the GPU.\n        There is no dedicated VRAM.\n\n        Returns\n        -------\n        list\n            The RAM in Megabytes for each available Apple Silicon SoC\n        \"\"\"\n        vram = [int((psutil.virtual_memory().total / self._device_count) / (1024 * 1024))\n                for _ in range(self._device_count)]\n        self._log(\"debug\", f\"SoC RAM: {vram}\")\n        return vram\n\n    def _get_free_vram(self) -> list[int]:\n        \"\"\" Obtain the amount of VRAM that is available, in Megabytes, for each available Apple\n        Silicon SoC.\n\n        Returns\n        -------\n        list\n             List of `float`s containing the amount of RAM available, in Megabytes, for each\n             available SoC as corresponding to the values in :attr:`_handles\n        \"\"\"\n        vram = [int((psutil.virtual_memory().available / self._device_count) / (1024 * 1024))\n                for _ in range(self._device_count)]\n        self._log(\"debug\", f\"SoC RAM free: {vram}\")\n        return vram\n", "lib/gpu_stats/_base.py": "#!/usr/bin/env python3\n\"\"\" Parent class for obtaining Stats for various GPU/TPU backends. All GPU Stats should inherit\nfrom the :class:`_GPUStats` class contained here. \"\"\"\n\nimport logging\n\nfrom dataclasses import dataclass\n\nfrom lib.utils import get_backend\n\n_EXCLUDE_DEVICES: list[int] = []\n\n\n@dataclass\nclass GPUInfo():\n    \"\"\"Dataclass for storing information about the available GPUs on the system.\n\n    Attributes:\n    ----------\n    vram: list[int]\n        List of integers representing the total VRAM available on each GPU, in MB.\n    vram_free: list[int]\n        List of integers representing the free VRAM available on each GPU, in MB.\n    driver: str\n        String representing the driver version being used for the GPUs.\n    devices: list[str]\n        List of strings representing the names of each GPU device.\n    devices_active: list[int]\n        List of integers representing the indices of the active GPU devices.\n    \"\"\"\n    vram: list[int]\n    vram_free: list[int]\n    driver: str\n    devices: list[str]\n    devices_active: list[int]\n\n\n@dataclass\nclass BiggestGPUInfo():\n    \"\"\" Dataclass for holding GPU Information about the card with most available VRAM.\n\n    Attributes\n    ----------\n    card_id: int\n        Integer representing the index of the GPU device.\n    device: str\n        The name of the device\n    free: float\n        The amount of available VRAM on the GPU\n    total: float\n        the total amount of VRAM on the GPU\n    \"\"\"\n    card_id: int\n    device: str\n    free: float\n    total: float\n\n\ndef set_exclude_devices(devices: list[int]) -> None:\n    \"\"\" Add any explicitly selected GPU devices to the global list of devices to be excluded\n    from use by Faceswap.\n\n    Parameters\n    ----------\n    devices: list[int]\n        list of GPU device indices to exclude\n\n    Example\n    -------\n    >>> set_exclude_devices([0, 1]) # Exclude the first two GPU devices\n    \"\"\"\n    logger = logging.getLogger(__name__)\n    logger.debug(\"Excluding GPU indicies: %s\", devices)\n    if not devices:\n        return\n    _EXCLUDE_DEVICES.extend(devices)\n\n\nclass _GPUStats():\n    \"\"\" Parent class for collecting GPU device information.\n\n    Parameters:\n    -----------\n    log : bool, optional\n        Flag indicating whether or not to log debug messages. Default: `True`.\n    \"\"\"\n\n    def __init__(self, log: bool = True) -> None:\n        # Logger is held internally, as we don't want to log when obtaining system stats on crash\n        # or when querying the backend for command line options\n        self._logger: logging.Logger | None = logging.getLogger(__name__) if log else None\n        self._log(\"debug\", f\"Initializing {self.__class__.__name__}\")\n\n        self._is_initialized = False\n        self._initialize()\n\n        self._device_count: int = self._get_device_count()\n        self._active_devices: list[int] = self._get_active_devices()\n        self._handles: list = self._get_handles()\n        self._driver: str = self._get_driver()\n        self._device_names: list[str] = self._get_device_names()\n        self._vram: list[int] = self._get_vram()\n        self._vram_free: list[int] = self._get_free_vram()\n\n        if get_backend() != \"cpu\" and not self._active_devices:\n            self._log(\"warning\", \"No GPU detected\")\n\n        self._shutdown()\n        self._log(\"debug\", f\"Initialized {self.__class__.__name__}\")\n\n    @property\n    def device_count(self) -> int:\n        \"\"\"int: The number of GPU devices discovered on the system. \"\"\"\n        return self._device_count\n\n    @property\n    def cli_devices(self) -> list[str]:\n        \"\"\" list[str]: Formatted index: name text string for each GPU \"\"\"\n        return [f\"{idx}: {device}\" for idx, device in enumerate(self._device_names)]\n\n    @property\n    def exclude_all_devices(self) -> bool:\n        \"\"\" bool: ``True`` if all GPU devices have been explicitly disabled otherwise ``False`` \"\"\"\n        return all(idx in _EXCLUDE_DEVICES for idx in range(self._device_count))\n\n    @property\n    def sys_info(self) -> GPUInfo:\n        \"\"\" :class:`GPUInfo`: The GPU Stats that are required for system information logging \"\"\"\n        return GPUInfo(vram=self._vram,\n                       vram_free=self._get_free_vram(),\n                       driver=self._driver,\n                       devices=self._device_names,\n                       devices_active=self._active_devices)\n\n    def _log(self, level: str, message: str) -> None:\n        \"\"\" If the class has been initialized with :attr:`log` as `True` then log the message\n        otherwise skip logging.\n\n        Parameters\n        ----------\n        level: str\n            The log level to log at\n        message: str\n            The message to log\n        \"\"\"\n        if self._logger is None:\n            return\n        logger = getattr(self._logger, level.lower())\n        logger(message)\n\n    def _initialize(self) -> None:\n        \"\"\" Override to initialize the GPU device handles and any other necessary resources. \"\"\"\n        self._is_initialized = True\n\n    def _shutdown(self) -> None:\n        \"\"\" Override to shutdown the GPU device handles and any other necessary resources. \"\"\"\n        self._is_initialized = False\n\n    def _get_device_count(self) -> int:\n        \"\"\" Override to obtain the number of GPU devices\n\n        Returns\n        -------\n        int\n            The total number of GPUs connected to the PC\n        \"\"\"\n        raise NotImplementedError()\n\n    def _get_active_devices(self) -> list[int]:\n        \"\"\" Obtain the indices of active GPUs (those that have not been explicitly excluded in\n        the command line arguments).\n\n        Notes\n        -----\n        Override for GPU specific checking\n\n        Returns\n        -------\n        list\n            The list of device indices that are available for Faceswap to use\n        \"\"\"\n        devices = [idx for idx in range(self._device_count) if idx not in _EXCLUDE_DEVICES]\n        self._log(\"debug\", f\"Active GPU Devices: {devices}\")\n        return devices\n\n    def _get_handles(self) -> list:\n        \"\"\" Override to obtain GPU specific device handles for all connected devices.\n\n        Returns\n        -------\n        list\n            The device handle for each connected GPU\n        \"\"\"\n        raise NotImplementedError()\n\n    def _get_driver(self) -> str:\n        \"\"\" Override to obtain the GPU specific driver version.\n\n        Returns\n        -------\n        str\n            The GPU driver currently in use\n        \"\"\"\n        raise NotImplementedError()\n\n    def _get_device_names(self) -> list[str]:\n        \"\"\" Override to obtain the names of all connected GPUs. The quality of this information\n        depends on the backend and OS being used, but it should be sufficient for identifying\n        cards.\n\n        Returns\n        -------\n        list\n            List of device names for connected GPUs as corresponding to the values in\n            :attr:`_handles`\n        \"\"\"\n        raise NotImplementedError()\n\n    def _get_vram(self) -> list[int]:\n        \"\"\" Override to obtain the total VRAM in Megabytes for each connected GPU.\n\n        Returns\n        -------\n        list\n             List of `float`s containing the total amount of VRAM in Megabytes for each\n             connected GPU as corresponding to the values in :attr:`_handles`\n        \"\"\"\n        raise NotImplementedError()\n\n    def _get_free_vram(self) -> list[int]:\n        \"\"\" Override to obtain the amount of VRAM that is available, in Megabytes, for each\n        connected GPU.\n\n        Returns\n        -------\n        list\n            List of `float`s containing the amount of VRAM available, in Megabytes, for each\n            connected GPU as corresponding to the values in :attr:`_handles\n        \"\"\"\n        raise NotImplementedError()\n\n    def get_card_most_free(self) -> BiggestGPUInfo:\n        \"\"\" Obtain statistics for the GPU with the most available free VRAM.\n\n        Returns\n        -------\n        :class:`BiggestGpuInfo`\n            If a GPU is not detected then the **card_id** is returned as ``-1`` and the amount\n            of free and total RAM available is fixed to 2048 Megabytes.\n        \"\"\"\n        if len(self._active_devices) == 0:\n            retval = BiggestGPUInfo(card_id=-1,\n                                    device=\"No GPU devices found\",\n                                    free=2048,\n                                    total=2048)\n        else:\n            free_vram = [self._vram_free[i] for i in self._active_devices]\n            vram_free = max(free_vram)\n            card_id = self._active_devices[free_vram.index(vram_free)]\n            retval = BiggestGPUInfo(card_id=card_id,\n                                    device=self._device_names[card_id],\n                                    free=vram_free,\n                                    total=self._vram[card_id])\n        self._log(\"debug\", f\"Active GPU Card with most free VRAM: {retval}\")\n        return retval\n", "lib/gpu_stats/nvidia_apple.py": "#!/usr/bin/env python3\n\"\"\" Collects and returns Information on available Nvidia GPUs connected to Apple Macs. \"\"\"\nimport pynvx\n\nfrom lib.utils import FaceswapError\n\nfrom ._base import _GPUStats\n\n\nclass NvidiaAppleStats(_GPUStats):\n    \"\"\" Holds information and statistics about Nvidia GPU(s) available on the currently\n    running Apple system.\n\n    Notes\n    -----\n    PyNvx is used for hooking in to Nvidia's Machine Learning Library and allows for pulling fairly\n    extensive statistics for Apple based Nvidia GPUs\n\n    Parameters\n    ----------\n    log: bool, optional\n        Whether the class should output information to the logger. There may be occasions where the\n        logger has not yet been set up when this class is queried. Attempting to log in these\n        instances will raise an error. If GPU stats are being queried prior to the logger being\n        available then this parameter should be set to ``False``. Otherwise set to ``True``.\n        Default: ``True``\n    \"\"\"\n\n    def _initialize(self) -> None:\n        \"\"\" Initialize PyNvx for Nvidia GPUs on Apple.\n\n        If :attr:`_is_initialized` is ``True`` then this function just returns performing no\n        action. Otherwise :attr:`is_initialized` is set to ``True`` after successfully\n        initializing NVML.\n\n        Raises\n        ------\n        FaceswapError\n            If the NVML library could not be successfully loaded\n        \"\"\"\n        if self._is_initialized:\n            return\n        self._log(\"debug\", \"Initializing Pynvx for Apple Nvidia GPU.\")\n        try:\n            pynvx.cudaInit()  # pylint:disable=no-member\n        except RuntimeError as err:\n            msg = (\"An unhandled exception occured reading from the Nvidia Machine Learning \"\n                   f\"Library. Original error: {str(err)}\")\n            raise FaceswapError(msg) from err\n        super()._initialize()\n\n    def _shutdown(self) -> None:\n        \"\"\" Set :attr:`_is_initialized` back to ``False``. \"\"\"\n        self._log(\"debug\", \"Shutting down NVML\")\n        super()._shutdown()\n\n    def _get_device_count(self) -> int:\n        \"\"\" Detect the number of GPUs attached to the system.\n\n        Returns\n        -------\n        int\n            The total number of GPUs connected to the PC\n        \"\"\"\n        retval = pynvx.cudaDeviceGetCount(ignore=True)  # pylint:disable=no-member\n        self._log(\"debug\", f\"GPU Device count: {retval}\")\n        return retval\n\n    def _get_handles(self) -> list:\n        \"\"\" Obtain the device handles for all Apple connected Nvidia GPUs.\n\n        Returns\n        -------\n        list\n            The list of pointers for connected Nvidia GPUs\n        \"\"\"\n        handles = pynvx.cudaDeviceGetHandles(ignore=True)  # pylint:disable=no-member\n        self._log(\"debug\", f\"GPU Handles found: {len(handles)}\")\n        return handles\n\n    def _get_driver(self) -> str:\n        \"\"\" Obtain the Nvidia driver version currently in use.\n\n        Returns\n        -------\n        str\n            The current GPU driver version\n        \"\"\"\n        driver = pynvx.cudaSystemGetDriverVersion(ignore=True)  # pylint:disable=no-member\n        self._log(\"debug\", f\"GPU Driver: {driver}\")\n        return driver\n\n    def _get_device_names(self) -> list[str]:\n        \"\"\" Obtain the list of names of connected Nvidia GPUs as identified in :attr:`_handles`.\n\n        Returns\n        -------\n        list\n            The list of connected Nvidia GPU names\n        \"\"\"\n        names = [pynvx.cudaGetName(handle, ignore=True)  # pylint:disable=no-member\n                 for handle in self._handles]\n        self._log(\"debug\", f\"GPU Devices: {names}\")\n        return names\n\n    def _get_vram(self) -> list[int]:\n        \"\"\" Obtain the VRAM in Megabytes for each connected Nvidia GPU as identified in\n        :attr:`_handles`.\n\n        Returns\n        -------\n        list\n            The VRAM in Megabytes for each connected Nvidia GPU\n        \"\"\"\n        vram = [\n            pynvx.cudaGetMemTotal(handle, ignore=True) / (1024 * 1024)  # pylint:disable=no-member\n            for handle in self._handles]\n        self._log(\"debug\", f\"GPU VRAM: {vram}\")\n        return vram\n\n    def _get_free_vram(self) -> list[int]:\n        \"\"\" Obtain the amount of VRAM that is available, in Megabytes, for each connected Nvidia\n        GPU.\n\n        Returns\n        -------\n        list\n             List of `float`s containing the amount of VRAM available, in Megabytes, for each\n             connected GPU as corresponding to the values in :attr:`_handles\n        \"\"\"\n        vram = [\n            pynvx.cudaGetMemFree(handle, ignore=True) / (1024 * 1024)  # pylint:disable=no-member\n            for handle in self._handles]\n        self._log(\"debug\", f\"GPU VRAM free: {vram}\")\n        return vram\n", "lib/gpu_stats/__init__.py": "#!/usr/bin/env python3\n\"\"\" Dynamically import the correct GPU Stats library based on the faceswap backend and the machine\nbeing used. \"\"\"\n\nimport platform\n\nfrom lib.utils import get_backend\n\nfrom ._base import set_exclude_devices, GPUInfo\n\nbackend = get_backend()\n\nif backend == \"nvidia\" and platform.system().lower() == \"darwin\":\n    from .nvidia_apple import NvidiaAppleStats as GPUStats  # type:ignore\nelif backend == \"nvidia\":\n    from .nvidia import NvidiaStats as GPUStats  # type:ignore\nelif backend == \"apple_silicon\":\n    from .apple_silicon import AppleSiliconStats as GPUStats  # type:ignore\nelif backend == \"directml\":\n    from .directml import DirectML as GPUStats  # type:ignore\nelif backend == \"rocm\":\n    from .rocm import ROCm as GPUStats  # type:ignore\nelse:\n    from .cpu import CPUStats as GPUStats  # type:ignore\n", "lib/gpu_stats/nvidia.py": "#!/usr/bin/env python3\n\"\"\" Collects and returns Information on available Nvidia GPUs. \"\"\"\nimport os\n\nimport pynvml\n\nfrom lib.utils import FaceswapError\n\nfrom ._base import _GPUStats\n\n\nclass NvidiaStats(_GPUStats):\n    \"\"\" Holds information and statistics about Nvidia GPU(s) available on the currently\n    running system.\n\n    Notes\n    -----\n    PyNVML is used for hooking in to Nvidia's Machine Learning Library and allows for pulling\n    fairly extensive statistics for Nvidia GPUs\n\n    Parameters\n    ----------\n    log: bool, optional\n        Whether the class should output information to the logger. There may be occasions where the\n        logger has not yet been set up when this class is queried. Attempting to log in these\n        instances will raise an error. If GPU stats are being queried prior to the logger being\n        available then this parameter should be set to ``False``. Otherwise set to ``True``.\n        Default: ``True``\n    \"\"\"\n\n    def _initialize(self) -> None:\n        \"\"\" Initialize PyNVML for Nvidia GPUs.\n\n        If :attr:`_is_initialized` is ``True`` then this function just returns performing no\n        action. Otherwise :attr:`is_initialized` is set to ``True`` after successfully\n        initializing NVML.\n\n        Raises\n        ------\n        FaceswapError\n            If the NVML library could not be successfully loaded\n        \"\"\"\n        if self._is_initialized:\n            return\n        try:\n            self._log(\"debug\", \"Initializing PyNVML for Nvidia GPU.\")\n            pynvml.nvmlInit()\n        except (pynvml.NVMLError_LibraryNotFound,  # pylint:disable=no-member\n                pynvml.NVMLError_DriverNotLoaded,  # pylint:disable=no-member\n                pynvml.NVMLError_NoPermission) as err:  # pylint:disable=no-member\n            msg = (\"There was an error reading from the Nvidia Machine Learning Library. The most \"\n                   \"likely cause is incorrectly installed drivers. If this is the case, Please \"\n                   \"remove and reinstall your Nvidia drivers before reporting. Original \"\n                   f\"Error: {str(err)}\")\n            raise FaceswapError(msg) from err\n        except Exception as err:  # pylint:disable=broad-except\n            msg = (\"An unhandled exception occured reading from the Nvidia Machine Learning \"\n                   f\"Library. Original error: {str(err)}\")\n            raise FaceswapError(msg) from err\n        super()._initialize()\n\n    def _shutdown(self) -> None:\n        \"\"\" Cleanly close access to NVML and set :attr:`_is_initialized` back to ``False``. \"\"\"\n        self._log(\"debug\", \"Shutting down NVML\")\n        pynvml.nvmlShutdown()\n        super()._shutdown()\n\n    def _get_device_count(self) -> int:\n        \"\"\" Detect the number of GPUs attached to the system.\n\n        Returns\n        -------\n        int\n            The total number of GPUs connected to the PC\n        \"\"\"\n        try:\n            retval = pynvml.nvmlDeviceGetCount()\n        except pynvml.NVMLError as err:\n            self._log(\"debug\", \"Error obtaining device count. Setting to 0. \"\n                               f\"Original error: {str(err)}\")\n            retval = 0\n        self._log(\"debug\", f\"GPU Device count: {retval}\")\n        return retval\n\n    def _get_active_devices(self) -> list[int]:\n        \"\"\" Obtain the indices of active GPUs (those that have not been explicitly excluded by\n        CUDA_VISIBLE_DEVICES environment variable or explicitly excluded in the command line\n        arguments).\n\n        Returns\n        -------\n        list\n            The list of device indices that are available for Faceswap to use\n        \"\"\"\n        devices = super()._get_active_devices()\n        env_devices = os.environ.get(\"CUDA_VISIBLE_DEVICES\")\n        if env_devices:\n            new_devices = [int(i) for i in env_devices.split(\",\")]\n            devices = [idx for idx in devices if idx in new_devices]\n        self._log(\"debug\", f\"Active GPU Devices: {devices}\")\n        return devices\n\n    def _get_handles(self) -> list:\n        \"\"\" Obtain the device handles for all connected Nvidia GPUs.\n\n        Returns\n        -------\n        list\n            The list of pointers for connected Nvidia GPUs\n        \"\"\"\n        handles = [pynvml.nvmlDeviceGetHandleByIndex(i)\n                   for i in range(self._device_count)]\n        self._log(\"debug\", f\"GPU Handles found: {len(handles)}\")\n        return handles\n\n    def _get_driver(self) -> str:\n        \"\"\" Obtain the Nvidia driver version currently in use.\n\n        Returns\n        -------\n        str\n            The current GPU driver version\n        \"\"\"\n        try:\n            driver = pynvml.nvmlSystemGetDriverVersion()\n        except pynvml.NVMLError as err:\n            self._log(\"debug\", f\"Unable to obtain driver. Original error: {str(err)}\")\n            driver = \"No Nvidia driver found\"\n        self._log(\"debug\", f\"GPU Driver: {driver}\")\n        return driver\n\n    def _get_device_names(self) -> list[str]:\n        \"\"\" Obtain the list of names of connected Nvidia GPUs as identified in :attr:`_handles`.\n\n        Returns\n        -------\n        list\n            The list of connected Nvidia GPU names\n        \"\"\"\n        names = [pynvml.nvmlDeviceGetName(handle)\n                 for handle in self._handles]\n        self._log(\"debug\", f\"GPU Devices: {names}\")\n        return names\n\n    def _get_vram(self) -> list[int]:\n        \"\"\" Obtain the VRAM in Megabytes for each connected Nvidia GPU as identified in\n        :attr:`_handles`.\n\n        Returns\n        -------\n        list\n            The VRAM in Megabytes for each connected Nvidia GPU\n        \"\"\"\n        vram = [pynvml.nvmlDeviceGetMemoryInfo(handle).total / (1024 * 1024)\n                for handle in self._handles]\n        self._log(\"debug\", f\"GPU VRAM: {vram}\")\n        return vram\n\n    def _get_free_vram(self) -> list[int]:\n        \"\"\" Obtain the amount of VRAM that is available, in Megabytes, for each connected Nvidia\n        GPU.\n\n        Returns\n        -------\n        list\n             List of `float`s containing the amount of VRAM available, in Megabytes, for each\n             connected GPU as corresponding to the values in :attr:`_handles\n        \"\"\"\n        is_initialized = self._is_initialized\n        if not is_initialized:\n            self._initialize()\n            self._handles = self._get_handles()\n\n        vram = [pynvml.nvmlDeviceGetMemoryInfo(handle).free / (1024 * 1024)\n                for handle in self._handles]\n        if not is_initialized:\n            self._shutdown()\n\n        self._log(\"debug\", f\"GPU VRAM free: {vram}\")\n        return vram\n", "lib/gpu_stats/directml.py": "#!/usr/bin/env python3\n\"\"\" Collects and returns Information on DirectX 12 hardware devices for DirectML. \"\"\"\nfrom __future__ import annotations\nimport os\nimport sys\nimport typing as T\nassert sys.platform == \"win32\"\n\nimport ctypes\nfrom ctypes import POINTER, Structure, windll\nfrom dataclasses import dataclass\nfrom enum import Enum, IntEnum\n\nfrom comtypes import COMError, IUnknown, GUID, STDMETHOD, HRESULT  # pylint:disable=import-error\n\nfrom ._base import _GPUStats\n\nif T.TYPE_CHECKING:\n    from collections.abc import Callable\n\n# Monkey patch default ctypes.c_uint32 value to Enum ctypes property for easier tracking of types\n# We can't just subclass as the attribute will be assumed to be part of the Enumeration, so we\n# attach it directly and suck up the typing errors.\nsetattr(Enum, \"ctype\", ctypes.c_uint32)\n\n\n#############################\n# CTYPES SUPPORTING OBJECTS #\n#############################\n# GUIDs\n@dataclass\nclass LookupGUID:\n    \"\"\" GUIDs that are required for creating COM objects which are used and discarded.\n\n    Reference\n    ---------\n    https://learn.microsoft.com/en-us/windows/win32/api/d3d12/nn-d3d12-id3d12device2\n    \"\"\"\n    IDXGIDevice = GUID(\"{54ec77fa-1377-44e6-8c32-88fd5f44c84c}\")\n    ID3D12Device = GUID(\"{189819f1-1db6-4b57-be54-1821339b85f7}\")\n\n\n# ENUMS\nclass DXGIGpuPreference(IntEnum):\n    \"\"\" The preference of GPU for the app to run on.\n\n    Reference\n    ---------\n    https://learn.microsoft.com/en-us/windows/win32/api/dxgi1_6/ne-dxgi1_6-dxgi_gpu_preference\n    \"\"\"\n    DXGI_GPU_PREFERENCE_UNSPECIFIED = 0\n    DXGI_GPU_PREFERENCE_MINIMUM_POWER = 1\n    DXGI_GPU_PREFERENCE_HIGH_PERFORMANCE = 2\n\n\nclass DXGIAdapterFlag(IntEnum):\n    \"\"\" Identifies the type of DXGI adapter.\n\n    Reference\n    ---------\n    https://learn.microsoft.com/en-us/windows/win32/api/dxgi/ne-dxgi-dxgi_adapter_flag\n    \"\"\"\n    DXGI_ADAPTER_FLAG_NONE = 0\n    DXGI_ADAPTER_FLAG_REMOTE = 1\n    DXGI_ADAPTER_FLAG_SOFTWARE = 2\n    DXGI_ADAPTER_FLAG_FORCE_DWORD = 0xffffffff\n\n\nclass DXGIMemorySegmentGroup(IntEnum):\n    \"\"\" Constants that specify an adapter's memory segment grouping.\n\n    Reference\n    ---------\n    https://learn.microsoft.com/en-us/windows/win32/api/dxgi1_4/ne-dxgi1_4-dxgi_memory_segment_group\n    \"\"\"\n    DXGI_MEMORY_SEGMENT_GROUP_LOCAL = 0\n    DXGI_MEMORY_SEGMENT_GROUP_NON_LOCAL = 1\n\n\nclass D3DFeatureLevel(Enum):\n    \"\"\" Describes the set of features targeted by a Direct3D device.\n\n    Reference\n    ---------\n    https://learn.microsoft.com/en-us/windows/win32/api/d3dcommon/ne-d3dcommon-d3d_feature_level\n    \"\"\"\n    D3D_FEATURE_LEVEL_1_0_CORE = 0x1000\n    D3D_FEATURE_LEVEL_9_1 = 0x9100\n    D3D_FEATURE_LEVEL_9_2 = 0x9200\n    D3D_FEATURE_LEVEL_9_3 = 0x9300\n    D3D_FEATURE_LEVEL_10_0 = 0xa000\n    D3D_FEATURE_LEVEL_10_1 = 0xa100\n    D3D_FEATURE_LEVEL_11_0 = 0xb000\n    D3D_FEATURE_LEVEL_11_1 = 0xb100\n    D3D_FEATURE_LEVEL_12_0 = 0xc000\n    D3D_FEATURE_LEVEL_12_1 = 0xc100\n    D3D_FEATURE_LEVEL_12_2 = 0xc200\n\n\nclass VendorID(Enum):\n    \"\"\" DirectX VendorID Enum \"\"\"\n    AMD = 0x1002\n    NVIDIA = 0x10DE\n    MICROSOFT = 0x1414\n    QUALCOMM = 0x4D4F4351\n    INTEL = 0x8086\n\n\n# STRUCTS\nclass StructureRepr(Structure):\n    \"\"\" Override the standard structure class to add a useful __repr__ for logging \"\"\"\n    def __repr__(self) -> str:\n        \"\"\" Output the class name and the structure contents \"\"\"\n        content = [\"=\".join([field[0], str(getattr(self, field[0]))])\n                   for field in self._fields_]\n        if self.__dict__:  # Add manually added parameters\n            content.extend(\"=\".join([key, str(val)]) for key, val in self.__dict__.items())\n        return f\"{self.__class__.__name__}({', '.join(content)})\"\n\n\nclass LUID(StructureRepr):  # pylint:disable=too-few-public-methods\n    \"\"\" Local Identifier for an adaptor\n\n    Reference\n    ---------\n    https://learn.microsoft.com/en-us/windows/win32/api/winnt/ns-winnt-luid \"\"\"\n    _fields_ = [(\"LowPart\", ctypes.c_ulong), (\"HighPart\", ctypes.c_long)]\n\n\nclass DriverVersion(StructureRepr):  # pylint:disable=too-few-public-methods\n    \"\"\" Stucture (based off LARGE_INTEGER) to hold the driver version\n\n    Reference\n    ---------\n    https://docs.microsoft.com/en-us/windows/win32/api/winnt/ns-winnt-large_integer-r1\"\"\"\n    _fields_ = [(\"parts_a\", ctypes.c_uint16),\n                (\"parts_b\", ctypes.c_uint16),\n                (\"parts_c\", ctypes.c_uint16),\n                (\"parts_d\", ctypes.c_uint16)]\n\n\nclass DXGIAdapterDesc1(StructureRepr):  # pylint:disable=too-few-public-methods\n    \"\"\" Describes an adapter (or video card) using DXGI 1.1\n\n    Reference\n    ---------\n    https://learn.microsoft.com/en-us/windows/win32/api/dxgi/ns-dxgi-DXGIAdapterDesc1 \"\"\"\n    _fields_ = [\n        (\"Description\", ctypes.c_wchar * 128),\n        (\"VendorId\", ctypes.c_uint),\n        (\"DeviceId\", ctypes.c_uint),\n        (\"SubSysId\", ctypes.c_uint),\n        (\"Revision\", ctypes.c_uint),\n        (\"DedicatedVideoMemory\", ctypes.c_size_t),\n        (\"DedicatedSystemMemory\", ctypes.c_size_t),\n        (\"SharedSystemMemory\", ctypes.c_size_t),\n        (\"AdapterLuid\", LUID),\n        (\"Flags\", DXGIAdapterFlag.ctype)]  # type:ignore[attr-defined] # pylint:disable=no-member\n\n\nclass DXGIQueryVideoMemoryInfo(StructureRepr):  # pylint:disable=too-few-public-methods\n    \"\"\" Describes the current video memory budgeting parameters.\n\n    Reference\n    ---------\n    https://learn.microsoft.com/en-us/windows/win32/api/dxgi1_4/ns-dxgi1_4-dxgi_query_video_memory_info\n    \"\"\"\n    _fields_ = [(\"Budget\", ctypes.c_uint64),\n                (\"CurrentUsage\", ctypes.c_uint64),\n                (\"AvailableForReservation\", ctypes.c_uint64),\n                (\"CurrentReservation\", ctypes.c_uint64)]\n\n\n# COM OBjects\nclass IDXObject(IUnknown):  # pylint:disable=too-few-public-methods\n    \"\"\" Base interface for all DXGI objects.\n\n    Reference\n    ---------\n    https://learn.microsoft.com/en-us/windows/win32/api/dxgi/nn-dxgi-idxgiobject\n    \"\"\"\n    _iid_ = GUID(\"{aec22fb8-76f3-4639-9be0-28eb43a67a2e}\")\n    _methods_ = [STDMETHOD(HRESULT, \"SetPrivateData\",\n                           [GUID, ctypes.c_uint, POINTER(ctypes.c_void_p)]),\n                 STDMETHOD(HRESULT, \"SetPrivateDataInterface\", [GUID, POINTER(IUnknown)]),\n                 STDMETHOD(HRESULT, \"GetPrivateData\",\n                           [GUID, POINTER(ctypes.c_uint), POINTER(ctypes.c_void_p)]),\n                 STDMETHOD(HRESULT, \"GetParent\", [GUID, POINTER(POINTER(ctypes.c_void_p))])]\n\n\nclass IDXGIFactory6(IDXObject):  # pylint:disable=too-few-public-methods\n    \"\"\" Implements methods for generating DXGI objects\n\n    Reference\n    ---------\n    https://learn.microsoft.com/en-us/windows/win32/api/dxgi/nn-dxgi-idxgifactory\n    \"\"\"\n    _iid_ = GUID(\"{c1b6694f-ff09-44a9-b03c-77900a0a1d17}\")\n\n    _methods_ = [STDMETHOD(HRESULT, \"EnumAdapters\"),  # IDXGIFactory\n                 STDMETHOD(HRESULT, \"MakeWindowAssociation\"),\n                 STDMETHOD(HRESULT, \"GetWindowAssociation\"),\n                 STDMETHOD(HRESULT, \"CreateSwapChain\"),\n                 STDMETHOD(HRESULT, \"CreateSoftwareAdapter\"),\n                 STDMETHOD(HRESULT, \"EnumAdapters1\"),  # IDXGIFactory1\n                 STDMETHOD(ctypes.c_bool, \"IsCurrent\"),\n                 STDMETHOD(ctypes.c_bool, \"IsWindowedStereoEnabled\"),  # IDXGIFactory2\n                 STDMETHOD(HRESULT, \"CreateSwapChainForHwnd\"),\n                 STDMETHOD(HRESULT, \"CreateSwapChainForCoreWindow\"),\n                 STDMETHOD(HRESULT, \"GetSharedResourceAdapterLuid\"),\n                 STDMETHOD(HRESULT, \"RegisterStereoStatusWindow\"),\n                 STDMETHOD(HRESULT, \"RegisterStereoStatusEvent\"),\n                 STDMETHOD(None, \"UnregisterStereoStatus\"),\n                 STDMETHOD(HRESULT, \"RegisterOcclusionStatusWindow\"),\n                 STDMETHOD(HRESULT, \"RegisterOcclusionStatusEvent\"),\n                 STDMETHOD(None, \"UnregisterOcclusionStatus\"),\n                 STDMETHOD(HRESULT, \"CreateSwapChainForComposition\"),\n                 STDMETHOD(ctypes.c_uint, \"GetCreationFlags\"),  # IDXGIFactory3\n                 STDMETHOD(HRESULT, \"EnumAdapterByLuid\",  # IDXGIFactory4\n                           [LUID, GUID, POINTER(POINTER(ctypes.c_void_p))]),\n                 STDMETHOD(HRESULT, \"EnumWarpAdapter\"),\n                 STDMETHOD(HRESULT, \"CheckFeatureSupport\"),  # IDXGIFactory5\n                 STDMETHOD(HRESULT,  # IDXGIFactory6\n                           \"EnumAdapterByGpuPreference\",\n                           [ctypes.c_uint,\n                            DXGIGpuPreference.ctype,  # type:ignore[attr-defined] # pylint:disable=no-member  # noqa:E501\n                            GUID,\n                            POINTER(ctypes.c_void_p)])]\n\n\nclass IDXGIAdapter3(IDXObject):  # pylint:disable=too-few-public-methods\n    \"\"\" Represents a display sub-system (including one or more GPU's, DACs and video memory).\n\n    Reference\n    ---------\n    https://learn.microsoft.com/en-us/windows/win32/api/dxgi1_4/nn-dxgi1_4-idxgiadapter3\n    \"\"\"\n    _iid_ = GUID(\"{645967a4-1392-4310-a798-8053ce3e93fd}\")\n    _methods_ = [STDMETHOD(HRESULT, \"EnumOutputs\"),  # v1.0 Methods\n                 STDMETHOD(HRESULT, \"GetDesc\"),\n                 STDMETHOD(HRESULT, \"CheckInterfaceSupport\",  # v1.1 Methods\n                           [GUID, POINTER(DriverVersion)]),\n                 STDMETHOD(HRESULT, \"GetDesc1\", [POINTER(DXGIAdapterDesc1)]),\n                 STDMETHOD(HRESULT, \"GetDesc2\"),  # v1.2 Methods\n                 STDMETHOD(HRESULT,    # v1.3 Methods\n                           \"RegisterHardwareContentProtectionTeardownStatusEvent\"),\n                 STDMETHOD(None, \"UnregisterHardwareContentProtectionTeardownStatus\"),\n                 STDMETHOD(HRESULT,\n                           \"QueryVideoMemoryInfo\",\n                           [ctypes.c_uint,\n                            DXGIMemorySegmentGroup.ctype,  # type:ignore[attr-defined] # pylint:disable=no-member  # noqa:E501\n                            POINTER(DXGIQueryVideoMemoryInfo)]),\n                 STDMETHOD(HRESULT, \"SetVideoMemoryReservation\"),\n                 STDMETHOD(HRESULT, \"RegisterVideoMemoryBudgetChangeNotificationEvent\"),\n                 STDMETHOD(None, \"UnregisterVideoMemoryBudgetChangeNotification\")]\n\n\n###########################\n# PYTHON COLLATED OBJECTS #\n###########################\n@dataclass\nclass Device:\n    \"\"\" Holds information about a device attached to an adapter.\n\n    Parameters\n    ----------\n    description: :class:`DXGIAdapterDesc1`\n        The information returned from DXGI.dll about the device\n    driver_version: str\n        The driver version of the device\n    local_mem: :class:`DXGIQueryVideoMemoryInfo`\n        The amount of local memory currently available\n    non_local_mem: :class:`DXGIQueryVideoMemoryInfo`\n        The amount of non-local memory currently available\n    is_d3d12: bool\n        ``True`` if the device supports DirectX12\n    is_compute_only: bool\n        ``True`` if the device is only compute (no graphics)\n    \"\"\"\n    description: DXGIAdapterDesc1\n    driver_version: str\n    local_mem: DXGIQueryVideoMemoryInfo\n    non_local_mem: DXGIQueryVideoMemoryInfo\n    is_d3d12: bool\n    is_compute_only: bool = False\n\n    @property\n    def is_software_adapter(self) -> bool:\n        \"\"\" bool: ``True`` if this is a software adapter. \"\"\"\n        return self.description.Flags == DXGIAdapterFlag.DXGI_ADAPTER_FLAG_SOFTWARE.value\n\n    @property\n    def is_valid(self) -> bool:\n        \"\"\" bool: ``True`` if this adapter is a hardware adaptor and is not the basic renderer \"\"\"\n        if self.is_software_adapter:\n            return False\n\n        if (self.description.VendorId == VendorID.MICROSOFT.value and\n                self.description.DeviceId == 0x8c):\n            return False\n\n        return True\n\n\nclass Adapters():  # pylint:disable=too-few-public-methods\n    \"\"\" Wrapper to obtain connected DirectX Graphics interface adapters from Windows\n\n    Parameters\n    ----------\n    log_func: :func:`~lib.gpu_stats._base._log`\n        The logging function to use from the parent GPUStats class\n    \"\"\"\n    def __init__(self, log_func: Callable[[str, str], None]) -> None:\n        self._log = log_func\n        self._log(\"debug\", f\"Initializing {self.__class__.__name__}: (log_func: {log_func})\")\n\n        self._factory = self._get_factory()\n        self._adapters = self._get_adapters()\n        self._devices = self._process_adapters()\n\n        self._valid_adaptors: list[Device] = []\n        self._log(\"debug\", f\"Initialized {self.__class__.__name__}\")\n\n    def _get_factory(self) -> ctypes._Pointer:\n        \"\"\" Get a DXGI 1.1 Factory object\n\n        Reference\n        ---------\n        https://learn.microsoft.com/en-us/windows/win32/api/dxgi/nf-dxgi-createdxgifactory1\n\n        Returns\n        -------\n        :class:`ctypes._Pointer`\n            A pointer to a :class:`IDXGIFactory6` COM instance\n        \"\"\"\n        factory_func = windll.dxgi.CreateDXGIFactory\n        factory_func.argtypes = (GUID, POINTER(ctypes.c_void_p))\n        factory_func.restype = HRESULT\n        handle = ctypes.c_void_p(0)\n        factory_func(IDXGIFactory6._iid_,  ctypes.byref(handle))  # pylint:disable=protected-access\n        retval = ctypes.POINTER(IDXGIFactory6)(T.cast(IDXGIFactory6, handle.value))\n        self._log(\"debug\", f\"factory: {retval}\")\n        return retval\n\n    @property\n    def valid_adapters(self) -> list[Device]:\n        \"\"\" list[:class:`Device`]: DirectX 12 compatible hardware :class:`Device` objects \"\"\"\n        if self._valid_adaptors:\n            return self._valid_adaptors\n\n        for device in self._devices:\n            if not device.is_valid:\n                # Sorted by most performant so everything after first basic adapter is skipped\n                break\n            if not device.is_d3d12:\n                continue\n            self._valid_adaptors.append(device)\n        self._log(\"debug\", f\"valid_adaptors: {self._valid_adaptors}\")\n        return self._valid_adaptors\n\n    def _get_adapters(self) -> list[ctypes._Pointer]:\n        \"\"\" Obtain DirectX 12 supporting hardware adapter objects and add a Device class for\n        obtaining details\n\n        Returns\n        -------\n        list\n            List of :class:`ctypes._Pointer` objects\n        \"\"\"\n        idx = 0\n        retval = []\n        while True:\n            try:\n                handle = ctypes.c_void_p(0)\n                success = self._factory.EnumAdapterByGpuPreference(  # type:ignore[attr-defined]\n                    idx,\n                    DXGIGpuPreference.DXGI_GPU_PREFERENCE_HIGH_PERFORMANCE.value,\n                    IDXGIAdapter3._iid_,  # pylint:disable=protected-access\n                    ctypes.byref(handle))\n                if success != 0:\n                    raise AttributeError(\"Error calling EnumAdapterByGpuPreference. Result: \"\n                                         f\"{hex(ctypes.c_ulong(success).value)}\")\n                adapter = POINTER(IDXGIAdapter3)(T.cast(IDXGIAdapter3, handle.value))\n                self._log(\"debug\", f\"found adapter: {adapter}\")\n                retval.append(adapter)\n            except COMError as err:\n                err_code = hex(ctypes.c_ulong(err.hresult).value)  # pylint:disable=no-member\n                self._log(\n                    \"debug\",\n                    \"COM Error. Breaking: \"\n                    f\"{err.text}({err_code})\")  # pylint:disable=no-member\n                break\n            finally:\n                idx += 1\n\n        self._log(\"debug\", f\"adapters: {retval}\")\n        return retval\n\n    def _query_adapter(self, func: Callable[[T.Any], T.Any], *args: T.Any) -> None:\n        \"\"\" Query an adapter function, logging if the HRESULT is not a success\n\n        Parameters\n        ----------\n        func: Callable[[Any], Any]\n            The adaptor function to call\n        args: Any\n            The arguments to pass to the adaptor function\n        \"\"\"\n        check = func(*args)\n        if check:\n            self._log(\"debug\", f\"Failed HRESULT for func {func}({args}): \"\n                               f\"{hex(ctypes.c_ulong(check).value)}\")\n\n    def _test_d3d12(self, adapter: ctypes._Pointer) -> bool:\n        \"\"\" Test whether the given adapter supports DirectX 12\n\n        Parameters\n        ----------\n        adapter: :class:`ctypes._Pointer`\n            A pointer to an adapter instance\n\n        Returns\n        -------\n        bool\n            ``True`` if the given adapter supports DirectX 12\n        \"\"\"\n        factory_func = windll.d3d12.D3D12CreateDevice\n        factory_func.argtypes = (\n            POINTER(IUnknown),\n            D3DFeatureLevel.ctype,  # type:ignore[attr-defined] # pylint:disable=no-member\n            GUID,\n            POINTER(ctypes.c_void_p))\n        handle = ctypes.c_void_p(0)\n        factory_func.restype = HRESULT\n        success = factory_func(adapter,\n                               D3DFeatureLevel.D3D_FEATURE_LEVEL_11_0.value,\n                               LookupGUID.ID3D12Device,\n                               ctypes.byref(handle))\n        return success in (0, 1)\n\n    def _process_adapters(self) -> list[Device]:\n        \"\"\" Process the adapters to add discovered information.\n\n        Returns\n        -------\n        list[:class:`Device`]\n            List of device of objects found in the adapters\n        \"\"\"\n        retval = []\n        for adapter in self._adapters:\n            # Description\n            desc = DXGIAdapterDesc1()\n            self._query_adapter(adapter.GetDesc1, ctypes.byref(desc))  # type:ignore[attr-defined]\n\n            # Driver Version\n            driver = DriverVersion()\n            self._query_adapter(adapter.CheckInterfaceSupport,  # type:ignore[attr-defined]\n                                LookupGUID.IDXGIDevice,\n                                ctypes.byref(driver))\n            driver_version = f\"{driver.parts_d}.{driver.parts_c}.{driver.parts_b}.{driver.parts_a}\"\n\n            # Current Memory\n            local_mem = DXGIQueryVideoMemoryInfo()\n            self._query_adapter(adapter.QueryVideoMemoryInfo,  # type:ignore[attr-defined]\n                                0,\n                                DXGIMemorySegmentGroup.DXGI_MEMORY_SEGMENT_GROUP_LOCAL.value,\n                                local_mem)\n            non_local_mem = DXGIQueryVideoMemoryInfo()\n            self._query_adapter(\n                adapter.QueryVideoMemoryInfo,  # type:ignore[attr-defined]\n                0,\n                DXGIMemorySegmentGroup.DXGI_MEMORY_SEGMENT_GROUP_NON_LOCAL.value,\n                non_local_mem)\n\n            # is_d3d12\n            is_d3d12 = self._test_d3d12(adapter)\n\n            retval.append(Device(desc, driver_version, local_mem, non_local_mem, is_d3d12))\n\n        return retval\n\n\nclass DirectML(_GPUStats):\n    \"\"\" Holds information and statistics about GPUs connected using Windows API\n\n    Parameters\n    ----------\n    log: bool, optional\n        Whether the class should output information to the logger. There may be occasions where the\n        logger has not yet been set up when this class is queried. Attempting to log in these\n        instances will raise an error. If GPU stats are being queried prior to the logger being\n        available then this parameter should be set to ``False``. Otherwise set to ``True``.\n        Default: ``True``\n    \"\"\"\n    def __init__(self, log: bool = True) -> None:\n        self._devices: list[Device] = []\n        super().__init__(log=log)\n\n    @property\n    def _all_vram(self) -> list[int]:\n        \"\"\" list: The VRAM of each GPU device that the DX API has discovered. \"\"\"\n        return [int(device.description.DedicatedVideoMemory / (1024 * 1024))\n                for device in self._devices]\n\n    @property\n    def names(self) -> list[str]:\n        \"\"\" list: The name of each GPU device that the DX API has discovered. \"\"\"\n        return [device.description.Description for device in self._devices]\n\n    def _get_active_devices(self) -> list[int]:\n        \"\"\" Obtain the indices of active GPUs (those that have not been explicitly excluded by\n        DML_VISIBLE_DEVICES environment variable or explicitly excluded in the command line\n        arguments).\n\n        Returns\n        -------\n        list\n            The list of device indices that are available for Faceswap to use\n        \"\"\"\n        devices = super()._get_active_devices()\n        env_devices = os.environ.get(\"DML_VISIBLE_DEVICES\")\n        if env_devices:\n            new_devices = [int(i) for i in env_devices.split(\",\")]\n            devices = [idx for idx in devices if idx in new_devices]\n        self._log(\"debug\", f\"Active GPU Devices: {devices}\")\n        return devices\n\n    def _get_devices(self) -> list[Device]:\n        \"\"\" Obtain all detected DX API devices.\n\n        Returns\n        -------\n        list\n            The :class:`~dx_lib.Device` objects for GPUs that the DX API has discovered.\n        \"\"\"\n        adapters = Adapters(log_func=self._log)\n        devices = adapters.valid_adapters\n        self._log(\"debug\", f\"Obtained Devices: {devices}\")\n        return devices\n\n    def _initialize(self) -> None:\n        \"\"\" Initialize DX Core for DirectML backend.\n\n        If :attr:`_is_initialized` is ``True`` then this function just returns performing no\n        action.\n\n        if ``False`` then DirectML is setup, if not already, and GPU information is extracted\n        from the DirectML context.\n        \"\"\"\n        if self._is_initialized:\n            return\n        self._log(\"debug\", \"Initializing Win DX API for DirectML.\")\n        self._devices = self._get_devices()\n        super()._initialize()\n\n    def _get_device_count(self) -> int:\n        \"\"\" Detect the number of GPUs available from the DX API.\n\n        Returns\n        -------\n        int\n            The total number of GPUs available\n        \"\"\"\n        retval = len(self._devices)\n        self._log(\"debug\", f\"GPU Device count: {retval}\")\n        return retval\n\n    def _get_handles(self) -> list:\n        \"\"\" The DX API doesn't really use device handles, so we just return the all devices list\n\n        Returns\n        -------\n        list\n            The list of all discovered GPUs\n        \"\"\"\n        handles = self._devices\n        self._log(\"debug\", f\"DirectML GPU Handles found: {handles}\")\n        return handles\n\n    def _get_driver(self) -> str:\n        \"\"\" Obtain the driver versions currently in use.\n\n        Returns\n        -------\n        str\n            The current DirectX 12 GPU driver versions\n        \"\"\"\n        drivers = \"|\".join([device.driver_version if device.driver_version else \"No Driver Found\"\n                            for device in self._devices])\n        self._log(\"debug\", f\"GPU Drivers: {drivers}\")\n        return drivers\n\n    def _get_device_names(self) -> list[str]:\n        \"\"\" Obtain the list of names of connected GPUs as identified in :attr:`_handles`.\n\n        Returns\n        -------\n        list\n            The list of connected Nvidia GPU names\n        \"\"\"\n        names = self.names\n        self._log(\"debug\", f\"GPU Devices: {names}\")\n        return names\n\n    def _get_vram(self) -> list[int]:\n        \"\"\" Obtain the VRAM in Megabytes for each connected DirectML GPU as identified in\n        :attr:`_handles`.\n\n        Returns\n        -------\n        list\n            The VRAM in Megabytes for each connected Nvidia GPU\n        \"\"\"\n        vram = self._all_vram\n        self._log(\"debug\", f\"GPU VRAM: {vram}\")\n        return vram\n\n    def _get_free_vram(self) -> list[int]:\n        \"\"\" Obtain the amount of VRAM that is available, in Megabytes, for each connected DirectX\n        12 supporting GPU.\n\n        Returns\n        -------\n        list\n             List of `float`s containing the amount of VRAM available, in Megabytes, for each\n             connected GPU as corresponding to the values in :attr:`_handles\n        \"\"\"\n        vram = [int(device.local_mem.Budget / (1024 * 1024)) for device in self._devices]\n        self._log(\"debug\", f\"GPU VRAM free: {vram}\")\n        return vram\n", "lib/model/autoclip.py": "\"\"\" Auto clipper for clipping gradients. \"\"\"\nimport numpy as np\nimport tensorflow as tf\n\n\nclass AutoClipper():\n    \"\"\" AutoClip: Adaptive Gradient Clipping for Source Separation Networks\n\n    Parameters\n    ----------\n    clip_percentile: int\n        The percentile to clip the gradients at\n    history_size: int, optional\n        The number of iterations of data to use to calculate the norm\n    Default: ``10000``\n\n    References\n    ----------\n    tf implementation: https://github.com/pseeth/autoclip\n    original paper: https://arxiv.org/abs/2007.14469\n    \"\"\"\n    def __init__(self, clip_percentile: int, history_size: int = 10000):\n        self._clip_percentile = tf.cast(clip_percentile, tf.float64)\n        self._grad_history = tf.Variable(tf.zeros(history_size), trainable=False)\n        self._index = tf.Variable(0, trainable=False)\n        self._history_size = history_size\n\n    def _percentile(self, grad_history: tf.Tensor) -> tf.Tensor:\n        \"\"\" Compute the clip percentile of the gradient history\n\n        Parameters\n        ----------\n        grad_history: :class:`tensorflow.Tensor`\n            Tge gradient history to calculate the clip percentile for\n\n        Returns\n        -------\n        :class:`tensorflow.Tensor`\n            A rank(:attr:`clip_percentile`) `Tensor`\n\n        Notes\n        -----\n        Adapted from\n        https://github.com/tensorflow/probability/blob/r0.14/tensorflow_probability/python/stats/quantiles.py\n        to remove reliance on full tensorflow_probability libraray\n        \"\"\"\n        with tf.name_scope(\"percentile\"):\n            frac_at_q_or_below = self._clip_percentile / 100.\n            sorted_hist = tf.sort(grad_history, axis=-1, direction=\"ASCENDING\")\n\n            num = tf.cast(tf.shape(grad_history)[-1], tf.float64)\n\n            # get indices\n            indices = tf.round((num - 1) * frac_at_q_or_below)\n            indices = tf.clip_by_value(tf.cast(indices, tf.int32),\n                                       0,\n                                       tf.shape(grad_history)[-1] - 1)\n            gathered_hist = tf.gather(sorted_hist, indices, axis=-1)\n\n            # Propagate NaNs. Apparently tf.is_nan doesn't like other dtypes\n            nan_batch_members = tf.reduce_any(tf.math.is_nan(grad_history), axis=None)\n            right_rank_matched_shape = tf.pad(tf.shape(nan_batch_members),\n                                              paddings=[[0, tf.rank(self._clip_percentile)]],\n                                              constant_values=1)\n            nan_batch_members = tf.reshape(nan_batch_members, shape=right_rank_matched_shape)\n\n            nan = np.array(np.nan, gathered_hist.dtype.as_numpy_dtype)\n            gathered_hist = tf.where(nan_batch_members, nan, gathered_hist)\n\n            return gathered_hist\n\n    def __call__(self, grads_and_vars: list[tf.Tensor]) -> list[tf.Tensor]:\n        \"\"\" Call the AutoClip function.\n\n        Parameters\n        ----------\n        grads_and_vars: list\n            The list of gradient tensors and variables for the optimizer\n        \"\"\"\n        grad_norms = [self._get_grad_norm(g) for g, _ in grads_and_vars]\n        total_norm = tf.norm(grad_norms)\n        assign_idx = tf.math.mod(self._index, self._history_size)\n        self._grad_history = self._grad_history[assign_idx].assign(total_norm)\n        self._index = self._index.assign_add(1)\n        clip_value = self._percentile(self._grad_history[: self._index])\n        return [(tf.clip_by_norm(g, clip_value), v) for g, v in grads_and_vars]\n\n    @classmethod\n    def _get_grad_norm(cls, gradients: tf.Tensor) -> tf.Tensor:\n        \"\"\" Obtain the L2 Norm for the gradients\n\n        Parameters\n        ----------\n        gradients: :class:`tensorflow.Tensor`\n            The gradients to calculate the L2 norm for\n\n        Returns\n        -------\n        :class:`tensorflow.Tensor`\n            The L2 Norm of the given gradients\n        \"\"\"\n        values = tf.convert_to_tensor(gradients.values\n                                      if isinstance(gradients, tf.IndexedSlices)\n                                      else gradients, name=\"t\")\n\n        # Calculate L2-norm, clip elements by ratio of clip_norm to L2-norm\n        l2sum = tf.math.reduce_sum(values * values, axis=None, keepdims=True)\n        pred = l2sum > 0\n        # Two-tap tf.where trick to bypass NaN gradients\n        l2sum_safe = tf.where(pred, l2sum, tf.ones_like(l2sum))\n        return tf.squeeze(tf.where(pred, tf.math.sqrt(l2sum_safe), l2sum))\n", "lib/model/normalization.py": "#!/usr/bin/env python3\n\"\"\" Normalization methods for faceswap.py specific to Tensorflow backend \"\"\"\nimport inspect\nimport sys\n\nimport tensorflow as tf\n\n# Fix intellisense/linting for tf.keras' thoroughly broken import system\nfrom tensorflow.python.keras.utils.conv_utils import normalize_data_format  # noqa:E501 # pylint:disable=no-name-in-module\nkeras = tf.keras\nlayers = keras.layers\nK = keras.backend\n\n\nclass AdaInstanceNormalization(layers.Layer):  # type:ignore[name-defined]\n    \"\"\" Adaptive Instance Normalization Layer for Keras.\n\n    Parameters\n    ----------\n    axis: int, optional\n        The axis that should be normalized (typically the features axis). For instance, after a\n        `Conv2D` layer with `data_format=\"channels_first\"`, set `axis=1` in\n        :class:`InstanceNormalization`. Setting `axis=None` will normalize all values in each\n        instance of the batch. Axis 0 is the batch dimension. `axis` cannot be set to 0 to avoid\n        errors. Default: ``None``\n    momentum: float, optional\n        Momentum for the moving mean and the moving variance. Default: `0.99`\n    epsilon: float, optional\n        Small float added to variance to avoid dividing by zero. Default: `1e-3`\n    center: bool, optional\n        If ``True``, add offset of `beta` to normalized tensor. If ``False``, `beta` is ignored.\n        Default: ``True``\n    scale: bool, optional\n        If ``True``, multiply by `gamma`. If ``False``, `gamma` is not used. When the next layer\n        is linear (also e.g. `relu`), this can be disabled since the scaling will be done by\n        the next layer. Default: ``True``\n\n    References\n    ----------\n        Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization - \\\n        https://arxiv.org/abs/1703.06868\n    \"\"\"\n    def __init__(self, axis=-1, momentum=0.99, epsilon=1e-3, center=True, scale=True, **kwargs):\n        super().__init__(**kwargs)\n        self.axis = axis\n        self.momentum = momentum\n        self.epsilon = epsilon\n        self.center = center\n        self.scale = scale\n\n    def build(self, input_shape):\n        \"\"\"Creates the layer weights.\n\n        Parameters\n        ----------\n        input_shape: tensor\n            Keras tensor (future input to layer) or ``list``/``tuple`` of Keras tensors to\n            reference for weight shape computations.\n        \"\"\"\n        dim = input_shape[0][self.axis]\n        if dim is None:\n            raise ValueError('Axis ' + str(self.axis) + ' of '\n                             'input tensor should have a defined dimension '\n                             'but the layer received an input with shape ' +\n                             str(input_shape[0]) + '.')\n\n        super().build(input_shape)\n\n    def call(self, inputs, training=None):  # pylint:disable=unused-argument,arguments-differ\n        \"\"\"This is where the layer's logic lives.\n\n        Parameters\n        ----------\n        inputs: tensor\n            Input tensor, or list/tuple of input tensors\n\n        Returns\n        -------\n        tensor\n            A tensor or list/tuple of tensors\n        \"\"\"\n        input_shape = K.int_shape(inputs[0])\n        reduction_axes = list(range(0, len(input_shape)))\n\n        beta = inputs[1]\n        gamma = inputs[2]\n\n        if self.axis is not None:\n            del reduction_axes[self.axis]\n\n        del reduction_axes[0]\n        mean = K.mean(inputs[0], reduction_axes, keepdims=True)\n        stddev = K.std(inputs[0], reduction_axes, keepdims=True) + self.epsilon\n        normed = (inputs[0] - mean) / stddev\n\n        return normed * gamma + beta\n\n    def get_config(self):\n        \"\"\"Returns the config of the layer.\n\n        The Keras configuration for the layer.\n\n        Returns\n        --------\n        dict\n            A python dictionary containing the layer configuration\n        \"\"\"\n        config = {\n            'axis': self.axis,\n            'momentum': self.momentum,\n            'epsilon': self.epsilon,\n            'center': self.center,\n            'scale': self.scale\n        }\n        base_config = super().get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n    def compute_output_shape(self, input_shape):\n        \"\"\" Calculate the output shape from this layer.\n\n        Parameters\n        ----------\n        input_shape: tuple\n            The input shape to the layer\n\n        Returns\n        -------\n        int\n            The output shape to the layer\n        \"\"\"\n        return input_shape[0]\n\n\nclass GroupNormalization(layers.Layer):  # type:ignore[name-defined]\n    \"\"\" Group Normalization\n\n    Parameters\n    ----------\n    axis: int, optional\n        The axis that should be normalized (typically the features axis). For instance, after a\n        `Conv2D` layer with `data_format=\"channels_first\"`, set `axis=1` in\n        :class:`InstanceNormalization`. Setting `axis=None` will normalize all values in each\n        instance of the batch. Axis 0 is the batch dimension. `axis` cannot be set to 0 to avoid\n        errors. Default: ``None``\n    gamma_init: str, optional\n        Initializer for the gamma weight. Default: `\"one\"`\n    beta_init: str, optional\n        Initializer for the beta weight. Default `\"zero\"`\n    gamma_regularizer: varies, optional\n        Optional regularizer for the gamma weight. Default: ``None``\n    beta_regularizer:  varies, optional\n        Optional regularizer for the beta weight. Default ``None``\n    epsilon: float, optional\n        Small float added to variance to avoid dividing by zero. Default: `1e-3`\n    group: int, optional\n        The group size. Default: `32`\n    data_format: [\"channels_first\", \"channels_last\"], optional\n        The required data format. Optional. Default: ``None``\n    kwargs: dict\n        Any additional standard Keras Layer key word arguments\n\n    References\n    ----------\n    Shaoanlu GAN: https://github.com/shaoanlu/faceswap-GAN\n    \"\"\"\n    # pylint:disable=too-many-instance-attributes\n    def __init__(self, axis=-1, gamma_init='one', beta_init='zero', gamma_regularizer=None,\n                 beta_regularizer=None, epsilon=1e-6, group=32, data_format=None, **kwargs):\n        self.beta = None\n        self.gamma = None\n        super().__init__(**kwargs)\n        self.axis = axis if isinstance(axis, (list, tuple)) else [axis]\n        self.gamma_init = keras.initializers.get(gamma_init)\n        self.beta_init = keras.initializers.get(beta_init)\n        self.gamma_regularizer = keras.regularizers.get(gamma_regularizer)\n        self.beta_regularizer = keras.regularizers.get(beta_regularizer)\n        self.epsilon = epsilon\n        self.group = group\n        self.data_format = normalize_data_format(data_format)\n\n        self.supports_masking = True\n\n    def build(self, input_shape):\n        \"\"\"Creates the layer weights.\n\n        Parameters\n        ----------\n        input_shape: tensor\n            Keras tensor (future input to layer) or ``list``/``tuple`` of Keras tensors to\n            reference for weight shape computations.\n        \"\"\"\n        input_spec = [layers.InputSpec(shape=input_shape)]\n        self.input_spec = input_spec  # pylint:disable=attribute-defined-outside-init\n        shape = [1 for _ in input_shape]\n        if self.data_format == 'channels_last':\n            channel_axis = -1\n            shape[channel_axis] = input_shape[channel_axis]\n        elif self.data_format == 'channels_first':\n            channel_axis = 1\n            shape[channel_axis] = input_shape[channel_axis]\n        # for i in self.axis:\n        #    shape[i] = input_shape[i]\n        self.gamma = self.add_weight(shape=shape,\n                                     initializer=self.gamma_init,\n                                     regularizer=self.gamma_regularizer,\n                                     name='gamma')\n        self.beta = self.add_weight(shape=shape,\n                                    initializer=self.beta_init,\n                                    regularizer=self.beta_regularizer,\n                                    name='beta')\n        self.built = True  # pylint:disable=attribute-defined-outside-init\n\n    def call(self, inputs, *args, **kwargs):  # noqa:C901\n        \"\"\"This is where the layer's logic lives.\n\n        Parameters\n        ----------\n        inputs: tensor\n            Input tensor, or list/tuple of input tensors\n\n        Returns\n        -------\n        tensor\n            A tensor or list/tuple of tensors\n        \"\"\"\n        input_shape = K.int_shape(inputs)\n        if len(input_shape) != 4 and len(input_shape) != 2:\n            raise ValueError('Inputs should have rank ' +\n                             str(4) + \" or \" + str(2) +\n                             '; Received input shape:', str(input_shape))\n\n        if len(input_shape) == 4:\n            if self.data_format == 'channels_last':\n                batch_size, height, width, channels = input_shape\n                if batch_size is None:\n                    batch_size = -1\n\n                if channels < self.group:\n                    raise ValueError('Input channels should be larger than group size' +\n                                     '; Received input channels: ' + str(channels) +\n                                     '; Group size: ' + str(self.group))\n\n                var_x = K.reshape(inputs, (batch_size,\n                                           height,\n                                           width,\n                                           self.group,\n                                           channels // self.group))\n                mean = K.mean(var_x, axis=[1, 2, 4], keepdims=True)\n                std = K.sqrt(K.var(var_x, axis=[1, 2, 4], keepdims=True) + self.epsilon)\n                var_x = (var_x - mean) / std\n\n                var_x = K.reshape(var_x, (batch_size, height, width, channels))\n                retval = self.gamma * var_x + self.beta\n            elif self.data_format == 'channels_first':\n                batch_size, channels, height, width = input_shape\n                if batch_size is None:\n                    batch_size = -1\n\n                if channels < self.group:\n                    raise ValueError('Input channels should be larger than group size' +\n                                     '; Received input channels: ' + str(channels) +\n                                     '; Group size: ' + str(self.group))\n\n                var_x = K.reshape(inputs, (batch_size,\n                                           self.group,\n                                           channels // self.group,\n                                           height,\n                                           width))\n                mean = K.mean(var_x, axis=[2, 3, 4], keepdims=True)\n                std = K.sqrt(K.var(var_x, axis=[2, 3, 4], keepdims=True) + self.epsilon)\n                var_x = (var_x - mean) / std\n\n                var_x = K.reshape(var_x, (batch_size, channels, height, width))\n                retval = self.gamma * var_x + self.beta\n\n        elif len(input_shape) == 2:\n            reduction_axes = list(range(0, len(input_shape)))\n            del reduction_axes[0]\n            batch_size, _ = input_shape\n            if batch_size is None:\n                batch_size = -1\n\n            mean = K.mean(inputs, keepdims=True)\n            std = K.sqrt(K.var(inputs, keepdims=True) + self.epsilon)\n            var_x = (inputs - mean) / std\n\n            retval = self.gamma * var_x + self.beta\n        return retval\n\n    def get_config(self):\n        \"\"\"Returns the config of the layer.\n\n        The Keras configuration for the layer.\n\n        Returns\n        --------\n        dict\n            A python dictionary containing the layer configuration\n        \"\"\"\n        config = {'epsilon': self.epsilon,\n                  'axis': self.axis,\n                  'gamma_init': keras.initializers.serialize(self.gamma_init),\n                  'beta_init': keras.initializers.serialize(self.beta_init),\n                  'gamma_regularizer': keras.regularizers.serialize(self.gamma_regularizer),\n                  'beta_regularizer': keras.regularizers.serialize(self.gamma_regularizer),\n                  'group': self.group}\n        base_config = super().get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nclass InstanceNormalization(layers.Layer):  # type:ignore[name-defined]\n    \"\"\"Instance normalization layer (Lei Ba et al, 2016, Ulyanov et al., 2016).\n\n    Normalize the activations of the previous layer at each step, i.e. applies a transformation\n    that maintains the mean activation close to 0 and the activation standard deviation close to 1.\n\n    Parameters\n    ----------\n    axis: int, optional\n        The axis that should be normalized (typically the features axis). For instance, after a\n        `Conv2D` layer with `data_format=\"channels_first\"`, set `axis=1` in\n        :class:`InstanceNormalization`. Setting `axis=None` will normalize all values in each\n        instance of the batch. Axis 0 is the batch dimension. `axis` cannot be set to 0 to avoid\n        errors. Default: ``None``\n    epsilon: float, optional\n        Small float added to variance to avoid dividing by zero. Default: `1e-3`\n    center: bool, optional\n        If ``True``, add offset of `beta` to normalized tensor. If ``False``, `beta` is ignored.\n        Default: ``True``\n    scale: bool, optional\n        If ``True``, multiply by `gamma`. If ``False``, `gamma` is not used. When the next layer\n        is linear (also e.g. `relu`), this can be disabled since the scaling will be done by\n        the next layer. Default: ``True``\n    beta_initializer: str, optional\n        Initializer for the beta weight. Default: `\"zeros\"`\n    gamma_initializer: str, optional\n        Initializer for the gamma weight. Default: `\"ones\"`\n    beta_regularizer: str, optional\n        Optional regularizer for the beta weight. Default: ``None``\n    gamma_regularizer: str, optional\n        Optional regularizer for the gamma weight. Default: ``None``\n    beta_constraint: float, optional\n        Optional constraint for the beta weight. Default: ``None``\n    gamma_constraint: float, optional\n        Optional constraint for the gamma weight. Default: ``None``\n\n    References\n    ----------\n        - Layer Normalization - https://arxiv.org/abs/1607.06450\n\n        - Instance Normalization: The Missing Ingredient for Fast Stylization - \\\n        https://arxiv.org/abs/1607.08022\n    \"\"\"\n    # pylint:disable=too-many-instance-attributes,too-many-arguments\n    def __init__(self,\n                 axis=None,\n                 epsilon=1e-3,\n                 center=True,\n                 scale=True,\n                 beta_initializer=\"zeros\",\n                 gamma_initializer=\"ones\",\n                 beta_regularizer=None,\n                 gamma_regularizer=None,\n                 beta_constraint=None,\n                 gamma_constraint=None,\n                 **kwargs):\n        self.beta = None\n        self.gamma = None\n        super().__init__(**kwargs)\n        self.supports_masking = True\n        self.axis = axis\n        self.epsilon = epsilon\n        self.center = center\n        self.scale = scale\n        self.beta_initializer = keras.initializers.get(beta_initializer)\n        self.gamma_initializer = keras.initializers.get(gamma_initializer)\n        self.beta_regularizer = keras.regularizers.get(beta_regularizer)\n        self.gamma_regularizer = keras.regularizers.get(gamma_regularizer)\n        self.beta_constraint = keras.constraints.get(beta_constraint)\n        self.gamma_constraint = keras.constraints.get(gamma_constraint)\n\n    def build(self, input_shape):\n        \"\"\"Creates the layer weights.\n\n        Parameters\n        ----------\n        input_shape: tensor\n            Keras tensor (future input to layer) or ``list``/``tuple`` of Keras tensors to\n            reference for weight shape computations.\n        \"\"\"\n        ndim = len(input_shape)\n        if self.axis == 0:\n            raise ValueError(\"Axis cannot be zero\")\n\n        if (self.axis is not None) and (ndim == 2):\n            raise ValueError(\"Cannot specify axis for rank 1 tensor\")\n\n        self.input_spec = layers.InputSpec(ndim=ndim)  # noqa:E501  pylint:disable=attribute-defined-outside-init\n\n        if self.axis is None:\n            shape = (1,)\n        else:\n            shape = (input_shape[self.axis],)\n\n        if self.scale:\n            self.gamma = self.add_weight(shape=shape,\n                                         name=\"gamma\",\n                                         initializer=self.gamma_initializer,\n                                         regularizer=self.gamma_regularizer,\n                                         constraint=self.gamma_constraint)\n        else:\n            self.gamma = None\n        if self.center:\n            self.beta = self.add_weight(shape=shape,\n                                        name=\"beta\",\n                                        initializer=self.beta_initializer,\n                                        regularizer=self.beta_regularizer,\n                                        constraint=self.beta_constraint)\n        else:\n            self.beta = None\n        self.built = True  # pylint:disable=attribute-defined-outside-init\n\n    def call(self, inputs, training=None):  # pylint:disable=arguments-differ,unused-argument\n        \"\"\"This is where the layer's logic lives.\n\n        Parameters\n        ----------\n        inputs: tensor\n            Input tensor, or list/tuple of input tensors\n\n        Returns\n        -------\n        tensor\n            A tensor or list/tuple of tensors\n        \"\"\"\n        input_shape = K.int_shape(inputs)\n        reduction_axes = list(range(0, len(input_shape)))\n\n        if self.axis is not None:\n            del reduction_axes[self.axis]\n\n        del reduction_axes[0]\n\n        mean = K.mean(inputs, reduction_axes, keepdims=True)\n        stddev = K.std(inputs, reduction_axes, keepdims=True) + self.epsilon\n        normed = (inputs - mean) / stddev\n\n        broadcast_shape = [1] * len(input_shape)\n        if self.axis is not None:\n            broadcast_shape[self.axis] = input_shape[self.axis]\n\n        if self.scale:\n            broadcast_gamma = K.reshape(self.gamma, broadcast_shape)\n            normed = normed * broadcast_gamma\n        if self.center:\n            broadcast_beta = K.reshape(self.beta, broadcast_shape)\n            normed = normed + broadcast_beta\n        return normed\n\n    def get_config(self):\n        \"\"\"Returns the config of the layer.\n\n        A layer config is a Python dictionary (serializable) containing the configuration of a\n        layer. The same layer can be reinstated later (without its trained weights) from this\n        configuration.\n\n        The configuration of a layer does not include connectivity information, nor the layer\n        class name. These are handled by `Network` (one layer of abstraction above).\n\n        Returns\n        --------\n        dict\n            A python dictionary containing the layer configuration\n        \"\"\"\n        config = {\n            \"axis\": self.axis,\n            \"epsilon\": self.epsilon,\n            \"center\": self.center,\n            \"scale\": self.scale,\n            \"beta_initializer\": keras.initializers.serialize(self.beta_initializer),\n            \"gamma_initializer\": keras.initializers.serialize(self.gamma_initializer),\n            \"beta_regularizer\": keras.regularizers.serialize(self.beta_regularizer),\n            \"gamma_regularizer\": keras.regularizers.serialize(self.gamma_regularizer),\n            \"beta_constraint\": keras.constraints.serialize(self.beta_constraint),\n            \"gamma_constraint\": keras.constraints.serialize(self.gamma_constraint)\n        }\n        base_config = super().get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nclass RMSNormalization(layers.Layer):  # type:ignore[name-defined]\n    \"\"\" Root Mean Square Layer Normalization (Biao Zhang, Rico Sennrich, 2019)\n\n    RMSNorm is a simplification of the original layer normalization (LayerNorm). LayerNorm is a\n    regularization technique that might handle the internal covariate shift issue so as to\n    stabilize the layer activations and improve model convergence. It has been proved quite\n    successful in NLP-based model. In some cases, LayerNorm has become an essential component\n    to enable model optimization, such as in the SOTA NMT model Transformer.\n\n    RMSNorm simplifies LayerNorm by removing the mean-centering operation, or normalizing layer\n    activations with RMS statistic.\n\n    Parameters\n    ----------\n    axis: int\n        The axis to normalize across. Typically this is the features axis. The left-out axes are\n        typically the batch axis/axes. This argument defaults to `-1`, the last dimension in the\n        input.\n    epsilon: float, optional\n        Small float added to variance to avoid dividing by zero. Default: `1e-8`\n    partial: float, optional\n        Partial multiplier for calculating pRMSNorm. Valid values are between `0.0` and `1.0`.\n        Setting to `0.0` or `1.0` disables. Default: `0.0`\n    bias: bool, optional\n        Whether to use a bias term for RMSNorm. Disabled by default because RMSNorm does not\n        enforce re-centering invariance. Default ``False``\n    kwargs: dict\n        Standard keras layer kwargs\n\n    References\n    ----------\n        - RMS Normalization - https://arxiv.org/abs/1910.07467\n        - Official implementation - https://github.com/bzhangGo/rmsnorm\n    \"\"\"\n    def __init__(self, axis=-1, epsilon=1e-8, partial=0.0, bias=False, **kwargs):\n        self.scale = None\n        self.offset = 0\n        super().__init__(**kwargs)\n\n        # Checks\n        if not isinstance(axis, int):\n            raise TypeError(f\"Expected an int for the argument 'axis', but received: {axis}\")\n\n        if not 0.0 <= partial <= 1.0:\n            raise ValueError(f\"partial must be between 0.0 and 1.0, but received {partial}\")\n\n        self.axis = axis\n        self.epsilon = epsilon\n        self.partial = partial\n        self.bias = bias\n        self.offset = 0.\n\n    def build(self, input_shape):\n        \"\"\" Validate and populate :attr:`axis`\n\n        Parameters\n        ----------\n        input_shape: tensor\n            Keras tensor (future input to layer) or ``list``/``tuple`` of Keras tensors to\n            reference for weight shape computations.\n        \"\"\"\n        ndims = len(input_shape)\n        if ndims is None:\n            raise ValueError(f\"Input shape {input_shape} has undefined rank.\")\n\n        # Resolve negative axis\n        if self.axis < 0:\n            self.axis += ndims\n\n        # Validate axes\n        if self.axis < 0 or self.axis >= ndims:\n            raise ValueError(f\"Invalid axis: {self.axis}\")\n\n        param_shape = [input_shape[self.axis]]\n        self.scale = self.add_weight(\n            name=\"scale\",\n            shape=param_shape,\n            initializer=\"ones\")\n        if self.bias:\n            self.offset = self.add_weight(\n                name=\"offset\",\n                shape=param_shape,\n                initializer=\"zeros\")\n\n        self.built = True  # pylint:disable=attribute-defined-outside-init\n\n    def call(self, inputs, *args, **kwargs):\n        \"\"\" Call Root Mean Square Layer Normalization\n\n        Parameters\n        ----------\n        inputs: tensor\n            Input tensor, or list/tuple of input tensors\n\n        Returns\n        -------\n        tensor\n            A tensor or list/tuple of tensors\n        \"\"\"\n        # Compute the axes along which to reduce the mean / variance\n        input_shape = K.int_shape(inputs)\n        layer_size = input_shape[self.axis]\n\n        if self.partial in (0.0, 1.0):\n            mean_square = K.mean(K.square(inputs), axis=self.axis, keepdims=True)\n        else:\n            partial_size = int(layer_size * self.partial)\n            partial_x, _ = tf.split(  # pylint:disable=redundant-keyword-arg,no-value-for-parameter\n                inputs,\n                [partial_size, layer_size - partial_size],\n                axis=self.axis)\n            mean_square = K.mean(K.square(partial_x), axis=self.axis, keepdims=True)\n\n        recip_square_root = tf.math.rsqrt(mean_square + self.epsilon)\n        output = self.scale * inputs * recip_square_root + self.offset\n        return output\n\n    def compute_output_shape(self, input_shape):\n        \"\"\" The output shape of the layer is the same as the input shape.\n\n        Parameters\n        ----------\n        input_shape: tuple\n            The input shape to the layer\n\n        Returns\n        -------\n        tuple\n            The output shape to the layer\n        \"\"\"\n        return input_shape\n\n    def get_config(self):\n        \"\"\"Returns the config of the layer.\n\n        A layer config is a Python dictionary (serializable) containing the configuration of a\n        layer. The same layer can be reinstated later (without its trained weights) from this\n        configuration.\n\n        The configuration of a layer does not include connectivity information, nor the layer\n        class name. These are handled by `Network` (one layer of abstraction above).\n\n        Returns\n        --------\n        dict\n            A python dictionary containing the layer configuration\n        \"\"\"\n        base_config = super().get_config()\n        config = {\"axis\": self.axis,\n                  \"epsilon\": self.epsilon,\n                  \"partial\": self.partial,\n                  \"bias\": self.bias}\n        return dict(list(base_config.items()) + list(config.items()))\n\n\n# Update normalization into Keras custom objects\nfor name, obj in inspect.getmembers(sys.modules[__name__]):\n    if inspect.isclass(obj) and obj.__module__ == __name__:\n        keras.utils.get_custom_objects().update({name: obj})\n", "lib/model/optimizers.py": "#!/usr/bin/env python3\n\"\"\" Custom Optimizers for TensorFlow 2.x/tf.keras \"\"\"\n\nimport inspect\nimport sys\n\nimport tensorflow as tf\n\n# Ignore linting errors from Tensorflow's thoroughly broken import system\nfrom tensorflow.keras.optimizers import Adam, Nadam, RMSprop  # noqa:E501,F401  pylint:disable=import-error,unused-import\nkeras = tf.keras\n\n\nclass AdaBelief(tf.keras.optimizers.Optimizer):\n    \"\"\" Implementation of the AdaBelief Optimizer\n\n    Inherits from: tf.keras.optimizers.Optimizer.\n\n    AdaBelief Optimizer is not a placement of the heuristic warmup, the settings should be kept if\n    warmup has already been employed and tuned in the baseline method. You can enable warmup by\n    setting `total_steps` and `warmup_proportion` (see examples)\n\n    Lookahead (see references) can be integrated with AdaBelief Optimizer, which is announced by\n    Less Wright and the new combined optimizer can also be called \"Ranger\". The mechanism can be\n    enabled by using the lookahead wrapper. (See examples)\n\n    Parameters\n    ----------\n    learning_rate: `Tensor`, float or :class: `tf.keras.optimizers.schedules.LearningRateSchedule`\n        The learning rate.\n    beta_1: float\n        The exponential decay rate for the 1st moment estimates.\n    beta_2: float\n        The exponential decay rate for the 2nd moment estimates.\n    epsilon: float\n        A small constant for numerical stability.\n    weight_decay: `Tensor`, float or :class: `tf.keras.optimizers.schedules.LearningRateSchedule`\n        Weight decay for each parameter.\n    rectify: bool\n        Whether to enable rectification as in RectifiedAdam\n    amsgrad: bool\n        Whether to apply AMSGrad variant of this algorithm from the paper \"On the Convergence\n        of Adam and beyond\".\n    sma_threshold. float\n        The threshold for simple mean average.\n    total_steps: int\n        Total number of training steps. Enable warmup by setting a positive value.\n    warmup_proportion: float\n        The proportion of increasing steps.\n    min_lr: float\n        Minimum learning rate after warmup.\n    name: str, optional\n        Name for the operations created when applying gradients. Default: ``\"AdaBeliefOptimizer\"``.\n    **kwargs: dict\n        Standard Keras Optimizer keyword arguments. Allowed to be {`clipnorm`, `clipvalue`, `lr`,\n        `decay`}. `clipnorm` is clip gradients by norm; `clipvalue` is clip gradients by value,\n        `decay` is included for backward compatibility to allow time inverse decay of learning\n        rate. `lr` is included for backward compatibility, recommended to use `learning_rate`\n        instead.\n\n    Examples\n    --------\n    >>> from adabelief_tf import AdaBelief\n    >>> opt = AdaBelief(lr=1e-3)\n\n    Example of serialization:\n\n    >>> optimizer = AdaBelief(learning_rate=lr_scheduler, weight_decay=wd_scheduler)\n    >>> config = tf.keras.optimizers.serialize(optimizer)\n    >>> new_optimizer = tf.keras.optimizers.deserialize(config,\n    ...                                                 custom_objects=dict(AdaBelief=AdaBelief))\n\n    Example of warm up:\n\n    >>> opt = AdaBelief(lr=1e-3, total_steps=10000, warmup_proportion=0.1, min_lr=1e-5)\n\n    In the above example, the learning rate will increase linearly from 0 to `lr` in 1000 steps,\n    then decrease linearly from `lr` to `min_lr` in 9000 steps.\n\n    Example of enabling Lookahead:\n\n    >>> adabelief = AdaBelief()\n    >>> ranger = tfa.optimizers.Lookahead(adabelief, sync_period=6, slow_step_size=0.5)\n\n    Notes\n    -----\n    `amsgrad` is not described in the original paper. Use it with caution.\n\n    References\n    ----------\n    Juntang Zhuang et al. - AdaBelief Optimizer: Adapting stepsizes by the belief in observed\n    gradients - https://arxiv.org/abs/2010.07468.\n\n    Original implementation - https://github.com/juntang-zhuang/Adabelief-Optimizer\n\n    Michael R. Zhang et.al - Lookahead Optimizer: k steps forward, 1 step back -\n    https://arxiv.org/abs/1907.08610v1\n\n    Adapted from https://github.com/juntang-zhuang/Adabelief-Optimizer\n\n    BSD 2-Clause License\n\n    Copyright (c) 2021, Juntang Zhuang\n    All rights reserved.\n\n    Redistribution and use in source and binary forms, with or without\n    modification, are permitted provided that the following conditions are met:\n\n    1. Redistributions of source code must retain the above copyright notice, this\n    list of conditions and the following disclaimer.\n\n    2. Redistributions in binary form must reproduce the above copyright notice,\n    this list of conditions and the following disclaimer in the documentation\n    and/or other materials provided with the distribution.\n\n    THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n    AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n    IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n    DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n    FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n    DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n    SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n    CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n    OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n    OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n    \"\"\"\n\n    def __init__(self, learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-14,\n                 weight_decay=0.0, rectify=True, amsgrad=False, sma_threshold=5.0, total_steps=0,\n                 warmup_proportion=0.1, min_lr=0.0, name=\"AdaBeliefOptimizer\", **kwargs):\n        # pylint:disable=too-many-arguments\n        super().__init__(name, **kwargs)\n        self._set_hyper(\"learning_rate\", kwargs.get(\"lr\", learning_rate))\n        self._set_hyper(\"beta_1\", beta_1)\n        self._set_hyper(\"beta_2\", beta_2)\n        self._set_hyper(\"decay\", self._initial_decay)\n        self._set_hyper(\"weight_decay\", weight_decay)\n        self._set_hyper(\"sma_threshold\", sma_threshold)\n        self._set_hyper(\"total_steps\", int(total_steps))\n        self._set_hyper(\"warmup_proportion\", warmup_proportion)\n        self._set_hyper(\"min_lr\", min_lr)\n        self.epsilon = epsilon or tf.keras.backend.epsilon()\n        self.amsgrad = amsgrad\n        self.rectify = rectify\n        self._has_weight_decay = weight_decay != 0.0\n        self._initial_total_steps = total_steps\n\n    def _create_slots(self, var_list):\n        \"\"\" Create slots for the first and second moments\n\n        Parameters\n        ----------\n        var_list: list\n            List of tensorflow variables to create slots for\n        \"\"\"\n        for var in var_list:\n            self.add_slot(var, \"m\")\n            self.add_slot(var, \"v\")\n            if self.amsgrad:\n                self.add_slot(var, \"vhat\")\n\n    def set_weights(self, weights):\n        \"\"\" Set the weights of the optimizer.\n\n        The weights of an optimizer are its state (IE, variables). This function takes the weight\n        values associated with this optimizer as a list of Numpy arrays. The first value is always\n        the iterations count of the optimizer, followed by the optimizers state variables in the\n        order they are created. The passed values are used to set the new state of the optimizer.\n\n        Parameters\n        ----------\n        weights: list\n            weight values as a list of numpy arrays.\n        \"\"\"\n        params = self.weights\n        num_vars = int((len(params) - 1) / 2)\n        if len(weights) == 3 * num_vars + 1:\n            weights = weights[: len(params)]\n        super().set_weights(weights)\n\n    def _decayed_wd(self, var_dtype):\n        \"\"\" Set the weight decay\n\n        Parameters\n        ----------\n        var_dtype: str\n            The data type to to set up weight decay for\n\n        Returns\n        -------\n        Tensor\n            The weight decay variable\n        \"\"\"\n        wd_t = self._get_hyper(\"weight_decay\", var_dtype)\n        if isinstance(wd_t, tf.keras.optimizers.schedules.LearningRateSchedule):\n            wd_t = tf.cast(wd_t(self.iterations), var_dtype)\n        return wd_t\n\n    def _resource_apply_dense(self, grad, handle, apply_state=None):\n        # pylint:disable=too-many-locals,unused-argument\n        \"\"\" Add ops to apply dense gradients to the variable handle.\n\n        Parameters\n        ----------\n        grad: Tensor\n            A tensor representing the gradient.\n        handle: Tensor\n            a Tensor of dtype resource which points to the variable to be updated.\n        apply_state: dict\n            A dict which is used across multiple apply calls.\n\n        Returns\n        -------\n            An Operation which updates the value of the variable.\n        \"\"\"\n        var_dtype = handle.dtype.base_dtype\n        lr_t = self._decayed_lr(var_dtype)\n        wd_t = self._decayed_wd(var_dtype)\n        var_m = self.get_slot(handle, \"m\")\n        var_v = self.get_slot(handle, \"v\")\n        beta_1_t = self._get_hyper(\"beta_1\", var_dtype)\n        beta_2_t = self._get_hyper(\"beta_2\", var_dtype)\n        epsilon_t = tf.convert_to_tensor(self.epsilon, var_dtype)\n        local_step = tf.cast(self.iterations + 1, var_dtype)\n        beta_1_power = tf.math.pow(beta_1_t, local_step)\n        beta_2_power = tf.math.pow(beta_2_t, local_step)\n\n        if self._initial_total_steps > 0:\n            total_steps = self._get_hyper(\"total_steps\", var_dtype)\n            warmup_steps = total_steps * self._get_hyper(\"warmup_proportion\", var_dtype)\n            min_lr = self._get_hyper(\"min_lr\", var_dtype)\n            decay_steps = tf.maximum(total_steps - warmup_steps, 1)\n            decay_rate = (min_lr - lr_t) / decay_steps\n            lr_t = tf.where(local_step <= warmup_steps,\n                            lr_t * (local_step / warmup_steps),\n                            lr_t + decay_rate * tf.minimum(local_step - warmup_steps, decay_steps))\n\n        m_t = var_m.assign(beta_1_t * var_m + (1.0 - beta_1_t) * grad,\n                           use_locking=self._use_locking)\n        m_corr_t = m_t / (1.0 - beta_1_power)\n\n        v_t = var_v.assign(\n            beta_2_t * var_v + (1.0 - beta_2_t) * tf.math.square(grad - m_t) + epsilon_t,\n            use_locking=self._use_locking)\n\n        if self.amsgrad:\n            vhat = self.get_slot(handle, \"vhat\")\n            vhat_t = vhat.assign(tf.maximum(vhat, v_t), use_locking=self._use_locking)\n            v_corr_t = tf.math.sqrt(vhat_t / (1.0 - beta_2_power))\n        else:\n            vhat_t = None\n            v_corr_t = tf.math.sqrt(v_t / (1.0 - beta_2_power))\n\n        if self.rectify:\n            sma_inf = 2.0 / (1.0 - beta_2_t) - 1.0\n            sma_t = sma_inf - 2.0 * local_step * beta_2_power / (1.0 - beta_2_power)\n            r_t = tf.math.sqrt((sma_t - 4.0) / (sma_inf - 4.0) *\n                               (sma_t - 2.0) / (sma_inf - 2.0) *\n                               sma_inf / sma_t)\n            sma_threshold = self._get_hyper(\"sma_threshold\", var_dtype)\n            var_t = tf.where(sma_t >= sma_threshold,\n                             r_t * m_corr_t / (v_corr_t + epsilon_t),\n                             m_corr_t)\n        else:\n            var_t = m_corr_t / (v_corr_t + epsilon_t)\n\n        if self._has_weight_decay:\n            var_t += wd_t * handle\n\n        var_update = handle.assign_sub(lr_t * var_t, use_locking=self._use_locking)\n        updates = [var_update, m_t, v_t]\n\n        if self.amsgrad:\n            updates.append(vhat_t)\n        return tf.group(*updates)\n\n    def _resource_apply_sparse(self, grad, handle, indices, apply_state=None):\n        # pylint:disable=too-many-locals, unused-argument\n        \"\"\" Add ops to apply sparse gradients to the variable handle.\n\n        Similar to _apply_sparse, the indices argument to this method has been de-duplicated.\n        Optimizers which deal correctly with non-unique indices may instead override\n        :func:`_resource_apply_sparse_duplicate_indices` to avoid this overhead.\n\n        Parameters\n        ----------\n        grad: Tensor\n            a Tensor representing the gradient for the affected indices.\n        handle: Tensor\n            a Tensor of dtype resource which points to the variable to be updated.\n        indices: Tensor\n            a Tensor of integral type representing the indices for which the gradient is nonzero.\n            Indices are unique.\n        apply_state: dict\n            A dict which is used across multiple apply calls.\n\n        Returns\n        -------\n            An Operation which updates the value of the variable.\n        \"\"\"\n        var_dtype = handle.dtype.base_dtype\n        lr_t = self._decayed_lr(var_dtype)\n        wd_t = self._decayed_wd(var_dtype)\n        beta_1_t = self._get_hyper(\"beta_1\", var_dtype)\n        beta_2_t = self._get_hyper(\"beta_2\", var_dtype)\n        epsilon_t = tf.convert_to_tensor(self.epsilon, var_dtype)\n        local_step = tf.cast(self.iterations + 1, var_dtype)\n        beta_1_power = tf.math.pow(beta_1_t, local_step)\n        beta_2_power = tf.math.pow(beta_2_t, local_step)\n\n        if self._initial_total_steps > 0:\n            total_steps = self._get_hyper(\"total_steps\", var_dtype)\n            warmup_steps = total_steps * self._get_hyper(\"warmup_proportion\", var_dtype)\n            min_lr = self._get_hyper(\"min_lr\", var_dtype)\n            decay_steps = tf.maximum(total_steps - warmup_steps, 1)\n            decay_rate = (min_lr - lr_t) / decay_steps\n            lr_t = tf.where(local_step <= warmup_steps,\n                            lr_t * (local_step / warmup_steps),\n                            lr_t + decay_rate * tf.minimum(local_step - warmup_steps, decay_steps))\n\n        var_m = self.get_slot(handle, \"m\")\n        m_scaled_g_values = grad * (1 - beta_1_t)\n        m_t = var_m.assign(var_m * beta_1_t, use_locking=self._use_locking)\n        m_t = self._resource_scatter_add(var_m, indices, m_scaled_g_values)\n        m_corr_t = m_t / (1.0 - beta_1_power)\n\n        var_v = self.get_slot(handle, \"v\")\n        m_t_indices = tf.gather(m_t, indices)  # pylint:disable=no-value-for-parameter\n        v_scaled_g_values = tf.math.square(grad - m_t_indices) * (1 - beta_2_t)\n        v_t = var_v.assign(var_v * beta_2_t + epsilon_t, use_locking=self._use_locking)\n        v_t = self._resource_scatter_add(var_v, indices, v_scaled_g_values)\n\n        if self.amsgrad:\n            vhat = self.get_slot(handle, \"vhat\")\n            vhat_t = vhat.assign(tf.maximum(vhat, v_t), use_locking=self._use_locking)\n            v_corr_t = tf.math.sqrt(vhat_t / (1.0 - beta_2_power))\n        else:\n            vhat_t = None\n            v_corr_t = tf.math.sqrt(v_t / (1.0 - beta_2_power))\n\n        if self.rectify:\n            sma_inf = 2.0 / (1.0 - beta_2_t) - 1.0\n            sma_t = sma_inf - 2.0 * local_step * beta_2_power / (1.0 - beta_2_power)\n            r_t = tf.math.sqrt((sma_t - 4.0) / (sma_inf - 4.0) *\n                               (sma_t - 2.0) / (sma_inf - 2.0) *\n                               sma_inf / sma_t)\n            sma_threshold = self._get_hyper(\"sma_threshold\", var_dtype)\n            var_t = tf.where(sma_t >= sma_threshold,\n                             r_t * m_corr_t / (v_corr_t + epsilon_t),\n                             m_corr_t)\n        else:\n            var_t = m_corr_t / (v_corr_t + epsilon_t)\n\n        if self._has_weight_decay:\n            var_t += wd_t * handle\n\n        var_update = self._resource_scatter_add(handle,\n                                                indices,\n                                                tf.gather(  # pylint:disable=no-value-for-parameter\n                                                    tf.math.negative(lr_t) * var_t,\n                                                    indices))\n\n        updates = [var_update, m_t, v_t]\n        if self.amsgrad:\n            updates.append(vhat_t)\n        return tf.group(*updates)\n\n    def get_config(self):\n        \"\"\" Returns the config of the optimizer.\n\n        An optimizer config is a Python dictionary (serializable) containing the configuration of\n        an optimizer. The same optimizer can be re-instantiated later (without any saved state)\n        from this configuration.\n\n        Returns\n        -------\n        dict\n            The optimizer configuration.\n        \"\"\"\n        config = super().get_config()\n        config.update({\"learning_rate\": self._serialize_hyperparameter(\"learning_rate\"),\n                       \"beta_1\": self._serialize_hyperparameter(\"beta_1\"),\n                       \"beta_2\": self._serialize_hyperparameter(\"beta_2\"),\n                       \"decay\": self._serialize_hyperparameter(\"decay\"),\n                       \"weight_decay\": self._serialize_hyperparameter(\"weight_decay\"),\n                       \"sma_threshold\": self._serialize_hyperparameter(\"sma_threshold\"),\n                       \"epsilon\": self.epsilon,\n                       \"amsgrad\": self.amsgrad,\n                       \"rectify\": self.rectify,\n                       \"total_steps\": self._serialize_hyperparameter(\"total_steps\"),\n                       \"warmup_proportion\": self._serialize_hyperparameter(\"warmup_proportion\"),\n                       \"min_lr\": self._serialize_hyperparameter(\"min_lr\")})\n        return config\n\n\n# Update layers into Keras custom objects\nfor _name, obj in inspect.getmembers(sys.modules[__name__]):\n    if inspect.isclass(obj) and obj.__module__ == __name__:\n        keras.utils.get_custom_objects().update({_name: obj})\n", "lib/model/backup_restore.py": "#!/usr/bin/env python3\n\n\"\"\" Functions for backing up, restoring and creating model snapshots. \"\"\"\n\nimport logging\nimport os\nfrom datetime import datetime\nfrom shutil import copyfile, copytree, rmtree\n\nfrom lib.serializer import get_serializer\nfrom lib.utils import get_folder\n\nlogger = logging.getLogger(__name__)\n\n\nclass Backup():\n    \"\"\" Performs the back up of models at each save iteration, and the restoring of models from\n    their back up location.\n\n    Parameters\n    ----------\n    model_dir: str\n        The folder that contains the model to be backed up\n    model_name: str\n        The name of the model that is to be backed up\n    \"\"\"\n    def __init__(self, model_dir, model_name):\n        logger.debug(\"Initializing %s: (model_dir: '%s', model_name: '%s')\",\n                     self.__class__.__name__, model_dir, model_name)\n        self.model_dir = str(model_dir)\n        self.model_name = model_name\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    def _check_valid(self, filename, for_restore=False):\n        \"\"\" Check if the passed in filename is valid for a backup or restore operation.\n\n        Parameters\n        ----------\n        filename: str\n            The filename that is to be checked for backup or restore\n        for_restore: bool, optional\n            ``True`` if the checks are to be performed for restoring a model, ``False`` if the\n            checks are to be performed for backing up a model. Default: ``False``\n\n        Returns\n        -------\n        bool\n            ``True`` if the given file is valid for a backup/restore operation otherwise ``False``\n        \"\"\"\n        fullpath = os.path.join(self.model_dir, filename)\n        if not filename.startswith(self.model_name):\n            # Any filename that does not start with the model name are invalid\n            # for all operations\n            retval = False\n        elif for_restore and filename.endswith(\".bk\"):\n            # Only filenames ending in .bk are valid for restoring\n            retval = True\n        elif not for_restore and ((os.path.isfile(fullpath) and not filename.endswith(\".bk\")) or\n                                  (os.path.isdir(fullpath) and\n                                   filename == \"{}_logs\".format(self.model_name))):\n            # Only filenames that do not end with .bk or folders that are the logs folder\n            # are valid for backup\n            retval = True\n        else:\n            retval = False\n        logger.debug(\"'%s' valid for backup operation: %s\", filename, retval)\n        return retval\n\n    @staticmethod\n    def backup_model(full_path):\n        \"\"\" Backup a model file.\n\n        The backed up file is saved with the original filename in the original location with `.bk`\n        appended to the end of the name.\n\n        Parameters\n        ----------\n        full_path: str\n            The full path to a `.h5` model file or a `.json` state file\n        \"\"\"\n        backupfile = full_path + \".bk\"\n        if os.path.exists(backupfile):\n            os.remove(backupfile)\n        if os.path.exists(full_path):\n            logger.verbose(\"Backing up: '%s' to '%s'\", full_path, backupfile)\n            os.rename(full_path, backupfile)\n\n    def snapshot_models(self, iterations):\n        \"\"\" Take a snapshot of the model at the current state and back it up.\n\n        The snapshot is a copy of the model folder located in the same root location\n        as the original model file, with the number of iterations appended to the end\n        of the folder name.\n\n        Parameters\n        ----------\n        iterations: int\n            The number of iterations that the model has trained when performing the snapshot.\n        \"\"\"\n        print(\"\")  # New line so log message doesn't append to last loss output\n        logger.verbose(\"Saving snapshot\")\n        snapshot_dir = \"{}_snapshot_{}_iters\".format(self.model_dir, iterations)\n\n        if os.path.isdir(snapshot_dir):\n            logger.debug(\"Removing previously existing snapshot folder: '%s'\", snapshot_dir)\n            rmtree(snapshot_dir)\n\n        dst = get_folder(snapshot_dir)\n        for filename in os.listdir(self.model_dir):\n            if not self._check_valid(filename, for_restore=False):\n                logger.debug(\"Not snapshotting file: '%s'\", filename)\n                continue\n            srcfile = os.path.join(self.model_dir, filename)\n            dstfile = os.path.join(dst, filename)\n            copyfunc = copytree if os.path.isdir(srcfile) else copyfile\n            logger.debug(\"Saving snapshot: '%s' > '%s'\", srcfile, dstfile)\n            copyfunc(srcfile, dstfile)\n        logger.info(\"Saved snapshot (%s iterations)\", iterations)\n\n    def restore(self):\n        \"\"\" Restores a model from backup.\n\n        The original model files are migrated into a folder within the original model folder\n        named `<model_name>_archived_<timestamp>`. The `.bk` backup files are then moved to\n        the location of the previously existing model files. Logs that were generated after the\n        the last backup was taken are removed. \"\"\"\n        archive_dir = self._move_archived()\n        self._restore_files()\n        self._restore_logs(archive_dir)\n\n    def _move_archived(self):\n        \"\"\" Move archived files to the archived folder.\n\n        Returns\n        -------\n        str\n            The name of the generated archive folder\n        \"\"\"\n        logger.info(\"Archiving existing model files...\")\n        now = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        archive_dir = os.path.join(self.model_dir, \"{}_archived_{}\".format(self.model_name, now))\n        os.mkdir(archive_dir)\n        for filename in os.listdir(self.model_dir):\n            if not self._check_valid(filename, for_restore=False):\n                logger.debug(\"Not moving file to archived: '%s'\", filename)\n                continue\n            logger.verbose(\"Moving '%s' to archived model folder: '%s'\", filename, archive_dir)\n            src = os.path.join(self.model_dir, filename)\n            dst = os.path.join(archive_dir, filename)\n            os.rename(src, dst)\n        logger.verbose(\"Archived existing model files\")\n        return archive_dir\n\n    def _restore_files(self):\n        \"\"\" Restore files from .bk \"\"\"\n        logger.info(\"Restoring models from backup...\")\n        for filename in os.listdir(self.model_dir):\n            if not self._check_valid(filename, for_restore=True):\n                logger.debug(\"Not restoring file: '%s'\", filename)\n                continue\n            dstfile = os.path.splitext(filename)[0]\n            logger.verbose(\"Restoring '%s' to '%s'\", filename, dstfile)\n            src = os.path.join(self.model_dir, filename)\n            dst = os.path.join(self.model_dir, dstfile)\n            copyfile(src, dst)\n        logger.verbose(\"Restored models from backup\")\n\n    def _restore_logs(self, archive_dir):\n        \"\"\" Restores the log files up to and including the last backup.\n\n        Parameters\n        ----------\n        archive_dir: str\n            The full path to the model's archive folder\n        \"\"\"\n        logger.info(\"Restoring Logs...\")\n        session_names = self._get_session_names()\n        log_dirs = self._get_log_dirs(archive_dir, session_names)\n        for log_dir in log_dirs:\n            src = os.path.join(archive_dir, log_dir)\n            dst = os.path.join(self.model_dir, log_dir)\n            logger.verbose(\"Restoring logfile: %s\", dst)\n            copytree(src, dst)\n        logger.verbose(\"Restored Logs\")\n\n    def _get_session_names(self):\n        \"\"\" Get the existing session names from a state file. \"\"\"\n        serializer = get_serializer(\"json\")\n        state_file = os.path.join(self.model_dir,\n                                  \"{}_state.{}\".format(self.model_name, serializer.file_extension))\n        state = serializer.load(state_file)\n        session_names = [\"session_{}\".format(key)\n                         for key in state[\"sessions\"].keys()]\n        logger.debug(\"Session to restore: %s\", session_names)\n        return session_names\n\n    def _get_log_dirs(self, archive_dir, session_names):\n        \"\"\" Get the session log directory paths in the archive folder.\n\n        Parameters\n        ----------\n        archive_dir: str\n            The full path to the model's archive folder\n        session_names: list\n            The name of the training sessions that exist for the model\n\n        Returns\n        -------\n        list\n            The full paths to the log folders\n        \"\"\"\n        archive_logs = os.path.join(archive_dir, \"{}_logs\".format(self.model_name))\n        paths = [os.path.join(dirpath.replace(archive_dir, \"\")[1:], folder)\n                 for dirpath, dirnames, _ in os.walk(archive_logs)\n                 for folder in dirnames\n                 if folder in session_names]\n        logger.debug(\"log folders to restore: %s\", paths)\n        return paths\n", "lib/model/__init__.py": "", "lib/model/layers.py": "#!/usr/bin/env python3\n\"\"\" Custom Layers for faceswap.py. \"\"\"\nfrom __future__ import annotations\n\nimport sys\nimport inspect\nimport typing as T\n\nimport tensorflow as tf\n\n# Fix intellisense/linting for tf.keras' thoroughly broken import system\nfrom tensorflow.python.keras.utils import conv_utils  # pylint:disable=no-name-in-module\nkeras = tf.keras\nlayers = keras.layers\nK = keras.backend\n\n\nclass _GlobalPooling2D(tf.keras.layers.Layer):\n    \"\"\"Abstract class for different global pooling 2D layers.\n\n    From keras as access to pooling is trickier in tensorflow.keras\n    \"\"\"\n    def __init__(self, data_format: str | None = None, **kwargs) -> None:\n        super().__init__(**kwargs)\n        self.data_format = conv_utils.normalize_data_format(data_format)\n        self.input_spec = keras.layers.InputSpec(ndim=4)\n\n    def compute_output_shape(self, input_shape):\n        \"\"\" Compute the output shape based on the input shape.\n\n        Parameters\n        ----------\n        input_shape: tuple\n            The input shape to the layer\n        \"\"\"\n        if self.data_format == 'channels_last':\n            return (input_shape[0], input_shape[3])\n        return (input_shape[0], input_shape[1])\n\n    def call(self, inputs: tf.Tensor, *args, **kwargs) -> tf.Tensor:\n        \"\"\" Override to call the layer.\n\n        Parameters\n        ----------\n        inputs: :class:`tf.Tensor`\n            The input to the layer\n        \"\"\"\n        raise NotImplementedError\n\n    def get_config(self) -> dict[str, T.Any]:\n        \"\"\" Set the Keras config \"\"\"\n        config = {'data_format': self.data_format}\n        base_config = super().get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nclass GlobalMinPooling2D(_GlobalPooling2D):\n    \"\"\"Global minimum pooling operation for spatial data. \"\"\"\n\n    def call(self, inputs: tf.Tensor, *args, **kwargs) -> tf.Tensor:\n        \"\"\"This is where the layer's logic lives.\n\n        Parameters\n        ----------\n        inputs: :class:`tf.Tensor`\n            Input tensor, or list/tuple of input tensors\n\n        Returns\n        -------\n        tensor\n            A tensor or list/tuple of tensors\n        \"\"\"\n        if self.data_format == 'channels_last':\n            pooled = K.min(inputs, axis=[1, 2])\n        else:\n            pooled = K.min(inputs, axis=[2, 3])\n        return pooled\n\n\nclass GlobalStdDevPooling2D(_GlobalPooling2D):\n    \"\"\"Global standard deviation pooling operation for spatial data. \"\"\"\n\n    def call(self, inputs: tf.Tensor, *args, **kwargs) -> tf.Tensor:\n        \"\"\"This is where the layer's logic lives.\n\n        Parameters\n        ----------\n        inputs: tensor\n            Input tensor, or list/tuple of input tensors\n\n        Returns\n        -------\n        tensor\n            A tensor or list/tuple of tensors\n        \"\"\"\n        if self.data_format == 'channels_last':\n            pooled = K.std(inputs, axis=[1, 2])\n        else:\n            pooled = K.std(inputs, axis=[2, 3])\n        return pooled\n\n\nclass KResizeImages(tf.keras.layers.Layer):\n    \"\"\" A custom upscale function that uses :class:`keras.backend.resize_images` to upsample.\n\n    Parameters\n    ----------\n    size: int or float, optional\n        The scale to upsample to. Default: `2`\n    interpolation: [\"nearest\", \"bilinear\"], optional\n        The interpolation to use. Default: `\"nearest\"`\n    kwargs: dict\n        The standard Keras Layer keyword arguments (if any)\n    \"\"\"\n    def __init__(self,\n                 size: int = 2,\n                 interpolation: T.Literal[\"nearest\", \"bilinear\"] = \"nearest\",\n                 **kwargs) -> None:\n        super().__init__(**kwargs)\n        self.size = size\n        self.interpolation = interpolation\n\n    def call(self, inputs: tf.Tensor, *args, **kwargs) -> tf.Tensor:\n        \"\"\" Call the upsample layer\n\n        Parameters\n        ----------\n        inputs: :class:`tf.Tensor`\n            Input tensor, or list/tuple of input tensors\n\n        Returns\n        -------\n        :class:`tf.Tensor`\n            A tensor or list/tuple of tensors\n        \"\"\"\n        if isinstance(self.size, int):\n            retval = K.resize_images(inputs,\n                                     self.size,\n                                     self.size,\n                                     \"channels_last\",\n                                     interpolation=self.interpolation)\n        else:\n            # Arbitrary resizing\n            size = int(round(K.int_shape(inputs)[1] * self.size))\n            retval = tf.image.resize(inputs, (size, size), method=self.interpolation)\n        return retval\n\n    def compute_output_shape(self, input_shape: tuple[int, ...]) -> tuple[int, ...]:\n        \"\"\"Computes the output shape of the layer.\n\n        This is the input shape with size dimensions multiplied by :attr:`size`\n\n        Parameters\n        ----------\n        input_shape: tuple or list of tuples\n            Shape tuple (tuple of integers) or list of shape tuples (one per output tensor of the\n            layer).  Shape tuples can include None for free dimensions, instead of an integer.\n\n        Returns\n        -------\n        tuple\n            An input shape tuple\n        \"\"\"\n        batch, height, width, channels = input_shape\n        return (batch, height * self.size, width * self.size, channels)\n\n    def get_config(self) -> dict[str, T.Any]:\n        \"\"\"Returns the config of the layer.\n\n        Returns\n        --------\n        dict\n            A python dictionary containing the layer configuration\n        \"\"\"\n        config = {\"size\": self.size, \"interpolation\": self.interpolation}\n        base_config = super().get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nclass L2_normalize(tf.keras.layers.Layer):  # pylint:disable=invalid-name\n    \"\"\" Normalizes a tensor w.r.t. the L2 norm alongside the specified axis.\n\n    Parameters\n    ----------\n    axis: int\n        The axis to perform normalization across\n    kwargs: dict\n        The standard Keras Layer keyword arguments (if any)\n    \"\"\"\n    def __init__(self, axis: int, **kwargs) -> None:\n        self.axis = axis\n        super().__init__(**kwargs)\n\n    def call(self, inputs: tf.Tensor, *args, **kwargs) -> tf.Tensor:\n        \"\"\"This is where the layer's logic lives.\n\n        Parameters\n        ----------\n        inputs: :class:`tf.Tensor`\n            Input tensor, or list/tuple of input tensors\n\n        Returns\n        -------\n        :class:`tf.Tensor`\n            A tensor or list/tuple of tensors\n        \"\"\"\n        return K.l2_normalize(inputs, self.axis)\n\n    def get_config(self) -> dict[str, T.Any]:\n        \"\"\"Returns the config of the layer.\n\n        A layer config is a Python dictionary (serializable) containing the configuration of a\n        layer. The same layer can be reinstated later (without its trained weights) from this\n        configuration.\n\n        The configuration of a layer does not include connectivity information, nor the layer\n        class name. These are handled by `Network` (one layer of abstraction above).\n\n        Returns\n        --------\n        dict\n            A python dictionary containing the layer configuration\n        \"\"\"\n        config = super().get_config()\n        config[\"axis\"] = self.axis\n        return config\n\n\nclass PixelShuffler(tf.keras.layers.Layer):\n    \"\"\" PixelShuffler layer for Keras.\n\n    This layer requires a Convolution2D prior to it, having output filters computed according to\n    the formula :math:`filters = k * (scale_factor * scale_factor)` where `k` is a user defined\n    number of filters (generally larger than 32) and `scale_factor` is the up-scaling factor\n    (generally 2).\n\n    This layer performs the depth to space operation on the convolution filters, and returns a\n    tensor with the size as defined below.\n\n    Notes\n    -----\n    In practice, it is useful to have a second convolution layer after the\n    :class:`PixelShuffler` layer to speed up the learning process. However, if you are stacking\n    multiple :class:`PixelShuffler` blocks, it may increase the number of parameters greatly,\n    so the Convolution layer after :class:`PixelShuffler` layer can be removed.\n\n    Example\n    -------\n    >>> # A standard sub-pixel up-scaling block\n    >>> x = Convolution2D(256, 3, 3, padding=\"same\", activation=\"relu\")(...)\n    >>> u = PixelShuffler(size=(2, 2))(x)\n    [Optional]\n    >>> x = Convolution2D(256, 3, 3, padding=\"same\", activation=\"relu\")(u)\n\n    Parameters\n    ----------\n    size: tuple, optional\n        The (`h`, `w`) scaling factor for up-scaling. Default: `(2, 2)`\n    data_format: [\"channels_first\", \"channels_last\", ``None``], optional\n        The data format for the input. Default: ``None``\n    kwargs: dict\n        The standard Keras Layer keyword arguments (if any)\n\n    References\n    ----------\n    https://gist.github.com/t-ae/6e1016cc188104d123676ccef3264981\n    \"\"\"\n    def __init__(self,\n                 size: int | tuple[int, int] = (2, 2),\n                 data_format: str | None = None,\n                 **kwargs) -> None:\n        super().__init__(**kwargs)\n        self.data_format = conv_utils.normalize_data_format(data_format)\n        self.size = conv_utils.normalize_tuple(size, 2, 'size')\n\n    def call(self, inputs: tf.Tensor, *args, **kwargs) -> tf.Tensor:\n        \"\"\"This is where the layer's logic lives.\n\n        Parameters\n        ----------\n        inputs: :class:`tf.Tensor`\n            Input tensor, or list/tuple of input tensors\n\n        Returns\n        -------\n        :class:`tf.Tensor`\n            A tensor or list/tuple of tensors\n        \"\"\"\n        input_shape = K.int_shape(inputs)\n        if len(input_shape) != 4:\n            raise ValueError('Inputs should have rank ' +\n                             str(4) +\n                             '; Received input shape:', str(input_shape))\n\n        if self.data_format == 'channels_first':\n            batch_size, channels, height, width = input_shape\n            if batch_size is None:\n                batch_size = -1\n            r_height, r_width = self.size\n            o_height, o_width = height * r_height, width * r_width\n            o_channels = channels // (r_height * r_width)\n\n            out = K.reshape(inputs, (batch_size, r_height, r_width, o_channels, height, width))\n            out = K.permute_dimensions(out, (0, 3, 4, 1, 5, 2))\n            out = K.reshape(out, (batch_size, o_channels, o_height, o_width))\n        elif self.data_format == 'channels_last':\n            batch_size, height, width, channels = input_shape\n            if batch_size is None:\n                batch_size = -1\n            r_height, r_width = self.size\n            o_height, o_width = height * r_height, width * r_width\n            o_channels = channels // (r_height * r_width)\n\n            out = K.reshape(inputs, (batch_size, height, width, r_height, r_width, o_channels))\n            out = K.permute_dimensions(out, (0, 1, 3, 2, 4, 5))\n            out = K.reshape(out, (batch_size, o_height, o_width, o_channels))\n        return out\n\n    def compute_output_shape(self, input_shape: tuple[int, ...]) -> tuple[int, ...]:\n        \"\"\"Computes the output shape of the layer.\n\n        Assumes that the layer will be built to match that input shape provided.\n\n        Parameters\n        ----------\n        input_shape: tuple or list of tuples\n            Shape tuple (tuple of integers) or list of shape tuples (one per output tensor of the\n            layer).  Shape tuples can include None for free dimensions, instead of an integer.\n\n        Returns\n        -------\n        tuple\n            An input shape tuple\n        \"\"\"\n        if len(input_shape) != 4:\n            raise ValueError('Inputs should have rank ' +\n                             str(4) +\n                             '; Received input shape:', str(input_shape))\n\n        if self.data_format == 'channels_first':\n            height = None\n            width = None\n            if input_shape[2] is not None:\n                height = input_shape[2] * self.size[0]\n            if input_shape[3] is not None:\n                width = input_shape[3] * self.size[1]\n            channels = input_shape[1] // self.size[0] // self.size[1]\n\n            if channels * self.size[0] * self.size[1] != input_shape[1]:\n                raise ValueError('channels of input and size are incompatible')\n\n            retval = (input_shape[0],\n                      channels,\n                      height,\n                      width)\n        elif self.data_format == 'channels_last':\n            height = None\n            width = None\n            if input_shape[1] is not None:\n                height = input_shape[1] * self.size[0]\n            if input_shape[2] is not None:\n                width = input_shape[2] * self.size[1]\n            channels = input_shape[3] // self.size[0] // self.size[1]\n\n            if channels * self.size[0] * self.size[1] != input_shape[3]:\n                raise ValueError('channels of input and size are incompatible')\n\n            retval = (input_shape[0],\n                      height,\n                      width,\n                      channels)\n        return retval\n\n    def get_config(self) -> dict[str, T.Any]:\n        \"\"\"Returns the config of the layer.\n\n        A layer config is a Python dictionary (serializable) containing the configuration of a\n        layer. The same layer can be reinstated later (without its trained weights) from this\n        configuration.\n\n        The configuration of a layer does not include connectivity information, nor the layer\n        class name. These are handled by `Network` (one layer of abstraction above).\n\n        Returns\n        --------\n        dict\n            A python dictionary containing the layer configuration\n        \"\"\"\n        config = {'size': self.size,\n                  'data_format': self.data_format}\n        base_config = super().get_config()\n\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nclass QuickGELU(tf.keras.layers.Layer):\n    \"\"\" Applies GELU approximation that is fast but somewhat inaccurate.\n\n    Parameters\n    ----------\n    name: str, optional\n        The name for the layer. Default: \"QuickGELU\"\n    kwargs: dict\n        The standard Keras Layer keyword arguments (if any)\n    \"\"\"\n\n    def __init__(self, name: str = \"QuickGELU\", **kwargs) -> None:\n        super().__init__(name=name, **kwargs)\n\n    def call(self, inputs: tf.Tensor, *args, **kwargs) -> tf.Tensor:\n        \"\"\" Call the QuickGELU layerr\n\n        Parameters\n        ----------\n        inputs : :class:`tf.Tensor`\n            The input Tensor\n\n        Returns\n        -------\n        :class:`tf.Tensor`\n            The output Tensor\n        \"\"\"\n        return inputs * K.sigmoid(1.702 * inputs)\n\n\nclass ReflectionPadding2D(tf.keras.layers.Layer):\n    \"\"\"Reflection-padding layer for 2D input (e.g. picture).\n\n    This layer can add rows and columns at the top, bottom, left and right side of an image tensor.\n\n    Parameters\n    ----------\n    stride: int, optional\n        The stride of the following convolution. Default: `2`\n    kernel_size: int, optional\n        The kernel size of the following convolution. Default: `5`\n    kwargs: dict\n        The standard Keras Layer keyword arguments (if any)\n    \"\"\"\n    def __init__(self, stride: int = 2, kernel_size: int = 5, **kwargs) -> None:\n        if isinstance(stride, (tuple, list)):\n            assert len(stride) == 2 and stride[0] == stride[1]\n            stride = stride[0]\n        self.stride = stride\n        self.kernel_size = kernel_size\n        self.input_spec: list[tf.Tensor] | None = None\n        super().__init__(**kwargs)\n\n    def build(self, input_shape: tf.Tensor) -> None:\n        \"\"\"Creates the layer weights.\n\n        Must be implemented on all layers that have weights.\n\n        Parameters\n        ----------\n        input_shape: :class:`tf.Tensor`\n            Keras tensor (future input to layer) or ``list``/``tuple`` of Keras tensors to\n            reference for weight shape computations.\n        \"\"\"\n        self.input_spec = [keras.layers.InputSpec(shape=input_shape)]\n        super().build(input_shape)\n\n    def compute_output_shape(self, input_shape: tuple[int, ...]) -> tuple[int, ...]:\n        \"\"\"Computes the output shape of the layer.\n\n        Assumes that the layer will be built to match that input shape provided.\n\n        Parameters\n        ----------\n        input_shape: tuple or list of tuples\n            Shape tuple (tuple of integers) or list of shape tuples (one per output tensor of the\n            layer).  Shape tuples can include None for free dimensions, instead of an integer.\n\n        Returns\n        -------\n        tuple\n            An input shape tuple\n        \"\"\"\n        assert self.input_spec is not None\n        input_shape = self.input_spec[0].shape\n        in_width, in_height = input_shape[2], input_shape[1]\n        kernel_width, kernel_height = self.kernel_size, self.kernel_size\n\n        if (in_height % self.stride) == 0:\n            padding_height = max(kernel_height - self.stride, 0)\n        else:\n            padding_height = max(kernel_height - (in_height % self.stride), 0)\n        if (in_width % self.stride) == 0:\n            padding_width = max(kernel_width - self.stride, 0)\n        else:\n            padding_width = max(kernel_width - (in_width % self.stride), 0)\n\n        return (input_shape[0],\n                input_shape[1] + padding_height,\n                input_shape[2] + padding_width,\n                input_shape[3])\n\n    def call(self, inputs: tf.Tensor, *args, **kwargs) -> tf.Tensor:\n        \"\"\"This is where the layer's logic lives.\n\n        Parameters\n        ----------\n        inputs: :class:`tf.Tensor`\n            Input tensor, or list/tuple of input tensors\n\n        Returns\n        -------\n        :class:`tf.Tensor`\n            A tensor or list/tuple of tensors\n        \"\"\"\n        assert self.input_spec is not None\n        input_shape = self.input_spec[0].shape\n        in_width, in_height = input_shape[2], input_shape[1]\n        kernel_width, kernel_height = self.kernel_size, self.kernel_size\n\n        if (in_height % self.stride) == 0:\n            padding_height = max(kernel_height - self.stride, 0)\n        else:\n            padding_height = max(kernel_height - (in_height % self.stride), 0)\n        if (in_width % self.stride) == 0:\n            padding_width = max(kernel_width - self.stride, 0)\n        else:\n            padding_width = max(kernel_width - (in_width % self.stride), 0)\n\n        padding_top = padding_height // 2\n        padding_bot = padding_height - padding_top\n        padding_left = padding_width // 2\n        padding_right = padding_width - padding_left\n\n        return tf.pad(inputs,\n                      [[0, 0],\n                       [padding_top, padding_bot],\n                       [padding_left, padding_right],\n                       [0, 0]],\n                      'REFLECT')\n\n    def get_config(self) -> dict[str, T.Any]:\n        \"\"\"Returns the config of the layer.\n\n        A layer config is a Python dictionary (serializable) containing the configuration of a\n        layer. The same layer can be reinstated later (without its trained weights) from this\n        configuration.\n\n        The configuration of a layer does not include connectivity information, nor the layer\n        class name. These are handled by `Network` (one layer of abstraction above).\n\n        Returns\n        --------\n        dict\n            A python dictionary containing the layer configuration\n        \"\"\"\n        config = {'stride': self.stride,\n                  'kernel_size': self.kernel_size}\n        base_config = super().get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nclass SubPixelUpscaling(tf.keras.layers.Layer):\n    \"\"\" Sub-pixel convolutional up-scaling layer.\n\n    This layer requires a Convolution2D prior to it, having output filters computed according to\n    the formula :math:`filters = k * (scale_factor * scale_factor)` where `k` is a user defined\n    number of filters (generally larger than 32) and `scale_factor` is the up-scaling factor\n    (generally 2).\n\n    This layer performs the depth to space operation on the convolution filters, and returns a\n    tensor with the size as defined below.\n\n    Notes\n    -----\n    This method is deprecated as it just performs the same as :class:`PixelShuffler`\n    using explicit Tensorflow ops. The method is kept in the repository to support legacy\n    models that have been created with this layer.\n\n    In practice, it is useful to have a second convolution layer after the\n    :class:`SubPixelUpscaling` layer to speed up the learning process. However, if you are stacking\n    multiple :class:`SubPixelUpscaling` blocks, it may increase the number of parameters greatly,\n    so the Convolution layer after :class:`SubPixelUpscaling` layer can be removed.\n\n    Example\n    -------\n    >>> # A standard sub-pixel up-scaling block\n    >>> x = Convolution2D(256, 3, 3, padding=\"same\", activation=\"relu\")(...)\n    >>> u = SubPixelUpscaling(scale_factor=2)(x)\n    [Optional]\n    >>> x = Convolution2D(256, 3, 3, padding=\"same\", activation=\"relu\")(u)\n\n    Parameters\n    ----------\n    size: int, optional\n        The up-scaling factor. Default: `2`\n    data_format: [\"channels_first\", \"channels_last\", ``None``], optional\n        The data format for the input. Default: ``None``\n    kwargs: dict\n        The standard Keras Layer keyword arguments (if any)\n\n    References\n    ----------\n    based on the paper \"Real-Time Single Image and Video Super-Resolution Using an Efficient\n    Sub-Pixel Convolutional Neural Network\" (https://arxiv.org/abs/1609.05158).\n    \"\"\"\n\n    def __init__(self, scale_factor: int = 2, data_format: str | None = None, **kwargs) -> None:\n        super().__init__(**kwargs)\n\n        self.scale_factor = scale_factor\n        self.data_format = conv_utils.normalize_data_format(data_format)\n\n    def build(self, input_shape: tuple[int, ...]) -> None:\n        \"\"\"Creates the layer weights.\n\n        Must be implemented on all layers that have weights.\n\n        Parameters\n        ----------\n        input_shape: tensor\n            Keras tensor (future input to layer) or ``list``/``tuple`` of Keras tensors to\n            reference for weight shape computations.\n        \"\"\"\n        pass  # pylint:disable=unnecessary-pass\n\n    def call(self, inputs: tf.Tensor, *args, **kwargs) -> tf.Tensor:\n        \"\"\"This is where the layer's logic lives.\n\n        Parameters\n        ----------\n        inputs: :class:`tf.Tensor`\n            Input tensor, or list/tuple of input tensors\n\n        Returns\n        -------\n        :class:`tf.Tensor`\n            A tensor or list/tuple of tensors\n        \"\"\"\n        retval = self._depth_to_space(inputs, self.scale_factor, self.data_format)\n        return retval\n\n    def compute_output_shape(self, input_shape: tuple[int, ...]) -> tuple[int, ...]:\n        \"\"\"Computes the output shape of the layer.\n\n        Assumes that the layer will be built to match that input shape provided.\n\n        Parameters\n        ----------\n        input_shape: tuple or list of tuples\n            Shape tuple (tuple of integers) or list of shape tuples (one per output tensor of the\n            layer).  Shape tuples can include None for free dimensions, instead of an integer.\n\n        Returns\n        -------\n        tuple\n            An input shape tuple\n        \"\"\"\n        if self.data_format == \"channels_first\":\n            batch, channels, rows, columns = input_shape\n            return (batch,\n                    channels // (self.scale_factor ** 2),\n                    rows * self.scale_factor,\n                    columns * self.scale_factor)\n        batch, rows, columns, channels = input_shape\n        return (batch,\n                rows * self.scale_factor,\n                columns * self.scale_factor,\n                channels // (self.scale_factor ** 2))\n\n    @classmethod\n    def _depth_to_space(cls,\n                        inputs: tf.Tensor,\n                        scale: int,\n                        data_format: str | None = None) -> tf.Tensor:\n        \"\"\" Uses phase shift algorithm to convert channels/depth for spatial resolution\n\n        Parameters\n        ----------\n        inputs : :class:`tf.Tensor`\n            The input Tensor\n        scale : int\n            Scale factor\n        data_format : str | None, optional\n            \"channels_first\" or \"channels_last\"\n\n        Returns\n        -------\n        :class:`tf.Tensor`\n            The output Tensor\n        \"\"\"\n        if data_format is None:\n            data_format = K.image_data_format()\n        data_format = data_format.lower()\n        inputs = cls._preprocess_conv2d_input(inputs, data_format)\n        out = tf.nn.depth_to_space(inputs, scale)\n        out = cls._postprocess_conv2d_output(out, data_format)\n        return out\n\n    @staticmethod\n    def _postprocess_conv2d_output(inputs: tf.Tensor, data_format: str | None) -> tf.Tensor:\n        \"\"\"Transpose and cast the output from conv2d if needed.\n\n        Parameters\n        ----------\n        inputs: :class:`tf.Tensor`\n            The input that requires transposing and casting\n        data_format: str\n            `\"channels_last\"` or `\"channels_first\"`\n\n        Returns\n        -------\n        :class:`tf.Tensor`\n            The transposed and cast input tensor\n        \"\"\"\n\n        if data_format == \"channels_first\":\n            inputs = tf.transpose(inputs, (0, 3, 1, 2))\n\n        if K.floatx() == \"float64\":\n            inputs = tf.cast(inputs, \"float64\")\n        return inputs\n\n    @staticmethod\n    def _preprocess_conv2d_input(inputs: tf.Tensor, data_format: str | None) -> tf.Tensor:\n        \"\"\"Transpose and cast the input before the conv2d.\n\n        Parameters\n        ----------\n        inputs: :class:`tf.Tensor`\n            The input that requires transposing and casting\n        data_format: str\n            `\"channels_last\"` or `\"channels_first\"`\n\n        Returns\n        -------\n        :class:`tf.Tensor`\n            The transposed and cast input tensor\n        \"\"\"\n        if K.dtype(inputs) == \"float64\":\n            inputs = tf.cast(inputs, \"float32\")\n        if data_format == \"channels_first\":\n            # Tensorflow uses the last dimension as channel dimension, instead of the 2nd one.\n            # Theano input shape: (samples, input_depth, rows, cols)\n            # Tensorflow input shape: (samples, rows, cols, input_depth)\n            inputs = tf.transpose(inputs, (0, 2, 3, 1))\n        return inputs\n\n    def get_config(self) -> dict[str, T.Any]:\n        \"\"\"Returns the config of the layer.\n\n        A layer config is a Python dictionary (serializable) containing the configuration of a\n        layer. The same layer can be reinstated later (without its trained weights) from this\n        configuration.\n\n        The configuration of a layer does not include connectivity information, nor the layer\n        class name. These are handled by `Network` (one layer of abstraction above).\n\n        Returns\n        --------\n        dict\n            A python dictionary containing the layer configuration\n        \"\"\"\n        config = {\"scale_factor\": self.scale_factor,\n                  \"data_format\": self.data_format}\n        base_config = super().get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nclass Swish(tf.keras.layers.Layer):\n    \"\"\" Swish Activation Layer implementation for Keras.\n\n    Parameters\n    ----------\n    beta: float, optional\n        The beta value to apply to the activation function. Default: `1.0`\n    kwargs: dict\n        The standard Keras Layer keyword arguments (if any)\n\n    References\n    -----------\n    Swish: a Self-Gated Activation Function: https://arxiv.org/abs/1710.05941v1\n    \"\"\"\n    def __init__(self, beta: float = 1.0, **kwargs) -> None:\n        super().__init__(**kwargs)\n        self.beta = beta\n\n    def call(self, inputs, *args, **kwargs):\n        \"\"\" Call the Swish Activation function.\n\n        Parameters\n        ----------\n        inputs: tensor\n            Input tensor, or list/tuple of input tensors\n\n        Returns\n        -------\n        :class:`tf.Tensor`\n            A tensor or list/tuple of tensors\n        \"\"\"\n        return tf.nn.swish(inputs * self.beta)\n\n    def get_config(self):\n        \"\"\"Returns the config of the layer.\n\n        Adds the :attr:`beta` to config.\n\n        Returns\n        --------\n        dict\n            A python dictionary containing the layer configuration\n        \"\"\"\n        config = super().get_config()\n        config[\"beta\"] = self.beta\n        return config\n\n\n# Update layers into Keras custom objects\nfor name_, obj in inspect.getmembers(sys.modules[__name__]):\n    if inspect.isclass(obj) and obj.__module__ == __name__:\n        keras.utils.get_custom_objects().update({name_: obj})\n", "lib/model/initializers.py": "#!/usr/bin/env python3\n\"\"\" Custom Initializers for faceswap.py \"\"\"\n\nimport logging\nimport sys\nimport inspect\n\nimport numpy as np\nimport tensorflow as tf\n\n# Fix intellisense/linting for tf.keras' thoroughly broken import system\nkeras = tf.keras\nK = keras.backend\n\n\nlogger = logging.getLogger(__name__)\n\n\ndef compute_fans(shape, data_format='channels_last'):\n    \"\"\"Computes the number of input and output units for a weight shape.\n\n    Ported directly from Keras as the location moves between keras and tensorflow-keras\n\n    Parameters\n    ----------\n    shape: tuple\n        shape tuple of integers\n    data_format: str\n        Image data format to use for convolution kernels. Note that all kernels in Keras are\n        standardized on the `\"channels_last\"` ordering (even when inputs are set to\n        `\"channels_first\"`).\n\n    Returns\n    -------\n    tuple\n            A tuple of scalars, `(fan_in, fan_out)`.\n\n    Raises\n    ------\n    ValueError\n        In case of invalid `data_format` argument.\n    \"\"\"\n    if len(shape) == 2:\n        fan_in = shape[0]\n        fan_out = shape[1]\n    elif len(shape) in {3, 4, 5}:\n        # Assuming convolution kernels (1D, 2D or 3D).\n        # Theano kernel shape: (depth, input_depth, ...)\n        # Tensorflow kernel shape: (..., input_depth, depth)\n        if data_format == 'channels_first':\n            receptive_field_size = np.prod(shape[2:])\n            fan_in = shape[1] * receptive_field_size\n            fan_out = shape[0] * receptive_field_size\n        elif data_format == 'channels_last':\n            receptive_field_size = np.prod(shape[:-2])\n            fan_in = shape[-2] * receptive_field_size\n            fan_out = shape[-1] * receptive_field_size\n        else:\n            raise ValueError('Invalid data_format: ' + data_format)\n    else:\n        # No specific assumptions.\n        fan_in = np.sqrt(np.prod(shape))\n        fan_out = np.sqrt(np.prod(shape))\n    return fan_in, fan_out\n\n\nclass ICNR(keras.initializers.Initializer):  # type:ignore[name-defined]\n    \"\"\" ICNR initializer for checkerboard artifact free sub pixel convolution\n\n    Parameters\n    ----------\n    initializer: :class:`keras.initializers.Initializer`\n        The initializer used for sub kernels (orthogonal, glorot uniform, etc.)\n    scale: int, optional\n        scaling factor of sub pixel convolution (up sampling from 8x8 to 16x16 is scale 2).\n        Default: `2`\n\n    Returns\n    -------\n    tensor\n        The modified kernel weights\n\n    Example\n    -------\n    >>> x = conv2d(... weights_initializer=ICNR(initializer=he_uniform(), scale=2))\n\n    References\n    ----------\n    Andrew Aitken et al. Checkerboard artifact free sub-pixel convolution\n    https://arxiv.org/pdf/1707.02937.pdf,  https://distill.pub/2016/deconv-checkerboard/\n    \"\"\"\n\n    def __init__(self, initializer, scale=2):\n        self.scale = scale\n        self.initializer = initializer\n\n    def __call__(self, shape, dtype=\"float32\", **kwargs):\n        \"\"\" Call function for the ICNR initializer.\n\n        Parameters\n        ----------\n        shape: tuple or list\n            The required resized shape for the output tensor\n        dtype: str\n            The data type for the tensor\n\n        Returns\n        -------\n        tensor\n            The modified kernel weights\n        \"\"\"\n        shape = list(shape)\n        if self.scale == 1:\n            return self.initializer(shape)\n        new_shape = shape[:3] + [shape[3] // (self.scale ** 2)]\n        if isinstance(self.initializer, dict):\n            self.initializer = keras.initializers.deserialize(self.initializer)\n        var_x = self.initializer(new_shape, dtype)\n        var_x = K.permute_dimensions(var_x, [2, 0, 1, 3])\n        var_x = K.resize_images(var_x,\n                                self.scale,\n                                self.scale,\n                                \"channels_last\",\n                                interpolation=\"nearest\")\n        var_x = self._space_to_depth(var_x)\n        var_x = K.permute_dimensions(var_x, [1, 2, 0, 3])\n        logger.debug(\"Output shape: %s\", var_x.shape)\n        return var_x\n\n    def _space_to_depth(self, input_tensor):\n        \"\"\" Space to depth implementation.\n\n        Parameters\n        ----------\n        input_tensor: tensor\n            The tensor to be manipulated\n\n        Returns\n        -------\n        tensor\n            The manipulated input tensor\n        \"\"\"\n        retval = tf.nn.space_to_depth(input_tensor, block_size=self.scale, data_format=\"NHWC\")\n        logger.debug(\"Input shape: %s, Output shape: %s\", input_tensor.shape, retval.shape)\n        return retval\n\n    def get_config(self):\n        \"\"\" Return the ICNR Initializer configuration.\n\n        Returns\n        -------\n        dict\n            The configuration for ICNR Initialization\n        \"\"\"\n        config = {\"scale\": self.scale,\n                  \"initializer\": self.initializer\n                  }\n        base_config = super().get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nclass ConvolutionAware(keras.initializers.Initializer):  # type:ignore[name-defined]\n    \"\"\"\n    Initializer that generates orthogonal convolution filters in the Fourier space. If this\n    initializer is passed a shape that is not 3D or 4D, orthogonal initialization will be used.\n\n    Adapted, fixed and optimized from:\n    https://github.com/keras-team/keras-contrib/blob/master/keras_contrib/initializers/convaware.py\n\n    Parameters\n    ----------\n    eps_std: float, optional\n        The Standard deviation for the random normal noise used to break symmetry in the inverse\n        Fourier transform. Default: 0.05\n    seed: int, optional\n        Used to seed the random generator. Default: ``None``\n    initialized: bool, optional\n        This should always be set to ``False``. To avoid Keras re-calculating the values every time\n        the model is loaded, this parameter is internally set on first time initialization.\n        Default:``False``\n\n    Returns\n    -------\n    tensor\n        The modified kernel weights\n\n    References\n    ----------\n    Armen Aghajanyan, https://arxiv.org/abs/1702.06295\n    \"\"\"\n\n    def __init__(self, eps_std=0.05, seed=None, initialized=False):\n        self.eps_std = eps_std\n        self.seed = seed\n        self.orthogonal = keras.initializers.Orthogonal()\n        self.he_uniform = keras.initializers.he_uniform()\n        self.initialized = initialized\n\n    def __call__(self, shape, dtype=None, **kwargs):\n        \"\"\" Call function for the ICNR initializer.\n\n        Parameters\n        ----------\n        shape: tuple or list\n            The required shape for the output tensor\n        dtype: str\n            The data type for the tensor\n\n        Returns\n        -------\n        tensor\n            The modified kernel weights\n        \"\"\"\n        # TODO Tensorflow appears to pass in a :class:`tensorflow.python.framework.dtypes.DType`\n        # object which causes this to error, so currently just reverts to default dtype if a string\n        # is not passed in.\n        if self.initialized:   # Avoid re-calculating initializer when loading a saved model\n            return self.he_uniform(shape, dtype=dtype)\n        dtype = K.floatx() if not isinstance(dtype, str) else dtype\n        logger.info(\"Calculating Convolution Aware Initializer for shape: %s\", shape)\n        rank = len(shape)\n        if self.seed is not None:\n            np.random.seed(self.seed)\n\n        fan_in, _ = compute_fans(shape)  # pylint:disable=protected-access\n        variance = 2 / fan_in\n\n        if rank == 3:\n            row, stack_size, filters_size = shape\n\n            transpose_dimensions = (2, 1, 0)\n            kernel_shape = (row,)\n            correct_ifft = lambda shape, s=[None]: np.fft.irfft(shape, s[0])  # noqa:E501,E731 # pylint:disable=unnecessary-lambda-assignment\n            correct_fft = np.fft.rfft\n\n        elif rank == 4:\n            row, column, stack_size, filters_size = shape\n\n            transpose_dimensions = (2, 3, 1, 0)\n            kernel_shape = (row, column)\n            correct_ifft = np.fft.irfft2\n            correct_fft = np.fft.rfft2\n\n        elif rank == 5:\n            var_x, var_y, var_z, stack_size, filters_size = shape\n\n            transpose_dimensions = (3, 4, 0, 1, 2)\n            kernel_shape = (var_x, var_y, var_z)\n            correct_fft = np.fft.rfftn\n            correct_ifft = np.fft.irfftn\n\n        else:\n            self.initialized = True\n            return K.variable(self.orthogonal(shape), dtype=dtype)\n\n        kernel_fourier_shape = correct_fft(np.zeros(kernel_shape)).shape\n\n        basis = self._create_basis(filters_size, stack_size, np.prod(kernel_fourier_shape), dtype)\n        basis = basis.reshape((filters_size, stack_size,) + kernel_fourier_shape)\n        randoms = np.random.normal(0, self.eps_std, basis.shape[:-2] + kernel_shape)\n        init = correct_ifft(basis, kernel_shape) + randoms\n        init = self._scale_filters(init, variance)\n        self.initialized = True\n        return K.variable(init.transpose(transpose_dimensions), dtype=dtype, name=\"conv_aware\")\n\n    def _create_basis(self, filters_size, filters, size, dtype):\n        \"\"\" Create the basis for convolutional aware initialization \"\"\"\n        logger.debug(\"filters_size: %s, filters: %s, size: %s, dtype: %s\",\n                     filters_size, filters, size, dtype)\n        if size == 1:\n            return np.random.normal(0.0, self.eps_std, (filters_size, filters, size))\n        nbb = filters // size + 1\n        var_a = np.random.normal(0.0, 1.0, (filters_size, nbb, size, size))\n        var_a = self._symmetrize(var_a)\n        var_u = np.linalg.svd(var_a)[0].transpose(0, 1, 3, 2)\n        var_p = np.reshape(var_u, (filters_size, nbb * size, size))[:, :filters, :].astype(dtype)\n        return var_p\n\n    @staticmethod\n    def _symmetrize(var_a):\n        \"\"\" Make the given tensor symmetrical. \"\"\"\n        var_b = np.transpose(var_a, axes=(0, 1, 3, 2))\n        diag = var_a.diagonal(axis1=2, axis2=3)\n        var_c = np.array([[np.diag(arr) for arr in batch] for batch in diag])\n        return var_a + var_b - var_c\n\n    @staticmethod\n    def _scale_filters(filters, variance):\n        \"\"\" Scale the given filters. \"\"\"\n        c_var = np.var(filters)\n        var_p = np.sqrt(variance / c_var)\n        return filters * var_p\n\n    def get_config(self):\n        \"\"\" Return the Convolutional Aware Initializer configuration.\n\n        Returns\n        -------\n        dict\n            The configuration for ICNR Initialization\n        \"\"\"\n        return {\"eps_std\": self.eps_std,\n                \"seed\": self.seed,\n                \"initialized\": self.initialized}\n\n\n# Update initializers into Keras custom objects\nfor name, obj in inspect.getmembers(sys.modules[__name__]):\n    if inspect.isclass(obj) and obj.__module__ == __name__:\n        keras.utils.get_custom_objects().update({name: obj})\n", "lib/model/session.py": "#!/usr/bin python3\n\"\"\" Settings manager for Keras Backend \"\"\"\nfrom __future__ import annotations\nfrom contextlib import nullcontext\nimport logging\nimport typing as T\n\nimport numpy as np\nimport tensorflow as tf\n\n# Ignore linting errors from Tensorflow's thoroughly broken import system\nfrom tensorflow.keras.layers import Activation  # pylint:disable=import-error\nfrom tensorflow.keras.models import load_model as k_load_model, Model  # noqa:E501  # pylint:disable=import-error\n\nfrom lib.utils import get_backend\n\nif T.TYPE_CHECKING:\n    from collections.abc import Callable\n\nlogger = logging.getLogger(__name__)\n\n\nclass KSession():\n    \"\"\" Handles the settings of backend sessions for inference models.\n\n    This class acts as a wrapper for various :class:`keras.Model()` functions, ensuring that\n    actions performed on a model are handled consistently and can be performed in parallel in\n    separate threads.\n\n    This is an early implementation of this class, and should be expanded out over time.\n\n    Notes\n    -----\n    The documentation refers to :mod:`keras`. This is a pseudonym for either :mod:`keras` or\n    :mod:`tensorflow.keras` depending on the backend in use.\n\n    Parameters\n    ----------\n    name: str\n        The name of the model that is to be loaded\n    model_path: str\n        The path to the keras model file\n    model_kwargs: dict, optional\n        Any kwargs that need to be passed to :func:`keras.models.load_models()`. Default: ``None``\n    allow_growth: bool, optional\n        Enable the Tensorflow GPU allow_growth configuration option. This option prevents\n        Tensorflow from allocating all of the GPU VRAM, but can lead to higher fragmentation and\n        slower performance. Default: ``False``\n    exclude_gpus: list, optional\n        A list of indices correlating to connected GPUs that Tensorflow should not use. Pass\n        ``None`` to not exclude any GPUs. Default: ``None``\n    cpu_mode: bool, optional\n        ``True`` run the model on CPU. Default: ``False``\n    \"\"\"\n    def __init__(self,\n                 name: str,\n                 model_path: str,\n                 model_kwargs: dict | None = None,\n                 allow_growth: bool = False,\n                 exclude_gpus: list[int] | None = None,\n                 cpu_mode: bool = False) -> None:\n        logger.trace(\"Initializing: %s (name: %s, model_path: %s, \"  # type:ignore\n                     \"model_kwargs: %s,  allow_growth: %s, exclude_gpus: %s, cpu_mode: %s)\",\n                     self.__class__.__name__, name, model_path, model_kwargs, allow_growth,\n                     exclude_gpus, cpu_mode)\n        self._name = name\n        self._backend = get_backend()\n        self._context = self._set_session(allow_growth,\n                                          [] if exclude_gpus is None else exclude_gpus,\n                                          cpu_mode)\n        self._model_path = model_path\n        self._model_kwargs = {} if not model_kwargs else model_kwargs\n        self._model: Model | None = None\n        logger.trace(\"Initialized: %s\", self.__class__.__name__,)  # type:ignore\n\n    def predict(self,\n                feed: list[np.ndarray] | np.ndarray,\n                batch_size: int | None = None) -> list[np.ndarray] | np.ndarray:\n        \"\"\" Get predictions from the model.\n\n        This method is a wrapper for :func:`keras.predict()` function. For Tensorflow backends\n        this is a straight call to the predict function.\n\n        Parameters\n        ----------\n        feed: numpy.ndarray or list\n            The feed to be provided to the model as input. This should be a :class:`numpy.ndarray`\n            for single inputs or a `list` of :class:`numpy.ndarray` objects for multiple inputs.\n        batchsize: int, optional\n            The batch size to run prediction at. Default ``None``\n\n        Returns\n        -------\n        :class:`numpy.ndarray`\n            The predictions from the model\n        \"\"\"\n        assert self._model is not None\n        with self._context:\n            return self._model.predict(feed, verbose=0, batch_size=batch_size)\n\n    def _set_session(self,\n                     allow_growth: bool,\n                     exclude_gpus: list,\n                     cpu_mode: bool) -> T.ContextManager:\n        \"\"\" Sets the backend session options.\n\n        For CPU backends, this hides any GPUs from Tensorflow.\n\n        For Nvidia backends, this hides any GPUs that Tensorflow should not use and applies\n        any allow growth settings\n\n        Parameters\n        ----------\n        allow_growth: bool\n            Enable the Tensorflow GPU allow_growth configuration option. This option prevents\n            Tensorflow from allocating all of the GPU VRAM, but can lead to higher fragmentation\n            and slower performance\n        exclude_gpus: list\n            A list of indices correlating to connected GPUs that Tensorflow should not use. Pass\n            ``None`` to not exclude any GPUs\n        cpu_mode: bool\n            ``True`` run the model on CPU. Default: ``False``\n        \"\"\"\n        retval = nullcontext()\n        if self._backend == \"cpu\":\n            logger.verbose(\"Hiding GPUs from Tensorflow\")  # type:ignore\n            tf.config.set_visible_devices([], \"GPU\")\n            return retval\n\n        gpus = tf.config.list_physical_devices('GPU')\n        if exclude_gpus:\n            gpus = [gpu for idx, gpu in enumerate(gpus) if idx not in exclude_gpus]\n            logger.debug(\"Filtering devices to: %s\", gpus)\n            tf.config.set_visible_devices(gpus, \"GPU\")\n\n        if allow_growth and self._backend == \"nvidia\":\n            for gpu in gpus:\n                logger.info(\"Setting allow growth for GPU: %s\", gpu)\n                tf.config.experimental.set_memory_growth(gpu, True)\n\n        if cpu_mode:\n            retval = tf.device(\"/device:cpu:0\")\n        return retval\n\n    def load_model(self) -> None:\n        \"\"\" Loads a model.\n\n        This method is a wrapper for :func:`keras.models.load_model()`. Loads a model and its\n        weights from :attr:`model_path` defined during initialization of this class. Any additional\n        ``kwargs`` to be passed to :func:`keras.models.load_model()` should also be defined during\n        initialization of the class.\n\n        For Tensorflow backends, the `make_predict_function` method is called on the model to make\n        it thread safe.\n        \"\"\"\n        logger.verbose(\"Initializing plugin model: %s\", self._name)  # type:ignore\n        with self._context:\n            self._model = k_load_model(self._model_path, compile=False, **self._model_kwargs)\n            self._model.make_predict_function()\n\n    def define_model(self, function: Callable) -> None:\n        \"\"\" Defines a model from the given function.\n\n        This method acts as a wrapper for :class:`keras.models.Model()`.\n\n        Parameters\n        ----------\n        function: function\n            A function that defines a :class:`keras.Model` and returns it's ``inputs`` and\n            ``outputs``. The function that generates these results should be passed in, NOT the\n            results themselves, as the function needs to be executed within the correct context.\n        \"\"\"\n        with self._context:\n            self._model = Model(*function())\n\n    def load_model_weights(self) -> None:\n        \"\"\" Load model weights for a defined model inside the correct session.\n\n        This method is a wrapper for :class:`keras.load_weights()`. Once a model has been defined\n        in :func:`define_model()` this method can be called to load its weights from the\n        :attr:`model_path` defined during initialization of this class.\n\n        For Tensorflow backends, the `make_predict_function` method is called on the model to make\n        it thread safe.\n        \"\"\"\n        logger.verbose(\"Initializing plugin model: %s\", self._name)  # type:ignore\n        assert self._model is not None\n        with self._context:\n            self._model.load_weights(self._model_path)\n            self._model.make_predict_function()\n\n    def append_softmax_activation(self, layer_index: int = -1) -> None:\n        \"\"\" Append a softmax activation layer to a model\n\n        Occasionally a softmax activation layer needs to be added to a model's output.\n        This is a convenience function to append this layer to the loaded model.\n\n        Parameters\n        ----------\n        layer_index: int, optional\n            The layer index of the model to select the output from to use as an input to the\n            softmax activation layer. Default: `-1` (The final layer of the model)\n        \"\"\"\n        logger.debug(\"Appending Softmax Activation to model: (layer_index: %s)\", layer_index)\n        assert self._model is not None\n        with self._context:\n            softmax = Activation(\"softmax\", name=\"softmax\")(self._model.layers[layer_index].output)\n            self._model = Model(inputs=self._model.input, outputs=[softmax])\n", "lib/model/nn_blocks.py": "#!/usr/bin/env python3\n\"\"\" Neural Network Blocks for faceswap.py. \"\"\"\nfrom __future__ import annotations\nimport logging\nimport typing as T\n\n# Ignore linting errors from Tensorflow's thoroughly broken import system\nfrom tensorflow.keras.layers import (  # pylint:disable=import-error\n    Activation, Add, BatchNormalization, Concatenate, Conv2D as KConv2D, Conv2DTranspose,\n    DepthwiseConv2D as KDepthwiseConv2d, LeakyReLU, PReLU, SeparableConv2D, UpSampling2D)\nfrom tensorflow.keras.initializers import he_uniform, VarianceScaling  # noqa:E501  # pylint:disable=import-error\n\nfrom .initializers import ICNR, ConvolutionAware\nfrom .layers import PixelShuffler, ReflectionPadding2D, Swish, KResizeImages\nfrom .normalization import InstanceNormalization\n\nif T.TYPE_CHECKING:\n    from tensorflow import keras\n    from tensorflow import Tensor\n\n\nlogger = logging.getLogger(__name__)\n\n\n_CONFIG: dict = {}\n_NAMES: dict[str, int] = {}\n\n\ndef set_config(configuration: dict) -> None:\n    \"\"\" Set the global configuration parameters from the user's config file.\n\n    These options are used when creating layers for new models.\n\n    Parameters\n    ----------\n    configuration: dict\n        The configuration options that exist in the training configuration files that pertain\n        specifically to Custom Faceswap Layers. The keys should be: `icnr_init`, `conv_aware_init`\n        and 'reflect_padding'\n     \"\"\"\n    global _CONFIG  # pylint:disable=global-statement\n    _CONFIG = configuration\n    logger.debug(\"Set NNBlock configuration to: %s\", _CONFIG)\n\n\ndef _get_name(name: str) -> str:\n    \"\"\" Return unique layer name for requested block.\n\n    As blocks can be used multiple times, auto appends an integer to the end of the requested\n    name to keep all block names unique\n\n    Parameters\n    ----------\n    name: str\n        The requested name for the layer\n\n    Returns\n    -------\n    str\n        The unique name for this layer\n    \"\"\"\n    global _NAMES  # pylint:disable=global-statement,global-variable-not-assigned\n    _NAMES[name] = _NAMES.setdefault(name, -1) + 1\n    name = f\"{name}_{_NAMES[name]}\"\n    logger.debug(\"Generating block name: %s\", name)\n    return name\n\n\n#  << CONVOLUTIONS >>\ndef _get_default_initializer(\n        initializer: keras.initializers.Initializer) -> keras.initializers.Initializer:\n    \"\"\" Returns a default initializer of Convolutional Aware or he_uniform for convolutional\n    layers.\n\n    Parameters\n    ----------\n    initializer: :class:`keras.initializers.Initializer` or None\n        The initializer that has been passed into the model. If this value is ``None`` then a\n        default initializer will be set to 'he_uniform'. If Convolutional Aware initialization\n        has been enabled, then any passed through initializer will be replaced with the\n        Convolutional Aware initializer.\n\n    Returns\n    -------\n    :class:`keras.initializers.Initializer`\n        The kernel initializer to use for this convolutional layer. Either the original given\n        initializer, he_uniform or convolutional aware (if selected in config options)\n    \"\"\"\n    if _CONFIG[\"conv_aware_init\"]:\n        retval = ConvolutionAware()\n    elif initializer is None:\n        retval = he_uniform()\n    else:\n        retval = initializer\n        logger.debug(\"Using model supplied initializer: %s\", retval)\n    logger.debug(\"Set default kernel_initializer: (original: %s current: %s)\", initializer, retval)\n\n    return retval\n\n\nclass Conv2D(KConv2D):  # pylint:disable=too-few-public-methods, too-many-ancestors\n    \"\"\" A standard Keras Convolution 2D layer with parameters updated to be more appropriate for\n    Faceswap architecture.\n\n    Parameters are the same, with the same defaults, as a standard :class:`keras.layers.Conv2D`\n    except where listed below. The default initializer is updated to `he_uniform` or `convolutional\n    aware` based on user configuration settings.\n\n    Parameters\n    ----------\n    padding: str, optional\n        One of `\"valid\"` or `\"same\"` (case-insensitive). Default: `\"same\"`. Note that `\"same\"` is\n        slightly inconsistent across backends with `strides` != 1, as described\n        `here <https://github.com/keras-team/keras/pull/9473#issuecomment-372166860/>`_.\n    is_upscale: `bool`, optional\n        ``True`` if the convolution is being called from an upscale layer. This causes the instance\n        to check the user configuration options to see if ICNR initialization has been selected and\n        should be applied. This should only be passed in as ``True`` from :class:`UpscaleBlock`\n        layers. Default: ``False``\n    \"\"\"\n    def __init__(self, *args, padding: str = \"same\", is_upscale: bool = False, **kwargs) -> None:\n        if kwargs.get(\"name\", None) is None:\n            filters = kwargs[\"filters\"] if \"filters\" in kwargs else args[0]\n            kwargs[\"name\"] = _get_name(f\"conv2d_{filters}\")\n        initializer = _get_default_initializer(kwargs.pop(\"kernel_initializer\", None))\n        if is_upscale and _CONFIG[\"icnr_init\"]:\n            initializer = ICNR(initializer=initializer)\n            logger.debug(\"Using ICNR Initializer: %s\", initializer)\n        super().__init__(*args, padding=padding, kernel_initializer=initializer, **kwargs)\n\n\nclass DepthwiseConv2D(KDepthwiseConv2d):  # noqa,pylint:disable=too-few-public-methods, too-many-ancestors\n    \"\"\" A standard Keras Depthwise Convolution 2D layer with parameters updated to be more\n    appropriate for Faceswap architecture.\n\n    Parameters are the same, with the same defaults, as a standard\n    :class:`keras.layers.DepthwiseConv2D` except where listed below. The default initializer is\n    updated to `he_uniform` or `convolutional aware` based on user configuration settings.\n\n    Parameters\n    ----------\n    padding: str, optional\n        One of `\"valid\"` or `\"same\"` (case-insensitive). Default: `\"same\"`. Note that `\"same\"` is\n        slightly inconsistent across backends with `strides` != 1, as described\n        `here <https://github.com/keras-team/keras/pull/9473#issuecomment-372166860/>`_.\n    is_upscale: `bool`, optional\n        ``True`` if the convolution is being called from an upscale layer. This causes the instance\n        to check the user configuration options to see if ICNR initialization has been selected and\n        should be applied. This should only be passed in as ``True`` from :class:`UpscaleBlock`\n        layers. Default: ``False``\n    \"\"\"\n    def __init__(self, *args, padding: str = \"same\", is_upscale: bool = False, **kwargs) -> None:\n        if kwargs.get(\"name\", None) is None:\n            kwargs[\"name\"] = _get_name(\"dwconv2d\")\n        initializer = _get_default_initializer(kwargs.pop(\"depthwise_initializer\", None))\n        if is_upscale and _CONFIG[\"icnr_init\"]:\n            initializer = ICNR(initializer=initializer)\n            logger.debug(\"Using ICNR Initializer: %s\", initializer)\n        super().__init__(*args, padding=padding, depthwise_initializer=initializer, **kwargs)\n\n\nclass Conv2DOutput():  # pylint:disable=too-few-public-methods\n    \"\"\" A Convolution 2D layer that separates out the activation layer to explicitly set the data\n    type on the activation to float 32 to fully support mixed precision training.\n\n    The Convolution 2D layer uses default parameters to be more appropriate for Faceswap\n    architecture.\n\n    Parameters are the same, with the same defaults, as a standard :class:`keras.layers.Conv2D`\n    except where listed below. The default initializer is updated to he_uniform or convolutional\n    aware based on user config settings.\n\n    Parameters\n    ----------\n    filters: int\n        The dimensionality of the output space (i.e. the number of output filters in the\n        convolution)\n    kernel_size: int or tuple/list of 2 ints\n        The height and width of the 2D convolution window. Can be a single integer to specify the\n        same value for all spatial dimensions.\n    activation: str, optional\n        The activation function to apply to the output. Default: `\"sigmoid\"`\n    padding: str, optional\n        One of `\"valid\"` or `\"same\"` (case-insensitive). Default: `\"same\"`. Note that `\"same\"` is\n        slightly inconsistent across backends with `strides` != 1, as described\n        `here <https://github.com/keras-team/keras/pull/9473#issuecomment-372166860/>`_.\n    kwargs: dict\n        Any additional Keras standard layer keyword arguments to pass to the Convolutional 2D layer\n    \"\"\"\n    def __init__(self,\n                 filters: int,\n                 kernel_size: int | tuple[int],\n                 activation: str = \"sigmoid\",\n                 padding: str = \"same\", **kwargs) -> None:\n        self._name = _get_name(kwargs.pop(\"name\")) if \"name\" in kwargs else _get_name(\n            f\"conv_output_{filters}\")\n        self._filters = filters\n        self._kernel_size = kernel_size\n        self._activation = activation\n        self._padding = padding\n        self._kwargs = kwargs\n\n    def __call__(self, inputs: Tensor) -> Tensor:\n        \"\"\" Call the Faceswap Convolutional Output Layer.\n\n        Parameters\n        ----------\n        inputs: Tensor\n            The input to the layer\n\n        Returns\n        -------\n        Tensor\n            The output tensor from the Convolution 2D Layer\n        \"\"\"\n        var_x = Conv2D(self._filters,\n                       self._kernel_size,\n                       padding=self._padding,\n                       name=f\"{self._name}_conv2d\",\n                       **self._kwargs)(inputs)\n        var_x = Activation(self._activation, dtype=\"float32\", name=self._name)(var_x)\n        return var_x\n\n\nclass Conv2DBlock():  # pylint:disable=too-few-public-methods\n    \"\"\" A standard Convolution 2D layer which applies user specified configuration to the\n    layer.\n\n    Adds reflection padding if it has been selected by the user, and other post-processing\n    if requested by the plugin.\n\n    Adds instance normalization if requested. Adds a LeakyReLU if a residual block follows.\n\n    Parameters\n    ----------\n    filters: int\n        The dimensionality of the output space (i.e. the number of output filters in the\n        convolution)\n    kernel_size: int, optional\n        An integer or tuple/list of 2 integers, specifying the height and width of the 2D\n        convolution window. Can be a single integer to specify the same value for all spatial\n        dimensions. NB: If `use_depthwise` is ``True`` then a value must still be provided here,\n        but it will be ignored. Default: 5\n    strides: tuple or int, optional\n        An integer or tuple/list of 2 integers, specifying the strides of the convolution along the\n        height and width. Can be a single integer to specify the same value for all spatial\n        dimensions. Default: `2`\n    padding: [\"valid\", \"same\"], optional\n        The padding to use. NB: If reflect padding has been selected in the user configuration\n        options, then this argument will be ignored in favor of reflect padding. Default: `\"same\"`\n    normalization: str or ``None``, optional\n        Normalization to apply after the Convolution Layer. Select one of \"batch\" or \"instance\".\n        Set to ``None`` to not apply normalization. Default: ``None``\n    activation: str or ``None``, optional\n        The activation function to use. This is applied at the end of the convolution block. Select\n        one of `\"leakyrelu\"`, `\"prelu\"` or `\"swish\"`. Set to ``None`` to not apply an activation\n        function. Default: `\"leakyrelu\"`\n    use_depthwise: bool, optional\n        Set to ``True`` to use a Depthwise Convolution 2D layer rather than a standard Convolution\n        2D layer. Default: ``False``\n    relu_alpha: float\n        The alpha to use for LeakyRelu Activation. Default=`0.1`\n    kwargs: dict\n        Any additional Keras standard layer keyword arguments to pass to the Convolutional 2D layer\n    \"\"\"\n    def __init__(self,\n                 filters: int,\n                 kernel_size: int | tuple[int, int] = 5,\n                 strides: int | tuple[int, int] = 2,\n                 padding: str = \"same\",\n                 normalization: str | None = None,\n                 activation: str | None = \"leakyrelu\",\n                 use_depthwise: bool = False,\n                 relu_alpha: float = 0.1,\n                 **kwargs) -> None:\n        self._name = kwargs.pop(\"name\") if \"name\" in kwargs else _get_name(f\"conv_{filters}\")\n\n        logger.debug(\"name: %s, filters: %s, kernel_size: %s, strides: %s, padding: %s, \"\n                     \"normalization: %s, activation: %s, use_depthwise: %s, kwargs: %s)\",\n                     self._name, filters, kernel_size, strides, padding, normalization,\n                     activation, use_depthwise, kwargs)\n\n        self._use_reflect_padding = _CONFIG[\"reflect_padding\"]\n\n        kernel_size = (kernel_size, kernel_size) if isinstance(kernel_size, int) else kernel_size\n        self._args = (kernel_size, ) if use_depthwise else (filters, kernel_size)\n        self._strides = (strides, strides) if isinstance(strides, int) else strides\n        self._padding = \"valid\" if self._use_reflect_padding else padding\n        self._kwargs = kwargs\n        self._normalization = None if not normalization else normalization.lower()\n        self._activation = None if not activation else activation.lower()\n        self._use_depthwise = use_depthwise\n        self._relu_alpha = relu_alpha\n\n        self._assert_arguments()\n\n    def _assert_arguments(self) -> None:\n        \"\"\" Validate the given arguments. \"\"\"\n        assert self._normalization in (\"batch\", \"instance\", None), (\n            \"normalization should be 'batch', 'instance' or None\")\n        assert self._activation in (\"leakyrelu\", \"swish\", \"prelu\", None), (\n            \"activation should be 'leakyrelu', 'prelu', 'swish' or None\")\n\n    def __call__(self, inputs: Tensor) -> Tensor:\n        \"\"\" Call the Faceswap Convolutional Layer.\n\n        Parameters\n        ----------\n        inputs: Tensor\n            The input to the layer\n\n        Returns\n        -------\n        Tensor\n            The output tensor from the Convolution 2D Layer\n        \"\"\"\n        if self._use_reflect_padding:\n            inputs = ReflectionPadding2D(stride=self._strides[0],\n                                         kernel_size=self._args[-1][0],  # type:ignore[index]\n                                         name=f\"{self._name}_reflectionpadding2d\")(inputs)\n        conv: keras.layers.Layer = DepthwiseConv2D if self._use_depthwise else Conv2D\n        var_x = conv(*self._args,\n                     strides=self._strides,\n                     padding=self._padding,\n                     name=f\"{self._name}_{'dw' if self._use_depthwise else ''}conv2d\",\n                     **self._kwargs)(inputs)\n        # normalization\n        if self._normalization == \"instance\":\n            var_x = InstanceNormalization(name=f\"{self._name}_instancenorm\")(var_x)\n        if self._normalization == \"batch\":\n            var_x = BatchNormalization(axis=3, name=f\"{self._name}_batchnorm\")(var_x)\n\n        # activation\n        if self._activation == \"leakyrelu\":\n            var_x = LeakyReLU(self._relu_alpha, name=f\"{self._name}_leakyrelu\")(var_x)\n        if self._activation == \"swish\":\n            var_x = Swish(name=f\"{self._name}_swish\")(var_x)\n        if self._activation == \"prelu\":\n            var_x = PReLU(name=f\"{self._name}_prelu\")(var_x)\n\n        return var_x\n\n\nclass SeparableConv2DBlock():  # pylint:disable=too-few-public-methods\n    \"\"\" Seperable Convolution Block.\n\n    Parameters\n    ----------\n    filters: int\n        The dimensionality of the output space (i.e. the number of output filters in the\n        convolution)\n    kernel_size: int, optional\n        An integer or tuple/list of 2 integers, specifying the height and width of the 2D\n        convolution window. Can be a single integer to specify the same value for all spatial\n        dimensions. Default: 5\n    strides: tuple or int, optional\n        An integer or tuple/list of 2 integers, specifying the strides of the convolution along\n        the height and width. Can be a single integer to specify the same value for all spatial\n        dimensions. Default: `2`\n    kwargs: dict\n        Any additional Keras standard layer keyword arguments to pass to the Separable\n        Convolutional 2D layer\n    \"\"\"\n    def __init__(self,\n                 filters: int,\n                 kernel_size: int | tuple[int, int] = 5,\n                 strides: int | tuple[int, int] = 2, **kwargs) -> None:\n        self._name = _get_name(f\"separableconv2d_{filters}\")\n        logger.debug(\"name: %s, filters: %s, kernel_size: %s, strides: %s, kwargs: %s)\",\n                     self._name, filters, kernel_size, strides, kwargs)\n\n        self._filters = filters\n        self._kernel_size = kernel_size\n        self._strides = strides\n\n        initializer = _get_default_initializer(kwargs.pop(\"kernel_initializer\", None))\n        kwargs[\"kernel_initializer\"] = initializer\n        self._kwargs = kwargs\n\n    def __call__(self, inputs: Tensor) -> Tensor:\n        \"\"\" Call the Faceswap Separable Convolutional 2D Block.\n\n        Parameters\n        ----------\n        inputs: Tensor\n            The input to the layer\n\n        Returns\n        -------\n        Tensor\n            The output tensor from the Upscale Layer\n        \"\"\"\n        var_x = SeparableConv2D(self._filters,\n                                kernel_size=self._kernel_size,\n                                strides=self._strides,\n                                padding=\"same\",\n                                name=f\"{self._name}_seperableconv2d\",\n                                **self._kwargs)(inputs)\n        var_x = Activation(\"relu\", name=f\"{self._name}_relu\")(var_x)\n        return var_x\n\n\n#  << UPSCALING >>\n\nclass UpscaleBlock():  # pylint:disable=too-few-public-methods\n    \"\"\" An upscale layer for sub-pixel up-scaling.\n\n    Adds reflection padding if it has been selected by the user, and other post-processing\n    if requested by the plugin.\n\n    Parameters\n    ----------\n    filters: int\n        The dimensionality of the output space (i.e. the number of output filters in the\n        convolution)\n    kernel_size: int, optional\n        An integer or tuple/list of 2 integers, specifying the height and width of the 2D\n        convolution window. Can be a single integer to specify the same value for all spatial\n        dimensions. Default: 3\n    padding: [\"valid\", \"same\"], optional\n        The padding to use. NB: If reflect padding has been selected in the user configuration\n        options, then this argument will be ignored in favor of reflect padding. Default: `\"same\"`\n    scale_factor: int, optional\n        The amount to upscale the image. Default: `2`\n    normalization: str or ``None``, optional\n        Normalization to apply after the Convolution Layer. Select one of \"batch\" or \"instance\".\n        Set to ``None`` to not apply normalization. Default: ``None``\n    activation: str or ``None``, optional\n        The activation function to use. This is applied at the end of the convolution block. Select\n        one of `\"leakyrelu\"`, `\"prelu\"` or `\"swish\"`. Set to ``None`` to not apply an activation\n        function. Default: `\"leakyrelu\"`\n    kwargs: dict\n        Any additional Keras standard layer keyword arguments to pass to the Convolutional 2D layer\n    \"\"\"\n\n    def __init__(self,\n                 filters: int,\n                 kernel_size: int | tuple[int, int] = 3,\n                 padding: str = \"same\",\n                 scale_factor: int = 2,\n                 normalization: str | None = None,\n                 activation: str | None = \"leakyrelu\",\n                 **kwargs) -> None:\n        self._name = _get_name(f\"upscale_{filters}\")\n        logger.debug(\"name: %s. filters: %s, kernel_size: %s, padding: %s, scale_factor: %s, \"\n                     \"normalization: %s, activation: %s, kwargs: %s)\",\n                     self._name, filters, kernel_size, padding, scale_factor, normalization,\n                     activation, kwargs)\n\n        self._filters = filters\n        self._kernel_size = kernel_size\n        self._padding = padding\n        self._scale_factor = scale_factor\n        self._normalization = normalization\n        self._activation = activation\n        self._kwargs = kwargs\n\n    def __call__(self, inputs: Tensor) -> Tensor:\n        \"\"\" Call the Faceswap Convolutional Layer.\n\n        Parameters\n        ----------\n        inputs: Tensor\n            The input to the layer\n\n        Returns\n        -------\n        Tensor\n            The output tensor from the Upscale Layer\n        \"\"\"\n        var_x = Conv2DBlock(self._filters * self._scale_factor * self._scale_factor,\n                            self._kernel_size,\n                            strides=(1, 1),\n                            padding=self._padding,\n                            normalization=self._normalization,\n                            activation=self._activation,\n                            name=f\"{self._name}_conv2d\",\n                            is_upscale=True,\n                            **self._kwargs)(inputs)\n        var_x = PixelShuffler(name=f\"{self._name}_pixelshuffler\",\n                              size=self._scale_factor)(var_x)\n        return var_x\n\n\nclass Upscale2xBlock():  # pylint:disable=too-few-public-methods\n    \"\"\" Custom hybrid upscale layer for sub-pixel up-scaling.\n\n    Most of up-scaling is approximating lighting gradients which can be accurately achieved\n    using linear fitting. This layer attempts to improve memory consumption by splitting\n    with bilinear and convolutional layers so that the sub-pixel update will get details\n    whilst the bilinear filter will get lighting.\n\n    Adds reflection padding if it has been selected by the user, and other post-processing\n    if requested by the plugin.\n\n    Parameters\n    ----------\n    filters: int\n        The dimensionality of the output space (i.e. the number of output filters in the\n        convolution)\n    kernel_size: int, optional\n        An integer or tuple/list of 2 integers, specifying the height and width of the 2D\n        convolution window. Can be a single integer to specify the same value for all spatial\n        dimensions. Default: 3\n    padding: [\"valid\", \"same\"], optional\n        The padding to use. Default: `\"same\"`\n    activation: str or ``None``, optional\n        The activation function to use. This is applied at the end of the convolution block. Select\n        one of `\"leakyrelu\"`, `\"prelu\"` or `\"swish\"`. Set to ``None`` to not apply an activation\n        function. Default: `\"leakyrelu\"`\n    interpolation: [\"nearest\", \"bilinear\"], optional\n        Interpolation to use for up-sampling. Default: `\"bilinear\"`\n    scale_factor: int, optional\n        The amount to upscale the image. Default: `2`\n    sr_ratio: float, optional\n        The proportion of super resolution (pixel shuffler) filters to use. Non-fast mode only.\n        Default: `0.5`\n    fast: bool, optional\n        Use a faster up-scaling method that may appear more rugged. Default: ``False``\n    kwargs: dict\n        Any additional Keras standard layer keyword arguments to pass to the Convolutional 2D layer\n    \"\"\"\n    def __init__(self,\n                 filters: int,\n                 kernel_size: int | tuple[int, int] = 3,\n                 padding: str = \"same\",\n                 activation: str | None = \"leakyrelu\",\n                 interpolation: str = \"bilinear\",\n                 sr_ratio: float = 0.5,\n                 scale_factor: int = 2,\n                 fast: bool = False, **kwargs) -> None:\n        self._name = _get_name(f\"upscale2x_{filters}_{'fast' if fast else 'hyb'}\")\n\n        self._fast = fast\n        self._filters = filters if self._fast else filters - int(filters * sr_ratio)\n        self._kernel_size = kernel_size\n        self._padding = padding\n        self._interpolation = interpolation\n        self._activation = activation\n        self._scale_factor = scale_factor\n        self._kwargs = kwargs\n\n    def __call__(self, inputs: Tensor) -> Tensor:\n        \"\"\" Call the Faceswap Upscale 2x Layer.\n\n        Parameters\n        ----------\n        inputs: Tensor\n            The input to the layer\n\n        Returns\n        -------\n        Tensor\n            The output tensor from the Upscale Layer\n        \"\"\"\n        var_x = inputs\n        if not self._fast:\n            var_x_sr = UpscaleBlock(self._filters,\n                                    kernel_size=self._kernel_size,\n                                    padding=self._padding,\n                                    scale_factor=self._scale_factor,\n                                    activation=self._activation,\n                                    **self._kwargs)(var_x)\n        if self._fast or (not self._fast and self._filters > 0):\n            var_x2 = Conv2D(self._filters, 3,\n                            padding=self._padding,\n                            is_upscale=True,\n                            name=f\"{self._name}_conv2d\",\n                            **self._kwargs)(var_x)\n            var_x2 = UpSampling2D(size=(self._scale_factor, self._scale_factor),\n                                  interpolation=self._interpolation,\n                                  name=f\"{self._name}_upsampling2D\")(var_x2)\n            if self._fast:\n                var_x1 = UpscaleBlock(self._filters,\n                                      kernel_size=self._kernel_size,\n                                      padding=self._padding,\n                                      scale_factor=self._scale_factor,\n                                      activation=self._activation,\n                                      **self._kwargs)(var_x)\n                var_x = Add()([var_x2, var_x1])\n            else:\n                var_x = Concatenate(name=f\"{self._name}_concatenate\")([var_x_sr, var_x2])\n        else:\n            var_x = var_x_sr\n        return var_x\n\n\nclass UpscaleResizeImagesBlock():  # pylint:disable=too-few-public-methods\n    \"\"\" Upscale block that uses the Keras Backend function resize_images to perform the up scaling\n    Similar in methodology to the :class:`Upscale2xBlock`\n\n    Adds reflection padding if it has been selected by the user, and other post-processing\n    if requested by the plugin.\n\n    Parameters\n    ----------\n    filters: int\n        The dimensionality of the output space (i.e. the number of output filters in the\n        convolution)\n    kernel_size: int, optional\n        An integer or tuple/list of 2 integers, specifying the height and width of the 2D\n        convolution window. Can be a single integer to specify the same value for all spatial\n        dimensions. Default: 3\n    padding: [\"valid\", \"same\"], optional\n        The padding to use. Default: `\"same\"`\n    activation: str or ``None``, optional\n        The activation function to use. This is applied at the end of the convolution block. Select\n        one of `\"leakyrelu\"`, `\"prelu\"` or `\"swish\"`. Set to ``None`` to not apply an activation\n        function. Default: `\"leakyrelu\"`\n    scale_factor: int, optional\n        The amount to upscale the image. Default: `2`\n    interpolation: [\"nearest\", \"bilinear\"], optional\n        Interpolation to use for up-sampling. Default: `\"bilinear\"`\n    kwargs: dict\n        Any additional Keras standard layer keyword arguments to pass to the Convolutional 2D layer\n    \"\"\"\n    def __init__(self,\n                 filters: int,\n                 kernel_size: int | tuple[int, int] = 3,\n                 padding: str = \"same\",\n                 activation: str | None = \"leakyrelu\",\n                 scale_factor: int = 2,\n                 interpolation: T.Literal[\"nearest\", \"bilinear\"] = \"bilinear\") -> None:\n        self._name = _get_name(f\"upscale_ri_{filters}\")\n        self._interpolation = interpolation\n        self._size = scale_factor\n        self._filters = filters\n        self._kernel_size = kernel_size\n        self._padding = padding\n        self._activation = activation\n\n    def __call__(self, inputs: Tensor) -> Tensor:\n        \"\"\" Call the Faceswap Resize Images Layer.\n\n        Parameters\n        ----------\n        inputs: Tensor\n            The input to the layer\n\n        Returns\n        -------\n        Tensor\n            The output tensor from the Upscale Layer\n        \"\"\"\n        var_x = inputs\n\n        var_x_sr = KResizeImages(size=self._size,\n                                 interpolation=self._interpolation,\n                                 name=f\"{self._name}_resize\")(var_x)\n        var_x_sr = Conv2D(self._filters, self._kernel_size,\n                          strides=1,\n                          padding=self._padding,\n                          is_upscale=True,\n                          name=f\"{self._name}_conv\")(var_x_sr)\n        var_x_us = Conv2DTranspose(self._filters, 3,\n                                   strides=2,\n                                   padding=self._padding,\n                                   name=f\"{self._name}_convtrans\")(var_x)\n        var_x = Add()([var_x_sr, var_x_us])\n\n        if self._activation == \"leakyrelu\":\n            var_x = LeakyReLU(0.2, name=f\"{self._name}_leakyrelu\")(var_x)\n        if self._activation == \"swish\":\n            var_x = Swish(name=f\"{self._name}_swish\")(var_x)\n        if self._activation == \"prelu\":\n            var_x = PReLU(name=f\"{self._name}_prelu\")(var_x)\n        return var_x\n\n\nclass UpscaleDNYBlock():  # pylint:disable=too-few-public-methods\n    \"\"\" Upscale block that implements methodology similar to the Disney Research Paper using an\n    upsampling2D block and 2 x convolutions\n\n    Adds reflection padding if it has been selected by the user, and other post-processing\n    if requested by the plugin.\n\n    References\n    ----------\n    https://studios.disneyresearch.com/2020/06/29/high-resolution-neural-face-swapping-for-visual-effects/\n\n    Parameters\n    ----------\n    filters: int\n        The dimensionality of the output space (i.e. the number of output filters in the\n        convolution)\n    kernel_size: int, optional\n        An integer or tuple/list of 2 integers, specifying the height and width of the 2D\n        convolution window. Can be a single integer to specify the same value for all spatial\n        dimensions. Default: 3\n    activation: str or ``None``, optional\n        The activation function to use. This is applied at the end of the convolution block. Select\n        one of `\"leakyrelu\"`, `\"prelu\"` or `\"swish\"`. Set to ``None`` to not apply an activation\n        function. Default: `\"leakyrelu\"`\n    size: int, optional\n        The amount to upscale the image. Default: `2`\n    interpolation: [\"nearest\", \"bilinear\"], optional\n        Interpolation to use for up-sampling. Default: `\"bilinear\"`\n    kwargs: dict\n        Any additional Keras standard layer keyword arguments to pass to the Convolutional 2D\n        layers\n    \"\"\"\n    def __init__(self,\n                 filters: int,\n                 kernel_size: int | tuple[int, int] = 3,\n                 padding: str = \"same\",\n                 activation: str | None = \"leakyrelu\",\n                 size: int = 2,\n                 interpolation: str = \"bilinear\",\n                 **kwargs) -> None:\n        self._name = _get_name(f\"upscale_dny_{filters}\")\n        self._interpolation = interpolation\n        self._size = size\n        self._filters = filters\n        self._kernel_size = kernel_size\n        self._padding = padding\n        self._activation = activation\n        self._kwargs = kwargs\n\n    def __call__(self, inputs: Tensor) -> Tensor:\n        var_x = UpSampling2D(size=self._size,\n                             interpolation=self._interpolation,\n                             name=f\"{self._name}_upsample2d\")(inputs)\n        for idx in range(2):\n            var_x = Conv2DBlock(self._filters,\n                                self._kernel_size,\n                                strides=1,\n                                padding=self._padding,\n                                activation=self._activation,\n                                relu_alpha=0.2,\n                                name=f\"{self._name}_conv2d_{idx + 1}\",\n                                is_upscale=True,\n                                **self._kwargs)(var_x)\n        return var_x\n\n\n# << OTHER BLOCKS >>\nclass ResidualBlock():  # pylint:disable=too-few-public-methods\n    \"\"\" Residual block from dfaker.\n\n    Parameters\n    ----------\n    filters: int\n        The dimensionality of the output space (i.e. the number of output filters in the\n        convolution)\n    kernel_size: int, optional\n        An integer or tuple/list of 2 integers, specifying the height and width of the 2D\n        convolution window. Can be a single integer to specify the same value for all spatial\n        dimensions. Default: 3\n    padding: [\"valid\", \"same\"], optional\n        The padding to use. Default: `\"same\"`\n    kwargs: dict\n        Any additional Keras standard layer keyword arguments to pass to the Convolutional 2D layer\n\n    Returns\n    -------\n    tensor\n        The output tensor from the Upscale layer\n    \"\"\"\n    def __init__(self,\n                 filters: int,\n                 kernel_size: int | tuple[int, int] = 3,\n                 padding: str = \"same\",\n                 **kwargs) -> None:\n        self._name = _get_name(f\"residual_{filters}\")\n        logger.debug(\"name: %s, filters: %s, kernel_size: %s, padding: %s, kwargs: %s)\",\n                     self._name, filters, kernel_size, padding, kwargs)\n        self._use_reflect_padding = _CONFIG[\"reflect_padding\"]\n\n        self._filters = filters\n        self._kernel_size = (kernel_size,\n                             kernel_size) if isinstance(kernel_size, int) else kernel_size\n        self._padding = \"valid\" if self._use_reflect_padding else padding\n        self._kwargs = kwargs\n\n    def __call__(self, inputs: Tensor) -> Tensor:\n        \"\"\" Call the Faceswap Residual Block.\n\n        Parameters\n        ----------\n        inputs: Tensor\n            The input to the layer\n\n        Returns\n        -------\n        Tensor\n            The output tensor from the Upscale Layer\n        \"\"\"\n        var_x = inputs\n        if self._use_reflect_padding:\n            var_x = ReflectionPadding2D(stride=1,\n                                        kernel_size=self._kernel_size[0],\n                                        name=f\"{self._name}_reflectionpadding2d_0\")(var_x)\n        var_x = Conv2D(self._filters,\n                       kernel_size=self._kernel_size,\n                       padding=self._padding,\n                       name=f\"{self._name}_conv2d_0\",\n                       **self._kwargs)(var_x)\n        var_x = LeakyReLU(alpha=0.2, name=f\"{self._name}_leakyrelu_1\")(var_x)\n        if self._use_reflect_padding:\n            var_x = ReflectionPadding2D(stride=1,\n                                        kernel_size=self._kernel_size[0],\n                                        name=f\"{self._name}_reflectionpadding2d_1\")(var_x)\n\n        kwargs = {key: val for key, val in self._kwargs.items() if key != \"kernel_initializer\"}\n        if not _CONFIG[\"conv_aware_init\"]:\n            kwargs[\"kernel_initializer\"] = VarianceScaling(scale=0.2,\n                                                           mode=\"fan_in\",\n                                                           distribution=\"uniform\")\n        var_x = Conv2D(self._filters,\n                       kernel_size=self._kernel_size,\n                       padding=self._padding,\n                       name=f\"{self._name}_conv2d_1\",\n                       **kwargs)(var_x)\n\n        var_x = Add()([var_x, inputs])\n        var_x = LeakyReLU(alpha=0.2, name=f\"{self._name}_leakyrelu_3\")(var_x)\n        return var_x\n", "lib/model/losses/loss.py": "#!/usr/bin/env python3\n\"\"\" Custom Loss Functions for faceswap.py \"\"\"\n\nfrom __future__ import annotations\nimport logging\nimport typing as T\n\nimport numpy as np\nimport tensorflow as tf\n\n# Ignore linting errors from Tensorflow's thoroughly broken import system\nfrom tensorflow.python.keras.engine import compile_utils  # pylint:disable=no-name-in-module\nfrom tensorflow.keras import backend as K  # pylint:disable=import-error\n\nif T.TYPE_CHECKING:\n    from collections.abc import Callable\n\nlogger = logging.getLogger(__name__)\n\n\nclass FocalFrequencyLoss():  # pylint:disable=too-few-public-methods\n    \"\"\" Focal Frequencey Loss Function.\n\n    A channels last implementation.\n\n    Notes\n    -----\n    There is a bug in this implementation that will do an incorrect FFT if\n    :attr:`patch_factor` >  ``1``, which means incorrect loss will be returned, so keep\n    patch factor at 1.\n\n    Parameters\n    ----------\n    alpha: float, Optional\n        Scaling factor of the spectrum weight matrix for flexibility. Default: ``1.0``\n    patch_factor: int, Optional\n        Factor to crop image patches for patch-based focal frequency loss.\n        Default: ``1``\n    ave_spectrum: bool, Optional\n        ``True`` to use minibatch average spectrum otherwise ``False``. Default: ``False``\n    log_matrix: bool, Optional\n        ``True`` to adjust the spectrum weight matrix by logarithm otherwise ``False``.\n        Default: ``False``\n    batch_matrix: bool, Optional\n        ``True`` to calculate the spectrum weight matrix using batch-based statistics otherwise\n        ``False``. Default: ``False``\n\n    References\n    ----------\n    https://arxiv.org/pdf/2012.12821.pdf\n    https://github.com/EndlessSora/focal-frequency-loss\n    \"\"\"\n\n    def __init__(self,\n                 alpha: float = 1.0,\n                 patch_factor: int = 1,\n                 ave_spectrum: bool = False,\n                 log_matrix: bool = False,\n                 batch_matrix: bool = False) -> None:\n        self._alpha = alpha\n        # TODO Fix bug where FFT will be incorrect if patch_factor > 1\n        self._patch_factor = patch_factor\n        self._ave_spectrum = ave_spectrum\n        self._log_matrix = log_matrix\n        self._batch_matrix = batch_matrix\n        self._dims: tuple[int, int] = (0, 0)\n\n    def _get_patches(self, inputs: tf.Tensor) -> tf.Tensor:\n        \"\"\" Crop the incoming batch of images into patches as defined by :attr:`_patch_factor.\n\n        Parameters\n        ----------\n        inputs: :class:`tf.Tensor`\n            A batch of images to be converted into patches\n\n        Returns\n        -------\n        :class`tf.Tensor``\n            The incoming batch converted into patches\n        \"\"\"\n        rows, cols = self._dims\n        patch_list = []\n        patch_rows = cols // self._patch_factor\n        patch_cols = rows // self._patch_factor\n        for i in range(self._patch_factor):\n            for j in range(self._patch_factor):\n                row_from = i * patch_rows\n                row_to = (i + 1) * patch_rows\n                col_from = j * patch_cols\n                col_to = (j + 1) * patch_cols\n                patch_list.append(inputs[:, row_from: row_to, col_from: col_to, :])\n\n        retval = K.stack(patch_list, axis=1)\n        return retval\n\n    def _tensor_to_frequency_spectrum(self, patch: tf.Tensor) -> tf.Tensor:\n        \"\"\" Perform FFT to create the orthonomalized DFT frequencies.\n\n        Parameters\n        ----------\n        inputs: :class:`tf.Tensor`\n            The incoming batch of patches to convert to the frequency spectrum\n\n        Returns\n        -------\n        :class:`tf.Tensor`\n            The DFT frequencies split into real and imaginary numbers as float32\n        \"\"\"\n        # TODO fix this for when self._patch_factor != 1.\n        rows, cols = self._dims\n        patch = K.permute_dimensions(patch, (0, 1, 4, 2, 3))  # move channels to first\n\n        patch = patch / np.sqrt(rows * cols)  # Orthonormalization\n\n        patch = K.cast(patch, \"complex64\")\n        freq = tf.signal.fft2d(patch)[..., None]\n\n        freq = K.concatenate([tf.math.real(freq), tf.math.imag(freq)], axis=-1)\n        freq = K.cast(freq, \"float32\")\n\n        freq = K.permute_dimensions(freq, (0, 1, 3, 4, 2, 5))  # channels to last\n\n        return freq\n\n    def _get_weight_matrix(self, freq_true: tf.Tensor, freq_pred: tf.Tensor) -> tf.Tensor:\n        \"\"\" Calculate a continuous, dynamic weight matrix based on current Euclidean distance.\n\n        Parameters\n        ----------\n        freq_true: :class:`tf.Tensor`\n            The real and imaginary DFT frequencies for the true batch of images\n        freq_pred: :class:`tf.Tensor`\n            The real and imaginary DFT frequencies for the predicted batch of images\n\n        Returns\n        -------\n        :class:`tf.Tensor`\n            The weights matrix for prioritizing hard frequencies\n        \"\"\"\n        weights = K.square(freq_pred - freq_true)\n        weights = K.sqrt(weights[..., 0] + weights[..., 1])\n        weights = K.pow(weights, self._alpha)\n\n        if self._log_matrix:  # adjust the spectrum weight matrix by logarithm\n            weights = K.log(weights + 1.0)\n\n        if self._batch_matrix:  # calculate the spectrum weight matrix using batch-based statistics\n            weights = weights / K.max(weights)\n        else:\n            weights = weights / K.max(K.max(weights, axis=-2), axis=-2)[..., None, None, :]\n\n        weights = K.switch(tf.math.is_nan(weights), K.zeros_like(weights), weights)\n        weights = K.clip(weights, min_value=0.0, max_value=1.0)\n\n        return weights\n\n    @classmethod\n    def _calculate_loss(cls,\n                        freq_true: tf.Tensor,\n                        freq_pred: tf.Tensor,\n                        weight_matrix: tf.Tensor) -> tf.Tensor:\n        \"\"\" Perform the loss calculation on the DFT spectrum applying the weights matrix.\n\n        Parameters\n        ----------\n        freq_true: :class:`tf.Tensor`\n            The real and imaginary DFT frequencies for the true batch of images\n        freq_pred: :class:`tf.Tensor`\n            The real and imaginary DFT frequencies for the predicted batch of images\n\n        Returns\n        :class:`tf.Tensor`\n            The final loss matrix\n        \"\"\"\n\n        tmp = K.square(freq_pred - freq_true)  # freq distance using squared Euclidean distance\n\n        freq_distance = tmp[..., 0] + tmp[..., 1]\n        loss = weight_matrix * freq_distance  # dynamic spectrum weighting (Hadamard product)\n\n        return loss\n\n    def __call__(self, y_true: tf.Tensor, y_pred: tf.Tensor) -> tf.Tensor:\n        \"\"\" Call the Focal Frequency Loss Function.\n\n        Parameters\n        ----------\n        y_true: :class:`tf.Tensor`\n            The ground truth batch of images\n        y_pred: :class:`tf.Tensor`\n            The predicted batch of images\n\n        Returns\n        -------\n        :class:`tf.Tensor`\n            The loss for this batch of images\n        \"\"\"\n        if not all(self._dims):\n            rows, cols = K.int_shape(y_true)[1:3]\n            assert cols % self._patch_factor == 0 and rows % self._patch_factor == 0, (\n                \"Patch factor must be a divisor of the image height and width\")\n            self._dims = (rows, cols)\n\n        patches_true = self._get_patches(y_true)\n        patches_pred = self._get_patches(y_pred)\n\n        freq_true = self._tensor_to_frequency_spectrum(patches_true)\n        freq_pred = self._tensor_to_frequency_spectrum(patches_pred)\n\n        if self._ave_spectrum:  # whether to use minibatch average spectrum\n            freq_true = K.mean(freq_true, axis=0, keepdims=True)\n            freq_pred = K.mean(freq_pred, axis=0, keepdims=True)\n\n        weight_matrix = self._get_weight_matrix(freq_true, freq_pred)\n        return self._calculate_loss(freq_true, freq_pred, weight_matrix)\n\n\nclass GeneralizedLoss():  # pylint:disable=too-few-public-methods\n    \"\"\"  Generalized function used to return a large variety of mathematical loss functions.\n\n    The primary benefit is a smooth, differentiable version of L1 loss.\n\n    References\n    ----------\n    Barron, J. A General and Adaptive Robust Loss Function - https://arxiv.org/pdf/1701.03077.pdf\n\n    Example\n    -------\n    >>> a=1.0, x>>c , c=1.0/255.0  # will give a smoothly differentiable version of L1 / MAE loss\n    >>> a=1.999999 (limit as a->2), beta=1.0/255.0 # will give L2 / RMSE loss\n\n    Parameters\n    ----------\n    alpha: float, optional\n        Penalty factor. Larger number give larger weight to large deviations. Default: `1.0`\n    beta: float, optional\n        Scale factor used to adjust to the input scale (i.e. inputs of mean `1e-4` or `256`).\n        Default: `1.0/255.0`\n    \"\"\"\n    def __init__(self, alpha: float = 1.0, beta: float = 1.0/255.0) -> None:\n        self._alpha = alpha\n        self._beta = beta\n\n    def __call__(self, y_true: tf.Tensor, y_pred: tf.Tensor) -> tf.Tensor:\n        \"\"\" Call the Generalized Loss Function\n\n        Parameters\n        ----------\n        y_true: :class:`tf.Tensor`\n            The ground truth value\n        y_pred: :class:`tf.Tensor`\n            The predicted value\n\n        Returns\n        -------\n        :class:`tf.Tensor`\n            The loss value from the results of function(y_pred - y_true)\n        \"\"\"\n        diff = y_pred - y_true\n        second = (K.pow(K.pow(diff/self._beta, 2.) / K.abs(2. - self._alpha) + 1.,\n                        (self._alpha / 2.)) - 1.)\n        loss = (K.abs(2. - self._alpha)/self._alpha) * second\n        loss = K.mean(loss, axis=-1) * self._beta\n        return loss\n\n\nclass GradientLoss():  # pylint:disable=too-few-public-methods\n    \"\"\" Gradient Loss Function.\n\n    Calculates the first and second order gradient difference between pixels of an image in the x\n    and y dimensions. These gradients are then compared between the ground truth and the predicted\n    image and the difference is taken. When used as a loss, its minimization will result in\n    predicted images approaching the same level of sharpness / blurriness as the ground truth.\n\n    References\n    ----------\n    TV+TV2 Regularization with Non-Convex Sparseness-Inducing Penalty for Image Restoration,\n    Chengwu Lu & Hua Huang, 2014 - http://downloads.hindawi.com/journals/mpe/2014/790547.pdf\n    \"\"\"\n    def __init__(self) -> None:\n        self.generalized_loss = GeneralizedLoss(alpha=1.9999)\n        self._tv_weight = 1.0\n        self._tv2_weight = 1.0\n\n    def __call__(self, y_true: tf.Tensor, y_pred: tf.Tensor) -> tf.Tensor:\n        \"\"\" Call the gradient loss function.\n\n        Parameters\n        ----------\n        y_true: :class:`tf.Tensor`\n            The ground truth value\n        y_pred: :class:`tf.Tensor`\n            The predicted value\n\n        Returns\n        -------\n        :class:`tf.Tensor`\n            The loss value\n        \"\"\"\n        loss = 0.0\n        loss += self._tv_weight * (self.generalized_loss(self._diff_x(y_true),\n                                                         self._diff_x(y_pred)) +\n                                   self.generalized_loss(self._diff_y(y_true),\n                                                         self._diff_y(y_pred)))\n        loss += self._tv2_weight * (self.generalized_loss(self._diff_xx(y_true),\n                                                          self._diff_xx(y_pred)) +\n                                    self.generalized_loss(self._diff_yy(y_true),\n                                    self._diff_yy(y_pred)) +\n                                    self.generalized_loss(self._diff_xy(y_true),\n                                    self._diff_xy(y_pred)) * 2.)\n        loss = loss / (self._tv_weight + self._tv2_weight)\n        # TODO simplify to use MSE instead\n        return loss\n\n    @classmethod\n    def _diff_x(cls, img: tf.Tensor) -> tf.Tensor:\n        \"\"\" X Difference \"\"\"\n        x_left = img[:, :, 1:2, :] - img[:, :, 0:1, :]\n        x_inner = img[:, :, 2:, :] - img[:, :, :-2, :]\n        x_right = img[:, :, -1:, :] - img[:, :, -2:-1, :]\n        x_out = K.concatenate([x_left, x_inner, x_right], axis=2)\n        return x_out * 0.5\n\n    @classmethod\n    def _diff_y(cls, img: tf.Tensor) -> tf.Tensor:\n        \"\"\" Y Difference \"\"\"\n        y_top = img[:, 1:2, :, :] - img[:, 0:1, :, :]\n        y_inner = img[:, 2:, :, :] - img[:, :-2, :, :]\n        y_bot = img[:, -1:, :, :] - img[:, -2:-1, :, :]\n        y_out = K.concatenate([y_top, y_inner, y_bot], axis=1)\n        return y_out * 0.5\n\n    @classmethod\n    def _diff_xx(cls, img: tf.Tensor) -> tf.Tensor:\n        \"\"\" X-X Difference \"\"\"\n        x_left = img[:, :, 1:2, :] + img[:, :, 0:1, :]\n        x_inner = img[:, :, 2:, :] + img[:, :, :-2, :]\n        x_right = img[:, :, -1:, :] + img[:, :, -2:-1, :]\n        x_out = K.concatenate([x_left, x_inner, x_right], axis=2)\n        return x_out - 2.0 * img\n\n    @classmethod\n    def _diff_yy(cls, img: tf.Tensor) -> tf.Tensor:\n        \"\"\" Y-Y Difference \"\"\"\n        y_top = img[:, 1:2, :, :] + img[:, 0:1, :, :]\n        y_inner = img[:, 2:, :, :] + img[:, :-2, :, :]\n        y_bot = img[:, -1:, :, :] + img[:, -2:-1, :, :]\n        y_out = K.concatenate([y_top, y_inner, y_bot], axis=1)\n        return y_out - 2.0 * img\n\n    @classmethod\n    def _diff_xy(cls, img: tf.Tensor) -> tf.Tensor:\n        \"\"\" X-Y Difference \"\"\"\n        # xout1\n        # Left\n        top = img[:, 1:2, 1:2, :] + img[:, 0:1, 0:1, :]\n        inner = img[:, 2:, 1:2, :] + img[:, :-2, 0:1, :]\n        bottom = img[:, -1:, 1:2, :] + img[:, -2:-1, 0:1, :]\n        xy_left = K.concatenate([top, inner, bottom], axis=1)\n        # Mid\n        top = img[:, 1:2, 2:, :] + img[:, 0:1, :-2, :]\n        mid = img[:, 2:, 2:, :] + img[:, :-2, :-2, :]\n        bottom = img[:, -1:, 2:, :] + img[:, -2:-1, :-2, :]\n        xy_mid = K.concatenate([top, mid, bottom], axis=1)\n        # Right\n        top = img[:, 1:2, -1:, :] + img[:, 0:1, -2:-1, :]\n        inner = img[:, 2:, -1:, :] + img[:, :-2, -2:-1, :]\n        bottom = img[:, -1:, -1:, :] + img[:, -2:-1, -2:-1, :]\n        xy_right = K.concatenate([top, inner, bottom], axis=1)\n\n        # Xout2\n        # Left\n        top = img[:, 0:1, 1:2, :] + img[:, 1:2, 0:1, :]\n        inner = img[:, :-2, 1:2, :] + img[:, 2:, 0:1, :]\n        bottom = img[:, -2:-1, 1:2, :] + img[:, -1:, 0:1, :]\n        xy_left = K.concatenate([top, inner, bottom], axis=1)\n        # Mid\n        top = img[:, 0:1, 2:, :] + img[:, 1:2, :-2, :]\n        mid = img[:, :-2, 2:, :] + img[:, 2:, :-2, :]\n        bottom = img[:, -2:-1, 2:, :] + img[:, -1:, :-2, :]\n        xy_mid = K.concatenate([top, mid, bottom], axis=1)\n        # Right\n        top = img[:, 0:1, -1:, :] + img[:, 1:2, -2:-1, :]\n        inner = img[:, :-2, -1:, :] + img[:, 2:, -2:-1, :]\n        bottom = img[:, -2:-1, -1:, :] + img[:, -1:, -2:-1, :]\n        xy_right = K.concatenate([top, inner, bottom], axis=1)\n\n        xy_out1 = K.concatenate([xy_left, xy_mid, xy_right], axis=2)\n        xy_out2 = K.concatenate([xy_left, xy_mid, xy_right], axis=2)\n        return (xy_out1 - xy_out2) * 0.25\n\n\nclass LaplacianPyramidLoss():  # pylint:disable=too-few-public-methods\n    \"\"\" Laplacian Pyramid Loss Function\n\n    Notes\n    -----\n    Channels last implementation on square images only.\n\n    Parameters\n    ----------\n    max_levels: int, Optional\n        The max number of laplacian pyramid levels to use. Default: `5`\n    gaussian_size: int, Optional\n        The size of the gaussian kernel. Default: `5`\n    gaussian_sigma: float, optional\n        The gaussian sigma. Default: 2.0\n\n    References\n    ----------\n    https://arxiv.org/abs/1707.05776\n    https://github.com/nathanaelbosch/generative-latent-optimization/blob/master/utils.py\n    \"\"\"\n    def __init__(self,\n                 max_levels: int = 5,\n                 gaussian_size: int = 5,\n                 gaussian_sigma: float = 1.0) -> None:\n        self._max_levels = max_levels\n        self._weights = K.constant([np.power(2., -2 * idx) for idx in range(max_levels + 1)])\n        self._gaussian_kernel = self._get_gaussian_kernel(gaussian_size, gaussian_sigma)\n\n    @classmethod\n    def _get_gaussian_kernel(cls, size: int, sigma: float) -> tf.Tensor:\n        \"\"\" Obtain the base gaussian kernel for the Laplacian Pyramid.\n\n        Parameters\n        ----------\n        size: int, Optional\n            The size of the gaussian kernel\n        sigma: float\n            The gaussian sigma\n\n        Returns\n        -------\n        :class:`tf.Tensor`\n            The base single channel Gaussian kernel\n        \"\"\"\n        assert size % 2 == 1, (\"kernel size must be uneven\")\n        x_1 = np.linspace(- (size // 2), size // 2, size, dtype=\"float32\")\n        x_1 /= np.sqrt(2)*sigma\n        x_2 = x_1 ** 2\n        kernel = np.exp(- x_2[:, None] - x_2[None, :])\n        kernel /= kernel.sum()\n        kernel = np.reshape(kernel, (size, size, 1, 1))\n        return K.constant(kernel)\n\n    def _conv_gaussian(self, inputs: tf.Tensor) -> tf.Tensor:\n        \"\"\" Perform Gaussian convolution on a batch of images.\n\n        Parameters\n        ----------\n        inputs: :class:`tf.Tensor`\n            The input batch of images to perform Gaussian convolution on.\n\n        Returns\n        -------\n        :class:`tf.Tensor`\n            The convolved images\n        \"\"\"\n        channels = K.int_shape(inputs)[-1]\n        gauss = K.tile(self._gaussian_kernel, (1, 1, 1, channels))\n\n        # TF doesn't implement replication padding like pytorch. This is an inefficient way to\n        # implement it for a square guassian kernel\n        size = self._gaussian_kernel.shape[1] // 2\n        padded_inputs = inputs\n        for _ in range(size):\n            padded_inputs = tf.pad(padded_inputs,  # noqa,pylint:disable=no-value-for-parameter,unexpected-keyword-arg\n                                   ([0, 0], [1, 1], [1, 1], [0, 0]),\n                                   mode=\"SYMMETRIC\")\n\n        retval = K.conv2d(padded_inputs, gauss, strides=1, padding=\"valid\")\n        return retval\n\n    def _get_laplacian_pyramid(self, inputs: tf.Tensor) -> list[tf.Tensor]:\n        \"\"\" Obtain the Laplacian Pyramid.\n\n        Parameters\n        ----------\n        inputs: :class:`tf.Tensor`\n            The input batch of images to run through the Laplacian Pyramid\n\n        Returns\n        -------\n        list\n            The tensors produced from the Laplacian Pyramid\n        \"\"\"\n        pyramid = []\n        current = inputs\n        for _ in range(self._max_levels):\n            gauss = self._conv_gaussian(current)\n            diff = current - gauss\n            pyramid.append(diff)\n            current = K.pool2d(gauss, (2, 2), strides=(2, 2), padding=\"valid\", pool_mode=\"avg\")\n        pyramid.append(current)\n        return pyramid\n\n    def __call__(self, y_true: tf.Tensor, y_pred: tf.Tensor) -> tf.Tensor:\n        \"\"\" Calculate the Laplacian Pyramid Loss.\n\n        Parameters\n        ----------\n        y_true: :class:`tf.Tensor`\n            The ground truth value\n        y_pred: :class:`tf.Tensor`\n            The predicted value\n\n        Returns\n        -------\n        :class: `tf.Tensor`\n            The loss value\n        \"\"\"\n        pyramid_true = self._get_laplacian_pyramid(y_true)\n        pyramid_pred = self._get_laplacian_pyramid(y_pred)\n\n        losses = K.stack([K.sum(K.abs(ppred - ptrue)) / K.cast(K.prod(K.shape(ptrue)), \"float32\")\n                          for ptrue, ppred in zip(pyramid_true, pyramid_pred)])\n        loss = K.sum(losses * self._weights)\n\n        return loss\n\n\nclass LInfNorm():  # pylint:disable=too-few-public-methods\n    \"\"\" Calculate the L-inf norm as a loss function. \"\"\"\n    def __call__(self, y_true: tf.Tensor, y_pred: tf.Tensor) -> tf.Tensor:\n        \"\"\" Call the L-inf norm loss function.\n\n        Parameters\n        ----------\n        y_true: :class:`tf.Tensor`\n            The ground truth value\n        y_pred: :class:`tf.Tensor`\n            The predicted value\n\n        Returns\n        -------\n        :class:`tf.Tensor`\n            The loss value\n        \"\"\"\n        diff = K.abs(y_true - y_pred)\n        max_loss = K.max(diff, axis=(1, 2), keepdims=True)\n        loss = K.mean(max_loss, axis=-1)\n        return loss\n\n\nclass LossWrapper(tf.keras.losses.Loss):\n    \"\"\" A wrapper class for multiple keras losses to enable multiple masked weighted loss\n    functions on a single output.\n\n    Notes\n    -----\n    Whilst Keras does allow for applying multiple weighted loss functions, it does not allow\n    for an easy mechanism to add additional data (in our case masks) that are batch specific\n    but are not fed in to the model.\n\n    This wrapper receives this additional mask data for the batch stacked onto the end of the\n    color channels of the received :attr:`y_true` batch of images. These masks are then split\n    off the batch of images and applied to both the :attr:`y_true` and :attr:`y_pred` tensors\n    prior to feeding into the loss functions.\n\n    For example, for an image of shape (4, 128, 128, 3) 3 additional masks may be stacked onto\n    the end of y_true, meaning we receive an input of shape (4, 128, 128, 6). This wrapper then\n    splits off (4, 128, 128, 3:6) from the end of the tensor, leaving the original y_true of\n    shape (4, 128, 128, 3) ready for masking and feeding through the loss functions.\n    \"\"\"\n    def __init__(self) -> None:\n        logger.debug(\"Initializing: %s\", self.__class__.__name__)\n        super().__init__(name=\"LossWrapper\")\n        self._loss_functions: list[compile_utils.LossesContainer] = []\n        self._loss_weights: list[float] = []\n        self._mask_channels: list[int] = []\n        logger.debug(\"Initialized: %s\", self.__class__.__name__)\n\n    def add_loss(self,\n                 function: Callable,\n                 weight: float = 1.0,\n                 mask_channel: int = -1) -> None:\n        \"\"\" Add the given loss function with the given weight to the loss function chain.\n\n        Parameters\n        ----------\n        function: :class:`tf.keras.losses.Loss`\n            The loss function to add to the loss chain\n        weight: float, optional\n            The weighting to apply to the loss function. Default: `1.0`\n        mask_channel: int, optional\n            The channel in the `y_true` image that the mask exists in. Set to `-1` if there is no\n            mask for the given loss function. Default: `-1`\n        \"\"\"\n        logger.debug(\"Adding loss: (function: %s, weight: %s, mask_channel: %s)\",\n                     function, weight, mask_channel)\n        # Loss must be compiled inside LossContainer for keras to handle distibuted strategies\n        self._loss_functions.append(compile_utils.LossesContainer(function))\n        self._loss_weights.append(weight)\n        self._mask_channels.append(mask_channel)\n\n    def call(self, y_true: tf.Tensor, y_pred: tf.Tensor) -> tf.Tensor:\n        \"\"\" Call the sub loss functions for the loss wrapper.\n\n        Loss is returned as the weighted sum of the chosen losses.\n\n        If masks are being applied to the loss function inputs, then they should be included as\n        additional channels at the end of :attr:`y_true`, so that they can be split off and\n        applied to the actual inputs to the selected loss function(s).\n\n        Parameters\n        ----------\n        y_true: :class:`tensorflow.Tensor`\n            The ground truth batch of images, with any required masks stacked on the end\n        y_pred: :class:`tensorflow.Tensor`\n            The batch of model predictions\n\n        Returns\n        -------\n        :class:`tensorflow.Tensor`\n            The final weighted loss\n        \"\"\"\n        loss = 0.0\n        for func, weight, mask_channel in zip(self._loss_functions,\n                                              self._loss_weights,\n                                              self._mask_channels):\n            logger.debug(\"Processing loss function: (func: %s, weight: %s, mask_channel: %s)\",\n                         func, weight, mask_channel)\n            n_true, n_pred = self._apply_mask(y_true, y_pred, mask_channel)\n            loss += (func(n_true, n_pred) * weight)\n        return loss\n\n    @classmethod\n    def _apply_mask(cls,\n                    y_true: tf.Tensor,\n                    y_pred: tf.Tensor,\n                    mask_channel: int,\n                    mask_prop: float = 1.0) -> tuple[tf.Tensor, tf.Tensor]:\n        \"\"\" Apply the mask to the input y_true and y_pred. If a mask is not required then\n        return the unmasked inputs.\n\n        Parameters\n        ----------\n        y_true: tensor or variable\n            The ground truth value\n        y_pred: tensor or variable\n            The predicted value\n        mask_channel: int\n            The channel within y_true that the required mask resides in\n        mask_prop: float, optional\n            The amount of mask propagation. Default: `1.0`\n\n        Returns\n        -------\n        tf.Tensor\n            The ground truth batch of images, with the required mask applied\n        tf.Tensor\n            The predicted batch of images with the required mask applied\n        \"\"\"\n        if mask_channel == -1:\n            logger.debug(\"No mask to apply\")\n            return y_true[..., :3], y_pred[..., :3]\n\n        logger.debug(\"Applying mask from channel %s\", mask_channel)\n\n        mask = K.tile(K.expand_dims(y_true[..., mask_channel], axis=-1), (1, 1, 1, 3))\n        mask_as_k_inv_prop = 1 - mask_prop\n        mask = (mask * mask_prop) + mask_as_k_inv_prop\n\n        m_true = y_true[..., :3] * mask\n        m_pred = y_pred[..., :3] * mask\n\n        return m_true, m_pred\n", "lib/model/losses/feature_loss.py": "#!/usr/bin/env python3\n\"\"\" Custom Feature Map Loss Functions for faceswap.py \"\"\"\nfrom __future__ import annotations\nfrom dataclasses import dataclass, field\nimport logging\nimport typing as T\n\n# Ignore linting errors from Tensorflow's thoroughly broken import system\nimport tensorflow as tf\nfrom tensorflow.keras import applications as kapp  # pylint:disable=import-error\nfrom tensorflow.keras.layers import Dropout, Conv2D, Input, Layer, Resizing  # noqa,pylint:disable=no-name-in-module,import-error\nfrom tensorflow.keras.models import Model  # pylint:disable=no-name-in-module,import-error\nimport tensorflow.keras.backend as K  # pylint:disable=no-name-in-module,import-error\n\nimport numpy as np\n\nfrom lib.model.networks import AlexNet, SqueezeNet\nfrom lib.utils import GetModel\n\nif T.TYPE_CHECKING:\n    from collections.abc import Callable\n\nlogger = logging.getLogger(__name__)\n\n\n@dataclass\nclass NetInfo:\n    \"\"\" Data class for holding information about Trunk and Linear Layer nets.\n\n    Parameters\n    ----------\n    model_id: int\n        The model ID for the model stored in the deepfakes Model repo\n    model_name: str\n        The filename of the decompressed model/weights file\n    net: callable, Optional\n        The net definition to load, if any. Default:``None``\n    init_kwargs: dict, optional\n        Keyword arguments to initialize any :attr:`net`. Default: empty ``dict``\n    needs_init: bool, optional\n        True if the net needs initializing otherwise False. Default: ``True``\n    \"\"\"\n    model_id: int = 0\n    model_name: str = \"\"\n    net: Callable | None = None\n    init_kwargs: dict[str, T.Any] = field(default_factory=dict)\n    needs_init: bool = True\n    outputs: list[Layer] = field(default_factory=list)\n\n\nclass _LPIPSTrunkNet():\n    \"\"\" Trunk neural network loader for LPIPS Loss function.\n\n    Parameters\n    ----------\n    net_name: str\n        The name of the trunk network to load. One of \"alex\", \"squeeze\" or \"vgg16\"\n    eval_mode: bool\n        ``True`` for evaluation mode, ``False`` for training mode\n    load_weights: bool\n        ``True`` if pretrained trunk network weights should be loaded, otherwise ``False``\n    \"\"\"\n    def __init__(self, net_name: str, eval_mode: bool, load_weights: bool) -> None:\n        logger.debug(\"Initializing: %s (net_name '%s', eval_mode: %s, load_weights: %s)\",\n                     self.__class__.__name__, net_name, eval_mode, load_weights)\n        self._eval_mode = eval_mode\n        self._load_weights = load_weights\n        self._net_name = net_name\n        self._net = self._nets[net_name]\n        logger.debug(\"Initialized: %s \", self.__class__.__name__)\n\n    @property\n    def _nets(self) -> dict[str, NetInfo]:\n        \"\"\" :class:`NetInfo`: The Information about the requested net.\"\"\"\n        return {\n            \"alex\": NetInfo(model_id=15,\n                            model_name=\"alexnet_imagenet_no_top_v1.h5\",\n                            net=AlexNet,\n                            outputs=[f\"features.{idx}\" for idx in (0, 3, 6, 8, 10)]),\n            \"squeeze\": NetInfo(model_id=16,\n                               model_name=\"squeezenet_imagenet_no_top_v1.h5\",\n                               net=SqueezeNet,\n                               outputs=[f\"features.{idx}\" for idx in (0, 4, 7, 9, 10, 11, 12)]),\n            \"vgg16\": NetInfo(model_id=17,\n                             model_name=\"vgg16_imagenet_no_top_v1.h5\",\n                             net=kapp.vgg16.VGG16,\n                             init_kwargs={\"include_top\": False, \"weights\": None},\n                             outputs=[f\"block{i + 1}_conv{2 if i < 2 else 3}\" for i in range(5)])}\n\n    @classmethod\n    def _normalize_output(cls, inputs: tf.Tensor, epsilon: float = 1e-10) -> tf.Tensor:\n        \"\"\" Normalize the output tensors from the trunk network.\n\n        Parameters\n        ----------\n        inputs: :class:`tensorflow.Tensor`\n            An output tensor from the trunk model\n        epsilon: float, optional\n            Epsilon to apply to the normalization operation. Default: `1e-10`\n        \"\"\"\n        norm_factor = K.sqrt(K.sum(K.square(inputs), axis=-1, keepdims=True))\n        return inputs / (norm_factor + epsilon)\n\n    def _process_weights(self, model: Model) -> Model:\n        \"\"\" Save and lock weights if requested.\n\n        Parameters\n        ----------\n        model :class:`keras.models.Model`\n            The loaded trunk or linear network\n\n        Returns\n        -------\n        :class:`keras.models.Model`\n            The network with weights loaded/not loaded and layers locked/unlocked\n        \"\"\"\n        if self._load_weights:\n            weights = GetModel(self._net.model_name, self._net.model_id).model_path\n            model.load_weights(weights)\n\n        if self._eval_mode:\n            model.trainable = False\n            for layer in model.layers:\n                layer.trainable = False\n        return model\n\n    def __call__(self) -> Model:\n        \"\"\" Load the Trunk net, add normalization to feature outputs, load weights and set\n        trainable state.\n\n        Returns\n        -------\n        :class:`tensorflow.keras.models.Model`\n            The trunk net with normalized feature output layers\n        \"\"\"\n        if self._net.net is None:\n            raise ValueError(\"No net loaded\")\n\n        model = self._net.net(**self._net.init_kwargs)\n        model = model if self._net_name == \"vgg16\" else model()\n        out_layers = [self._normalize_output(model.get_layer(name).output)\n                      for name in self._net.outputs]\n        model = Model(inputs=model.input, outputs=out_layers)\n        model = self._process_weights(model)\n        return model\n\n\nclass _LPIPSLinearNet(_LPIPSTrunkNet):\n    \"\"\" The Linear Network to be applied to the difference between the true and predicted outputs\n    of the trunk network.\n\n    Parameters\n    ----------\n    net_name: str\n        The name of the trunk network in use. One of \"alex\", \"squeeze\" or \"vgg16\"\n    eval_mode: bool\n        ``True`` for evaluation mode, ``False`` for training mode\n    load_weights: bool\n        ``True`` if pretrained linear network weights should be loaded, otherwise ``False``\n    trunk_net: :class:`keras.models.Model`\n        The trunk net to place the linear layer on.\n    use_dropout: bool\n        ``True`` if a dropout layer should be used in the Linear network otherwise ``False``\n    \"\"\"\n    def __init__(self,\n                 net_name: str,\n                 eval_mode: bool,\n                 load_weights: bool,\n                 trunk_net: Model,\n                 use_dropout: bool) -> None:\n        logger.debug(\n            \"Initializing: %s (trunk_net: %s, use_dropout: %s)\", self.__class__.__name__,\n            trunk_net, use_dropout)\n        super().__init__(net_name=net_name, eval_mode=eval_mode, load_weights=load_weights)\n\n        self._trunk = trunk_net\n        self._use_dropout = use_dropout\n\n        logger.debug(\"Initialized: %s\", self.__class__.__name__)\n\n    @property\n    def _nets(self) -> dict[str, NetInfo]:\n        \"\"\" :class:`NetInfo`: The Information about the requested net.\"\"\"\n        return {\n            \"alex\": NetInfo(model_id=18,\n                            model_name=\"alexnet_lpips_v1.h5\",),\n            \"squeeze\": NetInfo(model_id=19,\n                               model_name=\"squeezenet_lpips_v1.h5\"),\n            \"vgg16\": NetInfo(model_id=20,\n                             model_name=\"vgg16_lpips_v1.h5\")}\n\n    def _linear_block(self, net_output_layer: tf.Tensor) -> tuple[tf.Tensor, tf.Tensor]:\n        \"\"\" Build a linear block for a trunk network output.\n\n        Parameters\n        ----------\n        net_output_layer: :class:`tensorflow.Tensor`\n            An output from the selected trunk network\n\n        Returns\n        -------\n        :class:`tensorflow.Tensor`\n            The input to the linear block\n        :class:`tensorflow.Tensor`\n            The output from the linear block\n        \"\"\"\n        in_shape = K.int_shape(net_output_layer)[1:]\n        input_ = Input(in_shape)\n        var_x = Dropout(rate=0.5)(input_) if self._use_dropout else input_\n        var_x = Conv2D(1, 1, strides=1, padding=\"valid\", use_bias=False)(var_x)\n        return input_, var_x\n\n    def __call__(self) -> Model:\n        \"\"\" Build the linear network for the given trunk network's outputs. Load in trained weights\n        and set the model's trainable parameters.\n\n        Returns\n        -------\n        :class:`tensorflow.keras.models.Model`\n            The compiled Linear Net model\n        \"\"\"\n        inputs = []\n        outputs = []\n\n        for input_ in self._trunk.outputs:\n            in_, out = self._linear_block(input_)\n            inputs.append(in_)\n            outputs.append(out)\n\n        model = Model(inputs=inputs, outputs=outputs)\n        model = self._process_weights(model)\n        return model\n\n\nclass LPIPSLoss():\n    \"\"\" LPIPS Loss Function.\n\n    A perceptual loss function that uses linear outputs from pretrained CNNs feature layers.\n\n    Notes\n    -----\n    Channels Last implementation. All trunks implemented from the original paper.\n\n    References\n    ----------\n    https://richzhang.github.io/PerceptualSimilarity/\n\n    Parameters\n    ----------\n    trunk_network: str\n        The name of the trunk network to use. One of \"alex\", \"squeeze\" or \"vgg16\"\n    trunk_pretrained: bool, optional\n        ``True`` Load the imagenet pretrained weights for the trunk network. ``False`` randomly\n        initialize the trunk network. Default: ``True``\n    trunk_eval_mode: bool, optional\n        ``True`` for running inference on the trunk network (standard mode), ``False`` for training\n        the trunk network. Default: ``True``\n    linear_pretrained: bool, optional\n        ``True`` loads the pretrained weights for the linear network layers. ``False`` randomly\n        initializes the layers. Default: ``True``\n    linear_eval_mode: bool, optional\n        ``True`` for running inference on the linear network (standard mode), ``False`` for\n        training the linear network. Default: ``True``\n    linear_use_dropout: bool, optional\n        ``True`` if a dropout layer should be used in the Linear network otherwise ``False``.\n        Default: ``True``\n    lpips: bool, optional\n        ``True`` to use linear network on top of the trunk network. ``False`` to just average the\n        output from the trunk network. Default ``True``\n    spatial: bool, optional\n        ``True`` output the loss in the spatial domain (i.e. as a grayscale tensor of height and\n        width of the input image). ``Bool`` reduce the spatial dimensions for loss calculation.\n        Default: ``False``\n    normalize: bool, optional\n        ``True`` if the input Tensor needs to be normalized from the 0. to 1. range to the -1. to\n        1. range. Default: ``True``\n    ret_per_layer: bool, optional\n        ``True`` to return the loss value per feature output layer otherwise ``False``.\n        Default: ``False``\n    \"\"\"\n    def __init__(self,  # pylint:disable=too-many-arguments\n                 trunk_network: str,\n                 trunk_pretrained: bool = True,\n                 trunk_eval_mode: bool = True,\n                 linear_pretrained: bool = True,\n                 linear_eval_mode: bool = True,\n                 linear_use_dropout: bool = True,\n                 lpips: bool = True,\n                 spatial: bool = False,\n                 normalize: bool = True,\n                 ret_per_layer: bool = False) -> None:\n        logger.debug(\n            \"Initializing: %s (trunk_network '%s', trunk_pretrained: %s, trunk_eval_mode: %s, \"\n            \"linear_pretrained: %s, linear_eval_mode: %s, linear_use_dropout: %s, lpips: %s, \"\n            \"spatial: %s, normalize: %s, ret_per_layer: %s)\", self.__class__.__name__,\n            trunk_network, trunk_pretrained, trunk_eval_mode, linear_pretrained, linear_eval_mode,\n            linear_use_dropout, lpips, spatial, normalize, ret_per_layer)\n\n        self._spatial = spatial\n        self._use_lpips = lpips\n        self._normalize = normalize\n        self._ret_per_layer = ret_per_layer\n        self._shift = K.constant(np.array([-.030, -.088, -.188],\n                                          dtype=\"float32\")[None, None, None, :])\n        self._scale = K.constant(np.array([.458, .448, .450],\n                                          dtype=\"float32\")[None, None, None, :])\n\n        # Loss needs to be done as fp32. We could cast at output, but better to update the model\n        switch_mixed_precision = tf.keras.mixed_precision.global_policy().name == \"mixed_float16\"\n        if switch_mixed_precision:\n            logger.debug(\"Temporarily disabling mixed precision\")\n            tf.keras.mixed_precision.set_global_policy(\"float32\")\n\n        self._trunk_net = _LPIPSTrunkNet(trunk_network, trunk_eval_mode, trunk_pretrained)()\n        self._linear_net = _LPIPSLinearNet(trunk_network,\n                                           linear_eval_mode,\n                                           linear_pretrained,\n                                           self._trunk_net,\n                                           linear_use_dropout)()\n        if switch_mixed_precision:\n            logger.debug(\"Re-enabling mixed precision\")\n            tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")\n        logger.debug(\"Initialized: %s\", self.__class__.__name__)\n\n    def _process_diffs(self, inputs: list[tf.Tensor]) -> list[tf.Tensor]:\n        \"\"\" Perform processing on the Trunk Network outputs.\n\n        If :attr:`use_ldip` is enabled, process the diff values through the linear network,\n        otherwise return the diff values summed on the channels axis.\n\n        Parameters\n        ----------\n        inputs: list\n            List of the squared difference of the true and predicted outputs from the trunk network\n\n        Returns\n        -------\n        list\n            List of either the linear network outputs (when using lpips) or summed network outputs\n        \"\"\"\n        if self._use_lpips:\n            return self._linear_net(inputs)\n        return [K.sum(x, axis=-1) for x in inputs]\n\n    def _process_output(self, inputs: tf.Tensor, output_dims: tuple) -> tf.Tensor:\n        \"\"\" Process an individual output based on whether :attr:`is_spatial` has been selected.\n\n        When spatial output is selected, all outputs are sized to the shape of the original True\n        input Tensor. When not selected, the mean across the spatial axes (h, w) are returned\n\n        Parameters\n        ----------\n        inputs: :class:`tensorflow.Tensor`\n            An individual diff output tensor from the linear network or summed output\n        output_dims: tuple\n            The (height, width) of the original true image\n\n        Returns\n        -------\n        :class:`tensorflow.Tensor`\n            Either the original tensor resized to the true image dimensions, or the mean\n            value across the height, width axes.\n        \"\"\"\n        if self._spatial:\n            return Resizing(*output_dims, interpolation=\"bilinear\")(inputs)\n        return K.mean(inputs, axis=(1, 2), keepdims=True)\n\n    def __call__(self, y_true: tf.Tensor, y_pred: tf.Tensor) -> tf.Tensor:\n        \"\"\" Perform the LPIPS Loss Function.\n\n        Parameters\n        ----------\n        y_true: :class:`tensorflow.Tensor`\n            The ground truth batch of images\n        y_pred: :class:`tensorflow.Tensor`\n            The predicted batch of images\n\n        Returns\n        -------\n        :class:`tensorflow.Tensor`\n            The final  loss value\n        \"\"\"\n        if self._normalize:\n            y_true = (y_true * 2.0) - 1.0\n            y_pred = (y_pred * 2.0) - 1.0\n\n        y_true = (y_true - self._shift) / self._scale\n        y_pred = (y_pred - self._shift) / self._scale\n\n        net_true = self._trunk_net(y_true)\n        net_pred = self._trunk_net(y_pred)\n\n        diffs = [(out_true - out_pred) ** 2\n                 for out_true, out_pred in zip(net_true, net_pred)]\n\n        dims = K.int_shape(y_true)[1:3]\n        res = [self._process_output(diff, dims) for diff in self._process_diffs(diffs)]\n\n        axis = 0 if self._spatial else None\n        val = K.sum(res, axis=axis)\n\n        retval = (val, res) if self._ret_per_layer else val\n        return retval / 10.0   # Reduce by factor of 10 'cos this loss is STRONG\n", "lib/model/losses/__init__.py": "#!/usr/bin/env python3\n\"\"\" Custom Loss Functions for Faceswap \"\"\"\n\nfrom .feature_loss import LPIPSLoss\nfrom .loss import (FocalFrequencyLoss, GeneralizedLoss, GradientLoss,\n                   LaplacianPyramidLoss, LInfNorm, LossWrapper)\nfrom .perceptual_loss import DSSIMObjective, GMSDLoss, LDRFLIPLoss, MSSIMLoss\n", "lib/model/losses/perceptual_loss.py": "#!/usr/bin/env python3\n\"\"\" TF Keras implementation of Perceptual Loss Functions for faceswap.py \"\"\"\n\nimport logging\nimport typing as T\n\nimport numpy as np\nimport tensorflow as tf\n\n# Ignore linting errors from Tensorflow's thoroughly broken import system\nfrom tensorflow.keras import backend as K  # pylint:disable=import-error\n\nfrom lib.keras_utils import ColorSpaceConvert, frobenius_norm, replicate_pad\n\nlogger = logging.getLogger(__name__)\n\n\nclass DSSIMObjective():  # pylint:disable=too-few-public-methods\n    \"\"\" DSSIM Loss Functions\n\n    Difference of Structural Similarity (DSSIM loss function).\n\n    Adapted from :func:`tensorflow.image.ssim` for a pure keras implentation.\n\n    Notes\n    -----\n    Channels last only. Assumes all input images are the same size and square\n\n    Parameters\n    ----------\n    k_1: float, optional\n        Parameter of the SSIM. Default: `0.01`\n    k_2: float, optional\n        Parameter of the SSIM. Default: `0.03`\n    filter_size: int, optional\n        size of gaussian filter Default: `11`\n    filter_sigma: float, optional\n        Width of gaussian filter Default: `1.5`\n    max_value: float, optional\n        Max value of the output. Default: `1.0`\n\n    Notes\n    ------\n    You should add a regularization term like a l2 loss in addition to this one.\n    \"\"\"\n    def __init__(self,\n                 k_1: float = 0.01,\n                 k_2: float = 0.03,\n                 filter_size: int = 11,\n                 filter_sigma: float = 1.5,\n                 max_value: float = 1.0) -> None:\n        self._filter_size = filter_size\n        self._filter_sigma = filter_sigma\n        self._kernel = self._get_kernel()\n\n        compensation = 1.0\n        self._c1 = (k_1 * max_value) ** 2\n        self._c2 = ((k_2 * max_value) ** 2) * compensation\n\n    def _get_kernel(self) -> tf.Tensor:\n        \"\"\" Obtain the base kernel for performing depthwise convolution.\n\n        Returns\n        -------\n        :class:`tf.Tensor`\n            The gaussian kernel based on selected size and sigma\n        \"\"\"\n        coords = np.arange(self._filter_size, dtype=\"float32\")\n        coords -= (self._filter_size - 1) / 2.\n\n        kernel = np.square(coords)\n        kernel *= -0.5 / np.square(self._filter_sigma)\n        kernel = np.reshape(kernel, (1, -1)) + np.reshape(kernel, (-1, 1))\n        kernel = K.constant(np.reshape(kernel, (1, -1)))\n        kernel = K.softmax(kernel)\n        kernel = K.reshape(kernel, (self._filter_size, self._filter_size, 1, 1))\n        return kernel\n\n    @classmethod\n    def _depthwise_conv2d(cls, image: tf.Tensor, kernel: tf.Tensor) -> tf.Tensor:\n        \"\"\" Perform a standardized depthwise convolution.\n\n        Parameters\n        ----------\n        image: :class:`tf.Tensor`\n            Batch of images, channels last, to perform depthwise convolution\n        kernel: :class:`tf.Tensor`\n            convolution kernel\n\n        Returns\n        -------\n        :class:`tf.Tensor`\n            The output from the convolution\n        \"\"\"\n        return K.depthwise_conv2d(image, kernel, strides=(1, 1), padding=\"valid\")\n\n    def _get_ssim(self, y_true: tf.Tensor, y_pred: tf.Tensor) -> tuple[tf.Tensor, tf.Tensor]:\n        \"\"\" Obtain the structural similarity between a batch of true and predicted images.\n\n        Parameters\n        ----------\n        y_true: :class:`tf.Tensor`\n            The input batch of ground truth images\n        y_pred: :class:`tf.Tensor`\n            The input batch of predicted images\n\n        Returns\n        -------\n        :class:`tf.Tensor`\n            The SSIM for the given images\n        :class:`tf.Tensor`\n            The Contrast for the given images\n        \"\"\"\n        channels = K.int_shape(y_true)[-1]\n        kernel = K.tile(self._kernel, (1, 1, channels, 1))\n\n        # SSIM luminance measure is (2 * mu_x * mu_y + c1) / (mu_x ** 2 + mu_y ** 2 + c1)\n        mean_true = self._depthwise_conv2d(y_true, kernel)\n        mean_pred = self._depthwise_conv2d(y_pred, kernel)\n        num_lum = mean_true * mean_pred * 2.0\n        den_lum = K.square(mean_true) + K.square(mean_pred)\n        luminance = (num_lum + self._c1) / (den_lum + self._c1)\n\n        # SSIM contrast-structure measure is (2 * cov_{xy} + c2) / (cov_{xx} + cov_{yy} + c2)\n        num_con = self._depthwise_conv2d(y_true * y_pred, kernel) * 2.0\n        den_con = self._depthwise_conv2d(K.square(y_true) + K.square(y_pred), kernel)\n\n        contrast = (num_con - num_lum + self._c2) / (den_con - den_lum + self._c2)\n\n        # Average over the height x width dimensions\n        axes = (-3, -2)\n        ssim = K.mean(luminance * contrast, axis=axes)\n        contrast = K.mean(contrast, axis=axes)\n\n        return ssim, contrast\n\n    def __call__(self, y_true: tf.Tensor, y_pred: tf.Tensor) -> tf.Tensor:\n        \"\"\" Call the DSSIM  or MS-DSSIM Loss Function.\n\n        Parameters\n        ----------\n        y_true: :class:`tf.Tensor`\n            The input batch of ground truth images\n        y_pred: :class:`tf.Tensor`\n            The input batch of predicted images\n\n        Returns\n        -------\n        :class:`tf.Tensor`\n            The DSSIM or MS-DSSIM for the given images\n        \"\"\"\n        ssim = self._get_ssim(y_true, y_pred)[0]\n        retval = (1. - ssim) / 2.0\n        return K.mean(retval)\n\n\nclass GMSDLoss():  # pylint:disable=too-few-public-methods\n    \"\"\" Gradient Magnitude Similarity Deviation Loss.\n\n    Improved image quality metric over MS-SSIM with easier calculations\n\n    References\n    ----------\n    http://www4.comp.polyu.edu.hk/~cslzhang/IQA/GMSD/GMSD.htm\n    https://arxiv.org/ftp/arxiv/papers/1308/1308.3052.pdf\n    \"\"\"\n    def __call__(self, y_true: tf.Tensor, y_pred: tf.Tensor) -> tf.Tensor:\n        \"\"\" Return the Gradient Magnitude Similarity Deviation Loss.\n\n        Parameters\n        ----------\n        y_true: :class:`tf.Tensor`\n            The ground truth value\n        y_pred: :class:`tf.Tensor`\n            The predicted value\n\n        Returns\n        -------\n        :class:`tf.Tensor`\n            The loss value\n        \"\"\"\n        true_edge = self._scharr_edges(y_true, True)\n        pred_edge = self._scharr_edges(y_pred, True)\n        ephsilon = 0.0025\n        upper = 2.0 * true_edge * pred_edge\n        lower = K.square(true_edge) + K.square(pred_edge)\n        gms = (upper + ephsilon) / (lower + ephsilon)\n        gmsd = K.std(gms, axis=(1, 2, 3), keepdims=True)\n        gmsd = K.squeeze(gmsd, axis=-1)\n        return gmsd\n\n    @classmethod\n    def _scharr_edges(cls, image: tf.Tensor, magnitude: bool) -> tf.Tensor:\n        \"\"\" Returns a tensor holding modified Scharr edge maps.\n\n        Parameters\n        ----------\n        image: :class:`tf.Tensor`\n            Image tensor with shape [batch_size, h, w, d] and type float32. The image(s) must be\n            2x2 or larger.\n        magnitude: bool\n            Boolean to determine if the edge magnitude or edge direction is returned\n\n        Returns\n        -------\n        :class:`tf.Tensor`\n            Tensor holding edge maps for each channel. Returns a tensor with shape `[batch_size, h,\n            w, d, 2]` where the last two dimensions hold `[[dy[0], dx[0]], [dy[1], dx[1]], ...,\n            [dy[d-1], dx[d-1]]]` calculated using the Scharr filter.\n        \"\"\"\n\n        # Define vertical and horizontal Scharr filters.\n        static_image_shape = image.get_shape()\n        image_shape = K.shape(image)\n\n        # 5x5 modified Scharr kernel ( reshape to (5,5,1,2) )\n        matrix = np.array([[[[0.00070, 0.00070]],\n                            [[0.00520, 0.00370]],\n                            [[0.03700, 0.00000]],\n                            [[0.00520, -0.0037]],\n                            [[0.00070, -0.0007]]],\n                           [[[0.00370, 0.00520]],\n                            [[0.11870, 0.11870]],\n                            [[0.25890, 0.00000]],\n                            [[0.11870, -0.1187]],\n                            [[0.00370, -0.0052]]],\n                           [[[0.00000, 0.03700]],\n                            [[0.00000, 0.25890]],\n                            [[0.00000, 0.00000]],\n                            [[0.00000, -0.2589]],\n                            [[0.00000, -0.0370]]],\n                           [[[-0.0037, 0.00520]],\n                            [[-0.1187, 0.11870]],\n                            [[-0.2589, 0.00000]],\n                            [[-0.1187, -0.1187]],\n                            [[-0.0037, -0.0052]]],\n                           [[[-0.0007, 0.00070]],\n                            [[-0.0052, 0.00370]],\n                            [[-0.0370, 0.00000]],\n                            [[-0.0052, -0.0037]],\n                            [[-0.0007, -0.0007]]]])\n        num_kernels = [2]\n        kernels = K.constant(matrix, dtype='float32')\n        kernels = K.tile(kernels, [1, 1, image_shape[-1], 1])\n\n        # Use depth-wise convolution to calculate edge maps per channel.\n        # Output tensor has shape [batch_size, h, w, d * num_kernels].\n        pad_sizes = [[0, 0], [2, 2], [2, 2], [0, 0]]\n        padded = tf.pad(image,  # pylint:disable=unexpected-keyword-arg,no-value-for-parameter\n                        pad_sizes,\n                        mode='REFLECT')\n        output = K.depthwise_conv2d(padded, kernels)\n\n        if not magnitude:  # direction of edges\n            # Reshape to [batch_size, h, w, d, num_kernels].\n            shape = K.concatenate([image_shape, num_kernels], axis=0)\n            output = K.reshape(output, shape=shape)\n            output.set_shape(static_image_shape.concatenate(num_kernels))\n            output = tf.atan(K.squeeze(output[:, :, :, :, 0] / output[:, :, :, :, 1], axis=None))\n        # magnitude of edges -- unified x & y edges don't work well with Neural Networks\n        return output\n\n\nclass LDRFLIPLoss():  # pylint:disable=too-few-public-methods\n    \"\"\" Computes the LDR-FLIP error map between two LDR images, assuming the images are observed\n    at a certain number of pixels per degree of visual angle.\n\n    References\n    ----------\n    https://research.nvidia.com/sites/default/files/node/3260/FLIP_Paper.pdf\n    https://github.com/NVlabs/flip\n\n    License\n    -------\n    BSD 3-Clause License\n    Copyright (c) 2020-2022, NVIDIA Corporation & AFFILIATES. All rights reserved.\n    Redistribution and use in source and binary forms, with or without modification, are permitted\n    provided that the following conditions are met:\n    Redistributions of source code must retain the above copyright notice, this list of conditions\n    and the following disclaimer.\n    Redistributions in binary form must reproduce the above copyright notice, this list of\n    conditions and the following disclaimer in the documentation and/or other materials provided\n    with the distribution.\n    Neither the name of the copyright holder nor the names of its contributors may be used to\n    endorse or promote products derived from this software without specific prior written\n    permission.\n\n    THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR\n    IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY\n    AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n    CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n    CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n    SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n    THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n    OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n    POSSIBILITY OF SUCH DAMAGE.\n\n    Parameters\n    ----------\n    computed_distance_exponent: float, Optional\n        The computed distance exponent to apply to Hunt adjusted, filtered colors.\n        (`qc` in original paper). Default: `0.7`\n    feature_exponent: float, Optional\n        The feature exponent to apply for increasing the impact of feature difference on the\n        final loss value. (`qf` in original paper). Default: `0.5`\n    lower_threshold_exponent: float, Optional\n        The `pc` exponent for the color pipeline as described in the original paper: Default: `0.4`\n    upper_threshold_exponent: float, Optional\n        The `pt` exponent  for the color pipeline as described in the original paper.\n        Default: `0.95`\n    epsilon: float\n        A small value to improve training stability. Default: `1e-15`\n    pixels_per_degree: float, Optional\n        The estimated number of pixels per degree of visual angle of the observer. This effectively\n        impacts the tolerance when calculating loss. The default corresponds to viewing images on a\n        0.7m wide 4K monitor at 0.7m from the display. Default: ``None``\n    color_order: str\n        The `\"BGR\"` or `\"RGB\"` color order of the incoming images\n    \"\"\"\n    def __init__(self,\n                 computed_distance_exponent: float = 0.7,\n                 feature_exponent: float = 0.5,\n                 lower_threshold_exponent: float = 0.4,\n                 upper_threshold_exponent: float = 0.95,\n                 epsilon: float = 1e-15,\n                 pixels_per_degree: float | None = None,\n                 color_order: T.Literal[\"bgr\", \"rgb\"] = \"bgr\") -> None:\n        logger.debug(\"Initializing: %s (computed_distance_exponent '%s', feature_exponent: %s, \"\n                     \"lower_threshold_exponent: %s, upper_threshold_exponent: %s, epsilon: %s, \"\n                     \"pixels_per_degree: %s, color_order: %s)\", self.__class__.__name__,\n                     computed_distance_exponent, feature_exponent, lower_threshold_exponent,\n                     upper_threshold_exponent, epsilon, pixels_per_degree, color_order)\n\n        self._computed_distance_exponent = computed_distance_exponent\n        self._feature_exponent = feature_exponent\n        self._pc = lower_threshold_exponent\n        self._pt = upper_threshold_exponent\n        self._epsilon = epsilon\n        self._color_order = color_order.lower()\n\n        if pixels_per_degree is None:\n            pixels_per_degree = (0.7 * 3840 / 0.7) * np.pi / 180\n        self._pixels_per_degree = pixels_per_degree\n        self._spatial_filters = _SpatialFilters(pixels_per_degree)\n        self._feature_detector = _FeatureDetection(pixels_per_degree)\n        logger.debug(\"Initialized: %s \", self.__class__.__name__)\n\n    def __call__(self, y_true: tf.Tensor, y_pred: tf.Tensor) -> tf.Tensor:\n        \"\"\" Call the LDR Flip Loss Function\n\n        Parameters\n        ----------\n        y_true: :class:`tensorflow.Tensor`\n            The ground truth batch of images\n        y_pred: :class:`tensorflow.Tensor`\n            The predicted batch of images\n\n        Returns\n        -------\n        :class::class:`tensorflow.Tensor`\n            The calculated Flip loss value\n        \"\"\"\n        if self._color_order == \"bgr\":  # Switch models training in bgr order to rgb\n            y_true = y_true[..., 2::-1]\n            y_pred = y_pred[..., 2::-1]\n\n        y_true = K.clip(y_true, 0, 1.)\n        y_pred = K.clip(y_pred, 0, 1.)\n\n        rgb2ycxcz = ColorSpaceConvert(\"srgb\", \"ycxcz\")\n        true_ycxcz = rgb2ycxcz(y_true)\n        pred_ycxcz = rgb2ycxcz(y_pred)\n\n        delta_e_color = self._color_pipeline(true_ycxcz, pred_ycxcz)\n        delta_e_features = self._process_features(true_ycxcz, pred_ycxcz)\n\n        loss = K.pow(delta_e_color, 1 - delta_e_features)\n        return loss\n\n    def _color_pipeline(self, y_true: tf.Tensor, y_pred: tf.Tensor) -> tf.Tensor:\n        \"\"\" Perform the color processing part of the FLIP loss function\n\n        Parameters\n        ----------\n        y_true: :class:`tensorflow.Tensor`\n            The ground truth batch of images in YCxCz color space\n        y_pred: :class:`tensorflow.Tensor`\n            The predicted batch of images in YCxCz color space\n\n        Returns\n        -------\n        :class:`tensorflow.Tensor`\n            The exponentiated, maximum HyAB difference between two colors in Hunt-adjusted\n            L*A*B* space\n        \"\"\"\n        filtered_true = self._spatial_filters(y_true)\n        filtered_pred = self._spatial_filters(y_pred)\n\n        rgb2lab = ColorSpaceConvert(from_space=\"rgb\", to_space=\"lab\")\n        preprocessed_true = self._hunt_adjustment(rgb2lab(filtered_true))\n        preprocessed_pred = self._hunt_adjustment(rgb2lab(filtered_pred))\n        hunt_adjusted_green = self._hunt_adjustment(rgb2lab(K.constant([[[[0.0, 1.0, 0.0]]]],\n                                                                       dtype=\"float32\")))\n        hunt_adjusted_blue = self._hunt_adjustment(rgb2lab(K.constant([[[[0.0, 0.0, 1.0]]]],\n                                                                      dtype=\"float32\")))\n\n        delta = self._hyab(preprocessed_true, preprocessed_pred)\n        power_delta = K.pow(delta, self._computed_distance_exponent)\n        cmax = K.pow(self._hyab(hunt_adjusted_green, hunt_adjusted_blue),\n                     self._computed_distance_exponent)\n        return self._redistribute_errors(power_delta, cmax)\n\n    def _process_features(self, y_true: tf.Tensor, y_pred: tf.Tensor) -> tf.Tensor:\n        \"\"\" Perform the color processing part of the FLIP loss function\n\n        Parameters\n        ----------\n        y_true: :class:`tensorflow.Tensor`\n            The ground truth batch of images in YCxCz color space\n        y_pred: :class:`tensorflow.Tensor`\n            The predicted batch of images in YCxCz color space\n\n        Returns\n        -------\n        :class:`tensorflow.Tensor`\n            The exponentiated features delta\n        \"\"\"\n        col_y_true = (y_true[..., 0:1] + 16) / 116.\n        col_y_pred = (y_pred[..., 0:1] + 16) / 116.\n\n        edges_true = self._feature_detector(col_y_true, \"edge\")\n        points_true = self._feature_detector(col_y_true, \"point\")\n        edges_pred = self._feature_detector(col_y_pred, \"edge\")\n        points_pred = self._feature_detector(col_y_pred, \"point\")\n\n        delta = K.maximum(K.abs(frobenius_norm(edges_true) - frobenius_norm(edges_pred)),\n                          K.abs(frobenius_norm(points_pred) - frobenius_norm(points_true)))\n\n        delta = K.clip(delta, min_value=self._epsilon, max_value=None)\n        return K.pow(((1 / np.sqrt(2)) * delta), self._feature_exponent)\n\n    @classmethod\n    def _hunt_adjustment(cls, image: tf.Tensor) -> tf.Tensor:\n        \"\"\" Apply Hunt-adjustment to an image in L*a*b* color space\n\n        Parameters\n        ----------\n        image: :class:`tensorflow.Tensor`\n            The batch of images in L*a*b* to adjust\n\n        Returns\n        -------\n        :class:`tensorflow.Tensor`\n            The hunt adjusted batch of images in L*a*b color space\n        \"\"\"\n        ch_l = image[..., 0:1]\n        adjusted = K.concatenate([ch_l, image[..., 1:] * (ch_l * 0.01)], axis=-1)\n        return adjusted\n\n    def _hyab(self, y_true, y_pred):\n        \"\"\" Compute the HyAB distance between true and predicted images.\n\n        Parameters\n        ----------\n        y_true: :class:`tensorflow.Tensor`\n            The ground truth batch of images in standard or Hunt-adjusted L*A*B* color space\n        y_pred: :class:`tensorflow.Tensor`\n            The predicted batch of images in in standard or Hunt-adjusted L*A*B* color space\n\n        Returns\n        -------\n        :class:`tensorflow.Tensor`\n            image tensor containing the per-pixel HyAB distances between true and predicted images\n        \"\"\"\n        delta = y_true - y_pred\n        root = K.sqrt(K.clip(K.pow(delta[..., 0:1], 2), min_value=self._epsilon, max_value=None))\n        delta_norm = frobenius_norm(delta[..., 1:3])\n        return root + delta_norm\n\n    def _redistribute_errors(self, power_delta_e_hyab, cmax):\n        \"\"\" Redistribute exponentiated HyAB errors to the [0,1] range\n\n        Parameters\n        ----------\n        power_delta_e_hyab: :class:`tensorflow.Tensor`\n            The exponentiated HyAb distance\n        cmax: :class:`tensorflow.Tensor`\n            The exponentiated, maximum HyAB difference between two colors in Hunt-adjusted\n            L*A*B* space\n\n        Returns\n        -------\n        :class:`tensorflow.Tensor`\n            The redistributed per-pixel HyAB distances (in range [0,1])\n        \"\"\"\n        pccmax = self._pc * cmax\n        delta_e_c = K.switch(\n            power_delta_e_hyab < pccmax,\n            (self._pt / pccmax) * power_delta_e_hyab,\n            self._pt + ((power_delta_e_hyab - pccmax) / (cmax - pccmax)) * (1.0 - self._pt))\n        return delta_e_c\n\n\nclass _SpatialFilters():  # pylint:disable=too-few-public-methods\n    \"\"\" Filters an image with channel specific spatial contrast sensitivity functions and clips\n    result to the unit cube in linear RGB.\n\n    For use with LDRFlipLoss.\n\n    Parameters\n    ----------\n    pixels_per_degree: float\n        The estimated number of pixels per degree of visual angle of the observer. This effectively\n        impacts the tolerance when calculating loss.\n    \"\"\"\n    def __init__(self, pixels_per_degree: float) -> None:\n        self._pixels_per_degree = pixels_per_degree\n        self._spatial_filters, self._radius = self._generate_spatial_filters()\n        self._ycxcz2rgb = ColorSpaceConvert(from_space=\"ycxcz\", to_space=\"rgb\")\n\n    def _generate_spatial_filters(self) -> tuple[tf.Tensor, int]:\n        \"\"\" Generates spatial contrast sensitivity filters with width depending on the number of\n        pixels per degree of visual angle of the observer for channels \"A\", \"RG\" and \"BY\"\n\n        Returns\n        -------\n        dict\n            the channels (\"A\" (Achromatic CSF), \"RG\" (Red-Green CSF) or \"BY\" (Blue-Yellow CSF)) as\n            key with the Filter kernel corresponding to the spatial contrast sensitivity function\n            of channel and kernel's radius\n        \"\"\"\n        mapping = {\"A\": {\"a1\": 1, \"b1\": 0.0047, \"a2\": 0, \"b2\": 1e-5},\n                   \"RG\": {\"a1\": 1, \"b1\": 0.0053, \"a2\": 0, \"b2\": 1e-5},\n                   \"BY\": {\"a1\": 34.1, \"b1\": 0.04, \"a2\": 13.5, \"b2\": 0.025}}\n\n        domain, radius = self._get_evaluation_domain(mapping[\"A\"][\"b1\"],\n                                                     mapping[\"A\"][\"b2\"],\n                                                     mapping[\"RG\"][\"b1\"],\n                                                     mapping[\"RG\"][\"b2\"],\n                                                     mapping[\"BY\"][\"b1\"],\n                                                     mapping[\"BY\"][\"b2\"])\n\n        weights = np.array([self._generate_weights(mapping[channel], domain)\n                            for channel in (\"A\", \"RG\", \"BY\")])\n        weights = K.constant(np.moveaxis(weights, 0, -1), dtype=\"float32\")\n\n        return weights, radius\n\n    def _get_evaluation_domain(self,\n                               b1_a: float,\n                               b2_a: float,\n                               b1_rg: float,\n                               b2_rg: float,\n                               b1_by: float,\n                               b2_by: float) -> tuple[np.ndarray, int]:\n        \"\"\" TODO docstring \"\"\"\n        max_scale_parameter = max([b1_a, b2_a, b1_rg, b2_rg, b1_by, b2_by])\n        delta_x = 1.0 / self._pixels_per_degree\n        radius = int(np.ceil(3 * np.sqrt(max_scale_parameter / (2 * np.pi**2))\n                             * self._pixels_per_degree))\n        ax_x, ax_y = np.meshgrid(range(-radius, radius + 1), range(-radius, radius + 1))\n        domain = (ax_x * delta_x) ** 2 + (ax_y * delta_x) ** 2\n        return domain, radius\n\n    @classmethod\n    def _generate_weights(cls, channel: dict[str, float], domain: np.ndarray) -> tf.Tensor:\n        \"\"\" TODO docstring \"\"\"\n        a_1, b_1, a_2, b_2 = channel[\"a1\"], channel[\"b1\"], channel[\"a2\"], channel[\"b2\"]\n        grad = (a_1 * np.sqrt(np.pi / b_1) * np.exp(-np.pi ** 2 * domain / b_1) +\n                a_2 * np.sqrt(np.pi / b_2) * np.exp(-np.pi ** 2 * domain / b_2))\n        grad = grad / np.sum(grad)\n        grad = np.reshape(grad, (*grad.shape, 1))\n        return grad\n\n    def __call__(self, image: tf.Tensor) -> tf.Tensor:\n        \"\"\" Call the spacial filtering.\n\n        Parameters\n        ----------\n        image: Tensor\n            Image tensor to filter in YCxCz color space\n\n        Returns\n        -------\n        Tensor\n            The input image transformed to linear RGB after filtering with spatial contrast\n            sensitivity functions\n        \"\"\"\n        padded_image = replicate_pad(image, self._radius)\n        image_tilde_opponent = K.conv2d(padded_image,\n                                        self._spatial_filters,\n                                        strides=1,\n                                        padding=\"valid\")\n        rgb = K.clip(self._ycxcz2rgb(image_tilde_opponent), 0., 1.)\n        return rgb\n\n\nclass _FeatureDetection():  # pylint:disable=too-few-public-methods\n    \"\"\" Detect features (i.e. edges and points) in an achromatic YCxCz image.\n\n    For use with LDRFlipLoss.\n\n    Parameters\n    ----------\n    pixels_per_degree: float\n        The number of pixels per degree of visual angle of the observer\n    \"\"\"\n    def __init__(self, pixels_per_degree: float) -> None:\n        width = 0.082\n        self._std = 0.5 * width * pixels_per_degree\n        self._radius = int(np.ceil(3 * self._std))\n        self._grid = np.meshgrid(range(-self._radius, self._radius + 1),\n                                 range(-self._radius, self._radius + 1))\n        self._gradient = np.exp(-(self._grid[0] ** 2 + self._grid[1] ** 2)\n                                / (2 * (self._std ** 2)))\n\n    def __call__(self, image: tf.Tensor, feature_type: str) -> tf.Tensor:\n        \"\"\" Run the feature detection\n\n        Parameters\n        ----------\n        image: Tensor\n            Batch of images in YCxCz color space with normalized Y values\n        feature_type: str\n            Type of features to detect (`\"edge\"` or `\"point\"`)\n\n        Returns\n        -------\n        Tensor\n            Detected features in the 0-1 range\n        \"\"\"\n        feature_type = feature_type.lower()\n\n        if feature_type == 'edge':\n            grad_x = np.multiply(-self._grid[0], self._gradient)\n        else:\n            grad_x = np.multiply(self._grid[0] ** 2 / (self._std ** 2) - 1, self._gradient)\n\n        negative_weights_sum = -np.sum(grad_x[grad_x < 0])\n        positive_weights_sum = np.sum(grad_x[grad_x > 0])\n\n        grad_x = K.constant(grad_x)\n        grad_x = K.switch(grad_x < 0, grad_x / negative_weights_sum, grad_x / positive_weights_sum)\n        kernel = K.expand_dims(K.expand_dims(grad_x, axis=-1), axis=-1)\n\n        features_x = K.conv2d(replicate_pad(image, self._radius),\n                              kernel,\n                              strides=1,\n                              padding=\"valid\")\n        kernel = K.permute_dimensions(kernel, (1, 0, 2, 3))\n        features_y = K.conv2d(replicate_pad(image, self._radius),\n                              kernel,\n                              strides=1,\n                              padding=\"valid\")\n        features = K.concatenate([features_x, features_y], axis=-1)\n        return features\n\n\nclass MSSIMLoss():  # pylint:disable=too-few-public-methods\n    \"\"\" Multiscale Structural Similarity Loss Function\n\n    Parameters\n    ----------\n    k_1: float, optional\n        Parameter of the SSIM. Default: `0.01`\n    k_2: float, optional\n        Parameter of the SSIM. Default: `0.03`\n    filter_size: int, optional\n        size of gaussian filter Default: `11`\n    filter_sigma: float, optional\n        Width of gaussian filter Default: `1.5`\n    max_value: float, optional\n        Max value of the output. Default: `1.0`\n    power_factors: tuple, optional\n        Iterable of weights for each of the scales. The number of scales used is the length of the\n        list. Index 0 is the unscaled resolution's weight and each increasing scale corresponds to\n        the image being downsampled by 2. Defaults to the values obtained in the original paper.\n        Default: (0.0448, 0.2856, 0.3001, 0.2363, 0.1333)\n\n    Notes\n    ------\n    You should add a regularization term like a l2 loss in addition to this one.\n    \"\"\"\n    def __init__(self,\n                 k_1: float = 0.01,\n                 k_2: float = 0.03,\n                 filter_size: int = 11,\n                 filter_sigma: float = 1.5,\n                 max_value: float = 1.0,\n                 power_factors: tuple[float, ...] = (0.0448, 0.2856, 0.3001, 0.2363, 0.1333)\n                 ) -> None:\n        self.filter_size = filter_size\n        self.filter_sigma = filter_sigma\n        self.k_1 = k_1\n        self.k_2 = k_2\n        self.max_value = max_value\n        self.power_factors = power_factors\n\n    def __call__(self, y_true: tf.Tensor, y_pred: tf.Tensor) -> tf.Tensor:\n        \"\"\" Call the MS-SSIM Loss Function.\n\n        Parameters\n        ----------\n        y_true: :class:`tf.Tensor`\n            The ground truth value\n        y_pred: :class:`tf.Tensor`\n            The predicted value\n\n        Returns\n        -------\n        :class:`tf.Tensor`\n            The MS-SSIM Loss value\n        \"\"\"\n        im_size = K.int_shape(y_true)[1]\n        # filter size cannot be larger than the smallest scale\n        smallest_scale = self._get_smallest_size(im_size, len(self.power_factors) - 1)\n        filter_size = min(self.filter_size, smallest_scale)\n\n        ms_ssim = tf.image.ssim_multiscale(y_true,\n                                           y_pred,\n                                           self.max_value,\n                                           power_factors=self.power_factors,\n                                           filter_size=filter_size,\n                                           filter_sigma=self.filter_sigma,\n                                           k1=self.k_1,\n                                           k2=self.k_2)\n        ms_ssim_loss = 1. - ms_ssim\n        return K.mean(ms_ssim_loss)\n\n    def _get_smallest_size(self, size: int, idx: int) -> int:\n        \"\"\" Recursive function to obtain the smallest size that the image will be scaled to.\n\n        Parameters\n        ----------\n        size: int\n            The current scaled size to iterate through\n        idx: int\n            The current iteration to be performed. When iteration hits zero the value will\n            be returned\n\n        Returns\n        -------\n        int\n            The smallest size the image will be scaled to based on the original image size and\n            the amount of scaling factors that will occur\n        \"\"\"\n        logger.debug(\"scale id: %s, size: %s\", idx, size)\n        if idx > 0:\n            size = self._get_smallest_size(size // 2, idx - 1)\n        return size\n", "lib/model/networks/simple_nets.py": "#!/usr/bin/env python3\n\"\"\" Ports of existing NN Architecture for use in faceswap.py \"\"\"\nfrom __future__ import annotations\nimport logging\nimport typing as T\n\nimport tensorflow as tf\n\n# Fix intellisense/linting for tf.keras' thoroughly broken import system\nkeras = tf.keras\nlayers = keras.layers\nModel = keras.models.Model\n\nif T.TYPE_CHECKING:\n    from tensorflow import Tensor\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass _net():  # pylint:disable=too-few-public-methods\n    \"\"\" Base class for existing NeuralNet architecture\n\n    Notes\n    -----\n    All architectures assume channels_last format\n\n    Parameters\n    ----------\n    input_shape, Tuple, optional\n        The input shape for the model. Default: ``None``\n    \"\"\"\n    def __init__(self,\n                 input_shape: tuple[int, int, int] | None = None) -> None:\n        logger.debug(\"Initializing: %s (input_shape: %s)\", self.__class__.__name__, input_shape)\n        self._input_shape = (None, None, 3) if input_shape is None else input_shape\n        assert len(self._input_shape) == 3 and self._input_shape[-1] == 3, (\n            \"Input shape must be in the format (height, width, channels) and the number of \"\n            f\"channels must equal 3. Received: {self._input_shape}\")\n        logger.debug(\"Initialized: %s\", self.__class__.__name__)\n\n\nclass AlexNet(_net):\n    \"\"\" AlexNet ported from torchvision version.\n\n    Notes\n    -----\n    This port only contains the features portion of the model.\n\n    References\n    ----------\n    https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf\n\n    Parameters\n    ----------\n    input_shape, Tuple, optional\n        The input shape for the model. Default: ``None``\n    \"\"\"\n    def __init__(self, input_shape: tuple[int, int, int] | None = None) -> None:\n        super().__init__(input_shape)\n        self._feature_indices = [0, 3, 6, 8, 10]  # For naming equivalent to PyTorch\n        self._filters = [64, 192, 384, 256, 256]  # Filters at each block\n\n    @classmethod\n    def _conv_block(cls,\n                    inputs: Tensor,\n                    padding: int,\n                    filters: int,\n                    kernel_size: int,\n                    strides: int,\n                    block_idx: int,\n                    max_pool: bool) -> Tensor:\n        \"\"\"\n        The Convolutional block for AlexNet\n\n        Parameters\n        ----------\n        inputs: :class:`tf.Tensor`\n            The input tensor to the block\n        padding: int\n            The amount of zero paddin to apply prior to convolution\n        filters: int\n            The number of filters to apply during convolution\n        kernel_size: int\n            The kernel size of the convolution\n        strides: int\n            The number of strides for the convolution\n        block_idx: int\n            The index of the current block (for standardized naming convention)\n        max_pool: bool\n            ``True`` to apply a max pooling layer at the beginning of the block otherwise ``False``\n\n        Returns\n        -------\n        :class:`tf.Tensor`\n            The output of the Convolutional block\n        \"\"\"\n        name = f\"features.{block_idx}\"\n        var_x = inputs\n        if max_pool:\n            var_x = layers.MaxPool2D(pool_size=3, strides=2, name=f\"{name}.pool\")(var_x)\n        var_x = layers.ZeroPadding2D(padding=padding, name=f\"{name}.pad\")(var_x)\n        var_x = layers.Conv2D(filters,\n                              kernel_size=kernel_size,\n                              strides=strides,\n                              padding=\"valid\",\n                              activation=\"relu\",\n                              name=name)(var_x)\n        return var_x\n\n    def __call__(self) -> tf.keras.models.Model:\n        \"\"\" Create the AlexNet Model\n\n        Returns\n        -------\n        :class:`keras.models.Model`\n            The compiled AlexNet model\n        \"\"\"\n        inputs = layers.Input(self._input_shape)\n        var_x = inputs\n        kernel_size = 11\n        strides = 4\n\n        for idx, (filters, block_idx) in enumerate(zip(self._filters, self._feature_indices)):\n            padding = 2 if idx < 2 else 1\n            do_max_pool = 0 < idx < 3\n            var_x = self._conv_block(var_x,\n                                     padding,\n                                     filters,\n                                     kernel_size,\n                                     strides,\n                                     block_idx,\n                                     do_max_pool)\n            kernel_size = max(3, kernel_size // 2)\n            strides = 1\n        return Model(inputs=inputs, outputs=[var_x])\n\n\nclass SqueezeNet(_net):\n    \"\"\" SqueezeNet ported from torchvision version.\n\n    Notes\n    -----\n    This port only contains the features portion of the model.\n\n    References\n    ----------\n    https://arxiv.org/abs/1602.07360\n\n    Parameters\n    ----------\n    input_shape, Tuple, optional\n        The input shape for the model. Default: ``None``\n    \"\"\"\n\n    @classmethod\n    def _fire(cls,\n              inputs: Tensor,\n              squeeze_planes: int,\n              expand_planes: int,\n              block_idx: int) -> Tensor:\n        \"\"\" The fire block for SqueezeNet.\n\n        Parameters\n        ----------\n        inputs: :class:`tf.Tensor`\n            The input to the fire block\n        squeeze_planes: int\n            The number of filters for the squeeze convolution\n        expand_planes: int\n            The number of filters for the expand convolutions\n        block_idx: int\n            The index of the current block (for standardized naming convention)\n\n        Returns\n        -------\n        :class:`tf.Tensor`\n            The output of the SqueezeNet fire block\n        \"\"\"\n        name = f\"features.{block_idx}\"\n        squeezed = layers.Conv2D(squeeze_planes, 1,\n                                 activation=\"relu\", name=f\"{name}.squeeze\")(inputs)\n        expand1 = layers.Conv2D(expand_planes, 1,\n                                activation=\"relu\", name=f\"{name}.expand1x1\")(squeezed)\n        expand3 = layers.Conv2D(expand_planes,\n                                3,\n                                activation=\"relu\",\n                                padding=\"same\",\n                                name=f\"{name}.expand3x3\")(squeezed)\n        return layers.Concatenate(axis=-1, name=name)([expand1, expand3])\n\n    def __call__(self) -> tf.keras.models.Model:\n        \"\"\" Create the SqueezeNet Model\n\n        Returns\n        -------\n        :class:`keras.models.Model`\n            The compiled SqueezeNet model\n        \"\"\"\n        inputs = layers.Input(self._input_shape)\n        var_x = layers.Conv2D(64, 3, strides=2, activation=\"relu\", name=\"features.0\")(inputs)\n\n        block_idx = 2\n        squeeze = 16\n        expand = 64\n        for idx in range(4):\n            if idx < 3:\n                var_x = layers.MaxPool2D(pool_size=3, strides=2)(var_x)\n                block_idx += 1\n            var_x = self._fire(var_x, squeeze, expand, block_idx)\n            block_idx += 1\n            var_x = self._fire(var_x, squeeze, expand, block_idx)\n            block_idx += 1\n            squeeze += 16\n            expand += 64\n        return Model(inputs=inputs, outputs=[var_x])\n", "lib/model/networks/__init__.py": "#!/usr/bin/env python3\n\"\"\" Pre-defined networks for use in faceswap \"\"\"\nfrom .simple_nets import AlexNet, SqueezeNet\nfrom .clip import ViT, ViTConfig, TypeModels as TypeModelsViT\n", "lib/model/networks/clip.py": "#!/usr/bin/env python3\n\"\"\" CLIP: https://github.com/openai/CLIP. This implementation only ports the visual transformer\npart of the model.\n\"\"\"\n# TODO Fix Resnet. It is correct until final MHA\nfrom __future__ import annotations\nimport inspect\nimport logging\nimport typing as T\nimport sys\n\nfrom dataclasses import dataclass\n\nimport tensorflow as tf\n\nfrom lib.model.layers import QuickGELU\nfrom lib.utils import GetModel\n\nkeras = tf.keras\nlayers = tf.keras.layers\nK = tf.keras.backend\n\nlogger = logging.getLogger(__name__)\n\nTypeModels = T.Literal[\"RN50\", \"RN101\", \"RN50x4\", \"RN50x16\", \"RN50x64\", \"ViT-B-16\",\n                       \"ViT-B-32\", \"ViT-L-14\", \"ViT-L-14-336px\", \"FaRL-B-16-16\", \"FaRL-B-16-64\"]\n\n\n@dataclass\nclass ViTConfig:\n    \"\"\" Configuration settings for ViT\n\n    Parameters\n    ----------\n    embed_dim: int\n        Dimensionality of the final shared embedding space\n    resolution: int\n        Spatial resolution of the input images\n    layer_conf: tuple[int, int, int, int] | int\n        Number of layers in the visual encoder, or a tuple of layer configurations for a custom\n        ResNet visual encoder\n    width: int\n        Width of the visual encoder layers\n    patch: int\n        Size of the patches to be extracted from the images. Only used for Visual encoder.\n    git_id: int, optional\n        The id of the model weights file stored in deepfakes_models repo if they exist. Default: 0\n    \"\"\"\n    embed_dim: int\n    resolution: int\n    layer_conf: int | tuple[int, int, int, int]\n    width: int\n    patch: int\n    git_id: int = 0\n\n    def __post_init__(self):\n        \"\"\" Validate that patch_size is given correctly \"\"\"\n        assert (isinstance(self.layer_conf, (tuple, list)) and self.patch == 0) or (\n            isinstance(self.layer_conf, int) and self.patch > 0)\n\n\nModelConfig: dict[TypeModels, ViTConfig] = {  # Each model has a different set of parameters\n    \"RN50\": ViTConfig(\n        embed_dim=1024, resolution=224, layer_conf=(3, 4, 6, 3), width=64, patch=0, git_id=21),\n    \"RN101\": ViTConfig(\n        embed_dim=512, resolution=224, layer_conf=(3, 4, 23, 3), width=64, patch=0, git_id=22),\n    \"RN50x4\": ViTConfig(\n        embed_dim=640, resolution=288, layer_conf=(4, 6, 10, 6), width=80, patch=0, git_id=23),\n    \"RN50x16\": ViTConfig(\n        embed_dim=768, resolution=384, layer_conf=(6, 8, 18, 8), width=96, patch=0, git_id=24),\n    \"RN50x64\": ViTConfig(\n        embed_dim=1024, resolution=448, layer_conf=(3, 15, 36, 10), width=128, patch=0, git_id=25),\n    \"ViT-B-16\": ViTConfig(\n        embed_dim=512, resolution=224, layer_conf=12, width=768, patch=16, git_id=26),\n    \"ViT-B-32\": ViTConfig(\n        embed_dim=512, resolution=224, layer_conf=12, width=768, patch=32, git_id=27),\n    \"ViT-L-14\": ViTConfig(\n        embed_dim=768, resolution=224, layer_conf=24, width=1024, patch=14, git_id=28),\n    \"ViT-L-14-336px\": ViTConfig(\n        embed_dim=768, resolution=336, layer_conf=24, width=1024, patch=14, git_id=29),\n    \"FaRL-B-16-16\": ViTConfig(\n        embed_dim=512, resolution=224, layer_conf=12, width=768, patch=16, git_id=30),\n    \"FaRL-B-16-64\": ViTConfig(\n        embed_dim=512, resolution=224, layer_conf=12, width=768, patch=16, git_id=31)}\n\n\n# ################## #\n# VISUAL TRANSFORMER #\n# ################## #\n\nclass Transformer():  # pylint:disable=too-few-public-methods\n    \"\"\" A class representing a Transformer model with attention mechanism and residual connections.\n\n    Parameters\n    ----------\n    width: int\n        The dimension of the input and output vectors.\n    num_layers: int\n        The number of layers in the Transformer.\n    heads: int\n        The number of attention heads.\n    attn_mask: tf.Tensor, optional\n        The attention mask, by default None.\n    name: str, optional\n        The name of the Transformer model, by default \"transformer\".\n\n    Methods\n    -------\n    __call__() -> Model:\n        Calls the Transformer layers.\n    \"\"\"\n    _layer_names: dict[str, int] = {}\n    \"\"\" dict[str, int] for tracking unique layer names\"\"\"\n\n    def __init__(self,\n                 width: int,\n                 num_layers: int,\n                 heads: int,\n                 attn_mask: tf.Tensor = None,\n                 name: str = \"transformer\") -> None:\n        logger.debug(\"Initializing: %s (width: %s, num_layers: %s, heads: %s, attn_mask: %s, \"\n                     \"name: %s)\",\n                     self.__class__.__name__, width, num_layers, heads, attn_mask, name)\n        self._width = width\n        self._num_layers = num_layers\n        self._heads = heads\n        self._attn_mask = attn_mask\n        self._name = name\n        logger.debug(\"Initialized: %s \", self.__class__.__name__)\n\n    @classmethod\n    def _get_name(cls, name: str) -> str:\n        \"\"\" Return unique layer name for requested block.\n\n        As blocks can be used multiple times, auto appends an integer to the end of the requested\n        name to keep all block names unique\n\n        Parameters\n        ----------\n        name: str\n            The requested name for the layer\n\n        Returns\n        -------\n        str\n            The unique name for this layer\n        \"\"\"\n        cls._layer_names[name] = cls._layer_names.setdefault(name, -1) + 1\n        name = f\"{name}.{cls._layer_names[name]}\"\n        logger.debug(\"Generating block name: %s\", name)\n        return name\n\n    @classmethod\n    def _mlp(cls, inputs: tf.Tensor, key_dim: int, name: str) -> tf.Tensor:\n        \"\"\"\" Multilayer Perecptron for Block Ateention\n\n        Parameters\n        ----------\n        inputs: :class:`tensorflow.Tensor`\n            The input to the MLP\n        key_dim: int\n            key dimension per head for MultiHeadAttention\n        name: str\n            The name to prefix on the layers\n\n        Returns\n        -------\n        :class:`tensorflow.Tensor`\n            The output from the MLP\n        \"\"\"\n        name = f\"{name}.mlp\"\n        var_x = layers.Dense(key_dim * 4, name=f\"{name}.c_fc\")(inputs)\n        var_x = QuickGELU(name=f\"{name}.gelu\")(var_x)\n        var_x = layers.Dense(key_dim, name=f\"{name}.c_proj\")(var_x)\n        return var_x\n\n    def residual_attention_block(self,\n                                 inputs: tf.Tensor,\n                                 key_dim: int,\n                                 num_heads: int,\n                                 attn_mask: tf.Tensor,\n                                 name: str = \"ResidualAttentionBlock\") -> tf.Tensor:\n        \"\"\" Call the residual attention block\n\n        Parameters\n        ----------\n        inputs: :class:`tf.Tensor`\n            The input Tensor\n        key_dim: int\n            key dimension per head for MultiHeadAttention\n        num_heads: int\n            Number of heads for MultiHeadAttention\n        attn_mask: :class:`tensorflow.Tensor`, optional\n            Default: ``None``\n        name: str, optional\n            The name for the layer. Default: \"ResidualAttentionBlock\"\n\n        Returns\n        -------\n        :class:`tf.Tensor`\n            The return Tensor\n        \"\"\"\n        name = self._get_name(name)\n\n        var_x = layers.LayerNormalization(epsilon=1e-05, name=f\"{name}.ln_1\")(inputs)\n        var_x = layers.MultiHeadAttention(\n            num_heads=num_heads,\n            key_dim=key_dim // num_heads,\n            name=f\"{name}.attn\")(var_x, var_x, var_x, attention_mask=attn_mask)\n        var_x = layers.Add()([inputs, var_x])\n        var_y = var_x\n        var_x = layers.LayerNormalization(epsilon=1e-05, name=f\"{name}.ln_2\")(var_x)\n        var_x = layers.Add()([var_y, self._mlp(var_x, key_dim, name)])\n        return var_x\n\n    def __call__(self, inputs: tf.Tensor) -> tf.Tensor:\n        \"\"\" Call the Transformer layers\n\n        Parameters\n        ----------\n        inputs: :class:`tf.Tensor`\n            The input Tensor\n\n        Returns\n        -------\n        :class:`tf.Tensor`\n            The return Tensor\n        \"\"\"\n        logger.debug(\"Calling %s with input: %s\", self.__class__.__name__, inputs.shape)\n        var_x = inputs\n        for _ in range(self._num_layers):\n            var_x = self.residual_attention_block(var_x,\n                                                  self._width,\n                                                  self._heads,\n                                                  self._attn_mask,\n                                                  name=f\"{self._name}.resblocks\")\n        return var_x\n\n\nclass EmbeddingLayer(tf.keras.layers.Layer):\n    \"\"\" Parent class for trainable embedding variables\n\n    Parameters\n    ----------\n    input_shape: tuple[int, ...]\n        The shape of the variable\n    scale: int\n        Amount to scale the random initialization by\n    name: str\n        The name of the layer\n    dtype: str, optional\n        The datatype for the layer. Mixed precision can mess up the embeddings. Default: \"float32\"\n    \"\"\"\n    def __init__(self,\n                 input_shape: tuple[int, ...],\n                 scale: int,\n                 name: str,\n                 *args,\n                 dtype=\"float32\",\n                 **kwargs) -> None:\n        super().__init__(name=name, dtype=dtype, *args, **kwargs)\n        self._input_shape = input_shape\n        self._scale = scale\n        self._var: tf.Variable\n\n    def build(self, input_shape: tuple[int, ...]) -> None:\n        \"\"\" Add the weights\n\n        Parameters\n        ----------\n        input_shape: tuple[int, ...\n            The input shape of the incoming tensor\n        \"\"\"\n        self._var = tf.Variable(self._scale * tf.random.normal(self._input_shape,\n                                                               dtype=self.dtype),\n                                trainable=True,\n                                dtype=self.dtype)\n        super().build(input_shape)\n\n    def get_config(self) -> dict[str, T.Any]:\n        \"\"\" Get the config dictionary for the layer\n\n        Returns\n        -------\n        dict[str, Any]\n            The config dictionary for the layer\n        \"\"\"\n        retval = super().get_config()\n        retval[\"input_shape\"] = self._input_shape\n        retval[\"scale\"] = self._scale\n        return retval\n\n\nclass ClassEmbedding(EmbeddingLayer):\n    \"\"\" Trainable Class Embedding layer \"\"\"\n    def call(self, inputs: tf.Tensor, *args, **kwargs) -> tf.Tensor:\n        \"\"\" Get the Class Embedding layer\n\n        Parameters\n        ----------\n        inputs: :class:`tensorflow.Tensor`\n            Input tensor to the embedding layer\n\n        Returns\n        -------\n        :class:`tensorflow.Tensor`\n            The class embedding layer shaped for the input tensor\n        \"\"\"\n        return K.tile(self._var[None, None], [K.shape(inputs)[0], 1, 1])\n\n\nclass PositionalEmbedding(EmbeddingLayer):\n    \"\"\" Trainable Positional Embedding layer \"\"\"\n    def call(self, inputs: tf.Tensor, *args, **kwargs) -> tf.Tensor:\n        \"\"\" Get the Positional Embedding layer\n\n        Parameters\n        ----------\n        inputs: :class:`tensorflow.Tensor`\n            Input tensor to the embedding layer\n\n        Returns\n        -------\n        :class:`tensorflow.Tensor`\n            The positional embedding layer shaped for the input tensor\n        \"\"\"\n        return K.tile(self._var[None], [K.shape(inputs)[0], 1, 1])\n\n\nclass Projection(EmbeddingLayer):\n    \"\"\" Trainable Projection Embedding Layer \"\"\"\n    def call(self, inputs: tf.Tensor, *args, **kwargs) -> tf.Tensor:\n        \"\"\" Get the Projection layer\n\n        Parameters\n        ----------\n        inputs: :class:`tensorflow.Tensor`\n            Input tensor to the embedding layer\n\n        Returns\n        -------\n        :class:`tensorflow.Tensor`\n            The Projection layer expanded to the batch dimension and transposed for matmul\n        \"\"\"\n        return K.tile(K.transpose(self._var)[None], [K.shape(inputs)[0], 1, 1])\n\n\nclass VisualTransformer():  # pylint:disable=too-few-public-methods\n    \"\"\" A class representing a Visual Transformer model for image classification tasks.\n\n    Parameters\n    ----------\n    input_resolution: int\n        The input resolution of the images.\n    patch_size: int\n        The size of the patches to be extracted from the images.\n    width: int\n        The dimension of the input and output vectors.\n    num_layers: int\n        The number of layers in the Transformer.\n    heads: int\n        The number of attention heads.\n    output_dim: int\n        The dimension of the output vector.\n    name: str, optional\n        The name of the Visual Transformer model, Default: \"VisualTransformer\".\n\n    Methods\n    -------\n    __call__() -> Model:\n        Builds and returns the Visual Transformer model.\n    \"\"\"\n    def __init__(self,\n                 input_resolution: int,\n                 patch_size: int,\n                 width: int,\n                 num_layers: int,\n                 heads: int,\n                 output_dim: int,\n                 name: str = \"VisualTransformer\") -> None:\n        logger.debug(\"Initializing: %s (input_resolution: %s, patch_size: %s, width: %s, \"\n                     \"layers: %s, heads: %s, output_dim: %s, name: %s)\",\n                     self.__class__.__name__, input_resolution, patch_size, width, num_layers,\n                     heads, output_dim, name)\n        self._input_resolution = input_resolution\n        self._patch_size = patch_size\n        self._width = width\n        self._num_layers = num_layers\n        self._heads = heads\n        self._output_dim = output_dim\n        self._name = name\n        logger.debug(\"Initialized: %s\", self.__class__.__name__)\n\n    def __call__(self) -> tf.keras.models.Model:\n        \"\"\" Builds and returns the Visual Transformer model.\n\n        Returns\n        -------\n        Model\n            The Visual Transformer model.\n        \"\"\"\n        inputs = layers.Input([self._input_resolution, self._input_resolution, 3])\n        var_x: tf.Tensor = layers.Conv2D(self._width,  # shape = [*, grid, grid, width]\n                                         self._patch_size,\n                                         strides=self._patch_size,\n                                         use_bias=False,\n                                         name=f\"{self._name}.conv1\")(inputs)\n\n        var_x = layers.Reshape((-1, self._width))(var_x)  # shape = [*, grid ** 2, width]\n\n        class_embed = ClassEmbedding((self._width, ),\n                                     self._width ** -0.5,\n                                     name=f\"{self._name}.class_embedding\")(var_x)\n        var_x = layers.Concatenate(axis=1)([class_embed, var_x])\n\n        pos_embed = PositionalEmbedding(((self._input_resolution // self._patch_size) ** 2 + 1,\n                                        self._width),\n                                        self._width ** -0.5,\n                                        name=f\"{self._name}.positional_embedding\")(var_x)\n        var_x = layers.Add()([var_x, pos_embed])\n        var_x = layers.LayerNormalization(epsilon=1e-05, name=f\"{self._name}.ln_pre\")(var_x)\n        var_x = Transformer(self._width,\n                            self._num_layers,\n                            self._heads,\n                            name=f\"{self._name}.transformer\")(var_x)\n        var_x = layers.LayerNormalization(epsilon=1e-05,\n                                          name=f\"{self._name}.ln_post\")(var_x[:, 0, :])\n        proj = Projection((self._width, self._output_dim),\n                          self._width ** -0.5,\n                          name=f\"{self._name}.proj\")(var_x)\n        var_x = layers.Dot(axes=-1)([var_x, proj])\n        return keras.models.Model(inputs=inputs, outputs=[var_x], name=self._name)\n\n\n# ################ #\n# MODIEFIED RESNET #\n# ################ #\nclass Bottleneck():  # pylint:disable=too-few-public-methods\n    \"\"\" A ResNet bottleneck block that performs a sequence of convolutions, batch normalization,\n    and ReLU activation operations on an input tensor.\n\n    Parameters\n    ----------\n    inplanes: int\n        The number of input channels.\n    planes: int\n        The number of output channels.\n    stride: int, optional\n        The stride of the bottleneck block. Default: 1\n    name: str, optional\n        The name of the bottleneck block. Default: \"bottleneck\"\n    \"\"\"\n    expansion = 4\n    \"\"\" int: The factor by which the number of input channels is expanded to get the number of\n    output channels.\"\"\"\n\n    def __init__(self,\n                 inplanes: int,\n                 planes: int,\n                 stride: int = 1,\n                 name: str = \"bottleneck\") -> None:\n        logger.debug(\"Initializing: %s (inplanes: %s, planes: %s, stride: %s, name: %s)\",\n                     self.__class__.__name__, inplanes, planes, stride, name)\n        self._inplanes = inplanes\n        self._planes = planes\n        self._stride = stride\n        self._name = name\n        logger.debug(\"Initialized: %s\", self.__class__.__name__)\n\n    def _downsample(self, inputs: tf.Tensor) -> tf.Tensor:\n        \"\"\" Perform downsample if required\n\n        Parameters\n        ----------\n        inputs: :class:`tensorflow.Tensor`\n            The input the downsample\n\n        Returns\n        -------\n        :class:`tensorflow.Tensor`\n            The original tensor, if downsizing not required, otherwise the downsized tensor\n        \"\"\"\n        if self._stride <= 1 and self._inplanes == self._planes * self.expansion:\n            return inputs\n\n        name = f\"{self._name}.downsample\"\n        out = layers.AveragePooling2D(self._stride, name=f\"{name}.avgpool\")(inputs)\n        out = layers.Conv2D(self._planes * self.expansion,\n                            1,\n                            strides=1,\n                            use_bias=False,\n                            name=f\"{name}.0\")(out)\n        out = layers.BatchNormalization(name=f\"{name}.1\", epsilon=1e-5)(out)\n        return out\n\n    def __call__(self, inputs: tf.Tensor) -> tf.Tensor:\n        \"\"\" Performs the forward pass for a Bottleneck block.\n\n        All conv layers have stride 1. an avgpool is performed after the second convolution when\n        stride > 1\n\n        Parameters\n        ----------\n        inputs: :class:`tensorflow.Tensor`\n            The input tensor to the Bottleneck block.\n\n        Returns\n        -------\n        :class:`tensorflow.Tensor`\n            The result of the forward pass through the Bottleneck block.\n        \"\"\"\n        out = layers.Conv2D(self._planes, 1, use_bias=False, name=f\"{self._name}.conv1\")(inputs)\n        out = layers.BatchNormalization(name=f\"{self._name}.bn1\", epsilon=1e-5)(out)\n        out = layers.ReLU()(out)\n\n        out = layers.ZeroPadding2D(padding=((1, 1), (1, 1)))(out)\n        out = layers.Conv2D(self._planes, 3, use_bias=False, name=f\"{self._name}.conv2\")(out)\n        out = layers.BatchNormalization(name=f\"{self._name}.bn2\", epsilon=1e-5)(out)\n        out = layers.ReLU()(out)\n\n        if self._stride > 1:\n            out = layers.AveragePooling2D(self._stride)(out)\n\n        out = layers.Conv2D(self._planes * self.expansion,\n                            1,\n                            use_bias=False,\n                            name=f\"{self._name}.conv3\")(out)\n        out = layers.BatchNormalization(name=f\"{self._name}.bn3\", epsilon=1e-5)(out)\n\n        identity = self._downsample(inputs)\n\n        out += identity\n        out = layers.ReLU()(out)\n        return out\n\n\nclass AttentionPool2d():  # pylint:disable=too-few-public-methods\n    \"\"\" An Attention Pooling layer that applies a multi-head self-attention mechanism over a\n    spatial grid of features.\n\n    Parameters\n    ----------\n    spatial_dim: int\n        The dimensionality of the spatial grid of features.\n    embed_dim: int\n        The dimensionality of the feature embeddings.\n    num_heads: int\n        The number of attention heads.\n    output_dim: int\n        The output dimensionality of the attention layer. If None, it defaults to embed_dim.\n    name: str\n        The name of the layer.\n    \"\"\"\n    def __init__(self,\n                 spatial_dim: int,\n                 embed_dim: int,\n                 num_heads: int,\n                 output_dim: int | None = None,\n                 name=\"AttentionPool2d\"):\n        logger.debug(\"Initializing: %s (spatial_dim: %s, embed_dim: %s, num_heads: %s, \"\n                     \"output_dim: %s, name: %s)\",\n                     self.__class__.__name__, spatial_dim, embed_dim, num_heads, output_dim, name)\n\n        self._spatial_dim = spatial_dim\n        self._embed_dim = embed_dim\n        self._num_heads = num_heads\n        self._output_dim = output_dim\n        self._name = name\n        logger.debug(\"Initialized: %s\", self.__class__.__name__)\n\n    def __call__(self, inputs: tf.Tensor) -> tf.Tensor:\n        \"\"\"Performs the attention pooling operation on the input tensor.\n\n        Parameters\n        ----------\n        inputs: :class:`tensorflow.Tensor`:\n                The input tensor of shape [batch_size, height, width, embed_dim].\n\n        Returns\n        -------\n        :class:`tensorflow.Tensor`:: The result of the attention pooling operation\n        \"\"\"\n        var_x: tf.Tensor\n        var_x = layers.Reshape((-1, inputs.shape[-1]))(inputs)  # NHWC -> N(HW)C\n        var_x = layers.Concatenate(axis=1)([K.mean(var_x, axis=1,  # N(HW)C -> N(HW+1)C\n                                                   keepdims=True), var_x])\n        pos_embed = PositionalEmbedding((self._spatial_dim ** 2 + 1, self._embed_dim),  # N(HW+1)C\n                                        self._embed_dim ** 0.5,\n                                        name=f\"{self._name}.positional_embedding\")(var_x)\n        var_x = layers.Add()([var_x, pos_embed])\n        # TODO At this point torch + keras match. They mismatch after MHA\n        var_x = layers.MultiHeadAttention(num_heads=self._num_heads,\n                                          key_dim=self._embed_dim // self._num_heads,\n                                          output_shape=self._output_dim or self._embed_dim,\n                                          use_bias=True,\n                                          name=f\"{self._name}.mha\")(var_x[:, :1, ...],\n                                                                    var_x,\n                                                                    var_x)\n        # only return the first element in the sequence\n        return var_x[:, 0, ...]\n\n\nclass ModifiedResNet():  # pylint:disable=too-few-public-methods\n    \"\"\" A ResNet class that is similar to torchvision's but contains the following changes:\n\n    - There are now 3 \"stem\" convolutions as opposed to 1, with an average pool instead of a max\n      pool.\n    - Performs anti-aliasing strided convolutions, where an avgpool is prepended to convolutions\n      with stride > 1\n    - The final pooling layer is a QKV attention instead of an average pool\n\n    Parameters\n    ----------\n        input_resolution: int\n            The input resolution of the model. Default is 224.\n        width: int\n            The width of the model. Default is 64.\n        layer_config: list\n            A list containing the number of Bottleneck blocks for each layer.\n        output_dim: int\n            The output dimension of the model.\n        heads: int\n            The number of heads for the QKV attention.\n        name: str\n            The name of the model. Default is \"ModifiedResNet\".\n    \"\"\"\n    def __init__(self,\n                 input_resolution: int,\n                 width: int,\n                 layer_config: tuple[int, int, int, int],\n                 output_dim: int,\n                 heads: int,\n                 name=\"ModifiedResNet\"):\n        self._input_resolution = input_resolution\n        self._width = width\n        self._layer_config = layer_config\n        self._heads = heads\n        self._output_dim = output_dim\n        self._name = name\n\n    def _stem(self, inputs: tf.Tensor) -> tf.Tensor:\n        \"\"\" Applies the stem operation to the input tensor, which consists of 3 convolutional\n            layers with BatchNormalization and ReLU activation, followed by an average pooling\n            layer.\n\n        Parameters\n        ----------\n        inputs: :class:`tensorflow.Tensor`\n                The input tensor\n\n        Returns\n        -------\n        :class:`tensorflow.Tensor`\n            The output tensor after applying the stem operation.\n        \"\"\"\n        var_x = inputs\n        for i in range(1, 4):\n            width = self._width if i == 3 else self._width // 2\n            strides = 2 if i == 1 else 1\n            var_x = layers.ZeroPadding2D(padding=((1, 1), (1, 1)), name=f\"conv{i}_padding\")(var_x)\n            var_x = layers.Conv2D(width,\n                                  3,\n                                  strides=strides,\n                                  use_bias=False,\n                                  name=f\"conv{i}\")(var_x)\n            var_x = layers.BatchNormalization(name=f\"bn{i}\", epsilon=1e-5)(var_x)\n            var_x = layers.ReLU()(var_x)\n        var_x = layers.AveragePooling2D(2, name=\"avgpool\")(var_x)\n        return var_x\n\n    def _bottleneck(self,\n                    inputs: tf.Tensor,\n                    planes: int,\n                    blocks: int,\n                    stride: int = 1,\n                    name: str = \"layer\") -> tf.Tensor:\n        \"\"\" A private method that creates a sequential layer of Bottleneck blocks for the\n        ModifiedResNet model.\n\n        Parameters\n        ----------\n        inputs: :class:`tensorflow.Tensor`\n                The input tensor\n        planes: int\n            The number of output channels for the layer.\n        blocks: int\n            The number of Bottleneck blocks in the layer.\n        stride: int\n            The stride for the first Bottleneck block in the layer. Default is 1.\n        name: str\n            The name of the layer. Default is \"layer\".\n\n        Returns\n        -------\n        :class:`tensorflow.Tensor`\n            Sequential block of bottlenecks\n        \"\"\"\n        retval: tf.Tensor\n        retval = Bottleneck(planes, planes, stride, name=f\"{name}.0\")(inputs)\n        for i in range(1, blocks):\n            retval = Bottleneck(planes * Bottleneck.expansion,\n                                planes,\n                                name=f\"{name}.{i}\")(retval)\n        return retval\n\n    def __call__(self) -> tf.keras.models.Model:\n        \"\"\" Implements the forward pass of the ModifiedResNet model.\n\n        Returns\n        -------\n        :class:`tensorflow.keras.models.Model`\n            The modified resnet model.\n        \"\"\"\n        inputs = layers.Input((self._input_resolution, self._input_resolution, 3))\n        var_x = self._stem(inputs)\n\n        for i in range(4):\n            stride = 1 if i == 0 else 2\n            var_x = self._bottleneck(var_x,\n                                     self._width * (2 ** i),\n                                     self._layer_config[i],\n                                     stride=stride,\n                                     name=f\"{self._name}.layer{i + 1}\")\n\n        var_x = AttentionPool2d(self._input_resolution // 32,\n                                self._width * 32,  # the ResNet feature dimension\n                                self._heads,\n                                self._output_dim,\n                                name=f\"{self._name}.attnpool\")(var_x)\n        return keras.models.Model(inputs, outputs=[var_x], name=self._name)\n\n\n# ### #\n# VIT #\n# ### #\nclass ViT():  # pylint:disable=too-few-public-methods\n    \"\"\" Visiual Transform from CLIP\n\n    A Convolutional Language-Image Pre-Training (CLIP) model that encodes images and text into a\n    shared latent space.\n\n    Reference\n    ---------\n    https://arxiv.org/abs/2103.00020\n\n    Parameters\n    ----------\n        name: [\"RN50\", \"RN101\", \"RN50x4\", \"RN50x16\", \"RN50x64\", \"ViT-B-32\",\n               \"ViT-B-16\", \"ViT-L-14\", \"ViT-L-14-336px\", \"FaRL-B_16-64\"]\n            The model configuration to use\n        input_size: int, optional\n            The required resolution size for the model. ``None`` for default preset size\n        load_weights: bool, optional\n            ``True`` to load pretrained weights. Default: ``False``\n        \"\"\"\n    def __init__(self,\n                 name: TypeModels,\n                 input_size: int | None = None,\n                 load_weights: bool = False) -> None:\n        logger.debug(\"Initializing: %s (name: %s, input_size: %s, load_weights: %s)\",\n                     self.__class__.__name__, name, input_size, load_weights)\n        assert name in ModelConfig, (\"Name must be one of %s\", list(ModelConfig))\n\n        self._name = name\n        self._load_weights = load_weights\n\n        config = ModelConfig[name]\n        self._git_id = config.git_id\n\n        res = input_size if input_size is not None else config.resolution\n        self._net = self._get_vision_net(config.layer_conf,\n                                         config.width,\n                                         config.embed_dim,\n                                         res,\n                                         config.patch)\n        logger.debug(\"Initialized: %s\", self.__class__.__name__)\n\n    def _get_vision_net(self,\n                        layer_config: int | tuple[int, int, int, int],\n                        width: int,\n                        embed_dim: int,\n                        resolution: int,\n                        patch_size: int) -> tf.keras.models.Model:\n        \"\"\" Obtain the network for the vision layets\n\n        Parameters\n        ----------\n        layer_config: tuple[int, int, int, int] | int\n            Number of layers in the visual encoder, or a tuple of layer configurations for a custom\n            ResNet visual encoder.\n        width: int\n            Width of the visual encoder layers.\n        embed_dim: int\n            Dimensionality of the final shared embedding space.\n        resolution: int\n            Spatial resolution of the input images.\n        patch_size: int\n            Size of the patches to be extracted from the images.\n\n        Returns\n        -------\n        :class:`tensorflow.keras.models.Model`\n            The :class:`ModifiedResNet` or :class:`VisualTransformer` vision model to use\n        \"\"\"\n        if isinstance(layer_config, (tuple, list)):\n            vision_heads = width * 32 // 64\n            return ModifiedResNet(input_resolution=resolution,\n                                  width=width,\n                                  layer_config=layer_config,\n                                  output_dim=embed_dim,\n                                  heads=vision_heads,\n                                  name=\"visual\")\n        vision_heads = width // 64\n        return VisualTransformer(input_resolution=resolution,\n                                 width=width,\n                                 num_layers=layer_config,\n                                 output_dim=embed_dim,\n                                 heads=vision_heads,\n                                 patch_size=patch_size,\n                                 name=\"visual\")\n\n    def __call__(self) -> tf.keras.Model:\n        \"\"\" Get the configured ViT model\n\n        Returns\n        -------\n        :class:`tensorflow.keras.models.Model`\n            The requested Visual Transformer model\n        \"\"\"\n        net: tf.keras.models.Model = self._net()\n        if self._load_weights and not self._git_id:\n            logger.warning(\"Trained weights are not available for '%s'\", self._name)\n            return net\n        if self._load_weights:\n            model_path = GetModel(f\"CLIPv_{self._name}_v1.h5\", self._git_id).model_path\n            logger.info(\"Loading CLIPv trained weights for '%s'\", self._name)\n            net.load_weights(model_path, by_name=True, skip_mismatch=True)\n\n        return net\n\n\n# Update layers into Keras custom objects\nfor name_, obj in inspect.getmembers(sys.modules[__name__]):\n    if (inspect.isclass(obj) and issubclass(obj, tf.keras.layers.Layer)\n            and obj.__module__ == __name__):\n        keras.utils.get_custom_objects().update({name_: obj})\n", "lib/cli/actions.py": "#!/usr/bin/env python3\n\"\"\" Custom :class:`argparse.Action` objects for Faceswap's Command Line Interface.\n\nThe custom actions within this module allow for custom manipulation of Command Line Arguments\nas well as adding a mechanism for indicating to the GUI how specific options should be rendered.\n\"\"\"\n\nimport argparse\nimport os\nimport typing as T\n\n\n# << FILE HANDLING >>\n\nclass _FullPaths(argparse.Action):\n    \"\"\" Parent class for various file type and file path handling classes.\n\n    Expands out given paths to their full absolute paths. This class should not be\n    called directly. It is the base class for the various different file handling\n    methods.\n    \"\"\"\n    def __call__(self, parser, namespace, values, option_string=None) -> None:\n        if isinstance(values, (list, tuple)):\n            vals = [os.path.abspath(os.path.expanduser(val)) for val in values]\n        else:\n            vals = os.path.abspath(os.path.expanduser(values))\n        setattr(namespace, self.dest, vals)\n\n\nclass DirFullPaths(_FullPaths):\n    \"\"\" Adds support for a Directory browser in the GUI.\n\n    This is a standard :class:`argparse.Action` (with stock parameters) which indicates to the GUI\n    that a dialog box should be opened in order to browse for a folder.\n\n    No additional parameters are required.\n\n    Example\n    -------\n    >>> argument_list = []\n    >>> argument_list.append(dict(\n    >>>        opts=(\"-f\", \"--folder_location\"),\n    >>>        action=DirFullPaths)),\n    \"\"\"\n    pass  # pylint:disable=unnecessary-pass\n\n\nclass FileFullPaths(_FullPaths):\n    \"\"\" Adds support for a File browser to select a single file in the GUI.\n\n    This extends the standard :class:`argparse.Action` and adds an additional parameter\n    :attr:`filetypes`, indicating to the GUI that it should pop a file browser for opening a file\n    and limit the results to the file types listed. As well as the standard parameters, the\n    following parameter is required:\n\n    Parameters\n    ----------\n    filetypes: str\n        The accepted file types for this option. This is the key for the GUIs lookup table which\n        can be found in :class:`lib.gui.utils.FileHandler`\n\n    Example\n    -------\n    >>> argument_list = []\n    >>> argument_list.append(dict(\n    >>>        opts=(\"-f\", \"--video_location\"),\n    >>>        action=FileFullPaths,\n    >>>        filetypes=\"video))\"\n    \"\"\"\n    def __init__(self, *args, filetypes: str | None = None, **kwargs) -> None:\n        super().__init__(*args, **kwargs)\n        self.filetypes = filetypes\n\n    def _get_kwargs(self):\n        names = [\"option_strings\",\n                 \"dest\",\n                 \"nargs\",\n                 \"const\",\n                 \"default\",\n                 \"type\",\n                 \"choices\",\n                 \"help\",\n                 \"metavar\",\n                 \"filetypes\"]\n        return [(name, getattr(self, name)) for name in names]\n\n\nclass FilesFullPaths(FileFullPaths):\n    \"\"\" Adds support for a File browser to select multiple files in the GUI.\n\n    This extends the standard :class:`argparse.Action` and adds an additional parameter\n    :attr:`filetypes`, indicating to the GUI that it should pop a file browser, and limit\n    the results to the file types listed. Multiple files can be selected for opening, so the\n    :attr:`nargs` parameter must be set. As well as the standard parameters, the following\n    parameter is required:\n\n    Parameters\n    ----------\n    filetypes: str\n        The accepted file types for this option. This is the key for the GUIs lookup table which\n        can be found in :class:`lib.gui.utils.FileHandler`\n\n    Example\n    -------\n    >>> argument_list = []\n    >>> argument_list.append(dict(\n    >>>        opts=(\"-f\", \"--images\"),\n    >>>        action=FilesFullPaths,\n    >>>        filetypes=\"image\",\n    >>>        nargs=\"+\"))\n    \"\"\"\n    def __init__(self, *args, filetypes: str | None = None, **kwargs) -> None:\n        if kwargs.get(\"nargs\", None) is None:\n            opt = kwargs[\"option_strings\"]\n            raise ValueError(f\"nargs must be provided for FilesFullPaths: {opt}\")\n        super().__init__(*args, **kwargs)\n\n\nclass DirOrFileFullPaths(FileFullPaths):\n    \"\"\" Adds support to the GUI to launch either a file browser or a folder browser.\n\n    Some inputs (for example source frames) can come from a folder of images or from a\n    video file. This indicates to the GUI that it should place 2 buttons (one for a folder\n    browser, one for a file browser) for file/folder browsing.\n\n    The standard :class:`argparse.Action` is extended with the additional parameter\n    :attr:`filetypes`, indicating to the GUI that it should pop a file browser, and limit\n    the results to the file types listed. As well as the standard parameters, the following\n    parameter is required:\n\n    Parameters\n    ----------\n    filetypes: str\n        The accepted file types for this option. This is the key for the GUIs lookup table which\n        can be found in :class:`lib.gui.utils.FileHandler`. NB: This parameter is only used for\n        the file browser and not the folder browser\n\n    Example\n    -------\n    >>> argument_list = []\n    >>> argument_list.append(dict(\n    >>>        opts=(\"-f\", \"--input_frames\"),\n    >>>        action=DirOrFileFullPaths,\n    >>>        filetypes=\"video))\"\n    \"\"\"\n\n\nclass DirOrFilesFullPaths(FileFullPaths):\n    \"\"\" Adds support to the GUI to launch either a file browser for selecting multiple files\n    or a folder browser.\n\n    Some inputs (for example face filter) can come from a folder of images or from multiple\n    image file. This indicates to the GUI that it should place 2 buttons (one for a folder\n    browser, one for a multi-file browser) for file/folder browsing.\n\n    The standard :class:`argparse.Action` is extended with the additional parameter\n    :attr:`filetypes`, indicating to the GUI that it should pop a file browser, and limit\n    the results to the file types listed. As well as the standard parameters, the following\n    parameter is required:\n\n    Parameters\n    ----------\n    filetypes: str\n        The accepted file types for this option. This is the key for the GUIs lookup table which\n        can be found in :class:`lib.gui.utils.FileHandler`. NB: This parameter is only used for\n        the file browser and not the folder browser\n\n    Example\n    -------\n    >>> argument_list = []\n    >>> argument_list.append(dict(\n    >>>        opts=(\"-f\", \"--input_frames\"),\n    >>>        action=DirOrFileFullPaths,\n    >>>        filetypes=\"video))\"\n    \"\"\"\n    def __call__(self, parser, namespace, values, option_string=None) -> None:\n        \"\"\" Override :class:`_FullPaths` __call__ function.\n\n        The input for this option can be a space separated list of files or a single folder.\n        Folders can have spaces in them, so we don't want to blindly expand the paths.\n\n        We check whether the input can be resolved to a folder first before expanding.\n        \"\"\"\n        assert isinstance(values, (list, tuple))\n        folder = os.path.abspath(os.path.expanduser(\" \".join(values)))\n        if os.path.isdir(folder):\n            setattr(namespace, self.dest, [folder])\n        else:  # file list so call parent method\n            super().__call__(parser, namespace, values, option_string)\n\n\nclass SaveFileFullPaths(FileFullPaths):\n    \"\"\" Adds support for a Save File dialog in the GUI.\n\n    This extends the standard :class:`argparse.Action` and adds an additional parameter\n    :attr:`filetypes`, indicating to the GUI that it should pop a save file browser, and limit\n    the results to the file types listed. As well as the standard parameters, the following\n    parameter is required:\n\n    Parameters\n    ----------\n    filetypes: str\n        The accepted file types for this option. This is the key for the GUIs lookup table which\n        can be found in :class:`lib.gui.utils.FileHandler`\n\n    Example\n    -------\n    >>> argument_list = []\n    >>> argument_list.append(dict(\n    >>>        opts=(\"-f\", \"--video_out\"),\n    >>>        action=SaveFileFullPaths,\n    >>>        filetypes=\"video\"))\n    \"\"\"\n    pass  # pylint:disable=unnecessary-pass\n\n\nclass ContextFullPaths(FileFullPaths):\n    \"\"\" Adds support for context sensitive browser dialog opening in the GUI.\n\n    For some tasks, the type of action (file load, folder open, file save etc.) can vary\n    depending on the task to be performed (a good example of this is the effmpeg tool).\n    Using this action indicates to the GUI that the type of dialog to be launched can change\n    depending on another option. As well as the standard parameters, the below parameters are\n    required. NB: :attr:`nargs` are explicitly disallowed.\n\n    Parameters\n    ----------\n    filetypes: str\n        The accepted file types for this option. This is the key for the GUIs lookup table which\n        can be found in :class:`lib.gui.utils.FileHandler`\n    action_option: str\n        The command line option that dictates the context of the file dialog to be opened.\n        Bespoke actions are set in :class:`lib.gui.utils.FileHandler`\n\n    Example\n    -------\n    Assuming an argument has already been set with option string `-a` indicating the action to be\n    performed, the following will pop a different type of dialog depending on the action selected:\n\n    >>> argument_list = []\n    >>> argument_list.append(dict(\n    >>>        opts=(\"-f\", \"--input_video\"),\n    >>>        action=ContextFullPaths,\n    >>>        filetypes=\"video\",\n    >>>        action_option=\"-a\"))\n    \"\"\"\n    # pylint:disable=too-many-arguments\n    def __init__(self,\n                 *args,\n                 filetypes: str | None = None,\n                 action_option: str | None = None,\n                 **kwargs) -> None:\n        opt = kwargs[\"option_strings\"]\n        if kwargs.get(\"nargs\", None) is not None:\n            raise ValueError(f\"nargs not allowed for ContextFullPaths: {opt}\")\n        if filetypes is None:\n            raise ValueError(f\"filetypes is required for ContextFullPaths: {opt}\")\n        if action_option is None:\n            raise ValueError(f\"action_option is required for ContextFullPaths: {opt}\")\n        super().__init__(*args, filetypes=filetypes, **kwargs)\n        self.action_option = action_option\n\n    def _get_kwargs(self) -> list[tuple[str, T.Any]]:\n        names = [\"option_strings\",\n                 \"dest\",\n                 \"nargs\",\n                 \"const\",\n                 \"default\",\n                 \"type\",\n                 \"choices\",\n                 \"help\",\n                 \"metavar\",\n                 \"filetypes\",\n                 \"action_option\"]\n        return [(name, getattr(self, name)) for name in names]\n\n\n# << GUI DISPLAY OBJECTS >>\n\nclass Radio(argparse.Action):\n    \"\"\" Adds support for a GUI Radio options box.\n\n    This is a standard :class:`argparse.Action` (with stock parameters) which indicates to the GUI\n    that the options passed should be rendered as a group of Radio Buttons rather than a combo box.\n\n    No additional parameters are required, but the :attr:`choices` parameter must be provided as\n    these will be the Radio Box options. :attr:`nargs` are explicitly disallowed.\n\n    Example\n    -------\n    >>> argument_list = []\n    >>> argument_list.append(dict(\n    >>>        opts=(\"-f\", \"--foobar\"),\n    >>>        action=Radio,\n    >>>        choices=[\"foo\", \"bar\"))\n    \"\"\"\n    def __init__(self, *args, **kwargs) -> None:\n        opt = kwargs[\"option_strings\"]\n        if kwargs.get(\"nargs\", None) is not None:\n            raise ValueError(f\"nargs not allowed for Radio buttons: {opt}\")\n        if not kwargs.get(\"choices\", []):\n            raise ValueError(f\"Choices must be provided for Radio buttons: {opt}\")\n        super().__init__(*args, **kwargs)\n\n    def __call__(self, parser, namespace, values, option_string=None) -> None:\n        setattr(namespace, self.dest, values)\n\n\nclass MultiOption(argparse.Action):\n    \"\"\" Adds support for multiple option checkboxes in the GUI.\n\n    This is a standard :class:`argparse.Action` (with stock parameters) which indicates to the GUI\n    that the options passed should be rendered as a group of Radio Buttons rather than a combo box.\n\n    The :attr:`choices` parameter must be provided as this provides the valid option choices.\n\n    Example\n    -------\n    >>> argument_list = []\n    >>> argument_list.append(dict(\n    >>>        opts=(\"-f\", \"--foobar\"),\n    >>>        action=MultiOption,\n    >>>        choices=[\"foo\", \"bar\"))\n    \"\"\"\n    def __init__(self, *args, **kwargs) -> None:\n        opt = kwargs[\"option_strings\"]\n        if not kwargs.get(\"nargs\", []):\n            raise ValueError(f\"nargs must be provided for MultiOption: {opt}\")\n        if not kwargs.get(\"choices\", []):\n            raise ValueError(f\"Choices must be provided for MultiOption: {opt}\")\n        super().__init__(*args, **kwargs)\n\n    def __call__(self, parser, namespace, values, option_string=None) -> None:\n        setattr(namespace, self.dest, values)\n\n\nclass Slider(argparse.Action):\n    \"\"\" Adds support for a slider in the GUI.\n\n    The standard :class:`argparse.Action` is extended with the additional parameters listed below.\n    The :attr:`default` value must be supplied and the :attr:`type` must be either :class:`int` or\n    :class:`float`. :attr:`nargs` are explicitly disallowed.\n\n    Parameters\n    ----------\n    min_max: tuple\n        The (`min`, `max`) values that the slider's range should be set to. The values should be a\n        pair of `float` or `int` data types, depending on the data type of the slider. NB: These\n        min/max values are not enforced, they are purely for setting the slider range. Values\n        outside of this range can still be explicitly passed in from the cli.\n    rounding: int\n        If the underlying data type for the option is a `float` then this value is the number of\n        decimal places to round the slider values to. If the underlying data type for the option is\n        an `int` then this is the step interval between each value for the slider.\n\n    Examples\n    --------\n    For integer values:\n\n    >>> argument_list = []\n    >>> argument_list.append(dict(\n    >>>        opts=(\"-f\", \"--foobar\"),\n    >>>        action=Slider,\n    >>>        min_max=(0, 10)\n    >>>        rounding=1\n    >>>        type=int,\n    >>>        default=5))\n\n    For floating point values:\n\n    >>> argument_list = []\n    >>> argument_list.append(dict(\n    >>>        opts=(\"-f\", \"--foobar\"),\n    >>>        action=Slider,\n    >>>        min_max=(0.00, 1.00)\n    >>>        rounding=2\n    >>>        type=float,\n    >>>        default=5.00))\n    \"\"\"\n    def __init__(self,\n                 *args,\n                 min_max: tuple[int, int] | tuple[float, float] | None = None,\n                 rounding: int | None = None,\n                 **kwargs) -> None:\n        opt = kwargs[\"option_strings\"]\n        if kwargs.get(\"nargs\", None) is not None:\n            raise ValueError(f\"nargs not allowed for Slider: {opt}\")\n        if kwargs.get(\"default\", None) is None:\n            raise ValueError(f\"A default value must be supplied for Slider: {opt}\")\n        if kwargs.get(\"type\", None) not in (int, float):\n            raise ValueError(f\"Sliders only accept int and float data types: {opt}\")\n        if min_max is None:\n            raise ValueError(f\"min_max must be provided for Sliders: {opt}\")\n        if rounding is None:\n            raise ValueError(f\"rounding must be provided for Sliders: {opt}\")\n\n        super().__init__(*args, **kwargs)\n        self.min_max = min_max\n        self.rounding = rounding\n\n    def _get_kwargs(self) -> list[tuple[str, T.Any]]:\n        names = [\"option_strings\",\n                 \"dest\",\n                 \"nargs\",\n                 \"const\",\n                 \"default\",\n                 \"type\",\n                 \"choices\",\n                 \"help\",\n                 \"metavar\",\n                 \"min_max\",  # Tuple containing min and max values of scale\n                 \"rounding\"]  # Decimal places to round floats to or step interval for ints\n        return [(name, getattr(self, name)) for name in names]\n\n    def __call__(self, parser, namespace, values, option_string=None) -> None:\n        setattr(namespace, self.dest, values)\n", "lib/cli/args_train.py": "#!/usr/bin/env python3\n\"\"\" The Command Line Argument options for training with faceswap.py \"\"\"\nimport argparse\nimport gettext\nimport typing as T\n\nfrom plugins.plugin_loader import PluginLoader\n\nfrom .actions import DirFullPaths, FileFullPaths, Radio, Slider\nfrom .args import FaceSwapArgs\n\n\n# LOCALES\n_LANG = gettext.translation(\"lib.cli.args_train\", localedir=\"locales\", fallback=True)\n_ = _LANG.gettext\n\n\nclass TrainArgs(FaceSwapArgs):\n    \"\"\" Creates the command line arguments for training. \"\"\"\n\n    @staticmethod\n    def get_info() -> str:\n        \"\"\" The information text for the Train command.\n\n        Returns\n        -------\n        str\n            The information text for the Train command.\n        \"\"\"\n        return _(\"Train a model on extracted original (A) and swap (B) faces.\\n\"\n                 \"Training models can take a long time. Anything from 24hrs to over a week\\n\"\n                 \"Model plugins can be configured in the 'Settings' Menu\")\n\n    @staticmethod\n    def get_argument_list() -> list[dict[str, T.Any]]:\n        \"\"\" Returns the argument list for Train arguments.\n\n        Returns\n        -------\n        list\n            The list of command line options for training\n        \"\"\"\n        argument_list: list[dict[str, T.Any]] = []\n        argument_list.append({\n            \"opts\": (\"-A\", \"--input-A\"),\n            \"action\": DirFullPaths,\n            \"dest\": \"input_a\",\n            \"required\": True,\n            \"group\": _(\"faces\"),\n            \"help\": _(\n                \"Input directory. A directory containing training images for face A. This is the \"\n                \"original face, i.e. the face that you want to remove and replace with face B.\")})\n        argument_list.append({\n            \"opts\": (\"-B\", \"--input-B\"),\n            \"action\": DirFullPaths,\n            \"dest\": \"input_b\",\n            \"required\": True,\n            \"group\": _(\"faces\"),\n            \"help\": _(\n                \"Input directory. A directory containing training images for face B. This is the \"\n                \"swap face, i.e. the face that you want to place onto the head of person A.\")})\n        argument_list.append({\n            \"opts\": (\"-m\", \"--model-dir\"),\n            \"action\": DirFullPaths,\n            \"dest\": \"model_dir\",\n            \"required\": True,\n            \"group\": _(\"model\"),\n            \"help\": _(\n                \"Model directory. This is where the training data will be stored. You should \"\n                \"always specify a new folder for new models. If starting a new model, select \"\n                \"either an empty folder, or a folder which does not exist (which will be \"\n                \"created). If continuing to train an existing model, specify the location of the \"\n                \"existing model.\")})\n        argument_list.append({\n            \"opts\": (\"-l\", \"--load-weights\"),\n            \"action\": FileFullPaths,\n            \"filetypes\": \"model\",\n            \"dest\": \"load_weights\",\n            \"required\": False,\n            \"group\": _(\"model\"),\n            \"help\": _(\n                \"R|Load the weights from a pre-existing model into a newly created model. For \"\n                \"most models this will load weights from the Encoder of the given model into the \"\n                \"encoder of the newly created model. Some plugins may have specific configuration \"\n                \"options allowing you to load weights from other layers. Weights will only be \"\n                \"loaded when creating a new model. This option will be ignored if you are \"\n                \"resuming an existing model. Generally you will also want to 'freeze-weights' \"\n                \"whilst the rest of your model catches up with your Encoder.\\n\"\n                \"NB: Weights can only be loaded from models of the same plugin as you intend to \"\n                \"train.\")})\n        argument_list.append({\n            \"opts\": (\"-t\", \"--trainer\"),\n            \"action\": Radio,\n            \"type\": str.lower,\n            \"default\": PluginLoader.get_default_model(),\n            \"choices\": PluginLoader.get_available_models(),\n            \"group\": _(\"model\"),\n            \"help\": _(\n                \"R|Select which trainer to use. Trainers can be configured from the Settings menu \"\n                \"or the config folder.\"\n                \"\\nL|original: The original model created by /u/deepfakes.\"\n                \"\\nL|dfaker: 64px in/128px out model from dfaker. Enable 'warp-to-landmarks' for \"\n                \"full dfaker method.\"\n                \"\\nL|dfl-h128: 128px in/out model from deepfacelab\"\n                \"\\nL|dfl-sae: Adaptable model from deepfacelab\"\n                \"\\nL|dlight: A lightweight, high resolution DFaker variant.\"\n                \"\\nL|iae: A model that uses intermediate layers to try to get better details\"\n                \"\\nL|lightweight: A lightweight model for low-end cards. Don't expect great \"\n                \"results. Can train as low as 1.6GB with batch size 8.\"\n                \"\\nL|realface: A high detail, dual density model based on DFaker, with \"\n                \"customizable in/out resolution. The autoencoders are unbalanced so B>A swaps \"\n                \"won't work so well. By andenixa et al. Very configurable.\"\n                \"\\nL|unbalanced: 128px in/out model from andenixa. The autoencoders are \"\n                \"unbalanced so B>A swaps won't work so well. Very configurable.\"\n                \"\\nL|villain: 128px in/out model from villainguy. Very resource hungry (You will \"\n                \"require a GPU with a fair amount of VRAM). Good for details, but more \"\n                \"susceptible to color differences.\")})\n        argument_list.append({\n            \"opts\": (\"-u\", \"--summary\"),\n            \"action\": \"store_true\",\n            \"dest\": \"summary\",\n            \"default\": False,\n            \"group\": _(\"model\"),\n            \"help\": _(\n                \"Output a summary of the model and exit. If a model folder is provided then a \"\n                \"summary of the saved model is displayed. Otherwise a summary of the model that \"\n                \"would be created by the chosen plugin and configuration settings is displayed.\")})\n        argument_list.append({\n            \"opts\": (\"-f\", \"--freeze-weights\"),\n            \"action\": \"store_true\",\n            \"dest\": \"freeze_weights\",\n            \"default\": False,\n            \"group\": _(\"model\"),\n            \"help\": _(\n                \"Freeze the weights of the model. Freezing weights means that some of the \"\n                \"parameters in the model will no longer continue to learn, but those that are not \"\n                \"frozen will continue to learn. For most models, this will freeze the encoder, \"\n                \"but some models may have configuration options for freezing other layers.\")})\n        argument_list.append({\n            \"opts\": (\"-b\", \"--batch-size\"),\n            \"action\": Slider,\n            \"min_max\": (1, 256),\n            \"rounding\": 1,\n            \"type\": int,\n            \"dest\": \"batch_size\",\n            \"default\": 16,\n            \"group\": _(\"training\"),\n            \"help\": _(\n                \"Batch size. This is the number of images processed through the model for each \"\n                \"side per iteration. NB: As the model is fed 2 sides at a time, the actual number \"\n                \"of images within the model at any one time is double the number that you set \"\n                \"here. Larger batches require more GPU RAM.\")})\n        argument_list.append({\n            \"opts\": (\"-i\", \"--iterations\"),\n            \"action\": Slider,\n            \"min_max\": (0, 5000000),\n            \"rounding\": 20000,\n            \"type\": int,\n            \"default\": 1000000,\n            \"group\": _(\"training\"),\n            \"help\": _(\n                \"Length of training in iterations. This is only really used for automation. There \"\n                \"is no 'correct' number of iterations a model should be trained for. You should \"\n                \"stop training when you are happy with the previews. However, if you want the \"\n                \"model to stop automatically at a set number of iterations, you can set that \"\n                \"value here.\")})\n        argument_list.append({\n            \"opts\": (\"-D\", \"--distribution-strategy\"),\n            \"dest\": \"distribution_strategy\",\n            \"action\": Radio,\n            \"type\": str.lower,\n            \"choices\": [\"default\", \"central-storage\", \"mirrored\"],\n            \"default\": \"default\",\n            \"backend\": (\"nvidia\", \"directml\", \"rocm\", \"apple_silicon\"),\n            \"group\": _(\"training\"),\n            \"help\": _(\n                \"R|Select the distribution stategy to use.\"\n                \"\\nL|default: Use Tensorflow's default distribution strategy.\"\n                \"\\nL|central-storage: Centralizes variables on the CPU whilst operations are \"\n                \"performed on 1 or more local GPUs. This can help save some VRAM at the cost of \"\n                \"some speed by not storing variables on the GPU. Note: Mixed-Precision is not \"\n                \"supported on multi-GPU setups.\"\n                \"\\nL|mirrored: Supports synchronous distributed training across multiple local \"\n                \"GPUs. A copy of the model and all variables are loaded onto each GPU with \"\n                \"batches distributed to each GPU at each iteration.\")})\n        argument_list.append({\n            \"opts\": (\"-n\", \"--no-logs\"),\n            \"action\": \"store_true\",\n            \"dest\": \"no_logs\",\n            \"default\": False,\n            \"group\": _(\"training\"),\n            \"help\": _(\n                \"Disables TensorBoard logging. NB: Disabling logs means that you will not be able \"\n                \"to use the graph or analysis for this session in the GUI.\")})\n        argument_list.append({\n            \"opts\": (\"-r\", \"--use-lr-finder\"),\n            \"action\": \"store_true\",\n            \"dest\": \"use_lr_finder\",\n            \"default\": False,\n            \"group\": _(\"training\"),\n            \"help\": _(\n                \"Use the Learning Rate Finder to discover the optimal learning rate for training. \"\n                \"For new models, this will calculate the optimal learning rate for the model. For \"\n                \"existing models this will use the optimal learning rate that was discovered when \"\n                \"initializing the model. Setting this option will ignore the manually configured \"\n                \"learning rate (configurable in train settings).\")})\n        argument_list.append({\n            \"opts\": (\"-s\", \"--save-interval\"),\n            \"action\": Slider,\n            \"min_max\": (10, 1000),\n            \"rounding\": 10,\n            \"type\": int,\n            \"dest\": \"save_interval\",\n            \"default\": 250,\n            \"group\": _(\"Saving\"),\n            \"help\": _(\"Sets the number of iterations between each model save.\")})\n        argument_list.append({\n            \"opts\": (\"-I\", \"--snapshot-interval\"),\n            \"action\": Slider,\n            \"min_max\": (0, 100000),\n            \"rounding\": 5000,\n            \"type\": int,\n            \"dest\": \"snapshot_interval\",\n            \"default\": 25000,\n            \"group\": _(\"Saving\"),\n            \"help\": _(\n                \"Sets the number of iterations before saving a backup snapshot of the model in \"\n                \"it's current state. Set to 0 for off.\")})\n        argument_list.append({\n            \"opts\": (\"-x\", \"--timelapse-input-A\"),\n            \"action\": DirFullPaths,\n            \"dest\": \"timelapse_input_a\",\n            \"default\": None,\n            \"group\": _(\"timelapse\"),\n            \"help\": _(\n                \"Optional for creating a timelapse. Timelapse will save an image of your selected \"\n                \"faces into the timelapse-output folder at every save iteration. This should be \"\n                \"the input folder of 'A' faces that you would like to use for creating the \"\n                \"timelapse. You must also supply a --timelapse-output and a --timelapse-input-B \"\n                \"parameter.\")})\n        argument_list.append({\n            \"opts\": (\"-y\", \"--timelapse-input-B\"),\n            \"action\": DirFullPaths,\n            \"dest\": \"timelapse_input_b\",\n            \"default\": None,\n            \"group\": _(\"timelapse\"),\n            \"help\": _(\n                \"Optional for creating a timelapse. Timelapse will save an image of your selected \"\n                \"faces into the timelapse-output folder at every save iteration. This should be \"\n                \"the input folder of 'B' faces that you would like to use for creating the \"\n                \"timelapse. You must also supply a --timelapse-output and a --timelapse-input-A \"\n                \"parameter.\")})\n        argument_list.append({\n            \"opts\": (\"-z\", \"--timelapse-output\"),\n            \"action\": DirFullPaths,\n            \"dest\": \"timelapse_output\",\n            \"default\": None,\n            \"group\": _(\"timelapse\"),\n            \"help\": _(\n                \"Optional for creating a timelapse. Timelapse will save an image of your selected \"\n                \"faces into the timelapse-output folder at every save iteration. If the input \"\n                \"folders are supplied but no output folder, it will default to your model folder/\"\n                \"timelapse/\")})\n        argument_list.append({\n            \"opts\": (\"-p\", \"--preview\"),\n            \"action\": \"store_true\",\n            \"dest\": \"preview\",\n            \"default\": False,\n            \"group\": _(\"preview\"),\n            \"help\": _(\"Show training preview output. in a separate window.\")})\n        argument_list.append({\n            \"opts\": (\"-w\", \"--write-image\"),\n            \"action\": \"store_true\",\n            \"dest\": \"write_image\",\n            \"default\": False,\n            \"group\": _(\"preview\"),\n            \"help\": _(\n                \"Writes the training result to a file. The image will be stored in the root of \"\n                \"your FaceSwap folder.\")})\n        argument_list.append({\n            \"opts\": (\"-M\", \"--warp-to-landmarks\"),\n            \"action\": \"store_true\",\n            \"dest\": \"warp_to_landmarks\",\n            \"default\": False,\n            \"group\": _(\"augmentation\"),\n            \"help\": _(\n                \"Warps training faces to closely matched Landmarks from the opposite face-set \"\n                \"rather than randomly warping the face. This is the 'dfaker' way of doing \"\n                \"warping.\")})\n        argument_list.append({\n            \"opts\": (\"-P\", \"--no-flip\"),\n            \"action\": \"store_true\",\n            \"dest\": \"no_flip\",\n            \"default\": False,\n            \"group\": _(\"augmentation\"),\n            \"help\": _(\n                \"To effectively learn, a random set of images are flipped horizontally. Sometimes \"\n                \"it is desirable for this not to occur. Generally this should be left off except \"\n                \"for during 'fit training'.\")})\n        argument_list.append({\n            \"opts\": (\"-c\", \"--no-augment-color\"),\n            \"action\": \"store_true\",\n            \"dest\": \"no_augment_color\",\n            \"default\": False,\n            \"group\": _(\"augmentation\"),\n            \"help\": _(\n                \"Color augmentation helps make the model less susceptible to color differences \"\n                \"between the A and B sets, at an increased training time cost. Enable this option \"\n                \"to disable color augmentation.\")})\n        argument_list.append({\n            \"opts\": (\"-W\", \"--no-warp\"),\n            \"action\": \"store_true\",\n            \"dest\": \"no_warp\",\n            \"default\": False,\n            \"group\": _(\"augmentation\"),\n            \"help\": _(\n                \"Warping is integral to training the Neural Network. This option should only be \"\n                \"enabled towards the very end of training to try to bring out more detail. Think \"\n                \"of it as 'fine-tuning'. Enabling this option from the beginning is likely to \"\n                \"kill a model and lead to terrible results.\")})\n        # Deprecated multi-character switches\n        argument_list.append({\n            \"opts\": (\"-su\", ),\n            \"action\": \"store_true\",\n            \"dest\": \"depr_summary_su_u\",\n            \"help\": argparse.SUPPRESS})\n        argument_list.append({\n            \"opts\": (\"-bs\", ),\n            \"type\": int,\n            \"dest\": \"depr_batch-size_bs_b\",\n            \"help\": argparse.SUPPRESS})\n        argument_list.append({\n            \"opts\": (\"-it\", ),\n            \"type\": int,\n            \"dest\": \"depr_iterations_it_i\",\n            \"help\": argparse.SUPPRESS})\n        argument_list.append({\n            \"opts\": (\"-nl\", ),\n            \"action\": \"store_true\",\n            \"dest\": \"depr_no-logs_nl_n\",\n            \"help\": argparse.SUPPRESS})\n        argument_list.append({\n            \"opts\": (\"-ss\", ),\n            \"type\": int,\n            \"dest\": \"depr_snapshot-interval_ss_I\",\n            \"help\": argparse.SUPPRESS})\n        argument_list.append({\n            \"opts\": (\"-tia\", ),\n            \"type\": str,\n            \"dest\": \"depr_timelapse-input-A_tia_x\",\n            \"help\": argparse.SUPPRESS})\n        argument_list.append({\n            \"opts\": (\"-tib\", ),\n            \"type\": str,\n            \"dest\": \"depr_timelapse-input-B_tib_y\",\n            \"help\": argparse.SUPPRESS})\n        argument_list.append({\n            \"opts\": (\"-to\", ),\n            \"type\": str,\n            \"dest\": \"depr_timelapse-output_to_z\",\n            \"help\": argparse.SUPPRESS})\n        argument_list.append({\n            \"opts\": (\"-wl\", ),\n            \"action\": \"store_true\",\n            \"dest\": \"depr_warp-to-landmarks_wl_M\",\n            \"help\": argparse.SUPPRESS})\n        argument_list.append({\n            \"opts\": (\"-nf\", ),\n            \"action\": \"store_true\",\n            \"dest\": \"depr_no-flip_nf_P\",\n            \"help\": argparse.SUPPRESS})\n        argument_list.append({\n            \"opts\": (\"-nac\", ),\n            \"action\": \"store_true\",\n            \"dest\": \"depr_no-augment-color_nac_c\",\n            \"help\": argparse.SUPPRESS})\n        argument_list.append({\n            \"opts\": (\"-nw\", ),\n            \"action\": \"store_true\",\n            \"dest\": \"depr_no-warp_nw_W\",\n            \"help\": argparse.SUPPRESS})\n        return argument_list\n", "lib/cli/args_extract_convert.py": "#!/usr/bin/env python3\n\"\"\" The Command Line Argument options for extracting and converting with faceswap.py \"\"\"\nimport argparse\nimport gettext\nimport typing as T\n\nfrom lib.utils import get_backend\nfrom plugins.plugin_loader import PluginLoader\n\nfrom .actions import (DirFullPaths, DirOrFileFullPaths, DirOrFilesFullPaths, FileFullPaths,\n                      FilesFullPaths, MultiOption, Radio, Slider)\nfrom .args import FaceSwapArgs\n\n\n# LOCALES\n_LANG = gettext.translation(\"lib.cli.args_extract_convert\", localedir=\"locales\", fallback=True)\n_ = _LANG.gettext\n\n\nclass ExtractConvertArgs(FaceSwapArgs):\n    \"\"\" Parent class to capture arguments that will be used in both extract and convert processes.\n\n    Extract and Convert share a fair amount of arguments, so arguments that can be used in both of\n    these processes should be placed here.\n\n    No further processing is done in this class (this is handled by the children), this just\n    captures the shared arguments.\n    \"\"\"\n\n    @staticmethod\n    def get_argument_list() -> list[dict[str, T.Any]]:\n        \"\"\" Returns the argument list for shared Extract and Convert arguments.\n\n        Returns\n        -------\n        list\n            The list of command line options for the given Extract and Convert\n        \"\"\"\n        argument_list: list[dict[str, T.Any]] = []\n        argument_list.append({\n            \"opts\": (\"-i\", \"--input-dir\"),\n            \"action\": DirOrFileFullPaths,\n            \"filetypes\": \"video\",\n            \"dest\": \"input_dir\",\n            \"required\": True,\n            \"group\": _(\"Data\"),\n            \"help\": _(\n                \"Input directory or video. Either a directory containing the image files you wish \"\n                \"to process or path to a video file. NB: This should be the source video/frames \"\n                \"NOT the source faces.\")})\n        argument_list.append({\n            \"opts\": (\"-o\", \"--output-dir\"),\n            \"action\": DirFullPaths,\n            \"dest\": \"output_dir\",\n            \"required\": True,\n            \"group\": _(\"Data\"),\n            \"help\": _(\"Output directory. This is where the converted files will be saved.\")})\n        argument_list.append({\n            \"opts\": (\"-p\", \"--alignments\"),\n            \"action\": FileFullPaths,\n            \"filetypes\": \"alignments\",\n            \"type\": str,\n            \"dest\": \"alignments_path\",\n            \"group\": _(\"Data\"),\n            \"help\": _(\n                \"Optional path to an alignments file. Leave blank if the alignments file is at \"\n                \"the default location.\")})\n        # Deprecated multi-character switches\n        argument_list.append({\n            \"opts\": (\"-al\", ),\n            \"action\": FileFullPaths,\n            \"filetypes\": \"alignments\",\n            \"type\": str,\n            \"dest\": \"depr_alignments_al_p\",\n            \"help\": argparse.SUPPRESS})\n        return argument_list\n\n\nclass ExtractArgs(ExtractConvertArgs):\n    \"\"\" Creates the command line arguments for extraction.\n\n    This class inherits base options from :class:`ExtractConvertArgs` where arguments that are used\n    for both Extract and Convert should be placed.\n\n    Commands explicit to Extract should be added in :func:`get_optional_arguments`\n    \"\"\"\n\n    @staticmethod\n    def get_info() -> str:\n        \"\"\" The information text for the Extract command.\n\n        Returns\n        -------\n        str\n            The information text for the Extract command.\n        \"\"\"\n        return _(\"Extract faces from image or video sources.\\n\"\n                 \"Extraction plugins can be configured in the 'Settings' Menu\")\n\n    @staticmethod\n    def get_optional_arguments() -> list[dict[str, T.Any]]:\n        \"\"\" Returns the argument list unique to the Extract command.\n\n        Returns\n        -------\n        list\n            The list of optional command line options for the Extract command\n        \"\"\"\n        if get_backend() == \"cpu\":\n            default_detector = \"mtcnn\"\n            default_aligner = \"cv2-dnn\"\n        else:\n            default_detector = \"s3fd\"\n            default_aligner = \"fan\"\n\n        argument_list: list[dict[str, T.Any]] = []\n        argument_list.append({\n            \"opts\": (\"-b\", \"--batch-mode\"),\n            \"action\": \"store_true\",\n            \"dest\": \"batch_mode\",\n            \"default\": False,\n            \"group\": _(\"Data\"),\n            \"help\": _(\n                \"R|If selected then the input_dir should be a parent folder containing multiple \"\n                \"videos and/or folders of images you wish to extract from. The faces will be \"\n                \"output to separate sub-folders in the output_dir.\")})\n        argument_list.append({\n            \"opts\": (\"-D\", \"--detector\"),\n            \"action\": Radio,\n            \"type\": str.lower,\n            \"default\": default_detector,\n            \"choices\": PluginLoader.get_available_extractors(\"detect\"),\n            \"group\": _(\"Plugins\"),\n            \"help\": _(\n                \"R|Detector to use. Some of these have configurable settings in \"\n                \"'/config/extract.ini' or 'Settings > Configure Extract 'Plugins':\"\n                \"\\nL|cv2-dnn: A CPU only extractor which is the least reliable and least resource \"\n                \"intensive. Use this if not using a GPU and time is important.\"\n                \"\\nL|mtcnn: Good detector. Fast on CPU, faster on GPU. Uses fewer resources than \"\n                \"other GPU detectors but can often return more false positives.\"\n                \"\\nL|s3fd: Best detector. Slow on CPU, faster on GPU. Can detect more faces and \"\n                \"fewer false positives than other GPU detectors, but is a lot more resource \"\n                \"intensive.\"\n                \"\\nL|external: Import a face detection bounding box from a json file. (\"\n                \"configurable in Detect settings)\")})\n        argument_list.append({\n            \"opts\": (\"-A\", \"--aligner\"),\n            \"action\": Radio,\n            \"type\": str.lower,\n            \"default\": default_aligner,\n            \"choices\": PluginLoader.get_available_extractors(\"align\"),\n            \"group\": _(\"Plugins\"),\n            \"help\": _(\n                \"R|Aligner to use.\"\n                \"\\nL|cv2-dnn: A CPU only landmark detector. Faster, less resource intensive, but \"\n                \"less accurate. Only use this if not using a GPU and time is important.\"\n                \"\\nL|fan: Best aligner. Fast on GPU, slow on CPU.\"\n                \"\\nL|external: Import 68 point 2D landmarks or an aligned bounding box from a \"\n                \"json file. (configurable in Align settings)\")})\n        argument_list.append({\n            \"opts\": (\"-M\", \"--masker\"),\n            \"action\": MultiOption,\n            \"type\": str.lower,\n            \"nargs\": \"+\",\n            \"choices\": [mask for mask in PluginLoader.get_available_extractors(\"mask\")\n                        if mask not in (\"components\", \"extended\")],\n            \"group\": _(\"Plugins\"),\n            \"help\": _(\n                \"R|Additional Masker(s) to use. The masks generated here will all take up GPU \"\n                \"RAM. You can select none, one or multiple masks, but the extraction may take \"\n                \"longer the more you select. NB: The Extended and Components (landmark based) \"\n                \"masks are automatically generated on extraction.\"\n                \"\\nL|bisenet-fp: Relatively lightweight NN based mask that provides more refined \"\n                \"control over the area to be masked including full head masking (configurable in \"\n                \"mask settings).\"\n                \"\\nL|custom: A dummy mask that fills the mask area with all 1s or 0s (\"\n                \"configurable in settings). This is only required if you intend to manually edit \"\n                \"the custom masks yourself in the manual tool. This mask does not use the GPU so \"\n                \"will not use any additional VRAM.\"\n                \"\\nL|vgg-clear: Mask designed to provide smart segmentation of mostly frontal \"\n                \"faces clear of obstructions. Profile faces and obstructions may result in \"\n                \"sub-par performance.\"\n                \"\\nL|vgg-obstructed: Mask designed to provide smart segmentation of mostly \"\n                \"frontal faces. The mask model has been specifically trained to recognize some \"\n                \"facial obstructions (hands and eyeglasses). Profile faces may result in sub-par \"\n                \"performance.\"\n                \"\\nL|unet-dfl: Mask designed to provide smart segmentation of mostly frontal \"\n                \"faces. The mask model has been trained by community members and will need \"\n                \"testing for further description. Profile faces may result in sub-par \"\n                \"performance.\"\n                \"\\nThe auto generated masks are as follows:\"\n                \"\\nL|components: Mask designed to provide facial segmentation based on the \"\n                \"positioning of landmark locations. A convex hull is constructed around the \"\n                \"exterior of the landmarks to create a mask.\"\n                \"\\nL|extended: Mask designed to provide facial segmentation based on the \"\n                \"positioning of landmark locations. A convex hull is constructed around the \"\n                \"exterior of the landmarks and the mask is extended upwards onto the forehead.\"\n                \"\\n(eg: `-M unet-dfl vgg-clear`, `--masker vgg-obstructed`)\")})\n        argument_list.append({\n            \"opts\": (\"-O\", \"--normalization\"),\n            \"action\": Radio,\n            \"type\": str.lower,\n            \"dest\": \"normalization\",\n            \"default\": \"none\",\n            \"choices\": [\"none\", \"clahe\", \"hist\", \"mean\"],\n            \"group\": _(\"Plugins\"),\n            \"help\": _(\n                \"R|Performing normalization can help the aligner better align faces with \"\n                \"difficult lighting conditions at an extraction speed cost. Different methods \"\n                \"will yield different results on different sets. NB: This does not impact the \"\n                \"output face, just the input to the aligner.\"\n                \"\\nL|none: Don't perform normalization on the face.\"\n                \"\\nL|clahe: Perform Contrast Limited Adaptive Histogram Equalization on the face.\"\n                \"\\nL|hist: Equalize the histograms on the RGB channels.\"\n                \"\\nL|mean: Normalize the face colors to the mean.\")})\n        argument_list.append({\n            \"opts\": (\"-R\", \"--re-feed\"),\n            \"action\": Slider,\n            \"min_max\": (0, 10),\n            \"rounding\": 1,\n            \"type\": int,\n            \"dest\": \"re_feed\",\n            \"default\": 0,\n            \"group\": _(\"Plugins\"),\n            \"help\": _(\n                \"The number of times to re-feed the detected face into the aligner. Each time the \"\n                \"face is re-fed into the aligner the bounding box is adjusted by a small amount. \"\n                \"The final landmarks are then averaged from each iteration. Helps to remove \"\n                \"'micro-jitter' but at the cost of slower extraction speed. The more times the \"\n                \"face is re-fed into the aligner, the less micro-jitter should occur but the \"\n                \"longer extraction will take.\")})\n        argument_list.append({\n            \"opts\": (\"-a\", \"--re-align\"),\n            \"action\": \"store_true\",\n            \"dest\": \"re_align\",\n            \"default\": False,\n            \"group\": _(\"Plugins\"),\n            \"help\": _(\n                \"Re-feed the initially found aligned face through the aligner. Can help produce \"\n                \"better alignments for faces that are rotated beyond 45 degrees in the frame or \"\n                \"are at extreme angles. Slows down extraction.\")})\n        argument_list.append({\n            \"opts\": (\"-r\", \"--rotate-images\"),\n            \"type\": str,\n            \"dest\": \"rotate_images\",\n            \"default\": None,\n            \"group\": _(\"Plugins\"),\n            \"help\": _(\n                \"If a face isn't found, rotate the images to try to find a face. Can find more \"\n                \"faces at the cost of extraction speed. Pass in a single number to use increments \"\n                \"of that size up to 360, or pass in a list of numbers to enumerate exactly what \"\n                \"angles to check.\")})\n        argument_list.append({\n            \"opts\": (\"-I\", \"--identity\"),\n            \"action\": \"store_true\",\n            \"default\": False,\n            \"group\": _(\"Plugins\"),\n            \"help\": _(\n                \"Obtain and store face identity encodings from VGGFace2. Slows down extract a \"\n                \"little, but will save time if using 'sort by face'\")})\n        argument_list.append({\n            \"opts\": (\"-m\", \"--min-size\"),\n            \"action\": Slider,\n            \"min_max\": (0, 1080),\n            \"rounding\": 20,\n            \"type\": int,\n            \"dest\": \"min_size\",\n            \"default\": 0,\n            \"group\": _(\"Face Processing\"),\n            \"help\": _(\n                \"Filters out faces detected below this size. Length, in pixels across the \"\n                \"diagonal of the bounding box. Set to 0 for off\")})\n        argument_list.append({\n            \"opts\": (\"-n\", \"--nfilter\"),\n            \"action\": DirOrFilesFullPaths,\n            \"filetypes\": \"image\",\n            \"dest\": \"nfilter\",\n            \"default\": None,\n            \"nargs\": \"+\",\n            \"group\": _(\"Face Processing\"),\n            \"help\": _(\n                \"Optionally filter out people who you do not wish to extract by passing in images \"\n                \"of those people. Should be a small variety of images at different angles and in \"\n                \"different conditions. A folder containing the required images or multiple image \"\n                \"files, space separated, can be selected.\")})\n        argument_list.append({\n            \"opts\": (\"-f\", \"--filter\"),\n            \"action\": DirOrFilesFullPaths,\n            \"filetypes\": \"image\",\n            \"dest\": \"filter\",\n            \"default\": None,\n            \"nargs\": \"+\",\n            \"group\": _(\"Face Processing\"),\n            \"help\": _(\n                \"Optionally select people you wish to extract by passing in images of that \"\n                \"person. Should be a small variety of images at different angles and in different \"\n                \"conditions A folder containing the required images or multiple image files, \"\n                \"space separated, can be selected.\")})\n        argument_list.append({\n            \"opts\": (\"-l\", \"--ref_threshold\"),\n            \"action\": Slider,\n            \"min_max\": (0.01, 0.99),\n            \"rounding\": 2,\n            \"type\": float,\n            \"dest\": \"ref_threshold\",\n            \"default\": 0.60,\n            \"group\": _(\"Face Processing\"),\n            \"help\": _(\n                \"For use with the optional nfilter/filter files. Threshold for positive face \"\n                \"recognition. Higher values are stricter.\")})\n        argument_list.append({\n            \"opts\": (\"-z\", \"--size\"),\n            \"action\": Slider,\n            \"min_max\": (256, 1024),\n            \"rounding\": 64,\n            \"type\": int,\n            \"default\": 512,\n            \"group\": _(\"output\"),\n            \"help\": _(\n                \"The output size of extracted faces. Make sure that the model you intend to train \"\n                \"supports your required size. This will only need to be changed for hi-res \"\n                \"models.\")})\n        argument_list.append({\n            \"opts\": (\"-N\", \"--extract-every-n\"),\n            \"action\": Slider,\n            \"min_max\": (1, 100),\n            \"rounding\": 1,\n            \"type\": int,\n            \"dest\": \"extract_every_n\",\n            \"default\": 1,\n            \"group\": _(\"output\"),\n            \"help\": _(\n                \"Extract every 'nth' frame. This option will skip frames when extracting faces. \"\n                \"For example a value of 1 will extract faces from every frame, a value of 10 will \"\n                \"extract faces from every 10th frame.\")})\n        argument_list.append({\n            \"opts\": (\"-v\", \"--save-interval\"),\n            \"action\": Slider,\n            \"min_max\": (0, 1000),\n            \"rounding\": 10,\n            \"type\": int,\n            \"dest\": \"save_interval\",\n            \"default\": 0,\n            \"group\": _(\"output\"),\n            \"help\": _(\n                \"Automatically save the alignments file after a set amount of frames. By default \"\n                \"the alignments file is only saved at the end of the extraction process. NB: If \"\n                \"extracting in 2 passes then the alignments file will only start to be saved out \"\n                \"during the second pass. WARNING: Don't interrupt the script when writing the \"\n                \"file because it might get corrupted. Set to 0 to turn off\")})\n        argument_list.append({\n            \"opts\": (\"-B\", \"--debug-landmarks\"),\n            \"action\": \"store_true\",\n            \"dest\": \"debug_landmarks\",\n            \"default\": False,\n            \"group\": _(\"output\"),\n            \"help\": _(\"Draw landmarks on the ouput faces for debugging purposes.\")})\n        argument_list.append({\n            \"opts\": (\"-P\", \"--singleprocess\"),\n            \"action\": \"store_true\",\n            \"default\": False,\n            \"backend\": (\"nvidia\", \"directml\", \"rocm\", \"apple_silicon\"),\n            \"group\": _(\"settings\"),\n            \"help\": _(\n                \"Don't run extraction in parallel. Will run each part of the extraction process \"\n                \"separately (one after the other) rather than all at the same time. Useful if \"\n                \"VRAM is at a premium.\")})\n        argument_list.append({\n            \"opts\": (\"-s\", \"--skip-existing\"),\n            \"action\": \"store_true\",\n            \"dest\": \"skip_existing\",\n            \"default\": False,\n            \"group\": _(\"settings\"),\n            \"help\": _(\n                \"Skips frames that have already been extracted and exist in the alignments file\")})\n        argument_list.append({\n            \"opts\": (\"-e\", \"--skip-existing-faces\"),\n            \"action\": \"store_true\",\n            \"dest\": \"skip_faces\",\n            \"default\": False,\n            \"group\": _(\"settings\"),\n            \"help\": _(\"Skip frames that already have detected faces in the alignments file\")})\n        argument_list.append({\n            \"opts\": (\"-K\", \"--skip-saving-faces\"),\n            \"action\": \"store_true\",\n            \"dest\": \"skip_saving_faces\",\n            \"default\": False,\n            \"group\": _(\"settings\"),\n            \"help\": _(\"Skip saving the detected faces to disk. Just create an alignments file\")})\n        # Deprecated multi-character switches\n        argument_list.append({\n            \"opts\": (\"-min\", ),\n            \"type\": int,\n            \"dest\": \"depr_min-size_min_m\",\n            \"help\": argparse.SUPPRESS})\n        argument_list.append({\n            \"opts\": (\"-een\", ),\n            \"type\": int,\n            \"dest\": \"depr_extract-every-n_een_N\",\n            \"help\": argparse.SUPPRESS})\n        argument_list.append({\n            \"opts\": (\"-nm\",),\n            \"type\": str.lower,\n            \"dest\": \"depr_normalization_nm_O\",\n            \"choices\": [\"none\", \"clahe\", \"hist\", \"mean\"],\n            \"help\": argparse.SUPPRESS})\n        argument_list.append({\n            \"opts\": (\"-rf\", ),\n            \"type\": int,\n            \"dest\": \"depr_re-feed_rf_R\",\n            \"help\": argparse.SUPPRESS})\n        argument_list.append({\n            \"opts\": (\"-sz\", ),\n            \"type\": int,\n            \"dest\": \"depr_size_sz_z\",\n            \"help\": argparse.SUPPRESS})\n        argument_list.append({\n            \"opts\": (\"-si\", ),\n            \"type\": int,\n            \"dest\": \"depr_save-interval_si_v\",\n            \"help\": argparse.SUPPRESS})\n        argument_list.append({\n            \"opts\": (\"-dl\", ),\n            \"action\": \"store_true\",\n            \"dest\": \"depr_debug-landmarks_dl_B\",\n            \"help\": argparse.SUPPRESS})\n        argument_list.append({\n            \"opts\": (\"-sp\", ),\n            \"dest\": \"depr_singleprocess_sp_P\",\n            \"action\": \"store_true\",\n            \"help\": argparse.SUPPRESS})\n        argument_list.append({\n            \"opts\": (\"-sf\", ),\n            \"action\": \"store_true\",\n            \"dest\": \"depr_skip-existing-faces_sf_e\",\n            \"help\": argparse.SUPPRESS})\n        argument_list.append({\n            \"opts\": (\"-ssf\", ),\n            \"action\": \"store_true\",\n            \"dest\": \"depr_skip-saving-faces_ssf_K\",\n            \"help\": argparse.SUPPRESS})\n        return argument_list\n\n\nclass ConvertArgs(ExtractConvertArgs):\n    \"\"\" Creates the command line arguments for conversion.\n\n    This class inherits base options from :class:`ExtractConvertArgs` where arguments that are used\n    for both Extract and Convert should be placed.\n\n    Commands explicit to Convert should be added in :func:`get_optional_arguments`\n    \"\"\"\n\n    @staticmethod\n    def get_info() -> str:\n        \"\"\" The information text for the Convert command.\n\n        Returns\n        -------\n        str\n            The information text for the Convert command.\n        \"\"\"\n        return _(\"Swap the original faces in a source video/images to your final faces.\\n\"\n                 \"Conversion plugins can be configured in the 'Settings' Menu\")\n\n    @staticmethod\n    def get_optional_arguments() -> list[dict[str, T.Any]]:\n        \"\"\" Returns the argument list unique to the Convert command.\n\n        Returns\n        -------\n        list\n            The list of optional command line options for the Convert command\n        \"\"\"\n\n        argument_list: list[dict[str, T.Any]] = []\n        argument_list.append({\n            \"opts\": (\"-r\", \"--reference-video\"),\n            \"action\": FileFullPaths,\n            \"filetypes\": \"video\",\n            \"type\": str,\n            \"dest\": \"reference_video\",\n            \"group\": _(\"Data\"),\n            \"help\": _(\n                \"Only required if converting from images to video. Provide The original video \"\n                \"that the source frames were extracted from (for extracting the fps and audio).\")})\n        argument_list.append({\n            \"opts\": (\"-m\", \"--model-dir\"),\n            \"action\": DirFullPaths,\n            \"dest\": \"model_dir\",\n            \"required\": True,\n            \"group\": _(\"Data\"),\n            \"help\": _(\n                \"Model directory. The directory containing the trained model you wish to use for \"\n                \"conversion.\")})\n        argument_list.append({\n            \"opts\": (\"-c\", \"--color-adjustment\"),\n            \"action\": Radio,\n            \"type\": str.lower,\n            \"dest\": \"color_adjustment\",\n            \"default\": \"avg-color\",\n            \"choices\": PluginLoader.get_available_convert_plugins(\"color\", True),\n            \"group\": _(\"Plugins\"),\n            \"help\": _(\n                \"R|Performs color adjustment to the swapped face. Some of these options have \"\n                \"configurable settings in '/config/convert.ini' or 'Settings > Configure Convert \"\n                \"Plugins':\"\n                \"\\nL|avg-color: Adjust the mean of each color channel in the swapped \"\n                \"reconstruction to equal the mean of the masked area in the original image.\"\n                \"\\nL|color-transfer: Transfers the color distribution from the source to the \"\n                \"target image using the mean and standard deviations of the L*a*b* color space.\"\n                \"\\nL|manual-balance: Manually adjust the balance of the image in a variety of \"\n                \"color spaces. Best used with the Preview tool to set correct values.\"\n                \"\\nL|match-hist: Adjust the histogram of each color channel in the swapped \"\n                \"reconstruction to equal the histogram of the masked area in the original image.\"\n                \"\\nL|seamless-clone: Use cv2's seamless clone function to remove extreme \"\n                \"gradients at the mask seam by smoothing colors. Generally does not give very \"\n                \"satisfactory results.\"\n                \"\\nL|none: Don't perform color adjustment.\")})\n        argument_list.append({\n            \"opts\": (\"-M\", \"--mask-type\"),\n            \"action\": Radio,\n            \"type\": str.lower,\n            \"dest\": \"mask_type\",\n            \"default\": \"extended\",\n            \"choices\": PluginLoader.get_available_extractors(\"mask\",\n                                                             add_none=True,\n                                                             extend_plugin=True) + [\"predicted\"],\n            \"group\": _(\"Plugins\"),\n            \"help\": _(\n                \"R|Masker to use. NB: The mask you require must exist within the alignments file. \"\n                \"You can add additional masks with the Mask Tool.\"\n                \"\\nL|none: Don't use a mask.\"\n                \"\\nL|bisenet-fp_face: Relatively lightweight NN based mask that provides more \"\n                \"refined control over the area to be masked (configurable in mask settings). Use \"\n                \"this version of bisenet-fp if your model is trained with 'face' or \"\n                \"'legacy' centering.\"\n                \"\\nL|bisenet-fp_head: Relatively lightweight NN based mask that provides more \"\n                \"refined control over the area to be masked (configurable in mask settings). Use \"\n                \"this version of bisenet-fp if your model is trained with 'head' centering.\"\n                \"\\nL|custom_face: Custom user created, face centered mask.\"\n                \"\\nL|custom_head: Custom user created, head centered mask.\"\n                \"\\nL|components: Mask designed to provide facial segmentation based on the \"\n                \"positioning of landmark locations. A convex hull is constructed around the \"\n                \"exterior of the landmarks to create a mask.\"\n                \"\\nL|extended: Mask designed to provide facial segmentation based on the \"\n                \"positioning of landmark locations. A convex hull is constructed around the \"\n                \"exterior of the landmarks and the mask is extended upwards onto the forehead.\"\n                \"\\nL|vgg-clear: Mask designed to provide smart segmentation of mostly frontal \"\n                \"faces clear of obstructions. Profile faces and obstructions may result in sub-\"\n                \"par performance.\"\n                \"\\nL|vgg-obstructed: Mask designed to provide smart segmentation of mostly \"\n                \"frontal faces. The mask model has been specifically trained to recognize some \"\n                \"facial obstructions (hands and eyeglasses). Profile faces may result in sub-par \"\n                \"performance.\"\n                \"\\nL|unet-dfl: Mask designed to provide smart segmentation of mostly frontal \"\n                \"faces. The mask model has been trained by community members and will need \"\n                \"testing for further description. Profile faces may result in sub-par \"\n                \"performance.\"\n                \"\\nL|predicted: If the 'Learn Mask' option was enabled during training, this will \"\n                \"use the mask that was created by the trained model.\")})\n        argument_list.append({\n            \"opts\": (\"-w\", \"--writer\"),\n            \"action\": Radio,\n            \"type\": str,\n            \"default\": \"opencv\",\n            \"choices\": PluginLoader.get_available_convert_plugins(\"writer\", False),\n            \"group\": _(\"Plugins\"),\n            \"help\": _(\n                \"R|The plugin to use to output the converted images. The writers are configurable \"\n                \"in '/config/convert.ini' or 'Settings > Configure Convert Plugins:'\"\n                \"\\nL|ffmpeg: [video] Writes out the convert straight to video. When the input is \"\n                \"a series of images then the '-ref' (--reference-video) parameter must be set.\"\n                \"\\nL|gif: [animated image] Create an animated gif.\"\n                \"\\nL|opencv: [images] The fastest image writer, but less options and formats than \"\n                \"other plugins.\"\n                \"\\nL|patch: [images] Outputs the raw swapped face patch, along with the \"\n                \"transformation matrix required to re-insert the face back into the original \"\n                \"frame. Use this option if you wish to post-process and composite the final face \"\n                \"within external tools.\"\n                \"\\nL|pillow: [images] Slower than opencv, but has more options and supports more \"\n                \"formats.\")})\n        argument_list.append({\n            \"opts\": (\"-O\", \"--output-scale\"),\n            \"action\": Slider,\n            \"min_max\": (25, 400),\n            \"rounding\": 1,\n            \"type\": int,\n            \"dest\": \"output_scale\",\n            \"default\": 100,\n            \"group\": _(\"Frame Processing\"),\n            \"help\": _(\n                \"Scale the final output frames by this amount. 100%% will output the frames at \"\n                \"source dimensions. 50%% at half size 200%% at double size\")})\n        argument_list.append({\n            \"opts\": (\"-R\", \"--frame-ranges\"),\n            \"type\": str,\n            \"nargs\": \"+\",\n            \"dest\": \"frame_ranges\",\n            \"group\": _(\"Frame Processing\"),\n            \"help\": _(\n                \"Frame ranges to apply transfer to e.g. For frames 10 to 50 and 90 to 100 use \"\n                \"--frame-ranges 10-50 90-100. Frames falling outside of the selected range will \"\n                \"be discarded unless '-k' (--keep-unchanged) is selected. NB: If you are \"\n                \"converting from images, then the filenames must end with the frame-number!\")})\n        argument_list.append({\n            \"opts\": (\"-S\", \"--face-scale\"),\n            \"action\": Slider,\n            \"min_max\": (-10.0, 10.0),\n            \"rounding\": 2,\n            \"dest\": \"face_scale\",\n            \"type\": float,\n            \"default\": 0.0,\n            \"group\": _(\"Face Processing\"),\n            \"help\": _(\n                \"Scale the swapped face by this percentage. Positive values will enlarge the \"\n                \"face, Negative values will shrink the face.\")})\n        argument_list.append({\n            \"opts\": (\"-a\", \"--input-aligned-dir\"),\n            \"action\": DirFullPaths,\n            \"dest\": \"input_aligned_dir\",\n            \"default\": None,\n            \"group\": _(\"Face Processing\"),\n            \"help\": _(\n                \"If you have not cleansed your alignments file, then you can filter out faces by \"\n                \"defining a folder here that contains the faces extracted from your input files/\"\n                \"video. If this folder is defined, then only faces that exist within your \"\n                \"alignments file and also exist within the specified folder will be converted. \"\n                \"Leaving this blank will convert all faces that exist within the alignments \"\n                \"file.\")})\n        argument_list.append({\n            \"opts\": (\"-n\", \"--nfilter\"),\n            \"action\": FilesFullPaths,\n            \"filetypes\": \"image\",\n            \"dest\": \"nfilter\",\n            \"default\": None,\n            \"nargs\": \"+\",\n            \"group\": _(\"Face Processing\"),\n            \"help\": _(\n                \"Optionally filter out people who you do not wish to process by passing in an \"\n                \"image of that person. Should be a front portrait with a single person in the \"\n                \"image. Multiple images can be added space separated. NB: Using face filter will \"\n                \"significantly decrease extraction speed and its accuracy cannot be guaranteed.\")})\n        argument_list.append({\n            \"opts\": (\"-f\", \"--filter\"),\n            \"action\": FilesFullPaths,\n            \"filetypes\": \"image\",\n            \"dest\": \"filter\",\n            \"default\": None,\n            \"nargs\": \"+\",\n            \"group\": _(\"Face Processing\"),\n            \"help\": _(\n                \"Optionally select people you wish to process by passing in an image of that \"\n                \"person. Should be a front portrait with a single person in the image. Multiple \"\n                \"images can be added space separated. NB: Using face filter will significantly \"\n                \"decrease extraction speed and its accuracy cannot be guaranteed.\")})\n        argument_list.append({\n            \"opts\": (\"-l\", \"--ref_threshold\"),\n            \"action\": Slider,\n            \"min_max\": (0.01, 0.99),\n            \"rounding\": 2,\n            \"type\": float,\n            \"dest\": \"ref_threshold\",\n            \"default\": 0.4,\n            \"group\": _(\"Face Processing\"),\n            \"help\": _(\n                \"For use with the optional nfilter/filter files. Threshold for positive face \"\n                \"recognition. Lower values are stricter. NB: Using face filter will significantly \"\n                \"decrease extraction speed and its accuracy cannot be guaranteed.\")})\n        argument_list.append({\n            \"opts\": (\"-j\", \"--jobs\"),\n            \"action\": Slider,\n            \"min_max\": (0, 40),\n            \"rounding\": 1,\n            \"type\": int,\n            \"dest\": \"jobs\",\n            \"default\": 0,\n            \"group\": _(\"settings\"),\n            \"help\": _(\n                \"The maximum number of parallel processes for performing conversion. Converting \"\n                \"images is system RAM heavy so it is possible to run out of memory if you have a \"\n                \"lot of processes and not enough RAM to accommodate them all. Setting this to 0 \"\n                \"will use the maximum available. No matter what you set this to, it will never \"\n                \"attempt to use more processes than are available on your system. If \"\n                \"singleprocess is enabled this setting will be ignored.\")})\n        argument_list.append({\n            \"opts\": (\"-T\", \"--on-the-fly\"),\n            \"action\": \"store_true\",\n            \"dest\": \"on_the_fly\",\n            \"default\": False,\n            \"group\": _(\"settings\"),\n            \"help\": _(\n                \"Enable On-The-Fly Conversion. NOT recommended. You should generate a clean \"\n                \"alignments file for your destination video. However, if you wish you can \"\n                \"generate the alignments on-the-fly by enabling this option. This will use an \"\n                \"inferior extraction pipeline and will lead to substandard results. If an \"\n                \"alignments file is found, this option will be ignored.\")})\n        argument_list.append({\n            \"opts\": (\"-k\", \"--keep-unchanged\"),\n            \"action\": \"store_true\",\n            \"dest\": \"keep_unchanged\",\n            \"default\": False,\n            \"group\": _(\"Frame Processing\"),\n            \"help\": _(\n                \"When used with --frame-ranges outputs the unchanged frames that are not \"\n                \"processed instead of discarding them.\")})\n        argument_list.append({\n            \"opts\": (\"-s\", \"--swap-model\"),\n            \"action\": \"store_true\",\n            \"dest\": \"swap_model\",\n            \"default\": False,\n            \"group\": _(\"settings\"),\n            \"help\": _(\"Swap the model. Instead converting from of A -> B, converts B -> A\")})\n        argument_list.append({\n            \"opts\": (\"-P\", \"--singleprocess\"),\n            \"action\": \"store_true\",\n            \"default\": False,\n            \"group\": _(\"settings\"),\n            \"help\": _(\"Disable multiprocessing. Slower but less resource intensive.\")})\n        # Deprecated multi-character switches\n        argument_list.append({\n            \"opts\": (\"-sp\", ),\n            \"action\": \"store_true\",\n            \"dest\": \"depr_singleprocess_sp_P\",\n            \"help\": argparse.SUPPRESS})\n        argument_list.append({\n            \"opts\": (\"-ref\", ),\n            \"type\": str,\n            \"dest\": \"depr_reference-video_ref_r\",\n            \"help\": argparse.SUPPRESS})\n        argument_list.append({\n            \"opts\": (\"-fr\", ),\n            \"type\": str,\n            \"nargs\": \"+\",\n            \"dest\": \"depr_frame-ranges_fr_R\",\n            \"help\": argparse.SUPPRESS})\n        argument_list.append({\n            \"opts\": (\"-osc\", ),\n            \"type\": int,\n            \"dest\": \"depr_output-scale_osc_O\",\n            \"help\": argparse.SUPPRESS})\n        argument_list.append({\n            \"opts\": (\"-otf\", ),\n            \"action\": \"store_true\",\n            \"dest\": \"depr_on-the-fly_otf_T\",\n            \"help\": argparse.SUPPRESS})\n        return argument_list\n", "lib/cli/args.py": "#!/usr/bin/env python3\n\"\"\" The global and GUI Command Line Argument options for faceswap.py \"\"\"\n\nimport argparse\nimport gettext\nimport logging\nimport re\nimport sys\nimport textwrap\nimport typing as T\n\nfrom lib.utils import get_backend\nfrom lib.gpu_stats import GPUStats\n\nfrom .actions import FileFullPaths, MultiOption, SaveFileFullPaths\nfrom .launcher import ScriptExecutor\n\nlogger = logging.getLogger(__name__)\n_GPUS = GPUStats().cli_devices\n\n# LOCALES\n_LANG = gettext.translation(\"lib.cli.args\", localedir=\"locales\", fallback=True)\n_ = _LANG.gettext\n\n\nclass FullHelpArgumentParser(argparse.ArgumentParser):\n    \"\"\" Extends :class:`argparse.ArgumentParser` to output full help on bad arguments. \"\"\"\n    def error(self, message: str) -> T.NoReturn:\n        self.print_help(sys.stderr)\n        self.exit(2, f\"{self.prog}: error: {message}\\n\")\n\n\nclass SmartFormatter(argparse.HelpFormatter):\n    \"\"\" Extends the class :class:`argparse.HelpFormatter` to allow custom formatting in help text.\n\n    Adapted from: https://stackoverflow.com/questions/3853722\n\n    Notes\n    -----\n    Prefix help text with \"R|\" to override default formatting and use explicitly defined formatting\n    within the help text.\n    Prefixing a new line within the help text with \"L|\" will turn that line into a list item in\n    both the cli help text and the GUI.\n    \"\"\"\n    def __init__(self,\n                 prog: str,\n                 indent_increment: int = 2,\n                 max_help_position: int = 24,\n                 width: int | None = None) -> None:\n        super().__init__(prog, indent_increment, max_help_position, width)\n        self._whitespace_matcher_limited = re.compile(r'[ \\r\\f\\v]+', re.ASCII)\n\n    def _split_lines(self, text: str, width: int) -> list[str]:\n        \"\"\" Split the given text by the given display width.\n\n        If the text is not prefixed with \"R|\" then the standard\n        :func:`argparse.HelpFormatter._split_lines` function is used, otherwise raw\n        formatting is processed,\n\n        Parameters\n        ----------\n        text: str\n            The help text that is to be formatted for display\n        width: int\n            The display width, in characters, for the help text\n\n        Returns\n        -------\n        list\n            A list of split strings\n        \"\"\"\n        if text.startswith(\"R|\"):\n            text = self._whitespace_matcher_limited.sub(' ', text).strip()[2:]\n            output = []\n            for txt in text.splitlines():\n                indent = \"\"\n                if txt.startswith(\"L|\"):\n                    indent = \"    \"\n                    txt = f\"  - {txt[2:]}\"\n                output.extend(textwrap.wrap(txt, width, subsequent_indent=indent))\n            return output\n        return argparse.HelpFormatter._split_lines(self,  # pylint:disable=protected-access\n                                                   text,\n                                                   width)\n\n\nclass FaceSwapArgs():\n    \"\"\" Faceswap argument parser functions that are universal to all commands.\n\n    This is the parent class to all subsequent argparsers which holds global arguments that pertain\n    to all commands.\n\n    Process the incoming command line arguments, validates then launches the relevant faceswap\n    script with the given arguments.\n\n    Parameters\n    ----------\n    subparser: :class:`argparse._SubParsersAction` | None\n        The subparser for the given command. ``None`` if the class is being called for reading\n        rather than processing\n    command: str\n        The faceswap command that is to be executed\n    description: str, optional\n        The description for the given command. Default: \"default\"\n    \"\"\"\n    def __init__(self,\n                 subparser: argparse._SubParsersAction | None,\n                 command: str,\n                 description: str = \"default\") -> None:\n        self.global_arguments = self._get_global_arguments()\n        self.info: str = self.get_info()\n        self.argument_list = self.get_argument_list()\n        self.optional_arguments = self.get_optional_arguments()\n        self._process_suppressions()\n        if not subparser:\n            return\n        self.parser = self._create_parser(subparser, command, description)\n        self._add_arguments()\n        script = ScriptExecutor(command)\n        self.parser.set_defaults(func=script.execute_script)\n\n    @staticmethod\n    def get_info() -> str:\n        \"\"\" Returns the information text for the current command.\n\n        This function should be overridden with the actual command help text for each\n        commands' parser.\n\n        Returns\n        -------\n        str\n            The information text for this command.\n        \"\"\"\n        return \"\"\n\n    @staticmethod\n    def get_argument_list() -> list[dict[str, T.Any]]:\n        \"\"\" Returns the argument list for the current command.\n\n        The argument list should be a list of dictionaries pertaining to each option for a command.\n        This function should be overridden with the actual argument list for each command's\n        argument list.\n\n        See existing parsers for examples.\n\n        Returns\n        -------\n        list\n            The list of command line options for the given command\n        \"\"\"\n        argument_list: list[dict[str, T.Any]] = []\n        return argument_list\n\n    @staticmethod\n    def get_optional_arguments() -> list[dict[str, T.Any]]:\n        \"\"\" Returns the optional argument list for the current command.\n\n        The optional arguments list is not always required, but is used when there are shared\n        options between multiple commands (e.g. convert and extract). Only override if required.\n\n        Returns\n        -------\n        list\n            The list of optional command line options for the given command\n        \"\"\"\n        argument_list: list[dict[str, T.Any]] = []\n        return argument_list\n\n    @staticmethod\n    def _get_global_arguments() -> list[dict[str, T.Any]]:\n        \"\"\" Returns the global Arguments list that are required for ALL commands in Faceswap.\n\n        This method should NOT be overridden.\n\n        Returns\n        -------\n        list\n            The list of global command line options for all Faceswap commands.\n        \"\"\"\n        global_args: list[dict[str, T.Any]] = []\n        if _GPUS:\n            global_args.append({\n                \"opts\": (\"-X\", \"--exclude-gpus\"),\n                \"dest\": \"exclude_gpus\",\n                \"action\": MultiOption,\n                \"type\": str.lower,\n                \"nargs\": \"+\",\n                \"choices\": [str(idx) for idx in range(len(_GPUS))],\n                \"group\": _(\"Global Options\"),\n                \"help\": _(\n                    \"R|Exclude GPUs from use by Faceswap. Select the number(s) which correspond \"\n                    \"to any GPU(s) that you do not wish to be made available to Faceswap. \"\n                    \"Selecting all GPUs here will force Faceswap into CPU mode.\"\n                    \"\\nL|{}\".format(' \\nL|'.join(_GPUS)))})\n        global_args.append({\n            \"opts\": (\"-C\", \"--configfile\"),\n            \"action\": FileFullPaths,\n            \"filetypes\": \"ini\",\n            \"type\": str,\n            \"group\": _(\"Global Options\"),\n            \"help\": _(\n                \"Optionally overide the saved config with the path to a custom config file.\")})\n        global_args.append({\n            \"opts\": (\"-L\", \"--loglevel\"),\n            \"type\": str.upper,\n            \"dest\": \"loglevel\",\n            \"default\": \"INFO\",\n            \"choices\": (\"INFO\", \"VERBOSE\", \"DEBUG\", \"TRACE\"),\n            \"group\": _(\"Global Options\"),\n            \"help\": _(\n                \"Log level. Stick with INFO or VERBOSE unless you need to file an error report. \"\n                \"Be careful with TRACE as it will generate a lot of data\")})\n        global_args.append({\n            \"opts\": (\"-F\", \"--logfile\"),\n            \"action\": SaveFileFullPaths,\n            \"filetypes\": 'log',\n            \"type\": str,\n            \"dest\": \"logfile\",\n            \"default\": None,\n            \"group\": _(\"Global Options\"),\n            \"help\": _(\"Path to store the logfile. Leave blank to store in the faceswap folder\")})\n        # These are hidden arguments to indicate that the GUI/Colab is being used\n        global_args.append({\n            \"opts\": (\"-gui\", \"--gui\"),\n            \"action\": \"store_true\",\n            \"dest\": \"redirect_gui\",\n            \"default\": False,\n            \"help\": argparse.SUPPRESS})\n        # Deprecated multi-character switches\n        global_args.append({\n            \"opts\": (\"-LF\",),\n            \"action\": SaveFileFullPaths,\n            \"filetypes\": 'log',\n            \"type\": str,\n            \"dest\": \"depr_logfile_LF_F\",\n            \"help\": argparse.SUPPRESS})\n\n        return global_args\n\n    @staticmethod\n    def _create_parser(subparser: argparse._SubParsersAction,\n                       command: str,\n                       description: str) -> argparse.ArgumentParser:\n        \"\"\" Create the parser for the selected command.\n\n        Parameters\n        ----------\n        subparser: :class:`argparse._SubParsersAction`\n            The subparser for the given command\n        command: str\n            The faceswap command that is to be executed\n        description: str\n            The description for the given command\n\n\n        Returns\n        -------\n        :class:`~lib.cli.args.FullHelpArgumentParser`\n            The parser for the given command\n        \"\"\"\n        parser = subparser.add_parser(command,\n                                      help=description,\n                                      description=description,\n                                      epilog=\"Questions and feedback: https://faceswap.dev/forum\",\n                                      formatter_class=SmartFormatter)\n        return parser\n\n    def _add_arguments(self) -> None:\n        \"\"\" Parse the list of dictionaries containing the command line arguments and convert to\n        argparse parser arguments. \"\"\"\n        options = self.global_arguments + self.argument_list + self.optional_arguments\n        for option in options:\n            args = option[\"opts\"]\n            kwargs = {key: option[key] for key in option.keys() if key not in (\"opts\", \"group\")}\n            self.parser.add_argument(*args, **kwargs)\n\n    def _process_suppressions(self) -> None:\n        \"\"\" Certain options are only available for certain backends.\n\n        Suppresses command line options that are not available for the running backend.\n        \"\"\"\n        fs_backend = get_backend()\n        for opt_list in [self.global_arguments, self.argument_list, self.optional_arguments]:\n            for opts in opt_list:\n                if opts.get(\"backend\", None) is None:\n                    continue\n                opt_backend = opts.pop(\"backend\")\n                if isinstance(opt_backend, (list, tuple)):\n                    opt_backend = [backend.lower() for backend in opt_backend]\n                else:\n                    opt_backend = [opt_backend.lower()]\n                if fs_backend not in opt_backend:\n                    opts[\"help\"] = argparse.SUPPRESS\n\n\nclass GuiArgs(FaceSwapArgs):\n    \"\"\" Creates the command line arguments for the GUI. \"\"\"\n\n    @staticmethod\n    def get_argument_list() -> list[dict[str, T.Any]]:\n        \"\"\" Returns the argument list for GUI arguments.\n\n        Returns\n        -------\n        list\n            The list of command line options for the GUI\n        \"\"\"\n        argument_list: list[dict[str, T.Any]] = []\n        argument_list.append({\n            \"opts\": (\"-d\", \"--debug\"),\n            \"action\": \"store_true\",\n            \"dest\": \"debug\",\n            \"default\": False,\n            \"help\": _(\"Output to Shell console instead of GUI console\")})\n        return argument_list\n", "lib/cli/__init__.py": "", "lib/cli/launcher.py": "#!/usr/bin/env python3\n\"\"\" Launches the correct script with the given Command Line Arguments \"\"\"\nfrom __future__ import annotations\nimport logging\nimport os\nimport platform\nimport sys\nimport typing as T\n\nfrom importlib import import_module\n\nfrom lib.gpu_stats import set_exclude_devices, GPUStats\nfrom lib.logger import crash_log, log_setup\nfrom lib.utils import (FaceswapError, get_backend, get_tf_version,\n                       safe_shutdown, set_backend, set_system_verbosity)\n\nif T.TYPE_CHECKING:\n    import argparse\n    from collections.abc import Callable\n\nlogger = logging.getLogger(__name__)\n\n\nclass ScriptExecutor():\n    \"\"\" Loads the relevant script modules and executes the script.\n\n        This class is initialized in each of the argparsers for the relevant\n        command, then execute script is called within their set_default\n        function.\n\n        Parameters\n        ----------\n        command: str\n            The faceswap command that is being executed\n        \"\"\"\n    def __init__(self, command: str) -> None:\n        self._command = command.lower()\n\n    def _import_script(self) -> Callable:\n        \"\"\" Imports the relevant script as indicated by :attr:`_command` from the scripts folder.\n\n        Returns\n        -------\n        class: Faceswap Script\n            The uninitialized script from the faceswap scripts folder.\n        \"\"\"\n        self._set_environment_variables()\n        self._test_for_tf_version()\n        self._test_for_gui()\n        cmd = os.path.basename(sys.argv[0])\n        src = f\"tools.{self._command.lower()}\" if cmd == \"tools.py\" else \"scripts\"\n        mod = \".\".join((src, self._command.lower()))\n        module = import_module(mod)\n        script = getattr(module, self._command.title())\n        return script\n\n    def _set_environment_variables(self) -> None:\n        \"\"\" Set the number of threads that numexpr can use and TF environment variables. \"\"\"\n        # Allocate a decent number of threads to numexpr to suppress warnings\n        cpu_count = os.cpu_count()\n        allocate = cpu_count - cpu_count // 3 if cpu_count is not None else 1\n        if \"OMP_NUM_THREADS\" in os.environ:\n            # If this is set above NUMEXPR_MAX_THREADS, numexpr will error.\n            # ref: https://github.com/pydata/numexpr/issues/322\n            os.environ.pop(\"OMP_NUM_THREADS\")\n        os.environ[\"NUMEXPR_MAX_THREADS\"] = str(max(1, allocate))\n\n        # Ensure tensorflow doesn't pin all threads to one core when using Math Kernel Library\n        os.environ[\"TF_MIN_GPU_MULTIPROCESSOR_COUNT\"] = \"4\"\n        os.environ[\"KMP_AFFINITY\"] = \"disabled\"\n\n        # If running under CPU on Windows, the following error can be encountered:\n        # OMP: Error #15: Initializing libiomp5md.dll, but found libiomp5 already initialized.\n        # OMP: Hint This means that multiple copies of the OpenMP runtime have been linked into\n        # the program. That is dangerous, since it can degrade performance or cause incorrect\n        # results. The best thing to do is to ensure that only a single OpenMP runtime is linked\n        # into the process, e.g. by avoiding static linking of the OpenMP runtime in any library.\n        # As an unsafe, unsupported, undocumented workaround you can set the environment variable\n        # KMP_DUPLICATE_LIB_OK=TRUE to allow the program to continue to execute, but that may cause\n        # crashes or silently produce incorrect results. For more information,\n        # please see http://www.intel.com/software/products/support/.\n        #\n        # TODO find a better way than just allowing multiple libs\n        if get_backend() == \"cpu\" and platform.system() == \"Windows\":\n            logger.debug(\"Setting `KMP_DUPLICATE_LIB_OK` environment variable to `TRUE`\")\n            os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n\n        # There is a memory leak in TF2.10+ predict function. This fix will work for tf2.10 but not\n        # for later versions. This issue has been patched recently, but we'll probably need to\n        # skip some TF versions\n        # ref: https://github.com/tensorflow/tensorflow/issues/58676\n        # TODO remove this fix post TF2.10 and check memleak is fixed\n        logger.debug(\"Setting TF_RUN_EAGER_OP_AS_FUNCTION env var to False\")\n        os.environ[\"TF_RUN_EAGER_OP_AS_FUNCTION\"] = \"false\"\n\n    def _test_for_tf_version(self) -> None:\n        \"\"\" Check that the required Tensorflow version is installed.\n\n        Raises\n        ------\n        FaceswapError\n            If Tensorflow is not found, or is not between versions 2.4 and 2.9\n        \"\"\"\n        min_ver = (2, 10)\n        max_ver = (2, 10)\n        try:\n            import tensorflow as tf  # noqa pylint:disable=import-outside-toplevel,unused-import\n        except ImportError as err:\n            if \"DLL load failed while importing\" in str(err):\n                msg = (\n                    f\"A DLL library file failed to load. Make sure that you have Microsoft Visual \"\n                    \"C++ Redistributable (2015, 2017, 2019) installed for your machine from: \"\n                    \"https://support.microsoft.com/en-gb/help/2977003. Original error: \"\n                    f\"{str(err)}\")\n            else:\n                msg = (\n                    f\"There was an error importing Tensorflow. This is most likely because you do \"\n                    \"not have TensorFlow installed, or you are trying to run tensorflow-gpu on a \"\n                    \"system without an Nvidia graphics card. Original import \"\n                    f\"error: {str(err)}\")\n            self._handle_import_error(msg)\n\n        tf_ver = get_tf_version()\n        if tf_ver < min_ver:\n            msg = (f\"The minimum supported Tensorflow is version {min_ver} but you have version \"\n                   f\"{tf_ver} installed. Please upgrade Tensorflow.\")\n            self._handle_import_error(msg)\n        if tf_ver > max_ver:\n            msg = (f\"The maximum supported Tensorflow is version {max_ver} but you have version \"\n                   f\"{tf_ver} installed. Please downgrade Tensorflow.\")\n            self._handle_import_error(msg)\n        logger.debug(\"Installed Tensorflow Version: %s\", tf_ver)\n\n    @classmethod\n    def _handle_import_error(cls, message: str) -> None:\n        \"\"\" Display the error message to the console and wait for user input to dismiss it, if\n        running GUI under Windows, otherwise use standard error handling.\n\n        Parameters\n        ----------\n        message: str\n            The error message to display\n        \"\"\"\n        if \"gui\" in sys.argv and platform.system() == \"Windows\":\n            logger.error(message)\n            logger.info(\"Press \\\"ENTER\\\" to dismiss the message and close FaceSwap\")\n            input()\n            sys.exit(1)\n        else:\n            raise FaceswapError(message)\n\n    def _test_for_gui(self) -> None:\n        \"\"\" If running the gui, performs check to ensure necessary prerequisites are present. \"\"\"\n        if self._command != \"gui\":\n            return\n        self._test_tkinter()\n        self._check_display()\n\n    @classmethod\n    def _test_tkinter(cls) -> None:\n        \"\"\" If the user is running the GUI, test whether the tkinter app is available on their\n        machine. If not exit gracefully.\n\n        This avoids having to import every tkinter function within the GUI in a wrapper and\n        potentially spamming traceback errors to console.\n\n        Raises\n        ------\n        FaceswapError\n            If tkinter cannot be imported\n        \"\"\"\n        try:\n            import tkinter  # noqa pylint:disable=unused-import,import-outside-toplevel\n        except ImportError as err:\n            logger.error(\"It looks like TkInter isn't installed for your OS, so the GUI has been \"\n                         \"disabled. To enable the GUI please install the TkInter application. You \"\n                         \"can try:\")\n            logger.info(\"Anaconda: conda install tk\")\n            logger.info(\"Windows/macOS: Install ActiveTcl Community Edition from \"\n                        \"http://www.activestate.com\")\n            logger.info(\"Ubuntu/Mint/Debian: sudo apt install python3-tk\")\n            logger.info(\"Arch: sudo pacman -S tk\")\n            logger.info(\"CentOS/Redhat: sudo yum install tkinter\")\n            logger.info(\"Fedora: sudo dnf install python3-tkinter\")\n            raise FaceswapError(\"TkInter not found\") from err\n\n    @classmethod\n    def _check_display(cls) -> None:\n        \"\"\" Check whether there is a display to output the GUI to.\n\n        If running on Windows then it is assumed that we are not running in headless mode\n\n        Raises\n        ------\n        FaceswapError\n            If a DISPLAY environmental cannot be found\n        \"\"\"\n        if not os.environ.get(\"DISPLAY\", None) and os.name != \"nt\":\n            if platform.system() == \"Darwin\":\n                logger.info(\"macOS users need to install XQuartz. \"\n                            \"See https://support.apple.com/en-gb/HT201341\")\n            raise FaceswapError(\"No display detected. GUI mode has been disabled.\")\n\n    def execute_script(self, arguments: argparse.Namespace) -> None:\n        \"\"\" Performs final set up and launches the requested :attr:`_command` with the given\n        command line arguments.\n\n        Monitors for errors and attempts to shut down the process cleanly on exit.\n\n        Parameters\n        ----------\n        arguments: :class:`argparse.Namespace`\n            The command line arguments to be passed to the executing script.\n        \"\"\"\n        set_system_verbosity(arguments.loglevel)\n        is_gui = hasattr(arguments, \"redirect_gui\") and arguments.redirect_gui\n        log_setup(arguments.loglevel, arguments.logfile, self._command, is_gui)\n        success = False\n\n        if self._command != \"gui\":\n            self._configure_backend(arguments)\n        try:\n            script = self._import_script()\n            process = script(arguments)\n            process.process()\n            success = True\n        except FaceswapError as err:\n            for line in str(err).splitlines():\n                logger.error(line)\n        except KeyboardInterrupt:  # pylint:disable=try-except-raise\n            raise\n        except SystemExit:\n            pass\n        except Exception:  # pylint:disable=broad-except\n            crash_file = crash_log()\n            logger.exception(\"Got Exception on main handler:\")\n            logger.critical(\"An unexpected crash has occurred. Crash report written to '%s'. \"\n                            \"You MUST provide this file if seeking assistance. Please verify you \"\n                            \"are running the latest version of faceswap before reporting\",\n                            crash_file)\n\n        finally:\n            safe_shutdown(got_error=not success)\n\n    def _configure_backend(self, arguments: argparse.Namespace) -> None:\n        \"\"\" Configure the backend.\n\n        Exclude any GPUs for use by Faceswap when requested.\n\n        Set Faceswap backend to CPU if all GPUs have been deselected.\n\n        Parameters\n        ----------\n        arguments: :class:`argparse.Namespace`\n            The command line arguments passed to Faceswap.\n        \"\"\"\n        if not hasattr(arguments, \"exclude_gpus\"):\n            # CPU backends and systems where no GPU was detected will not have this attribute\n            logger.debug(\"Adding missing exclude gpus argument to namespace\")\n            setattr(arguments, \"exclude_gpus\", None)\n            return\n\n        if arguments.exclude_gpus:\n            if not all(idx.isdigit() for idx in arguments.exclude_gpus):\n                logger.error(\"GPUs passed to the ['-X', '--exclude-gpus'] argument must all be \"\n                             \"integers.\")\n                sys.exit(1)\n            arguments.exclude_gpus = [int(idx) for idx in arguments.exclude_gpus]\n            set_exclude_devices(arguments.exclude_gpus)\n\n        if GPUStats().exclude_all_devices:\n            msg = \"Switching backend to CPU\"\n            set_backend(\"cpu\")\n            logger.info(msg)\n\n        logger.debug(\"Executing: %s. PID: %s\", self._command, os.getpid())\n", "lib/training/preview_cv.py": "#!/usr/bin/python\n\"\"\" The pop up preview window for Faceswap.\n\nIf Tkinter is installed, then this will be used to manage the preview image, otherwise we\nfallback to opencv's imshow\n\"\"\"\nfrom __future__ import annotations\nimport logging\nimport typing as T\n\nfrom threading import Event, Lock\nfrom time import sleep\n\nimport cv2\n\nif T.TYPE_CHECKING:\n    from collections.abc import Generator\n    import numpy as np\n\nlogger = logging.getLogger(__name__)\nTriggerType = dict[T.Literal[\"toggle_mask\", \"refresh\", \"save\", \"quit\", \"shutdown\"], Event]\nTriggerKeysType = T.Literal[\"m\", \"r\", \"s\", \"enter\"]\nTriggerNamesType = T.Literal[\"toggle_mask\", \"refresh\", \"save\", \"quit\"]\n\n\nclass PreviewBuffer():\n    \"\"\" A thread safe class for holding preview images \"\"\"\n    def __init__(self) -> None:\n        logger.debug(\"Initializing: %s\", self.__class__.__name__)\n        self._images: dict[str, np.ndarray] = {}\n        self._lock = Lock()\n        self._updated = Event()\n        logger.debug(\"Initialized: %s\", self.__class__.__name__)\n\n    @property\n    def is_updated(self) -> bool:\n        \"\"\" bool: ``True`` when new images have been loaded into the  preview buffer \"\"\"\n        return self._updated.is_set()\n\n    def add_image(self, name: str, image: np.ndarray) -> None:\n        \"\"\" Add an image to the preview buffer in a thread safe way \"\"\"\n        logger.debug(\"Adding image: (name: '%s', shape: %s)\", name, image.shape)\n        with self._lock:\n            self._images[name] = image\n        logger.debug(\"Added images: %s\", list(self._images))\n        self._updated.set()\n\n    def get_images(self) -> Generator[tuple[str, np.ndarray], None, None]:\n        \"\"\" Get the latest images from the preview buffer. When iterator is exhausted clears the\n        :attr:`updated` event.\n\n        Yields\n        ------\n        name: str\n            The name of the image\n        :class:`numpy.ndarray`\n            The image in BGR format\n        \"\"\"\n        logger.debug(\"Retrieving images: %s\", list(self._images))\n        with self._lock:\n            for name, image in self._images.items():\n                logger.debug(\"Yielding: '%s' (%s)\", name, image.shape)\n                yield name, image\n            if self.is_updated:\n                logger.debug(\"Clearing updated event\")\n                self._updated.clear()\n                logger.debug(\"Retrieved images\")\n\n\nclass PreviewBase():  # pylint:disable=too-few-public-methods\n    \"\"\" Parent class for OpenCV and Tkinter Preview Windows\n\n    Parameters\n    ----------\n    preview_buffer: :class:`PreviewBuffer`\n        The thread safe object holding the preview images\n    triggers: dict, optional\n        Dictionary of event triggers for pop-up preview. Not required when running inside the GUI.\n        Default: `None`\n     \"\"\"\n    def __init__(self,\n                 preview_buffer: PreviewBuffer,\n                 triggers: TriggerType | None = None) -> None:\n        logger.debug(\"Initializing %s parent (triggers: %s)\",\n                     self.__class__.__name__, triggers)\n        self._triggers = triggers\n        self._buffer = preview_buffer\n        self._keymaps: dict[TriggerKeysType, TriggerNamesType] = {\"m\": \"toggle_mask\",\n                                                                  \"r\": \"refresh\",\n                                                                  \"s\": \"save\",\n                                                                  \"enter\": \"quit\"}\n        self._title = \"\"\n        logger.debug(\"Initialized %s parent\", self.__class__.__name__)\n\n    @property\n    def _should_shutdown(self) -> bool:\n        \"\"\" bool: ``True`` if the preview has received an external signal to shutdown otherwise\n        ``False`` \"\"\"\n        if self._triggers is None or not self._triggers[\"shutdown\"].is_set():\n            return False\n        logger.debug(\"Shutdown signal received\")\n        return True\n\n    def _launch(self) -> None:\n        \"\"\" Wait until an image is loaded into the preview buffer and call the child's\n        :func:`_display_preview` function \"\"\"\n        logger.debug(\"Launching %s\", self.__class__.__name__)\n        while True:\n            if not self._buffer.is_updated:\n                logger.debug(\"Waiting for preview image\")\n                sleep(1)\n                continue\n            break\n        logger.debug(\"Launching preview\")\n        self._display_preview()\n\n    def _display_preview(self) -> None:\n        \"\"\" Override for preview viewer's display loop \"\"\"\n        raise NotImplementedError()\n\n\nclass PreviewCV(PreviewBase):  # pylint:disable=too-few-public-methods\n    \"\"\" Simple fall back preview viewer using OpenCV for when TKinter is not available\n\n    Parameters\n    ----------\n    preview_buffer: :class:`PreviewBuffer`\n        The thread safe object holding the preview images\n    triggers: dict\n        Dictionary of event triggers for pop-up preview.\n     \"\"\"\n    def __init__(self,\n                 preview_buffer: PreviewBuffer,\n                 triggers: TriggerType) -> None:\n        logger.debug(\"Unable to import Tkinter. Falling back to OpenCV\")\n        super().__init__(preview_buffer, triggers=triggers)\n        self._triggers: TriggerType = self._triggers\n        self._windows: list[str] = []\n\n        self._lookup = {ord(key): val\n                        for key, val in self._keymaps.items() if key != \"enter\"}\n        self._lookup[ord(\"\\n\")] = self._keymaps[\"enter\"]\n        self._lookup[ord(\"\\r\")] = self._keymaps[\"enter\"]\n\n        self._launch()\n\n    @property\n    def _window_closed(self) -> bool:\n        \"\"\" bool: ``True`` if any window has been closed otherwise ``False`` \"\"\"\n        retval = any(cv2.getWindowProperty(win, cv2.WND_PROP_VISIBLE) < 1 for win in self._windows)\n        if retval:\n            logger.debug(\"Window closed detected\")\n        return retval\n\n    def _check_keypress(self, key: int):\n        \"\"\" Check whether we have received a valid key press from OpenCV window and handle\n        accordingly.\n\n        Parameters\n        ----------\n        key_press: int\n            The key press received from OpenCV\n        \"\"\"\n        if not key or key == -1 or key not in self._lookup:\n            return\n\n        if key == ord(\"r\"):\n            print(\"\")  # Let log print on different line from loss output\n            logger.info(\"Refresh preview requested...\")\n\n        self._triggers[self._lookup[key]].set()\n        logger.debug(\"Processed keypress '%s'. Set event for '%s'\", key, self._lookup[key])\n\n    def _display_preview(self):\n        \"\"\" Handle the displaying of the images currently in :attr:`_preview_buffer`\"\"\"\n        while True:\n            if self._buffer.is_updated or self._window_closed:\n                for name, image in self._buffer.get_images():\n                    logger.debug(\"showing image: '%s' (%s)\", name, image.shape)\n                    cv2.imshow(name, image)\n                    self._windows.append(name)\n\n            key = cv2.waitKey(1000)\n            self._check_keypress(key)\n\n            if self._triggers[\"shutdown\"].is_set():\n                logger.debug(\"Shutdown received\")\n                break\n        logger.debug(\"%s shutdown\", self.__class__.__name__)\n", "lib/training/augmentation.py": "#!/usr/bin/env python3\n\"\"\" Processes the augmentation of images for feeding into a Faceswap model. \"\"\"\nfrom __future__ import annotations\nimport logging\nimport typing as T\n\nimport cv2\nimport numexpr as ne\nimport numpy as np\nfrom scipy.interpolate import griddata\n\nfrom lib.image import batch_convert_color\nfrom lib.logger import parse_class_init\n\nif T.TYPE_CHECKING:\n    from lib.config import ConfigValueType\n\nlogger = logging.getLogger(__name__)\n\n\nclass AugConstants:  # pylint:disable=too-many-instance-attributes,too-few-public-methods\n    \"\"\" Dataclass for holding constants for Image Augmentation.\n\n    Parameters\n    ----------\n    config: dict[str, ConfigValueType]\n        The user training configuration options\n    processing_size: int:\n        The size of image to augment the data for\n    batch_size: int\n        The batch size that augmented data is being prepared for\n    \"\"\"\n    def __init__(self,\n                 config: dict[str, ConfigValueType],\n                 processing_size: int,\n                 batch_size: int) -> None:\n        logger.debug(parse_class_init(locals()))\n        self.clahe_base_contrast: int = 0\n        \"\"\"int: The base number for Contrast Limited Adaptive Histogram Equalization\"\"\"\n        self.clahe_chance: float = 0.0\n        \"\"\"float: Probability to perform Contrast Limited Adaptive Histogram Equilization\"\"\"\n        self.clahe_max_size: int = 0\n        \"\"\"int: Maximum clahe window size\"\"\"\n\n        self.lab_adjust: np.ndarray\n        \"\"\":class:`numpy.ndarray`: Adjustment amounts for L*A*B augmentation\"\"\"\n        self.transform_rotation: int = 0\n        \"\"\"int: Rotation range for transformations\"\"\"\n        self.transform_zoom: float = 0.0\n        \"\"\"float: Zoom range for transformations\"\"\"\n        self.transform_shift: float = 0.0\n        \"\"\"float: Shift range for transformations\"\"\"\n        self.warp_maps: np.ndarray\n        \"\"\":class:`numpy.ndarray`The stacked (x, y) mappings for image warping\"\"\"\n        self.warp_pad: tuple[int, int] = (0, 0)\n        \"\"\":tuple[int, int]: The padding to apply for image warping\"\"\"\n        self.warp_slices: slice\n        \"\"\":slice: The slices for extracting a warped image\"\"\"\n        self.warp_lm_edge_anchors: np.ndarray\n        \"\"\"::class:`numpy.ndarray`: The edge anchors for landmark based warping\"\"\"\n        self.warp_lm_grids: np.ndarray\n        \"\"\"::class:`numpy.ndarray`: The grids for landmark based warping\"\"\"\n\n        self._config = config\n        self._size = processing_size\n        self._load_config(batch_size)\n        logger.debug(\"Initialized: %s\", self.__class__.__name__)\n\n    def _load_clahe(self) -> None:\n        \"\"\" Load the CLAHE constants from user config \"\"\"\n        color_clahe_chance = self._config.get(\"color_clahe_chance\", 50)\n        color_clahe_max_size = self._config.get(\"color_clahe_max_size\", 4)\n        assert isinstance(color_clahe_chance, int)\n        assert isinstance(color_clahe_max_size, int)\n\n        self.clahe_base_contrast = max(2, self._size // 128)\n        self.clahe_chance = color_clahe_chance / 100\n        self.clahe_max_size = color_clahe_max_size\n        logger.debug(\"clahe_base_contrast: %s, clahe_chance: %s, clahe_max_size: %s\",\n                     self.clahe_base_contrast, self.clahe_chance, self.clahe_max_size)\n\n    def _load_lab(self) -> None:\n        \"\"\" Load the random L*A*B augmentation constants \"\"\"\n        color_lightness = self._config.get(\"color_lightness\", 30)\n        color_ab = self._config.get(\"color_ab\", 8)\n        assert isinstance(color_lightness, int)\n        assert isinstance(color_ab, int)\n\n        amount_l = int(color_lightness) / 100\n        amount_ab = int(color_ab) / 100\n\n        self.lab_adjust = np.array([amount_l, amount_ab, amount_ab], dtype=\"float32\")\n        logger.debug(\"lab_adjust: %s\", self.lab_adjust)\n\n    def _load_transform(self) -> None:\n        \"\"\" Load the random transform constants \"\"\"\n        shift_range = self._config.get(\"shift_range\", 5)\n        rotation_range = self._config.get(\"rotation_range\", 10)\n        zoom_amount = self._config.get(\"zoom_amount\", 5)\n        assert isinstance(shift_range, int)\n        assert isinstance(rotation_range, int)\n        assert isinstance(zoom_amount, int)\n\n        self.transform_shift = (shift_range / 100) * self._size\n        self.transform_rotation = rotation_range\n        self.transform_zoom = zoom_amount / 100\n        logger.debug(\"transform_shift: %s, transform_rotation: %s, transform_zoom: %s\",\n                     self.transform_shift, self.transform_rotation, self.transform_zoom)\n\n    def _load_warp(self, batch_size: int) -> None:\n        \"\"\" Load the warp augmentation constants\n\n        Parameters\n        ----------\n        batch_size: int\n            The batch size that augmented data is being prepared for\n        \"\"\"\n        warp_range = np.linspace(0, self._size, 5, dtype='float32')\n        warp_mapx = np.broadcast_to(warp_range, (batch_size, 5, 5)).astype(\"float32\")\n        warp_mapy = np.broadcast_to(warp_mapx[0].T, (batch_size, 5, 5)).astype(\"float32\")\n        warp_pad = int(1.25 * self._size)\n\n        self.warp_maps = np.stack((warp_mapx, warp_mapy), axis=1)\n        self.warp_pad = (warp_pad, warp_pad)\n        self.warp_slices = slice(warp_pad // 10, -warp_pad // 10)\n        logger.debug(\"warp_maps: (%s, %s), warp_pad: %s, warp_slices: %s\",\n                     self.warp_maps.shape, self.warp_maps.dtype,\n                     self.warp_pad, self.warp_slices)\n\n    def _load_warp_to_landmarks(self, batch_size: int) -> None:\n        \"\"\" Load the warp-to-landmarks augmentation constants\n\n        Parameters\n        ----------\n        batch_size: int\n            The batch size that augmented data is being prepared for\n        \"\"\"\n        p_mx = self._size - 1\n        p_hf = (self._size // 2) - 1\n        edge_anchors = np.array([(0, 0), (0, p_mx), (p_mx, p_mx), (p_mx, 0),\n                                 (p_hf, 0), (p_hf, p_mx), (p_mx, p_hf), (0, p_hf)]).astype(\"int32\")\n        edge_anchors = np.broadcast_to(edge_anchors, (batch_size, 8, 2))\n        grids = np.mgrid[0: p_mx: complex(self._size),  # type:ignore[misc]\n                         0: p_mx: complex(self._size)]  # type:ignore[misc]\n\n        self.warp_lm_edge_anchors = edge_anchors\n        self.warp_lm_grids = grids\n        logger.debug(\"warp_lm_edge_anchors: (%s, %s), warp_lm_grids: (%s, %s)\",\n                     self.warp_lm_edge_anchors.shape, self.warp_lm_edge_anchors.dtype,\n                     self.warp_lm_grids.shape, self.warp_lm_grids.dtype)\n\n    def _load_config(self, batch_size: int) -> None:\n        \"\"\" Load the constants into the class from user config\n\n        Parameters\n        ----------\n        batch_size: int\n            The batch size that augmented data is being prepared for\n        \"\"\"\n        logger.debug(\"Loading augmentation constants\")\n        self._load_clahe()\n        self._load_lab()\n        self._load_transform()\n        self._load_warp(batch_size)\n        self._load_warp_to_landmarks(batch_size)\n        logger.debug(\"Loaded augmentation constants\")\n\n\nclass ImageAugmentation():\n    \"\"\" Performs augmentation on batches of training images.\n\n    Parameters\n    ----------\n    batch_size: int\n        The number of images that will be fed through the augmentation functions at once.\n    processing_size: int\n        The largest input or output size of the model. This is the size that images are processed\n        at.\n    config: dict\n        The configuration `dict` generated from :file:`config.train.ini` containing the trainer\n        plugin configuration options.\n    \"\"\"\n    def __init__(self,\n                 batch_size: int,\n                 processing_size: int,\n                 config: dict[str, ConfigValueType]) -> None:\n        logger.debug(parse_class_init(locals()))\n        self._processing_size = processing_size\n        self._batch_size = batch_size\n\n        # flip_args\n        flip_chance = config.get(\"random_flip\", 50)\n        assert isinstance(flip_chance, int)\n        self._flip_chance = flip_chance\n\n        # Warp args\n        self._warp_scale = 5 / 256 * self._processing_size  # Normal random variable scale\n        self._warp_lm_scale = 2 / 256 * self._processing_size  # Normal random variable scale\n\n        self._constants = AugConstants(config, processing_size, batch_size)\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    # <<< COLOR AUGMENTATION >>> #\n    def color_adjust(self, batch: np.ndarray) -> np.ndarray:\n        \"\"\" Perform color augmentation on the passed in batch.\n\n        The color adjustment parameters are set in :file:`config.train.ini`\n\n        Parameters\n        ----------\n        batch: :class:`numpy.ndarray`\n            The batch should be a 4-dimensional array of shape (`batchsize`, `height`, `width`,\n            `3`) and in `BGR` format.\n\n        Returns\n        ----------\n        :class:`numpy.ndarray`\n            A 4-dimensional array of the same shape as :attr:`batch` with color augmentation\n            applied.\n        \"\"\"\n        logger.trace(\"Augmenting color\")  # type:ignore[attr-defined]\n        batch = batch_convert_color(batch, \"BGR2LAB\")\n        self._random_lab(batch)\n        self._random_clahe(batch)\n        batch = batch_convert_color(batch, \"LAB2BGR\")\n        return batch\n\n    def _random_clahe(self, batch: np.ndarray) -> None:\n        \"\"\" Randomly perform Contrast Limited Adaptive Histogram Equalization on\n        a batch of images \"\"\"\n        base_contrast = self._constants.clahe_base_contrast\n\n        batch_random = np.random.rand(self._batch_size)\n        indices = np.where(batch_random < self._constants.clahe_chance)[0]\n        if not np.any(indices):\n            return\n        grid_bases = np.random.randint(self._constants.clahe_max_size + 1,\n                                       size=indices.shape[0],\n                                       dtype=\"uint8\")\n        grid_sizes = (grid_bases * (base_contrast // 2)) + base_contrast\n        logger.trace(\"Adjusting Contrast. Grid Sizes: %s\", grid_sizes)  # type:ignore[attr-defined]\n\n        clahes = [cv2.createCLAHE(clipLimit=2.0,\n                                  tileGridSize=(grid_size, grid_size))\n                  for grid_size in grid_sizes]\n\n        for idx, clahe in zip(indices, clahes):\n            batch[idx, :, :, 0] = clahe.apply(batch[idx, :, :, 0], )\n\n    def _random_lab(self, batch: np.ndarray) -> None:\n        \"\"\" Perform random color/lightness adjustment in L*a*b* color space on a batch of\n        images \"\"\"\n        randoms = np.random.uniform(-self._constants.lab_adjust,\n                                    self._constants.lab_adjust,\n                                    size=(self._batch_size, 1, 1, 3)).astype(\"float32\")\n        logger.trace(\"Random LAB adjustments: %s\", randoms)  # type:ignore[attr-defined]\n        # Iterating through the images and channels is much faster than numpy.where and slightly\n        # faster than numexpr.where.\n        for image, rand in zip(batch, randoms):\n            for idx in range(rand.shape[-1]):\n                adjustment = rand[:, :, idx]\n                if adjustment >= 0:\n                    image[:, :, idx] = ((255 - image[:, :, idx]) * adjustment) + image[:, :, idx]\n                else:\n                    image[:, :, idx] = image[:, :, idx] * (1 + adjustment)\n\n    # <<< IMAGE AUGMENTATION >>> #\n    def transform(self, batch: np.ndarray):\n        \"\"\" Perform random transformation on the passed in batch.\n\n        The transformation parameters are set in :file:`config.train.ini`\n\n        Parameters\n        ----------\n        batch: :class:`numpy.ndarray`\n            The batch should be a 4-dimensional array of shape (`batchsize`, `height`, `width`,\n            `channels`) and in `BGR` format.\n        \"\"\"\n        logger.trace(\"Randomly transforming image\")  # type:ignore[attr-defined]\n\n        rotation = np.random.uniform(-self._constants.transform_rotation,\n                                     self._constants.transform_rotation,\n                                     size=self._batch_size).astype(\"float32\")\n        scale = np.random.uniform(1 - self._constants.transform_zoom,\n                                  1 + self._constants.transform_zoom,\n                                  size=self._batch_size).astype(\"float32\")\n\n        tform = np.random.uniform(-self._constants.transform_shift,\n                                  self._constants.transform_shift,\n                                  size=(self._batch_size, 2)).astype(\"float32\")\n        mats = np.array(\n            [cv2.getRotationMatrix2D((self._processing_size // 2, self._processing_size // 2),\n                                     rot,\n                                     scl)\n             for rot, scl in zip(rotation, scale)]).astype(\"float32\")\n        mats[..., 2] += tform\n\n        for image, mat in zip(batch, mats):\n            cv2.warpAffine(image,\n                           mat,\n                           (self._processing_size, self._processing_size),\n                           dst=image,\n                           borderMode=cv2.BORDER_REPLICATE)\n\n        logger.trace(\"Randomly transformed image\")  # type:ignore[attr-defined]\n\n    def random_flip(self, batch: np.ndarray):\n        \"\"\" Perform random horizontal flipping on the passed in batch.\n\n        The probability of flipping an image is set in :file:`config.train.ini`\n\n        Parameters\n        ----------\n        batch: :class:`numpy.ndarray`\n            The batch should be a 4-dimensional array of shape (`batchsize`, `height`, `width`,\n            `channels`) and in `BGR` format.\n        \"\"\"\n        logger.trace(\"Randomly flipping image\")  # type:ignore[attr-defined]\n        randoms = np.random.rand(self._batch_size)\n        indices = np.where(randoms <= self._flip_chance / 100)[0]\n        batch[indices] = batch[indices, :, ::-1]\n        logger.trace(\"Randomly flipped %s images of %s\",  # type:ignore[attr-defined]\n                     len(indices), self._batch_size)\n\n    def warp(self, batch: np.ndarray, to_landmarks: bool = False, **kwargs) -> np.ndarray:\n        \"\"\" Perform random warping on the passed in batch by one of two methods.\n\n        Parameters\n        ----------\n        batch: :class:`numpy.ndarray`\n            The batch should be a 4-dimensional array of shape (`batchsize`, `height`, `width`,\n            `3`) and in `BGR` format.\n        to_landmarks: bool, optional\n            If ``False`` perform standard random warping of the input image. If ``True`` perform\n            warping to semi-random similar corresponding landmarks from the other side. Default:\n            ``False``\n        kwargs: dict\n            If :attr:`to_landmarks` is ``True`` the following additional kwargs must be passed in:\n\n            * **batch_src_points** (:class:`numpy.ndarray`) - A batch of 68 point landmarks for \\\n            the source faces. This is a 3-dimensional array in the shape (`batchsize`, `68`, `2`).\n\n            * **batch_dst_points** (:class:`numpy.ndarray`) - A batch of randomly chosen closest \\\n            match destination faces landmarks. This is a 3-dimensional array in the shape \\\n            (`batchsize`, `68`, `2`).\n\n        Returns\n        ----------\n        :class:`numpy.ndarray`\n            A 4-dimensional array of the same shape as :attr:`batch` with warping applied.\n        \"\"\"\n        if to_landmarks:\n            return self._random_warp_landmarks(batch, **kwargs)\n        return self._random_warp(batch)\n\n    def _random_warp(self, batch: np.ndarray) -> np.ndarray:\n        \"\"\" Randomly warp the input batch\n\n        Parameters\n        ----------\n        batch: :class:`numpy.ndarray`\n            The batch should be a 4-dimensional array of shape (`batchsize`, `height`, `width`,\n            `3`) and in `BGR` format.\n\n        Returns\n        ----------\n        :class:`numpy.ndarray`\n            A 4-dimensional array of the same shape as :attr:`batch` with warping applied.\n        \"\"\"\n        logger.trace(\"Randomly warping batch\")  # type:ignore[attr-defined]\n        slices = self._constants.warp_slices\n        rands = np.random.normal(size=(self._batch_size, 2, 5, 5),\n                                 scale=self._warp_scale).astype(\"float32\")\n        batch_maps = ne.evaluate(\"m + r\", local_dict={\"m\": self._constants.warp_maps, \"r\": rands})\n        batch_interp = np.array([[cv2.resize(map_, self._constants.warp_pad)[slices, slices]\n                                  for map_ in maps]\n                                 for maps in batch_maps])\n        warped_batch = np.array([cv2.remap(image, interp[0], interp[1], cv2.INTER_LINEAR)\n                                 for image, interp in zip(batch, batch_interp)])\n\n        logger.trace(\"Warped image shape: %s\", warped_batch.shape)  # type:ignore[attr-defined]\n        return warped_batch\n\n    def _random_warp_landmarks(self,\n                               batch: np.ndarray,\n                               batch_src_points: np.ndarray,\n                               batch_dst_points: np.ndarray) -> np.ndarray:\n        \"\"\" From dfaker. Warp the image to a similar set of landmarks from the opposite side\n\n        batch: :class:`numpy.ndarray`\n            The batch should be a 4-dimensional array of shape (`batchsize`, `height`, `width`,\n            `3`) and in `BGR` format.\n        batch_src_points :class:`numpy.ndarray`\n            A batch of 68 point landmarks for the source faces. This is a 3-dimensional array in\n            the shape (`batchsize`, `68`, `2`).\n        batch_dst_points :class:`numpy.ndarray`\n            A batch of randomly chosen closest match destination faces landmarks. This is a\n            3-dimensional array in the shape (`batchsize`, `68`, `2`).\n\n        Returns\n        ----------\n        :class:`numpy.ndarray`\n            A 4-dimensional array of the same shape as :attr:`batch` with warping applied.\n        \"\"\"\n        logger.trace(\"Randomly warping landmarks\")  # type:ignore[attr-defined]\n        edge_anchors = self._constants.warp_lm_edge_anchors\n        grids = self._constants.warp_lm_grids\n\n        batch_dst = (batch_dst_points + np.random.normal(size=batch_dst_points.shape,\n                                                         scale=self._warp_lm_scale))\n\n        face_cores = [cv2.convexHull(np.concatenate([src[17:], dst[17:]], axis=0))\n                      for src, dst in zip(batch_src_points.astype(\"int32\"),\n                                          batch_dst.astype(\"int32\"))]\n\n        batch_src = np.append(batch_src_points, edge_anchors, axis=1)\n        batch_dst = np.append(batch_dst, edge_anchors, axis=1)\n\n        rem_indices = [list(set(idx for fpl in (src, dst)\n                                for idx, (pty, ptx) in enumerate(fpl)\n                                if cv2.pointPolygonTest(face_core, (pty, ptx), False) >= 0))\n                       for src, dst, face_core in zip(batch_src[:, :18, :],\n                                                      batch_dst[:, :18, :],\n                                                      face_cores)]\n        lbatch_src = [np.delete(src, idxs, axis=0) for idxs, src in zip(rem_indices, batch_src)]\n        lbatch_dst = [np.delete(dst, idxs, axis=0) for idxs, dst in zip(rem_indices, batch_dst)]\n\n        grid_z = np.array([griddata(dst, src, (grids[0], grids[1]), method=\"linear\")\n                           for src, dst in zip(lbatch_src, lbatch_dst)])\n        maps = grid_z.reshape((self._batch_size,\n                               self._processing_size,\n                               self._processing_size,\n                               2)).astype(\"float32\")\n\n        warped_batch = np.array([cv2.remap(image,\n                                           map_[..., 1],\n                                           map_[..., 0],\n                                           cv2.INTER_LINEAR,\n                                           borderMode=cv2.BORDER_TRANSPARENT)\n                                 for image, map_ in zip(batch, maps)])\n        logger.trace(\"Warped batch shape: %s\", warped_batch.shape)  # type:ignore[attr-defined]\n        return warped_batch\n", "lib/training/generator.py": "#!/usr/bin/env python3\n\"\"\" Handles Data Augmentation for feeding Faceswap Models \"\"\"\nfrom __future__ import annotations\nimport logging\nimport os\nimport typing as T\n\nfrom concurrent import futures\nfrom random import shuffle, choice\n\nimport cv2\nimport numpy as np\nimport numexpr as ne\nfrom lib.align import AlignedFace, DetectedFace\nfrom lib.align.aligned_face import CenteringType\nfrom lib.image import read_image_batch\nfrom lib.multithreading import BackgroundGenerator\nfrom lib.utils import FaceswapError\n\nfrom . import ImageAugmentation\nfrom .cache import get_cache, RingBuffer\n\nif T.TYPE_CHECKING:\n    from collections.abc import Generator\n    from lib.config import ConfigValueType\n    from plugins.train.model._base import ModelBase\n    from .cache import _Cache\n\nlogger = logging.getLogger(__name__)\nBatchType = tuple[np.ndarray, list[np.ndarray]]\n\n\nclass DataGenerator():\n    \"\"\" Parent class for Training and Preview Data Generators.\n\n    This class is called from :mod:`plugins.train.trainer._base` and launches a background\n    iterator that compiles augmented data, target data and sample data.\n\n    Parameters\n    ----------\n    model: :class:`~plugins.train.model.ModelBase`\n        The model that this data generator is feeding\n    config: dict\n        The configuration `dict` generated from :file:`config.train.ini` containing the trainer\n        plugin configuration options.\n    side: {'a' or 'b'}\n        The side of the model that this iterator is for.\n    images: list\n        A list of image paths that will be used to compile the final augmented data from.\n    batch_size: int\n        The batch size for this iterator. Images will be returned in :class:`numpy.ndarray`\n        objects of this size from the iterator.\n    \"\"\"\n    def __init__(self,\n                 config: dict[str, ConfigValueType],\n                 model: ModelBase,\n                 side: T.Literal[\"a\", \"b\"],\n                 images: list[str],\n                 batch_size: int) -> None:\n        logger.debug(\"Initializing %s: (model: %s, side: %s, images: %s , \"\n                     \"batch_size: %s, config: %s)\", self.__class__.__name__, model.name, side,\n                     len(images), batch_size, config)\n        self._config = config\n        self._side = side\n        self._images = images\n        self._batch_size = batch_size\n\n        self._process_size = max(img[1] for img in model.input_shapes + model.output_shapes)\n        self._output_sizes = self._get_output_sizes(model)\n        self._model_input_size = max(img[1] for img in model.input_shapes)\n\n        self._coverage_ratio = model.coverage_ratio\n        self._color_order = model.color_order.lower()\n        self._use_mask = self._config[\"mask_type\"] and (self._config[\"penalized_mask_loss\"] or\n                                                        self._config[\"learn_mask\"])\n\n        self._validate_samples()\n        self._buffer = RingBuffer(batch_size,\n                                  (self._process_size, self._process_size, self._total_channels),\n                                  dtype=\"uint8\")\n        self._face_cache: _Cache = get_cache(side,\n                                             filenames=images,\n                                             config=self._config,\n                                             size=self._process_size,\n                                             coverage_ratio=self._coverage_ratio)\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    @property\n    def _total_channels(self) -> int:\n        \"\"\"int: The total number of channels, including mask channels that the target image\n        should hold. \"\"\"\n        channels = 3\n        if self._config[\"mask_type\"] and (self._config[\"learn_mask\"] or\n                                          self._config[\"penalized_mask_loss\"]):\n            channels += 1\n\n        mults = [area for area in [\"eye\", \"mouth\"]\n                 if T.cast(int, self._config[f\"{area}_multiplier\"]) > 1]\n        if self._config[\"penalized_mask_loss\"] and mults:\n            channels += len(mults)\n        return channels\n\n    def _get_output_sizes(self, model: ModelBase) -> list[int]:\n        \"\"\" Obtain the size of each output tensor for the model.\n\n        Parameters\n        ----------\n        model: :class:`~plugins.train.model.ModelBase`\n            The model that this data generator is feeding\n\n        Returns\n        -------\n        list\n            A list of integers for the model output size for the current side\n        \"\"\"\n        out_shapes = model.output_shapes\n        split = len(out_shapes) // 2\n        side_out = out_shapes[:split] if self._side == \"a\" else out_shapes[split:]\n        retval = [shape[1] for shape in side_out if shape[-1] != 1]\n        logger.debug(\"side: %s, model output shapes: %s, output sizes: %s\",\n                     self._side, model.output_shapes, retval)\n        return retval\n\n    def minibatch_ab(self, do_shuffle: bool = True) -> Generator[BatchType, None, None]:\n        \"\"\" A Background iterator to return augmented images, samples and targets.\n\n        The exit point from this class and the sole attribute that should be referenced. Called\n        from :mod:`plugins.train.trainer._base`. Returns an iterator that yields images for\n        training, preview and time-lapses.\n\n        Parameters\n        ----------\n        do_shuffle: bool, optional\n            Whether data should be shuffled prior to loading from disk. If true, each time the full\n            list of filenames are processed, the data will be reshuffled to make sure they are not\n            returned in the same order. Default: ``True``\n\n        Yields\n        ------\n        feed: list\n            4-dimensional array of faces to feed the training the model (:attr:`x` parameter for\n            :func:`keras.models.model.train_on_batch`.). The array returned is in the format\n            (`batch size`, `height`, `width`, `channels`).\n        targets: list\n            List of 4-dimensional :class:`numpy.ndarray` objects in the order and size of each\n            output of the model. The format of these arrays will be (`batch size`, `height`,\n            `width`, `x`). This is the :attr:`y` parameter for\n            :func:`keras.models.model.train_on_batch`. The number of channels here will vary.\n            The first 3 channels are (rgb/bgr). The 4th channel is the face mask. Any subsequent\n            channels are area masks (e.g. eye/mouth masks)\n        \"\"\"\n        logger.debug(\"do_shuffle: %s\", do_shuffle)\n        args = (do_shuffle, )\n        batcher = BackgroundGenerator(self._minibatch, args=args)\n        return batcher.iterator()\n\n    # << INTERNAL METHODS >> #\n    def _validate_samples(self) -> None:\n        \"\"\" Ensures that the total number of images within :attr:`images` is greater or equal to\n        the selected :attr:`batch_size`.\n\n        Raises\n        ------\n        :class:`FaceswapError`\n            If the number of images loaded is smaller than the selected batch size\n        \"\"\"\n        length = len(self._images)\n        msg = (\"Number of images is lower than batch-size (Note that too few images may lead to \"\n               f\"bad training). # images: {length}, batch-size: {self._batch_size}\")\n        try:\n            assert length >= self._batch_size, msg\n        except AssertionError as err:\n            msg += (\"\\nYou should increase the number of images in your training set or lower \"\n                    \"your batch-size.\")\n            raise FaceswapError(msg) from err\n\n    def _minibatch(self, do_shuffle: bool) -> Generator[BatchType, None, None]:\n        \"\"\" A generator function that yields the augmented, target and sample images for the\n        current batch on the current side.\n\n        Parameters\n        ----------\n        do_shuffle: bool, optional\n            Whether data should be shuffled prior to loading from disk. If true, each time the full\n            list of filenames are processed, the data will be reshuffled to make sure they are not\n            returned in the same order. Default: ``True``\n\n        Yields\n        ------\n        feed: list\n            4-dimensional array of faces to feed the training the model (:attr:`x` parameter for\n            :func:`keras.models.model.train_on_batch`.). The array returned is in the format\n            (`batch size`, `height`, `width`, `channels`).\n        targets: list\n            List of 4-dimensional :class:`numpy.ndarray` objects in the order and size of each\n            output of the model. The format of these arrays will be (`batch size`, `height`,\n            `width`, `x`). This is the :attr:`y` parameter for\n            :func:`keras.models.model.train_on_batch`. The number of channels here will vary.\n            The first 3 channels are (rgb/bgr). The 4th channel is the face mask. Any subsequent\n            channels are area masks (e.g. eye/mouth masks)\n        \"\"\"\n        logger.debug(\"Loading minibatch generator: (image_count: %s, do_shuffle: %s)\",\n                     len(self._images), do_shuffle)\n\n        def _img_iter(imgs):\n            \"\"\" Infinite iterator for recursing through image list and reshuffling at each epoch\"\"\"\n            while True:\n                if do_shuffle:\n                    shuffle(imgs)\n                for img in imgs:\n                    yield img\n\n        img_iter = _img_iter(self._images[:])\n        while True:\n            img_paths = [next(img_iter)  # pylint:disable=stop-iteration-return\n                         for _ in range(self._batch_size)]\n            retval = self._process_batch(img_paths)\n            yield retval\n\n    def _get_images_with_meta(self, filenames: list[str]) -> tuple[np.ndarray, list[DetectedFace]]:\n        \"\"\" Obtain the raw face images with associated :class:`DetectedFace` objects for this\n        batch.\n\n        If this is the first time a face has been loaded, then it's meta data is extracted\n        from the png header and added to :attr:`_face_cache`.\n\n        Parameters\n        ----------\n        filenames: list\n            List of full paths to image file names\n\n        Returns\n        -------\n        raw_faces: :class:`numpy.ndarray`\n            The full sized batch of training images for the given filenames\n        list\n            Batch of :class:`~lib.align.DetectedFace` objects for the given filename including the\n            aligned face objects for the model output size\n        \"\"\"\n        if not self._face_cache.cache_full:\n            raw_faces = self._face_cache.cache_metadata(filenames)\n        else:\n            raw_faces = read_image_batch(filenames)\n\n        detected_faces = self._face_cache.get_items(filenames)\n        logger.trace(  # type:ignore[attr-defined]\n            \"filenames: %s, raw_faces: '%s', detected_faces: %s\",\n            filenames, raw_faces.shape, len(detected_faces))\n        return raw_faces, detected_faces\n\n    def _crop_to_coverage(self,\n                          filenames: list[str],\n                          images: np.ndarray,\n                          detected_faces: list[DetectedFace],\n                          batch: np.ndarray) -> None:\n        \"\"\" Crops the training image out of the full extract image based on the centering and\n        coveage used in the user's configuration settings.\n\n        If legacy extract images are being used then this just returns the extracted batch with\n        their corresponding landmarks.\n\n        Uses thread pool execution for about a 33% speed increase @ 64 batch size\n\n        Parameters\n        ----------\n        filenames: list\n            The list of filenames that correspond to this batch\n        images: :class:`numpy.ndarray`\n            The batch of faces that have been loaded from disk\n        detected_faces: list\n            The list of :class:`lib.align.DetectedFace` items corresponding to the batch\n        batch: :class:`np.ndarray`\n            The pre-allocated array to hold this batch\n        \"\"\"\n        logger.trace(  # type:ignore[attr-defined]\n            \"Cropping training images info: (filenames: %s, side: '%s')\", filenames, self._side)\n\n        with futures.ThreadPoolExecutor() as executor:\n            proc = {executor.submit(face.aligned.extract_face, img): idx\n                    for idx, (face, img) in enumerate(zip(detected_faces, images))}\n\n            for future in futures.as_completed(proc):\n                batch[proc[future], ..., :3] = future.result()\n\n    def _apply_mask(self, detected_faces: list[DetectedFace], batch: np.ndarray) -> None:\n        \"\"\" Applies the masks to the 4th channel of the batch.\n\n        If the configuration options `eye_multiplier` and/or `mouth_multiplier` are greater than 1\n        then these masks are applied to the final channels of the batch respectively.\n\n        If masks are not being used then this function returns having done nothing\n\n        Parameters\n        ----------\n        detected_face: list\n            The list of :class:`~lib.align.DetectedFace` objects corresponding to the batch\n        batch: :class:`numpy.ndarray`\n            The preallocated array to apply masks to\n        side: str\n            '\"a\"' or '\"b\"' the side that is being processed\n        \"\"\"\n        if not self._use_mask:\n            return\n\n        masks = np.array([face.get_training_masks() for face in detected_faces])\n        batch[..., 3:] = masks\n\n        logger.trace(\"side: %s, masks: %s, batch: %s\",  # type:ignore[attr-defined]\n                     self._side, masks.shape, batch.shape)\n\n    def _process_batch(self, filenames: list[str]) -> BatchType:\n        \"\"\" Prepares data for feeding through subclassed methods.\n\n        If this is the first time a face has been loaded, then it's meta data is extracted from the\n        png header and added to :attr:`_face_cache`\n\n        Parameters\n        ----------\n        filenames: list\n            List of full paths to image file names for a single batch\n\n        Returns\n        -------\n        :class:`numpy.ndarray`\n            4-dimensional array of faces to feed the training the model.\n        list\n            List of 4-dimensional :class:`numpy.ndarray`. The number of channels here will vary.\n            The first 3 channels are (rgb/bgr). The 4th channel is the face mask. Any subsequent\n            channels are area masks (e.g. eye/mouth masks)\n        \"\"\"\n        raw_faces, detected_faces = self._get_images_with_meta(filenames)\n        batch = self._buffer()\n        self._crop_to_coverage(filenames, raw_faces, detected_faces, batch)\n        self._apply_mask(detected_faces, batch)\n        feed, targets = self.process_batch(filenames, raw_faces, detected_faces, batch)\n\n        logger.trace(  # type:ignore[attr-defined]\n            \"Processed %s batch side %s. (filenames: %s, feed: %s, targets: %s)\",\n            self.__class__.__name__, self._side, filenames, feed.shape, [t.shape for t in targets])\n\n        return feed, targets\n\n    def process_batch(self,\n                      filenames: list[str],\n                      images: np.ndarray,\n                      detected_faces: list[DetectedFace],\n                      batch: np.ndarray) -> BatchType:\n        \"\"\" Override for processing the batch for the current generator.\n\n        Parameters\n        ----------\n        filenames: list\n            List of full paths to image file names for a single batch\n        images: :class:`numpy.ndarray`\n            The batch of faces corresponding to the filenames\n        detected_faces: list\n            List of :class:`~lib.align.DetectedFace` objects with aligned data and masks loaded for\n            the current batch\n        batch: :class:`numpy.ndarray`\n            The pre-allocated batch with images and masks populated for the selected coverage and\n            centering\n\n        Returns\n        -------\n        list\n            4-dimensional array of faces to feed the training the model.\n        list\n            List of 4-dimensional :class:`numpy.ndarray`. The number of channels here will vary.\n            The first 3 channels are (rgb/bgr). The 4th channel is the face mask. Any subsequent\n            channels are area masks (e.g. eye/mouth masks)\n        \"\"\"\n        raise NotImplementedError()\n\n    def _set_color_order(self, batch) -> None:\n        \"\"\" Set the color order correctly for the model's input type.\n\n        batch: :class:`numpy.ndarray`\n            The pre-allocated batch with images in the first 3 channels in BGR order\n        \"\"\"\n        if self._color_order == \"rgb\":\n            batch[..., :3] = batch[..., [2, 1, 0]]\n\n    def _to_float32(self, in_array: np.ndarray) -> np.ndarray:\n        \"\"\" Cast an UINT8 array in 0-255 range to float32 in 0.0-1.0 range.\n\n        in_array: :class:`numpy.ndarray`\n            The input uint8 array\n        \"\"\"\n        return ne.evaluate(\"x / c\",\n                           local_dict={\"x\": in_array, \"c\": np.float32(255)},\n                           casting=\"unsafe\")\n\n\nclass TrainingDataGenerator(DataGenerator):\n    \"\"\" A Training Data Generator for compiling data for feeding to a model.\n\n    This class is called from :mod:`plugins.train.trainer._base` and launches a background\n    iterator that compiles augmented data, target data and sample data.\n\n    Parameters\n    ----------\n    model: :class:`~plugins.train.model.ModelBase`\n        The model that this data generator is feeding\n    config: dict\n        The configuration `dict` generated from :file:`config.train.ini` containing the trainer\n        plugin configuration options.\n    side: {'a' or 'b'}\n        The side of the model that this iterator is for.\n    images: list\n        A list of image paths that will be used to compile the final augmented data from.\n    batch_size: int\n        The batch size for this iterator. Images will be returned in :class:`numpy.ndarray`\n        objects of this size from the iterator.\n    \"\"\"\n    def __init__(self,\n                 config: dict[str, ConfigValueType],\n                 model: ModelBase,\n                 side: T.Literal[\"a\", \"b\"],\n                 images: list[str],\n                 batch_size: int) -> None:\n        super().__init__(config, model, side, images, batch_size)\n        self._augment_color = not model.command_line_arguments.no_augment_color\n        self._no_flip = model.command_line_arguments.no_flip\n        self._no_warp = model.command_line_arguments.no_warp\n        self._warp_to_landmarks = (not self._no_warp\n                                   and model.command_line_arguments.warp_to_landmarks)\n\n        if self._warp_to_landmarks:\n            self._face_cache.pre_fill(images, side)\n        self._processing = ImageAugmentation(batch_size,\n                                             self._process_size,\n                                             self._config)\n        self._nearest_landmarks: dict[str, tuple[str, ...]] = {}\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    def _create_targets(self, batch: np.ndarray) -> list[np.ndarray]:\n        \"\"\" Compile target images, with masks, for the model output sizes.\n\n        Parameters\n        ----------\n        batch: :class:`numpy.ndarray`\n            This should be a 4-dimensional array of training images in the format (`batch size`,\n            `height`, `width`, `channels`). Targets should be requested after performing image\n            transformations but prior to performing warps. The 4th channel should be the mask.\n            Any channels above the 4th should be any additional area masks (e.g. eye/mouth) that\n            are required.\n\n        Returns\n        -------\n        list\n            List of 4-dimensional target images, at all model output sizes, with masks compiled\n            into channels 4+ for each output size\n        \"\"\"\n        logger.trace(\"Compiling targets: batch shape: %s\",  # type:ignore[attr-defined]\n                     batch.shape)\n        if len(self._output_sizes) == 1 and self._output_sizes[0] == self._process_size:\n            # Rolling buffer here makes next to no difference, so just create array on the fly\n            retval = [self._to_float32(batch)]\n        else:\n            retval = [self._to_float32(np.array([cv2.resize(image,\n                                                            (size, size),\n                                                            interpolation=cv2.INTER_AREA)\n                                                 for image in batch]))\n                      for size in self._output_sizes]\n        logger.trace(\"Processed targets: %s\",  # type:ignore[attr-defined]\n                     [t.shape for t in retval])\n        return retval\n\n    def process_batch(self,\n                      filenames: list[str],\n                      images: np.ndarray,\n                      detected_faces: list[DetectedFace],\n                      batch: np.ndarray) -> BatchType:\n        \"\"\" Performs the augmentation and compiles target images and samples.\n\n        Parameters\n        ----------\n        filenames: list\n            List of full paths to image file names for a single batch\n        images: :class:`numpy.ndarray`\n            The batch of faces corresponding to the filenames\n        detected_faces: list\n            List of :class:`~lib.align.DetectedFace` objects with aligned data and masks loaded for\n            the current batch\n        batch: :class:`numpy.ndarray`\n            The pre-allocated batch with images and masks populated for the selected coverage and\n            centering\n\n        Returns\n        -------\n        feed: :class:`numpy.ndarray`\n            4-dimensional array of faces to feed the training the model (:attr:`x` parameter for\n            :func:`keras.models.model.train_on_batch`.). The array returned is in the format\n            (`batch size`, `height`, `width`, `channels`).\n        targets: list\n            List of 4-dimensional :class:`numpy.ndarray` objects in the order and size of each\n            output of the model. The format of these arrays will be (`batch size`, `height`,\n            `width`, `x`). This is the :attr:`y` parameter for\n            :func:`keras.models.model.train_on_batch`. The number of channels here will vary.\n            The first 3 channels are (rgb/bgr). The 4th channel is the face mask. Any subsequent\n            channels are area masks (e.g. eye/mouth masks)\n        \"\"\"\n        logger.trace(\"Process training: (side: '%s', filenames: '%s', images: %s, \"  # type:ignore\n                     \"batch: %s, detected_faces: %s)\", self._side, filenames, images.shape,\n                     batch.shape, len(detected_faces))\n\n        # Color Augmentation of the image only\n        if self._augment_color:\n            batch[..., :3] = self._processing.color_adjust(batch[..., :3])\n\n        # Random Transform and flip\n        self._processing.transform(batch)\n\n        if not self._no_flip:\n            self._processing.random_flip(batch)\n\n        # Switch color order for RGB models\n        self._set_color_order(batch)\n\n        # Get Targets\n        targets = self._create_targets(batch)\n\n        # TODO Look at potential for applying mask on input\n        # Random Warp\n        if self._warp_to_landmarks:\n            landmarks = np.array([face.aligned.landmarks for face in detected_faces])\n            batch_dst_pts = self._get_closest_match(filenames, landmarks)\n            warp_kwargs = {\"batch_src_points\": landmarks, \"batch_dst_points\": batch_dst_pts}\n        else:\n            warp_kwargs = {}\n\n        warped = batch[..., :3] if self._no_warp else self._processing.warp(\n            batch[..., :3],\n            self._warp_to_landmarks,\n            **warp_kwargs)\n\n        if self._model_input_size != self._process_size:\n            feed = self._to_float32(np.array([cv2.resize(image,\n                                                         (self._model_input_size,\n                                                          self._model_input_size),\n                                                         interpolation=cv2.INTER_AREA)\n                                              for image in warped]))\n        else:\n            feed = self._to_float32(warped)\n\n        return feed, targets\n\n    def _get_closest_match(self, filenames: list[str], batch_src_points: np.ndarray) -> np.ndarray:\n        \"\"\" Only called if the :attr:`_warp_to_landmarks` is ``True``. Gets the closest\n        matched 68 point landmarks from the opposite training set.\n\n        Parameters\n        ----------\n        filenames: list\n            Filenames for current batch\n        batch_src_points: :class:`np.ndarray`\n            The source landmarks for the current batch\n\n        Returns\n        -------\n        :class:`np.ndarray`\n            Randomly selected closest matches from the other side's landmarks\n        \"\"\"\n        logger.trace(  # type:ignore[attr-defined]\n            \"Retrieving closest matched landmarks: (filenames: '%s', src_points: '%s')\",\n            filenames, batch_src_points)\n        lm_side: T.Literal[\"a\", \"b\"] = \"a\" if self._side == \"b\" else \"b\"\n        other_cache = get_cache(lm_side)\n        landmarks = other_cache.aligned_landmarks\n\n        try:\n            closest_matches = [self._nearest_landmarks[os.path.basename(filename)]\n                               for filename in filenames]\n        except KeyError:\n            # Resize mismatched training image size landmarks\n            sizes = {side: cache.size for side, cache in zip((self._side, lm_side),\n                                                             (self._face_cache, other_cache))}\n            if len(set(sizes.values())) > 1:\n                scale = sizes[self._side] / sizes[lm_side]\n                landmarks = {key: lms * scale for key, lms in landmarks.items()}\n            closest_matches = self._cache_closest_matches(filenames, batch_src_points, landmarks)\n\n        batch_dst_points = np.array([landmarks[choice(fname)] for fname in closest_matches])\n        logger.trace(\"Returning: (batch_dst_points: %s)\",  # type:ignore[attr-defined]\n                     batch_dst_points.shape)\n        return batch_dst_points\n\n    def _cache_closest_matches(self,\n                               filenames: list[str],\n                               batch_src_points: np.ndarray,\n                               landmarks: dict[str, np.ndarray]) -> list[tuple[str, ...]]:\n        \"\"\" Cache the nearest landmarks for this batch\n\n        Parameters\n        ----------\n        filenames: list\n            Filenames for current batch\n        batch_src_points: :class:`np.ndarray`\n            The source landmarks for the current batch\n        landmarks: dict\n            The destination landmarks with associated filenames\n\n        \"\"\"\n        logger.trace(\"Caching closest matches\")  # type:ignore\n        dst_landmarks = list(landmarks.items())\n        dst_points = np.array([lm[1] for lm in dst_landmarks])\n        batch_closest_matches: list[tuple[str, ...]] = []\n\n        for filename, src_points in zip(filenames, batch_src_points):\n            closest = (np.mean(np.square(src_points - dst_points), axis=(1, 2))).argsort()[:10]\n            closest_matches = tuple(dst_landmarks[i][0] for i in closest)\n            self._nearest_landmarks[os.path.basename(filename)] = closest_matches\n            batch_closest_matches.append(closest_matches)\n        logger.trace(\"Cached closest matches\")  # type:ignore\n        return batch_closest_matches\n\n\nclass PreviewDataGenerator(DataGenerator):\n    \"\"\" Generator for compiling images for generating previews.\n\n    This class is called from :mod:`plugins.train.trainer._base` and launches a background\n    iterator that compiles sample preview data for feeding the model's predict function and for\n    display.\n\n    Parameters\n    ----------\n    model: :class:`~plugins.train.model.ModelBase`\n        The model that this data generator is feeding\n    config: dict\n        The configuration `dict` generated from :file:`config.train.ini` containing the trainer\n        plugin configuration options.\n    side: {'a' or 'b'}\n        The side of the model that this iterator is for.\n    images: list\n        A list of image paths that will be used to compile the final images.\n    batch_size: int\n        The batch size for this iterator. Images will be returned in :class:`numpy.ndarray`\n        objects of this size from the iterator.\n    \"\"\"\n    def _create_samples(self,\n                        images: np.ndarray,\n                        detected_faces: list[DetectedFace]) -> list[np.ndarray]:\n        \"\"\" Compile the 'sample' images. These are the 100% coverage images which hold the model\n        output in the preview window.\n\n        Parameters\n        ----------\n        images: :class:`numpy.ndarray`\n            The original batch of images as loaded from disk.\n        detected_faces: list\n            List of :class:`~lib.align.DetectedFace` for the current batch\n\n        Returns\n        -------\n        list\n            List of 4-dimensional target images, at final model output size\n        \"\"\"\n        logger.trace(  # type:ignore[attr-defined]\n            \"Compiling samples: images shape: %s, detected_faces: %s \",\n            images.shape, len(detected_faces))\n        output_size = self._output_sizes[-1]\n        full_size = 2 * int(np.rint((output_size / self._coverage_ratio) / 2))\n\n        assert self._config[\"centering\"] in T.get_args(CenteringType)\n        retval = np.empty((full_size, full_size, 3), dtype=\"float32\")\n        retval = self._to_float32(np.array([\n            AlignedFace(face.landmarks_xy,\n                        image=images[idx],\n                        centering=T.cast(CenteringType,\n                                         self._config[\"centering\"]),\n                        size=full_size,\n                        dtype=\"uint8\",\n                        is_aligned=True).face\n            for idx, face in enumerate(detected_faces)]))\n\n        logger.trace(\"Processed samples: %s\", retval.shape)  # type:ignore[attr-defined]\n        return [retval]\n\n    def process_batch(self,\n                      filenames: list[str],\n                      images: np.ndarray,\n                      detected_faces: list[DetectedFace],\n                      batch: np.ndarray) -> BatchType:\n        \"\"\" Creates the full size preview images and the sub-cropped images for feeding the model's\n        predict function.\n\n        Parameters\n        ----------\n        filenames: list\n            List of full paths to image file names for a single batch\n        images: :class:`numpy.ndarray`\n            The batch of faces corresponding to the filenames\n        detected_faces: list\n            List of :class:`~lib.align.DetectedFace` objects with aligned data and masks loaded for\n            the current batch\n        batch: :class:`numpy.ndarray`\n            The pre-allocated batch with images and masks populated for the selected coverage and\n            centering\n\n        Returns\n        -------\n        feed: :class:`numpy.ndarray`\n            List of 4-dimensional :class:`numpy.ndarray` objects at model output size for feeding\n            the model's predict function. The first 3 channels are (rgb/bgr). The 4th channel is\n            the face mask.\n        samples: list\n            4-dimensional array containing the 100% coverage images at the model's centering for\n            for generating previews. The array returned is in the format\n            (`batch size`, `height`, `width`, `channels`).\n        \"\"\"\n        logger.trace(\"Process preview: (side: '%s', filenames: '%s', images: %s, \"  # type:ignore\n                     \"batch: %s, detected_faces: %s)\", self._side, filenames, images.shape,\n                     batch.shape, len(detected_faces))\n\n        # Switch color order for RGB models\n        self._set_color_order(batch)\n        self._set_color_order(images)\n\n        if not self._use_mask:\n            mask = np.zeros_like(batch[..., 0])[..., None] + 255\n            batch = np.concatenate([batch, mask], axis=-1)\n\n        feed = self._to_float32(batch[..., :4])  # Don't resize here: we want masks at output res.\n\n        # If user sets model input size as larger than output size, the preview will error, so\n        # resize in these rare instances\n        out_size = max(self._output_sizes)\n        if self._process_size > out_size:\n            feed = np.array([cv2.resize(img, (out_size, out_size), interpolation=cv2.INTER_AREA)\n                             for img in feed])\n\n        samples = self._create_samples(images, detected_faces)\n\n        return feed, samples\n\n\nclass Feeder():\n    \"\"\" Handles the processing of a Batch for training the model and generating samples.\n\n    Parameters\n    ----------\n    images: dict\n        The list of full paths to the training images for this :class:`_Feeder` for each side\n    model: plugin from :mod:`plugins.train.model`\n        The selected model that will be running this trainer\n    batch_size: int\n        The size of the batch to be processed for each side at each iteration\n    config: dict\n        The configuration for this trainer\n    include_preview: bool, optional\n        ``True`` to create a feeder for generating previews. Default: ``True``\n    \"\"\"\n    def __init__(self,\n                 images: dict[T.Literal[\"a\", \"b\"], list[str]],\n                 model: ModelBase,\n                 batch_size: int,\n                 config: dict[str, ConfigValueType],\n                 include_preview: bool = True) -> None:\n        logger.debug(\"Initializing %s: num_images: %s, batch_size: %s, config: %s, \"\n                     \"include_preview: %s)\", self.__class__.__name__,\n                     {k: len(v) for k, v in images.items()}, batch_size, config, include_preview)\n        self._model = model\n        self._images = images\n        self._batch_size = batch_size\n        self._config = config\n        self._feeds = {\n            side: self._load_generator(side, False).minibatch_ab()\n            for side in T.get_args(T.Literal[\"a\", \"b\"])}\n\n        self._display_feeds = {\"preview\": self._set_preview_feed() if include_preview else {},\n                               \"timelapse\": {}}\n        logger.debug(\"Initialized %s:\", self.__class__.__name__)\n\n    def _load_generator(self,\n                        side: T.Literal[\"a\", \"b\"],\n                        is_display: bool,\n                        batch_size: int | None = None,\n                        images: list[str] | None = None) -> DataGenerator:\n        \"\"\" Load the :class:`~lib.training_data.TrainingDataGenerator` for this feeder.\n\n        Parameters\n        ----------\n        side: [\"a\", \"b\"]\n            The side of the model to load the generator for\n        is_display: bool\n            ``True`` if the generator is for creating preview/time-lapse images. ``False`` if it is\n            for creating training images\n        batch_size: int, optional\n            If ``None`` then the batch size selected in command line arguments is used, otherwise\n            the batch size provided here is used.\n        images: list, optional. Default: ``None``\n            If provided then this will be used as the list of images for the generator. If ``None``\n            then the training folder images for the side will be used. Default: ``None``\n\n        Returns\n        -------\n        :class:`~lib.training_data.TrainingDataGenerator`\n            The training data generator\n        \"\"\"\n        logger.debug(\"Loading generator, side: %s, is_display: %s,  batch_size: %s\",\n                     side, is_display, batch_size)\n        generator = PreviewDataGenerator if is_display else TrainingDataGenerator\n        retval = generator(self._config,\n                           self._model,\n                           side,\n                           self._images[side] if images is None else images,\n                           self._batch_size if batch_size is None else batch_size)\n        return retval\n\n    def _set_preview_feed(self) -> dict[T.Literal[\"a\", \"b\"], Generator[BatchType, None, None]]:\n        \"\"\" Set the preview feed for this feeder.\n\n        Creates a generator from :class:`lib.training_data.PreviewDataGenerator` specifically\n        for previews for the feeder.\n\n        Returns\n        -------\n        dict\n            The side (\"a\" or \"b\") as key, :class:`~lib.training_data.PreviewDataGenerator` as\n            value.\n        \"\"\"\n        retval: dict[T.Literal[\"a\", \"b\"], Generator[BatchType, None, None]] = {}\n        num_images = self._config.get(\"preview_images\", 14)\n        assert isinstance(num_images, int)\n        for side in T.get_args(T.Literal[\"a\", \"b\"]):\n            logger.debug(\"Setting preview feed: (side: '%s')\", side)\n            preview_images = min(max(num_images, 2), 16)\n            batchsize = min(len(self._images[side]), preview_images)\n            retval[side] = self._load_generator(side,\n                                                True,\n                                                batch_size=batchsize).minibatch_ab()\n        return retval\n\n    def get_batch(self) -> tuple[list[list[np.ndarray]], ...]:\n        \"\"\" Get the feed data and the targets for each training side for feeding into the model's\n        train function.\n\n        Returns\n        -------\n        model_inputs: list\n            The inputs to the model for each side A and B\n        model_targets: list\n            The targets for the model for each side A and B\n        \"\"\"\n        model_inputs: list[list[np.ndarray]] = []\n        model_targets: list[list[np.ndarray]] = []\n        for side in (\"a\", \"b\"):\n            side_feed, side_targets = next(self._feeds[side])\n            if self._model.config[\"learn_mask\"]:  # Add the face mask as it's own target\n                side_targets += [side_targets[-1][..., 3][..., None]]\n            logger.trace(  # type:ignore[attr-defined]\n                \"side: %s, input_shapes: %s, target_shapes: %s\",\n                side, side_feed.shape, [i.shape for i in side_targets])\n            model_inputs.append([side_feed])\n            model_targets.append(side_targets)\n\n        return model_inputs, model_targets\n\n    def generate_preview(self, is_timelapse: bool = False\n                         ) -> dict[T.Literal[\"a\", \"b\"], list[np.ndarray]]:\n        \"\"\" Generate the images for preview window or timelapse\n\n        Parameters\n        ----------\n        is_timelapse, bool, optional\n            ``True`` if preview is to be generated for a Timelapse otherwise ``False``.\n            Default: ``False``\n\n        Returns\n        -------\n        dict\n            Dictionary for side A and B of list of numpy arrays corresponding to the\n            samples, targets and masks for this preview\n        \"\"\"\n        logger.debug(\"Generating preview (is_timelapse: %s)\", is_timelapse)\n\n        batchsizes: list[int] = []\n        feed: dict[T.Literal[\"a\", \"b\"], np.ndarray] = {}\n        samples: dict[T.Literal[\"a\", \"b\"], np.ndarray] = {}\n        masks: dict[T.Literal[\"a\", \"b\"], np.ndarray] = {}\n\n        # MyPy can't recurse into nested dicts to get the type :(\n        iterator = T.cast(dict[T.Literal[\"a\", \"b\"], \"Generator[BatchType, None, None]\"],\n                          self._display_feeds[\"timelapse\" if is_timelapse else \"preview\"])\n        for side in T.get_args(T.Literal[\"a\", \"b\"]):\n            side_feed, side_samples = next(iterator[side])\n            batchsizes.append(len(side_samples[0]))\n            samples[side] = side_samples[0]\n            feed[side] = side_feed[..., :3]\n            masks[side] = side_feed[..., 3][..., None]\n\n        logger.debug(\"Generated samples: is_timelapse: %s, images: %s\", is_timelapse,\n                     {key: {k: v.shape for k, v in item.items()}\n                      for key, item\n                      in zip((\"feed\", \"samples\", \"sides\"), (feed, samples, masks))})\n        return self.compile_sample(min(batchsizes), feed, samples, masks)\n\n    def compile_sample(self,\n                       image_count: int,\n                       feed: dict[T.Literal[\"a\", \"b\"], np.ndarray],\n                       samples: dict[T.Literal[\"a\", \"b\"], np.ndarray],\n                       masks: dict[T.Literal[\"a\", \"b\"], np.ndarray]\n                       ) -> dict[T.Literal[\"a\", \"b\"], list[np.ndarray]]:\n        \"\"\" Compile the preview samples for display.\n\n        Parameters\n        ----------\n        image_count: int\n            The number of images to limit the sample output to.\n        feed: dict\n            Dictionary for side \"a\", \"b\" of :class:`numpy.ndarray`. The images that should be fed\n            into the model for obtaining a prediction\n        samples: dict\n            Dictionary for side \"a\", \"b\" of :class:`numpy.ndarray`. The 100% coverage target images\n            that should be used for creating the preview.\n        masks: dict\n            Dictionary for side \"a\", \"b\" of :class:`numpy.ndarray`. The masks that should be used\n            for creating the preview.\n\n        Returns\n        -------\n        list\n            The list of samples, targets and masks as :class:`numpy.ndarrays` for creating a\n            preview image\n         \"\"\"\n        num_images = self._config.get(\"preview_images\", 14)\n        assert isinstance(num_images, int)\n        num_images = min(image_count, num_images)\n        retval: dict[T.Literal[\"a\", \"b\"], list[np.ndarray]] = {}\n        for side in T.get_args(T.Literal[\"a\", \"b\"]):\n            logger.debug(\"Compiling samples: (side: '%s', samples: %s)\", side, num_images)\n            retval[side] = [feed[side][0:num_images],\n                            samples[side][0:num_images],\n                            masks[side][0:num_images]]\n        logger.debug(\"Compiled Samples: %s\", {k: [i.shape for i in v] for k, v in retval.items()})\n        return retval\n\n    def set_timelapse_feed(self,\n                           images: dict[T.Literal[\"a\", \"b\"], list[str]],\n                           batch_size: int) -> None:\n        \"\"\" Set the time-lapse feed for this feeder.\n\n        Creates a generator from :class:`lib.training_data.PreviewDataGenerator` specifically\n        for generating time-lapse previews for the feeder.\n\n        Parameters\n        ----------\n        images: dict\n            The list of full paths to the images for creating the time-lapse for each side\n        batch_size: int\n            The number of images to be used to create the time-lapse preview.\n        \"\"\"\n        logger.debug(\"Setting time-lapse feed: (input_images: '%s', batch_size: %s)\",\n                     images, batch_size)\n\n        # MyPy can't recurse into nested dicts to get the type :(\n        iterator = T.cast(dict[T.Literal[\"a\", \"b\"], \"Generator[BatchType, None, None]\"],\n                          self._display_feeds[\"timelapse\"])\n\n        for side in T.get_args(T.Literal[\"a\", \"b\"]):\n            imgs = images[side]\n            logger.debug(\"Setting preview feed: (side: '%s', images: %s)\", side, len(imgs))\n\n            iterator[side] = self._load_generator(side,\n                                                  True,\n                                                  batch_size=batch_size,\n                                                  images=imgs).minibatch_ab(do_shuffle=False)\n        logger.debug(\"Set time-lapse feed: %s\", self._display_feeds[\"timelapse\"])\n", "lib/training/preview_tk.py": "#!/usr/bin/python\n\"\"\" The pop up preview window for Faceswap.\n\nIf Tkinter is installed, then this will be used to manage the preview image, otherwise we\nfallback to opencv's imshow\n\"\"\"\nfrom __future__ import annotations\nimport logging\nimport os\nimport sys\nimport tkinter as tk\nimport typing as T\n\nfrom datetime import datetime\nfrom platform import system\nfrom tkinter import ttk\nfrom math import ceil, floor\n\nfrom PIL import Image, ImageTk\n\nimport cv2\n\nfrom .preview_cv import PreviewBase, TriggerKeysType\n\nif T.TYPE_CHECKING:\n    import numpy as np\n    from .preview_cv import PreviewBuffer, TriggerType\n\nlogger = logging.getLogger(__name__)\n\n\nclass _Taskbar():\n    \"\"\" Taskbar at bottom of Preview window\n\n    Parameters\n    ----------\n    parent: :class:`tkinter.Frame`\n        The parent frame that holds the canvas and taskbar\n    taskbar: :class:`tkinter.ttk.Frame` or ``None``\n        None if preview is a pop-up window otherwise ttk.Frame if taskbar is managed by the GUI\n    \"\"\"\n    def __init__(self, parent: tk.Frame, taskbar: ttk.Frame | None) -> None:\n        logger.debug(\"Initializing %s (parent: '%s', taskbar: %s)\",\n                     self.__class__.__name__, parent, taskbar)\n        self._is_standalone = taskbar is None\n        self._gui_mapped: list[tk.Widget] = []\n        self._frame = tk.Frame(parent) if taskbar is None else taskbar\n\n        self._min_max_scales = (20, 400)\n        self._vars = {\"save\": tk.BooleanVar(),\n                      \"scale\": tk.StringVar(),\n                      \"slider\": tk.IntVar(),\n                      \"interpolator\": tk.IntVar()}\n        self._interpolators = [(\"nearest_neighbour\", cv2.INTER_NEAREST),\n                               (\"bicubic\", cv2.INTER_CUBIC)]\n        self._scale = self._add_scale_combo()\n        self._slider = self._add_scale_slider()\n        self._add_interpolator_radio()\n\n        if self._is_standalone:\n            self._add_save_button()\n            self._frame.pack(side=tk.BOTTOM, fill=tk.X, padx=2, pady=2)\n\n        logger.debug(\"Initialized %s ('%s')\", self.__class__.__name__, self)\n\n    @property\n    def min_scale(self) -> int:\n        \"\"\" int: The minimum allowed scale \"\"\"\n        return self._min_max_scales[0]\n\n    @property\n    def max_scale(self) -> int:\n        \"\"\" int: The maximum allowed scale \"\"\"\n        return self._min_max_scales[1]\n\n    @property\n    def save_var(self) -> tk.BooleanVar:\n        \"\"\":class:`tkinter.IntVar`: Variable which is set to ``True`` when the save button has\n        been. pressed \"\"\"\n        retval = self._vars[\"save\"]\n        assert isinstance(retval, tk.BooleanVar)\n        return retval\n\n    @property\n    def scale_var(self) -> tk.StringVar:\n        \"\"\":class:`tkinter.StringVar`: The variable holding the currently selected \"##%\" formatted\n        percentage scaling amount displayed in the Combobox. \"\"\"\n        retval = self._vars[\"scale\"]\n        assert isinstance(retval, tk.StringVar)\n        return retval\n\n    @property\n    def slider_var(self) -> tk.IntVar:\n        \"\"\":class:`tkinter.IntVar`: The variable holding the currently selected percentage scaling\n        amount in the slider. \"\"\"\n        retval = self._vars[\"slider\"]\n        assert isinstance(retval, tk.IntVar)\n        return retval\n\n    @property\n    def interpolator_var(self) -> tk.IntVar:\n        \"\"\":class:`tkinter.IntVar`: The variable holding the CV2 Interpolator Enum. \"\"\"\n        retval = self._vars[\"interpolator\"]\n        assert isinstance(retval, tk.IntVar)\n        return retval\n\n    def _track_widget(self, widget: tk.Widget) -> None:\n        \"\"\" If running embedded in the GUI track the widgets so that they can be destroyed if\n        the preview is disabled \"\"\"\n        if self._is_standalone:\n            return\n        logger.debug(\"Tracking option bar widget for GUI: %s\", widget)\n        self._gui_mapped.append(widget)\n\n    def _add_scale_combo(self) -> ttk.Combobox:\n        \"\"\" Add a scale combo for selecting zoom amount.\n\n        Returns\n        -------\n        :class:`tkinter.ttk.Combobox`\n            The Combobox widget\n        \"\"\"\n        logger.debug(\"Adding scale combo\")\n        self.scale_var.set(\"100%\")\n        scale = ttk.Combobox(self._frame,\n                             textvariable=self.scale_var,\n                             values=[\"Fit\"],\n                             state=\"readonly\",\n                             width=10)\n        scale.pack(side=tk.RIGHT)\n        scale.bind(\"<FocusIn>\", self._clear_combo_focus)  # Remove auto-focus on widget text box\n        self._track_widget(scale)\n        logger.debug(\"Added scale combo: '%s'\", scale)\n        return scale\n\n    def _clear_combo_focus(self, *args) -> None:  # pylint:disable=unused-argument\n        \"\"\" Remove the highlighting and stealing of focus that the combobox annoyingly\n        implements. \"\"\"\n        logger.debug(\"Clearing scale combo focus\")\n        self._scale.selection_clear()\n        self._scale.winfo_toplevel().focus_set()\n        logger.debug(\"Cleared scale combo focus\")\n\n    def _add_scale_slider(self) -> tk.Scale:\n        \"\"\" Add a scale slider for zooming the image.\n\n        Returns\n        -------\n        :class:`tkinter.Scale`\n            The scale widget\n        \"\"\"\n        logger.debug(\"Adding scale slider\")\n        self.slider_var.set(100)\n        slider = tk.Scale(self._frame,\n                          orient=tk.HORIZONTAL,\n                          to=self.max_scale,\n                          showvalue=False,\n                          variable=self.slider_var,\n                          command=self._on_slider_update)\n        slider.pack(side=tk.RIGHT)\n        self._track_widget(slider)\n        logger.debug(\"Added scale slider: '%s'\", slider)\n        return slider\n\n    def _add_interpolator_radio(self) -> None:\n        \"\"\" Add a radio box to choose interpolator \"\"\"\n        frame = tk.Frame(self._frame)\n        for text, mode in self._interpolators:\n            logger.debug(\"Adding %s radio button\", text)\n            radio = tk.Radiobutton(frame, text=text, value=mode, variable=self.interpolator_var)\n            radio.pack(side=tk.LEFT, anchor=tk.W)\n            self._track_widget(radio)\n\n            logger.debug(\"Added %s radio button\", radio)\n        self.interpolator_var.set(cv2.INTER_NEAREST)\n        frame.pack(side=tk.RIGHT)\n        self._track_widget(frame)\n\n    def _add_save_button(self) -> None:\n        \"\"\" Add a save button for saving out original preview \"\"\"\n        logger.debug(\"Adding save button\")\n        button = tk.Button(self._frame,\n                           text=\"Save\",\n                           cursor=\"hand2\",\n                           command=lambda: self.save_var.set(True))\n        button.pack(side=tk.LEFT)\n        logger.debug(\"Added save burron: '%s'\", button)\n\n    def _on_slider_update(self, value) -> None:\n        \"\"\" Callback for when the scale slider is adjusted. Adjusts the combo box display to the\n        current slider value.\n\n        Parameters\n        ----------\n        value: int\n            The value that the slider has been set to\n         \"\"\"\n        self.scale_var.set(f\"{value}%\")\n\n    def set_min_max_scale(self, min_scale: int, max_scale: int) -> None:\n        \"\"\" Set the minimum and maximum value that we allow an image to be scaled down to. This\n        impacts the slider and combo box min/max values:\n\n        Parameters\n        ----------\n        min_scale: int\n            The minimum percentage scale that is permitted\n        max_scale: int\n            The maximum percentage scale that is permitted\n        \"\"\"\n        logger.debug(\"Setting min/max scales: (min: %s, max: %s)\", min_scale, max_scale)\n        self._min_max_scales = (min_scale, max_scale)\n        self._slider.config(from_=self.min_scale, to=max_scale)\n        scales = [10, 25, 50, 75, 100, 200, 300, 400, 800]\n        if min_scale not in scales:\n            scales.insert(0, min_scale)\n        if max_scale not in scales:\n            scales.append(max_scale)\n        choices = [\"Fit\", *[f\"{x}%\" for x in scales if self.max_scale >= x >= self.min_scale]]\n        self._scale.config(values=choices)\n        logger.debug(\"Set min/max scale. min_max_scales: %s, scale combo choices: %s\",\n                     self._min_max_scales, choices)\n\n    def cycle_interpolators(self, *args) -> None:  # pylint:disable=unused-argument\n        \"\"\" Cycle interpolators on a keypress callback \"\"\"\n        current = next(i for i in self._interpolators if i[1] == self.interpolator_var.get())\n        next_idx = self._interpolators.index(current) + 1\n        next_idx = 0 if next_idx == len(self._interpolators) else next_idx\n        self.interpolator_var.set(self._interpolators[next_idx][1])\n\n    def destroy_widgets(self) -> None:\n        \"\"\" Remove the taskbar widgets when the preview within the GUI has been disabled \"\"\"\n        if self._is_standalone:\n            return\n\n        for widget in self._gui_mapped:\n            if widget.winfo_ismapped():\n                logger.debug(\"Removing widget: %s\", widget)\n                widget.pack_forget()\n                widget.destroy()\n                del widget\n\n        for var in list(self._vars):\n            logger.debug(\"Deleting tk variable: %s\", var)\n            del self._vars[var]\n\n\nclass _PreviewCanvas(tk.Canvas):  # pylint:disable=too-many-ancestors\n    \"\"\" The canvas that holds the preview image\n\n    Parameters\n    ----------\n    parent: :class:`tkinter.Frame`\n        The parent frame that will hold the Canvas and taskbar\n    scale_var: :class:`tkinter.StringVar`\n        The variable that holds the value from the scale combo box\n    screen_dimensions: tuple\n        The (`width`, `height`) of the displaying monitor\n    is_standalone: bool\n        ``True`` if the preview is standalone, ``False`` if it is in the GUI\n    \"\"\"\n    def __init__(self,\n                 parent: tk.Frame,\n                 scale_var: tk.StringVar,\n                 screen_dimensions: tuple[int, int],\n                 is_standalone: bool) -> None:\n        logger.debug(\"Initializing %s (parent: '%s', scale_var: %s, screen_dimensions: %s)\",\n                     self.__class__.__name__, parent, scale_var, screen_dimensions)\n        frame = tk.Frame(parent)\n        super().__init__(frame)\n\n        self._is_standalone = is_standalone\n        self._screen_dimensions = screen_dimensions\n        self._var_scale = scale_var\n        self._configure_scrollbars(frame)\n        self._image: ImageTk.PhotoImage | None = None\n        self._image_id = self.create_image(self.width / 2,\n                                           self.height / 2,\n                                           anchor=tk.CENTER,\n                                           image=self._image)\n        self.pack(fill=tk.BOTH, expand=True)\n        self.bind(\"<Configure>\", self._resize)\n        frame.pack(side=tk.TOP, fill=tk.BOTH, expand=True)\n        logger.debug(\"Initialized %s ('%s')\", self.__class__.__name__, self)\n\n    @property\n    def image_id(self) -> int:\n        \"\"\" int: The ID of the preview image item within the canvas \"\"\"\n        return self._image_id\n\n    @property\n    def width(self) -> int:\n        \"\"\"int: The pixel width of canvas\"\"\"\n        return self.winfo_width()\n\n    @property\n    def height(self) -> int:\n        \"\"\"int: The pixel width of the canvas\"\"\"\n        return self.winfo_height()\n\n    def _configure_scrollbars(self, frame: tk.Frame) -> None:\n        \"\"\" Add X and Y scrollbars to the frame and set to scroll the canvas.\n\n        Parameters\n        ----------\n        frame: :class:`tkinter.Frame`\n            The parent frame to the canvas\n        \"\"\"\n        logger.debug(\"Configuring scrollbars\")\n        x_scrollbar = tk.Scrollbar(frame, orient=\"horizontal\", command=self.xview)\n        x_scrollbar.pack(side=tk.BOTTOM, fill=tk.X)\n\n        y_scrollbar = tk.Scrollbar(frame, command=self.yview)\n        y_scrollbar.pack(side=tk.RIGHT, fill=tk.Y)\n\n        self.configure(xscrollcommand=x_scrollbar.set, yscrollcommand=y_scrollbar.set)\n        logger.debug(\"Configured scrollbars. x: '%s', y: '%s'\", x_scrollbar, y_scrollbar)\n\n    def _resize(self, event: tk.Event) -> None:  # pylint:disable=unused-argument\n        \"\"\" Place the image in center of canvas on resize event and move to top left\n\n        Parameters\n        ----------\n        event: :class:`tkinter.Event`\n            The canvas resize event. Unused.\n        \"\"\"\n        if self._var_scale.get() == \"Fit\":  # Trigger an update to resize image\n            logger.debug(\"Triggering redraw for 'Fit' Scaling\")\n            self._var_scale.set(\"Fit\")\n            return\n\n        self.configure(scrollregion=self.bbox(\"all\"))\n        self.update_idletasks()\n\n        assert self._image is not None\n        self._center_image(self.width / 2, self.height / 2)\n\n        # Move to top left when resizing into screen dimensions (initial startup)\n        if self.width > self._screen_dimensions[0]:\n            logger.debug(\"Moving image to left edge\")\n            self.xview_moveto(0.0)\n        if self.height > self._screen_dimensions[1]:\n            logger.debug(\"Moving image to top edge\")\n            self.yview_moveto(0.0)\n\n    def _center_image(self, point_x: float, point_y: float) -> None:\n        \"\"\" Center the image on the canvas on a resize or image update.\n\n        Parameters\n        ----------\n        point_x: int\n            The x point to center on\n        point_y: int\n            The y point to center on\n        \"\"\"\n        canvas_location = (self.canvasx(point_x), self.canvasy(point_y))\n        logger.debug(\"Centering canvas for size (%s, %s). New image coordinates: %s\",\n                     point_x, point_y, canvas_location)\n        self.coords(self.image_id, canvas_location)\n\n    def set_image(self,\n                  image: ImageTk.PhotoImage,\n                  center_image: bool = False) -> None:\n        \"\"\" Update the canvas with the given image and update area/scrollbars accordingly\n\n        Parameters\n        ----------\n        image: :class:`ImageTK.PhotoImage`\n            The preview image to display in the canvas\n        bool, optional\n            ``True`` if the image should be re-centered. Default ``True``\n        \"\"\"\n        logger.debug(\"Setting canvas image. ID: %s, size: %s for canvas size: %s (recenter: %s)\",\n                     self.image_id, (image.width(), image.height()), (self.width, self.height),\n                     center_image)\n        self._image = image\n        self.itemconfig(self.image_id, image=self._image)\n\n        if self._is_standalone:  # canvas size should not be updated inside GUI\n            self.config(width=self._image.width(), height=self._image.height())\n\n        self.update_idletasks()\n        if center_image:\n            self._center_image(self.width / 2, self.height / 2)\n        self.configure(scrollregion=self.bbox(\"all\"))\n        logger.debug(\"set canvas image. Canvas size: %s\", (self.width, self.height))\n\n\nclass _Image():\n    \"\"\" Holds the source image and the resized display image for the canvas\n\n    Parameters\n    ----------\n    save_variable: :class:`tkinter.BooleanVar`\n        Variable that indicates a save preview has been requested in standalone mode\n    is_standalone: bool\n        ``True`` if the preview is running in standalone mode. ``False`` if it is running in the\n        GUI\n    \"\"\"\n    def __init__(self, save_variable: tk.BooleanVar, is_standalone: bool) -> None:\n        logger.debug(\"Initializing %s: (save_variable: %s, is_standalone: %s)\",\n                     self.__class__.__name__, save_variable, is_standalone)\n        self._is_standalone = is_standalone\n        self._source: np.ndarray | None = None\n        self._display: ImageTk.PhotoImage | None = None\n        self._scale = 1.0\n        self._interpolation = cv2.INTER_NEAREST\n\n        self._save_var = save_variable\n        self._save_var.trace(\"w\", self.save_preview)\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    @property\n    def display_image(self) -> ImageTk.PhotoImage:\n        \"\"\" :class:`PIL.ImageTk.PhotoImage`: The current display image \"\"\"\n        assert self._display is not None\n        return self._display\n\n    @property\n    def source(self) -> np.ndarray:\n        \"\"\" :class:`PIL.Image.Image`: The current source preview image \"\"\"\n        assert self._source is not None\n        return self._source\n\n    @property\n    def scale(self) -> int:\n        \"\"\"int: The current display scale as a percentage of original image size \"\"\"\n        return int(self._scale * 100)\n\n    def set_source_image(self, name: str, image: np.ndarray) -> None:\n        \"\"\" Set the source image to :attr:`source`\n\n        Parameters\n        ----------\n        name: str\n            The name of the preview image to load\n        image: :class:`numpy.ndarray`\n            The image to use in RGB format\n        \"\"\"\n        logger.debug(\"Setting source image. name: '%s', shape: %s\", name, image.shape)\n        self._source = image\n\n    def set_display_image(self) -> None:\n        \"\"\" Obtain the scaled image and set to :attr:`display_image` \"\"\"\n        logger.debug(\"Setting display image. Scale: %s\", self._scale)\n        image = self.source[..., 2::-1]  # TO RGB\n        if self._scale not in (0.0, 1.0):  # Scale will be 0,0 on initial load in GUI\n            interp = self._interpolation if self._scale > 1.0 else cv2.INTER_NEAREST\n            dims = (int(round(self.source.shape[1] * self._scale, 0)),\n                    int(round(self.source.shape[0] * self._scale, 0)))\n            image = cv2.resize(image, dims, interpolation=interp)\n        self._display = ImageTk.PhotoImage(Image.fromarray(image))\n        logger.debug(\"Set display image. Size: %s\",\n                     (self._display.width(), self._display.height()))\n\n    def set_scale(self, scale: float) -> bool:\n        \"\"\" Set the display scale to the given value.\n\n        Parameters\n        ----------\n        scale: float\n            The value to set scaling to\n\n        Returns\n        -------\n        bool\n            ``True`` if the scale has been changed otherwise ``False``\n        \"\"\"\n        if self._scale == scale:\n            return False\n        logger.debug(\"Setting scale: %s\", scale)\n        self._scale = scale\n        return True\n\n    def set_interpolation(self, interpolation: int) -> bool:\n        \"\"\" Set the interpolation enum to the given value.\n\n        Parameters\n        ----------\n        interpolation: int\n            The value to set interpolation to\n\n        Returns\n        -------\n        bool\n            ``True`` if the interpolation has been changed otherwise ``False``\n        \"\"\"\n        if self._interpolation == interpolation:\n            return False\n        logger.debug(\"Setting interpolation: %s\")\n        self._interpolation = interpolation\n        return True\n\n    def save_preview(self, *args) -> None:\n        \"\"\" Save out the full size preview to the faceswap folder on a save button press\n\n        Parameters\n        ----------\n        args: tuple\n            Tuple containing either the key press event (Ctrl+s shortcut), the tk variable\n            arguments (standalone save button press) or the folder location (GUI save button press)\n        \"\"\"\n        if self._is_standalone and not self._save_var.get() and not isinstance(args[0], tk.Event):\n            return\n\n        if self._is_standalone:\n            root_path = os.path.join(os.path.realpath(os.path.dirname(sys.argv[0])))\n        else:\n            root_path = args[0]\n\n        now = datetime.now().strftime(\"%Y-%m-%d_%H.%M.%S\")\n        filename = os.path.join(root_path, f\"preview_{now}.png\")\n        cv2.imwrite(filename, self.source)\n        print(\"\")\n        logger.info(\"Saved preview to: '%s'\", filename)\n\n        if self._is_standalone:\n            self._save_var.set(False)\n\n\nclass _Bindings():  # pylint:disable=too-few-public-methods\n    \"\"\" Handle Mouse and Keyboard bindings for the canvas.\n\n    Parameters\n    ----------\n    canvas: :class:`_PreviewCanvas`\n        The canvas that holds the preview image\n    taskbar: :class:`_Taskbar`\n        The taskbar widget which holds the scaling variables\n    image: :class:`_Image`\n        The object which holds the source and display version of the preview image\n    is_standalone: bool\n        ``True`` if the preview is standalone, ``False`` if it is embedded in the GUI\n    \"\"\"\n    def __init__(self,\n                 canvas: _PreviewCanvas,\n                 taskbar: _Taskbar,\n                 image: _Image,\n                 is_standalone: bool) -> None:\n        logger.debug(\"Initializing %s (canvas: '%s', taskbar: '%s', image: '%s')\",\n                     self.__class__.__name__, canvas, taskbar, image)\n        self._canvas = canvas\n        self._taskbar = taskbar\n        self._image = image\n\n        self._drag_data: list[float] = [0., 0.]\n        self._set_mouse_bindings()\n        self._set_key_bindings(is_standalone)\n        logger.debug(\"Initialized %s\", self.__class__.__name__,)\n\n    def _on_bound_zoom(self, event: tk.Event) -> None:\n        \"\"\" Action to perform on a valid zoom key press or mouse wheel action\n\n        Parameters\n        ----------\n        event: :class:`tkinter.Event`\n            The key press or mouse wheel event\n        \"\"\"\n        if event.keysym in (\"KP_Add\", \"plus\") or event.num == 4 or event.delta > 0:\n            scale = min(self._taskbar.max_scale, self._image.scale + 25)\n        else:\n            scale = max(self._taskbar.min_scale, self._image.scale - 25)\n        logger.trace(\"Bound zoom action: (event: %s, scale: %s)\", event, scale)  # type: ignore\n        self._taskbar.scale_var.set(f\"{scale}%\")\n\n    def _on_mouse_click(self, event: tk.Event) -> None:\n        \"\"\" log initial click coordinates for mouse click + drag action\n\n        Parameters\n        ----------\n        event: :class:`tkinter.Event`\n            The mouse event\n        \"\"\"\n        self._drag_data = [event.x / self._image.display_image.width(),\n                           event.y / self._image.display_image.height()]\n        logger.trace(\"Mouse click action: (event: %s, drag_data: %s)\",  # type: ignore\n                     event, self._drag_data)\n\n    def _on_mouse_drag(self, event: tk.Event) -> None:\n        \"\"\" Drag image left, right, up or down\n\n        Parameters\n        ----------\n        event: :class:`tkinter.Event`\n            The mouse event\n        \"\"\"\n        location_x = event.x / self._image.display_image.width()\n        location_y = event.y / self._image.display_image.height()\n\n        if self._canvas.xview() != (0.0, 1.0):\n            to_x = min(1.0, max(0.0, self._drag_data[0] - location_x + self._canvas.xview()[0]))\n            self._canvas.xview_moveto(to_x)\n        if self._canvas.yview() != (0.0, 1.0):\n            to_y = min(1.0, max(0.0, self._drag_data[1] - location_y + self._canvas.yview()[0]))\n            self._canvas.yview_moveto(to_y)\n\n        self._drag_data = [location_x, location_y]\n\n    def _on_key_move(self, event: tk.Event) -> None:\n        \"\"\" Action to perform on a valid move key press\n\n        Parameters\n        ----------\n        event: :class:`tkinter.Event`\n            The key press event\n        \"\"\"\n        move_axis = self._canvas.xview if event.keysym in (\"Left\", \"Right\") else self._canvas.yview\n        visible = move_axis()[1] - move_axis()[0]\n        amount = -visible / 25 if event.keysym in (\"Up\", \"Left\") else visible / 25\n        logger.trace(\"Key move event: (event: %s, move_axis: %s, visible: %s, \"  # type: ignore\n                     \"amount: %s)\", move_axis, visible, amount)\n        move_axis(tk.MOVETO, min(1.0, max(0.0, move_axis()[0] + amount)))\n\n    def _set_mouse_bindings(self) -> None:\n        \"\"\" Set the mouse bindings for interacting with the preview image\n\n        Mousewheel: Zoom in and out\n        Mouse click: Move image\n        \"\"\"\n        logger.debug(\"Binding mouse events\")\n        if system() == \"Linux\":\n            self._canvas.tag_bind(self._canvas.image_id, \"<Button-4>\", self._on_bound_zoom)\n            self._canvas.tag_bind(self._canvas.image_id, \"<Button-5>\", self._on_bound_zoom)\n        else:\n            self._canvas.bind(\"<MouseWheel>\", self._on_bound_zoom)\n\n        self._canvas.tag_bind(self._canvas.image_id, \"<Button-1>\", self._on_mouse_click)\n        self._canvas.tag_bind(self._canvas.image_id, \"<B1-Motion>\", self._on_mouse_drag)\n        logger.debug(\"Bound mouse events\")\n\n    def _set_key_bindings(self, is_standalone: bool) -> None:\n        \"\"\" Set the keyboard bindings.\n\n        Up/Down/Left/Right: Moves image\n        +/-: Zooms image\n        ctrl+s: Save\n        i: Cycle interpolators\n\n        Parameters\n        ----------\n        ``True`` if the preview is standalone, ``False`` if it is embedded in the GUI\n        \"\"\"\n        if not is_standalone:\n            # Don't bind keys for GUI as it adds complication\n            return\n        logger.debug(\"Binding key events\")\n        root = self._canvas.winfo_toplevel()\n        for key in (\"Left\", \"Right\", \"Up\", \"Down\"):\n            root.bind(f\"<{key}>\", self._on_key_move)\n        for key in (\"Key-plus\", \"Key-minus\", \"Key-KP_Add\", \"Key-KP_Subtract\"):\n            root.bind(f\"<{key}>\", self._on_bound_zoom)\n        root.bind(\"<Control-s>\", self._image.save_preview)\n        root.bind(\"<i>\", self._taskbar.cycle_interpolators)\n        logger.debug(\"Bound key events\")\n\n\nclass PreviewTk(PreviewBase):\n    \"\"\" Holds a preview window for displaying the pop out preview.\n\n    Parameters\n    ----------\n    preview_buffer: :class:`PreviewBuffer`\n        The thread safe object holding the preview images\n    parent: tkinter widget, optional\n        If this viewer is being called from the GUI the parent widget should be passed in here.\n        If this is a standalone pop-up window then pass ``None``. Default: ``None``\n    taskbar: :class:`tkinter.ttk.Frame`, optional\n        If this viewer is being called from the GUI the parent's option frame should be passed in\n        here. If this is a standalone pop-up window then pass ``None``. Default: ``None``\n    triggers: dict, optional\n        Dictionary of event triggers for pop-up preview. Not required when running inside the GUI.\n        Default: `None`\n    \"\"\"\n    def __init__(self,\n                 preview_buffer: PreviewBuffer,\n                 parent: tk.Widget | None = None,\n                 taskbar: ttk.Frame | None = None,\n                 triggers: TriggerType | None = None) -> None:\n        logger.debug(\"Initializing %s (parent: '%s')\", self.__class__.__name__, parent)\n        super().__init__(preview_buffer, triggers=triggers)\n        self._is_standalone = parent is None\n        self._initialized = False\n        self._root = parent if parent is not None else tk.Tk()\n        self._master_frame = tk.Frame(self._root)\n\n        self._taskbar = _Taskbar(self._master_frame, taskbar)\n\n        self._screen_dimensions = self._get_geometry()\n        self._canvas = _PreviewCanvas(self._master_frame,\n                                      self._taskbar.scale_var,\n                                      self._screen_dimensions,\n                                      self._is_standalone)\n\n        self._image = _Image(self._taskbar.save_var, self._is_standalone)\n\n        _Bindings(self._canvas, self._taskbar, self._image, self._is_standalone)\n\n        self._taskbar.scale_var.trace(\"w\", self._set_scale)\n        self._taskbar.interpolator_var.trace(\"w\", self._set_interpolation)\n\n        self._process_triggers()\n\n        if self._is_standalone:\n            self.pack(fill=tk.BOTH, expand=True)\n\n        self._output_helptext()\n\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n        self._launch()\n\n    @property\n    def master_frame(self) -> tk.Frame:\n        \"\"\" :class:`tkinter.Frame`: The master frame that holds the preview window \"\"\"\n        return self._master_frame\n\n    def pack(self, *args, **kwargs):\n        \"\"\" Redirect calls to pack the widget to pack the actual :attr:`_master_frame`.\n\n        Takes standard :class:`tkinter.Frame` pack arguments\n        \"\"\"\n        logger.debug(\"Packing master frame: (args: %s, kwargs: %s)\", args, kwargs)\n        self._master_frame.pack(*args, **kwargs)\n\n    def save(self, location: str) -> None:\n        \"\"\" Save action to be performed when save button pressed from the GUI.\n\n        location: str\n            Full path to the folder to save the preview image to\n        \"\"\"\n        self._image.save_preview(location)\n\n    def remove_option_controls(self) -> None:\n        \"\"\" Remove the taskbar options controls when the preview is disabled in the GUI \"\"\"\n        self._taskbar.destroy_widgets()\n\n    def _output_helptext(self) -> None:\n        \"\"\" Output the keybindings to Console. \"\"\"\n        if not self._is_standalone:\n            return\n        logger.info(\"---------------------------------------------------\")\n        logger.info(\"  Preview key bindings:\")\n        logger.info(\"    Zoom:              +/-\")\n        logger.info(\"    Toggle Zoom Mode:  i\")\n        logger.info(\"    Move:              arrow keys\")\n        logger.info(\"    Save Preview:      Ctrl+s\")\n        logger.info(\"---------------------------------------------------\")\n\n    def _get_geometry(self) -> tuple[int, int]:\n        \"\"\" Obtain the geometry of the current screen (standalone) or the dimensions of the widget\n        holding the preview window (GUI).\n\n        Just pulling screen width and height does not account for multiple monitors, so dummy in a\n        window to pull actual dimensions before hiding it again.\n\n        Returns\n        -------\n        Tuple\n            The (`width`, `height`) of the current monitor's display\n        \"\"\"\n        if not self._is_standalone:\n            root = self._root.winfo_toplevel()  # Get dims of whole GUI\n            retval = root.winfo_width(), root.winfo_height()\n            logger.debug(\"Obtained frame geometry: %s\", retval)\n            return retval\n\n        assert isinstance(self._root, tk.Tk)\n        logger.debug(\"Obtaining screen geometry\")\n        self._root.update_idletasks()\n        self._root.attributes(\"-fullscreen\", True)\n        self._root.state(\"iconic\")\n        retval = self._root.winfo_width(), self._root.winfo_height()\n        self._root.attributes(\"-fullscreen\", False)\n        self._root.state(\"withdraw\")\n        logger.debug(\"Obtained screen geometry: %s\", retval)\n        return retval\n\n    def _set_min_max_scales(self) -> None:\n        \"\"\" Set the minimum and maximum area that we allow to scale image to. \"\"\"\n        logger.debug(\"Calculating minimum scale for screen dimensions %s\", self._screen_dimensions)\n        half_screen = tuple(x // 2 for x in self._screen_dimensions)\n        min_scales = (half_screen[0] / self._image.source.shape[1],\n                      half_screen[1] / self._image.source.shape[0])\n        min_scale = min(1.0, *min_scales)\n        min_scale = (ceil(min_scale * 10)) * 10\n\n        eight_screen = tuple(x * 8 for x in self._screen_dimensions)\n        max_scales = (eight_screen[0] / self._image.source.shape[1],\n                      eight_screen[1] / self._image.source.shape[0])\n        max_scale = min(8.0, max(1.0, min(max_scales)))\n        max_scale = (floor(max_scale * 10)) * 10\n\n        logger.debug(\"Calculated minimum scale: %s, maximum_scale: %s\", min_scale, max_scale)\n        self._taskbar.set_min_max_scale(min_scale, max_scale)\n\n    def _initialize_window(self) -> None:\n        \"\"\" Initialize the window to fit into the current screen \"\"\"\n        logger.debug(\"Initializing window\")\n        assert isinstance(self._root, tk.Tk)\n        width = min(self._master_frame.winfo_reqwidth(), self._screen_dimensions[0])\n        height = min(self._master_frame.winfo_reqheight(), self._screen_dimensions[1])\n        self._set_min_max_scales()\n        self._root.state(\"normal\")\n        self._root.geometry(f\"{width}x{height}\")\n        self._root.protocol(\"WM_DELETE_WINDOW\", lambda: None)  # Intercept close window\n        self._initialized = True\n        logger.debug(\"Initialized window: (width: %s, height: %s)\", width, height)\n\n    def _update_image(self, center_image: bool = False) -> None:\n        \"\"\" Update the image displayed in the canvas and set the canvas size and scroll region\n        accordingly\n\n        center_image: bool = ``True``\n            ``True`` if the image in the canvas should be recentered. Defaul:``True``\n        \"\"\"\n        logger.debug(\"Updating image (center_image: %s)\", center_image)\n        self._image.set_display_image()\n        self._canvas.set_image(self._image.display_image, center_image)\n        logger.debug(\"Updated image\")\n\n    def _convert_fit_scale(self) -> str:\n        \"\"\" Convert \"Fit\" scale to the actual scaling amount\n\n        Returns\n        -------\n        str\n            The fit scaling in '##%' format\n         \"\"\"\n        logger.debug(\"Converting 'Fit' scaling\")\n        width_scale = self._canvas.width / self._image.source.shape[1]\n        height_scale = self._canvas.height / self._image.source.shape[0]\n        scale = min(width_scale, height_scale) * 100\n        retval = f\"{floor(scale)}%\"\n        logger.debug(\"Converted 'Fit' scaling: (width_scale: %s, height_scale: %s, scale: %s, \"\n                     \"retval: '%s'\", width_scale, height_scale, scale, retval)\n        return retval\n\n    def _set_scale(self, *args) -> None:  # pylint:disable=unused-argument\n        \"\"\" Update the image on a scale request \"\"\"\n        txtscale = self._taskbar.scale_var.get()\n        logger.debug(\"Setting scale: '%s'\", txtscale)\n        txtscale = self._convert_fit_scale() if txtscale == \"Fit\" else txtscale\n        scale = int(txtscale[:-1])  # Strip percentage and convert to int\n        logger.debug(\"Got scale: %s\", scale)\n\n        if self._image.set_scale(scale / 100):\n            logger.debug(\"Updating for new scale\")\n            self._taskbar.slider_var.set(scale)\n            self._update_image(center_image=True)\n\n    def _set_interpolation(self, *args) -> None:  # pylint:disable=unused-argument\n        \"\"\" Callback for when the interpolator is change\"\"\"\n        interp = self._taskbar.interpolator_var.get()\n        if not self._image.set_interpolation(interp) or self._image.scale <= 1.0:\n            return\n        self._update_image(center_image=False)\n\n    def _process_triggers(self) -> None:\n        \"\"\" Process the standard faceswap key press triggers:\n\n        m = toggle_mask\n        r = refresh\n        s = save\n        enter = quit\n        \"\"\"\n        if self._triggers is None:  # Don't need triggers for GUI\n            return\n        logger.debug(\"Processing triggers\")\n        root = self._canvas.winfo_toplevel()\n        for key in self._keymaps:\n            bindkey = \"Return\" if key == \"enter\" else key\n            logger.debug(\"Adding trigger for key: '%s'\", bindkey)\n\n            root.bind(f\"<{bindkey}>\", self._on_keypress)\n        logger.debug(\"Processed triggers\")\n\n    def _on_keypress(self, event: tk.Event) -> None:\n        \"\"\" Update the triggers on a keypress event for picking up by main faceswap process.\n\n        Parameters\n        ----------\n        event: :class:`tkinter.Event`\n            The valid preview trigger keypress\n        \"\"\"\n        if self._triggers is None:  # Don't need triggers for GUI\n            return\n        keypress = \"enter\" if event.keysym == \"Return\" else event.keysym\n        key = T.cast(TriggerKeysType, keypress)\n        logger.debug(\"Processing keypress '%s'\", key)\n        if key == \"r\":\n            print(\"\")  # Let log print on different line from loss output\n            logger.info(\"Refresh preview requested...\")\n\n        self._triggers[self._keymaps[key]].set()\n        logger.debug(\"Processed keypress '%s'. Set event for '%s'\", key, self._keymaps[key])\n\n    def _display_preview(self) -> None:\n        \"\"\" Handle the displaying of the images currently in :attr:`_preview_buffer`\"\"\"\n        if self._should_shutdown:\n            self._root.destroy()\n\n        if not self._buffer.is_updated:\n            self._root.after(1000, self._display_preview)\n            return\n\n        for name, image in self._buffer.get_images():\n            logger.debug(\"Updating image: (name: '%s', shape: %s)\", name, image.shape)\n            if self._is_standalone and not self._title:\n                assert isinstance(self._root, tk.Tk)\n                self._title = name\n                logger.debug(\"Setting title: '%s;\", self._title)\n                self._root.title(self._title)\n            self._image.set_source_image(name, image)\n            self._update_image(center_image=not self._initialized)\n\n        self._root.after(1000, self._display_preview)\n\n        if not self._initialized and self._is_standalone:\n            self._initialize_window()\n            self._root.mainloop()\n        if not self._initialized:  # Set initialized to True for GUI\n            self._set_min_max_scales()\n            self._taskbar.scale_var.set(\"Fit\")\n            self._initialized = True\n\n\ndef main():\n    \"\"\" Load image from first given argument and display\n\n    python -m lib.training.preview_tk <filename>\n    \"\"\"\n    from lib.logger import log_setup  # pylint:disable=import-outside-toplevel\n    from .preview_cv import PreviewBuffer  # pylint:disable=import-outside-toplevel\n    log_setup(\"DEBUG\", \"faceswap_preview.log\", \"Test\", False)\n\n    img = cv2.imread(sys.argv[-1], cv2.IMREAD_UNCHANGED)\n    buff = PreviewBuffer()  # pylint:disable=used-before-assignment\n    buff.add_image(\"test_image\", img)\n    PreviewTk(buff)\n\n\nif __name__ == \"__main__\":\n    main()\n", "lib/training/cache.py": "#!/usr/bin/env python3\n\"\"\" Holds the data cache for training data generators \"\"\"\nfrom __future__ import annotations\nimport logging\nimport os\nimport typing as T\n\nfrom threading import Lock\n\nimport cv2\nimport numpy as np\nfrom tqdm import tqdm\n\nfrom lib.align import CenteringType, DetectedFace, LandmarkType\nfrom lib.image import read_image_batch, read_image_meta_batch\nfrom lib.utils import FaceswapError\n\nif T.TYPE_CHECKING:\n    from lib.align.alignments import PNGHeaderAlignmentsDict, PNGHeaderDict\n    from lib.config import ConfigValueType\n\nlogger = logging.getLogger(__name__)\n_FACE_CACHES: dict[str, \"_Cache\"] = {}\n\n\ndef get_cache(side: T.Literal[\"a\", \"b\"],\n              filenames: list[str] | None = None,\n              config: dict[str, ConfigValueType] | None = None,\n              size: int | None = None,\n              coverage_ratio: float | None = None) -> \"_Cache\":\n    \"\"\" Obtain a :class:`_Cache` object for the given side. If the object does not pre-exist then\n    create it.\n\n    Parameters\n    ----------\n    side: str\n        `\"a\"` or `\"b\"`. The side of the model to obtain the cache for\n    filenames: list\n        The filenames of all the images. This can either be the full path or the base name. If the\n        full paths are passed in, they are stripped to base name for use as the cache key. Must be\n        passed for the first call of this function for each side. For subsequent calls this\n        parameter is ignored. Default: ``None``\n    config: dict, optional\n        The user selected training configuration options. Must be passed for the first call of this\n        function for each side. For subsequent calls this parameter is ignored. Default: ``None``\n    size: int, optional\n        The largest output size of the model. Must be passed for the first call of this function\n        for each side. For subsequent calls this parameter is ignored. Default: ``None``\n    coverage_ratio: float: optional\n        The coverage ratio that the model is using. Must be passed for the first call of this\n        function for each side. For subsequent calls this parameter is ignored. Default: ``None``\n\n    Returns\n    -------\n    :class:`_Cache`\n        The face meta information cache for the requested side\n    \"\"\"\n    if not _FACE_CACHES.get(side):\n        assert config is not None, (\"config must be provided for first call to cache\")\n        assert filenames is not None, (\"filenames must be provided for first call to cache\")\n        assert size is not None, (\"size must be provided for first call to cache\")\n        assert coverage_ratio is not None, (\"coverage_ratio must be provided for first call to \"\n                                            \"cache\")\n        logger.debug(\"Creating cache. side: %s, size: %s, coverage_ratio: %s\",\n                     side, size, coverage_ratio)\n        _FACE_CACHES[side] = _Cache(filenames, config, size, coverage_ratio)\n    return _FACE_CACHES[side]\n\n\ndef _check_reset(face_cache: \"_Cache\") -> bool:\n    \"\"\" Check whether a given cache needs to be reset because a face centering change has been\n    detected in the other cache.\n\n    Parameters\n    ----------\n    face_cache: :class:`_Cache`\n        The cache object that is checking whether it should reset\n\n    Returns\n    -------\n    bool\n        ``True`` if the given object should reset the cache, otherwise ``False``\n    \"\"\"\n    check_cache = next((cache for cache in _FACE_CACHES.values() if cache != face_cache), None)\n    retval = False if check_cache is None else check_cache.check_reset()\n    return retval\n\n\nclass _Cache():\n    \"\"\" A thread safe mechanism for collecting and holding face meta information (masks, \"\n    \"alignments data etc.) for multiple :class:`TrainingDataGenerator`s.\n\n    Each side may have up to 3 generators (training, preview and time-lapse). To conserve VRAM\n    these need to share access to the same face information for the images they are processing.\n\n    As the cache is populated at run-time, thread safe writes are required for the first epoch.\n    Following that, the cache is only used for reads, which is thread safe intrinsically.\n\n    It would probably be quicker to set locks on each individual face, but for code complexity\n    reasons, and the fact that the lock is only taken up during cache population, and it should\n    only be being read multiple times on save iterations, we lock the whole cache during writes.\n\n    Parameters\n    ----------\n    filenames: list\n        The filenames of all the images. This can either be the full path or the base name. If the\n        full paths are passed in, they are stripped to base name for use as the cache key.\n    config: dict\n        The user selected training configuration options\n    size: int\n        The largest output size of the model\n    coverage_ratio: float\n        The coverage ratio that the model is using.\n    \"\"\"\n    def __init__(self,\n                 filenames: list[str],\n                 config: dict[str, ConfigValueType],\n                 size: int,\n                 coverage_ratio: float) -> None:\n        logger.debug(\"Initializing: %s (filenames: %s, size: %s, coverage_ratio: %s)\",\n                     self.__class__.__name__, len(filenames), size, coverage_ratio)\n        self._lock = Lock()\n        self._cache_info = {\"cache_full\": False, \"has_reset\": False}\n        self._partially_loaded: list[str] = []\n\n        self._image_count = len(filenames)\n        self._cache: dict[str, DetectedFace] = {}\n        self._aligned_landmarks: dict[str, np.ndarray] = {}\n        self._extract_version = 0.0\n        self._size = size\n\n        assert config[\"centering\"] in T.get_args(CenteringType)\n        self._centering: CenteringType = T.cast(CenteringType, config[\"centering\"])\n        self._config = config\n        self._coverage_ratio = coverage_ratio\n\n        logger.debug(\"Initialized: %s\", self.__class__.__name__)\n\n    @property\n    def cache_full(self) -> bool:\n        \"\"\"bool: ``True`` if the cache has been fully populated. ``False`` if there are items still\n        to be cached. \"\"\"\n        if self._cache_info[\"cache_full\"]:\n            return self._cache_info[\"cache_full\"]\n        with self._lock:\n            return self._cache_info[\"cache_full\"]\n\n    @property\n    def aligned_landmarks(self) -> dict[str, np.ndarray]:\n        \"\"\" dict: The filename as key, aligned landmarks as value. \"\"\"\n        # Note: Aligned landmarks are only used for warp-to-landmarks, so this can safely populate\n        # all of the aligned landmarks for the entire cache.\n        if not self._aligned_landmarks:\n            with self._lock:\n                # For Warp-To-Landmarks a race condition can occur where this is referenced from\n                # the opposite side prior to it being populated, so block on a lock.\n                self._aligned_landmarks = {key: face.aligned.landmarks\n                                           for key, face in self._cache.items()}\n        return self._aligned_landmarks\n\n    @property\n    def size(self) -> int:\n        \"\"\" int: The pixel size of the cropped aligned face \"\"\"\n        return self._size\n\n    def check_reset(self) -> bool:\n        \"\"\" Check whether this cache has been reset due to a face centering change, and reset the\n        flag if it has.\n\n        Returns\n        -------\n        bool\n            ``True`` if the cache has been reset because of a face centering change due to\n            legacy alignments, otherwise ``False``. \"\"\"\n        retval = self._cache_info[\"has_reset\"]\n        if retval:\n            logger.debug(\"Resetting 'has_reset' flag\")\n            self._cache_info[\"has_reset\"] = False\n        return retval\n\n    def get_items(self, filenames: list[str]) -> list[DetectedFace]:\n        \"\"\" Obtain the cached items for a list of filenames. The returned list is in the same order\n        as the provided filenames.\n\n        Parameters\n        ----------\n        filenames: list\n            A list of image filenames to obtain the cached data for\n\n        Returns\n        -------\n        list\n            List of DetectedFace objects holding the cached metadata. The list returns in the same\n            order as the filenames received\n        \"\"\"\n        return [self._cache[os.path.basename(filename)] for filename in filenames]\n\n    def cache_metadata(self, filenames: list[str]) -> np.ndarray:\n        \"\"\" Obtain the batch with metadata for items that need caching and cache DetectedFace\n        objects to :attr:`_cache`.\n\n        Parameters\n        ----------\n        filenames: list\n            List of full paths to image file names\n\n        Returns\n        -------\n        :class:`numpy.ndarray`\n            The batch of face images loaded from disk\n        \"\"\"\n        keys = [os.path.basename(filename) for filename in filenames]\n        with self._lock:\n            if _check_reset(self):\n                self._reset_cache(False)\n\n            needs_cache = [filename\n                           for filename, key in zip(filenames, keys)\n                           if key not in self._cache or key in self._partially_loaded]\n            logger.trace(\"Needs cache: %s\", needs_cache)  # type: ignore\n\n            if not needs_cache:\n                # Don't bother reading the metadata if no images in this batch need caching\n                logger.debug(\"All metadata already cached for: %s\", keys)\n                return read_image_batch(filenames)\n\n            try:\n                batch, metadata = read_image_batch(filenames, with_metadata=True)\n            except ValueError as err:\n                if \"inhomogeneous\" in str(err):\n                    raise FaceswapError(\n                        \"There was an error loading a batch of images. This is most likely due to \"\n                        \"non-faceswap extracted faces in your training folder.\"\n                        \"\\nAll training images should be Faceswap extracted faces.\"\n                        \"\\nAll training images should be the same size.\"\n                        f\"\\nThe files that caused this error are: {filenames}\") from err\n                raise\n            if len(batch.shape) == 1:\n                folder = os.path.dirname(filenames[0])\n                details = [\n                    f\"{key} ({f'{img.shape[1]}px' if isinstance(img, np.ndarray) else type(img)})\"\n                    for key, img in zip(keys, batch)]\n                msg = (f\"There are mismatched image sizes in the folder '{folder}'. All training \"\n                       \"images for each side must have the same dimensions.\\nThe batch that \"\n                       f\"failed contains the following files:\\n{details}.\")\n                raise FaceswapError(msg)\n\n            # Populate items into cache\n            for filename in needs_cache:\n                key = os.path.basename(filename)\n                meta = metadata[filenames.index(filename)]\n\n                # Version Check\n                self._validate_version(meta, filename)\n                if self._partially_loaded:  # Faces already loaded for Warp-to-landmarks\n                    self._partially_loaded.remove(key)\n                    detected_face = self._cache[key]\n                else:\n                    detected_face = self._load_detected_face(filename, meta[\"alignments\"])\n\n                self._prepare_masks(filename, detected_face)\n                self._cache[key] = detected_face\n\n            # Update the :attr:`cache_full` attribute\n            cache_full = not self._partially_loaded and len(self._cache) == self._image_count\n            if cache_full:\n                logger.verbose(\"Cache filled: '%s'\", os.path.dirname(filenames[0]))  # type: ignore\n                self._cache_info[\"cache_full\"] = cache_full\n\n        return batch\n\n    def pre_fill(self, filenames: list[str], side: T.Literal[\"a\", \"b\"]) -> None:\n        \"\"\" When warp to landmarks is enabled, the cache must be pre-filled, as each side needs\n        access to the other side's alignments.\n\n        Parameters\n        ----------\n        filenames: list\n            The list of full paths to the images to load the metadata from\n        side: str\n            `\"a\"` or `\"b\"`. The side of the model being cached. Used for info output\n\n        Raises\n        ------\n        FaceSwapError\n            If unsupported landmark type exists\n        \"\"\"\n        with self._lock:\n            for filename, meta in tqdm(read_image_meta_batch(filenames),\n                                       desc=f\"WTL: Caching Landmarks ({side.upper()})\",\n                                       total=len(filenames),\n                                       leave=False):\n                if \"itxt\" not in meta or \"alignments\" not in meta[\"itxt\"]:\n                    raise FaceswapError(f\"Invalid face image found. Aborting: '{filename}'\")\n\n                meta = meta[\"itxt\"]\n                key = os.path.basename(filename)\n                # Version Check\n                self._validate_version(meta, filename)\n                detected_face = self._load_detected_face(filename, meta[\"alignments\"])\n\n                aligned = detected_face.aligned\n                assert aligned is not None\n                if aligned.landmark_type != LandmarkType.LM_2D_68:\n                    raise FaceswapError(\"68 Point facial Landmarks are required for Warp-to-\"\n                                        f\"landmarks. The face that failed was: '{filename}'\")\n\n                self._cache[key] = detected_face\n                self._partially_loaded.append(key)\n\n    def _validate_version(self, png_meta: PNGHeaderDict, filename: str) -> None:\n        \"\"\" Validate that there are not a mix of v1.0 extracted faces and v2.x faces.\n\n        Parameters\n        ----------\n        png_meta: dict\n            The information held within the Faceswap PNG Header\n        filename: str\n            The full path to the file being validated\n\n        Raises\n        ------\n        FaceswapError\n            If a version 1.0 face appears in a 2.x set or vice versa\n        \"\"\"\n        alignment_version = png_meta[\"source\"][\"alignments_version\"]\n\n        if not self._extract_version:\n            logger.debug(\"Setting initial extract version: %s\", alignment_version)\n            self._extract_version = alignment_version\n            if alignment_version == 1.0 and self._centering != \"legacy\":\n                self._reset_cache(True)\n            return\n\n        if (self._extract_version == 1.0 and alignment_version > 1.0) or (\n                alignment_version == 1.0 and self._extract_version > 1.0):\n            raise FaceswapError(\"Mixing legacy and full head extracted facesets is not supported. \"\n                                \"The following folder contains a mix of extracted face types: \"\n                                f\"'{os.path.dirname(filename)}'\")\n\n        self._extract_version = min(alignment_version, self._extract_version)\n\n    def _reset_cache(self, set_flag: bool) -> None:\n        \"\"\" In the event that a legacy extracted face has been seen, and centering is not legacy\n        the cache will need to be reset for legacy centering.\n\n        Parameters\n        ----------\n        set_flag: bool\n            ``True`` if the flag should be set to indicate that the cache is being reset because of\n            a legacy face set/centering mismatch. ``False`` if the cache is being reset because it\n            has detected a reset flag from the opposite cache.\n        \"\"\"\n        if set_flag:\n            logger.warning(\"You are using legacy extracted faces but have selected '%s' centering \"\n                           \"which is incompatible. Switching centering to 'legacy'\",\n                           self._centering)\n        self._config[\"centering\"] = \"legacy\"\n        self._centering = \"legacy\"\n        self._cache = {}\n        self._cache_info[\"cache_full\"] = False\n        if set_flag:\n            self._cache_info[\"has_reset\"] = True\n\n    def _load_detected_face(self,\n                            filename: str,\n                            alignments: PNGHeaderAlignmentsDict) -> DetectedFace:\n        \"\"\" Load a :class:`DetectedFace` object and load its associated `aligned` property.\n\n        Parameters\n        ----------\n        filename: str\n            The file path for the current image\n        alignments: dict\n            The alignments for a single face, extracted from a PNG header\n\n        Returns\n        -------\n        :class:`lib.align.DetectedFace`\n            The loaded Detected Face object\n        \"\"\"\n        detected_face = DetectedFace()\n        detected_face.from_png_meta(alignments)\n        detected_face.load_aligned(None,\n                                   size=self._size,\n                                   centering=self._centering,\n                                   coverage_ratio=self._coverage_ratio,\n                                   is_aligned=True,\n                                   is_legacy=self._extract_version == 1.0)\n        logger.trace(\"Cached aligned face for: %s\", filename)  # type: ignore\n        return detected_face\n\n    def _prepare_masks(self, filename: str, detected_face: DetectedFace) -> None:\n        \"\"\" Prepare the masks required from training, and compile into a single compressed array\n\n        Parameters\n        ----------\n        filename: str\n            The file path for the current image\n        detected_face: :class:`lib.align.DetectedFace`\n            The detected face object that holds the masks\n        \"\"\"\n        masks = [(self._get_face_mask(filename, detected_face))]\n        for area in T.get_args(T.Literal[\"eye\", \"mouth\"]):\n            masks.append(self._get_localized_mask(filename, detected_face, area))\n\n        detected_face.store_training_masks(masks, delete_masks=True)\n        logger.trace(\"Stored masks for filename: %s)\", filename)  # type: ignore\n\n    def _get_face_mask(self, filename: str, detected_face: DetectedFace) -> np.ndarray | None:\n        \"\"\" Obtain the training sized face mask from the :class:`DetectedFace` for the requested\n        mask type.\n\n        Parameters\n        ----------\n        filename: str\n            The file path for the current image\n        detected_face: :class:`lib.align.DetectedFace`\n            The detected face object that holds the masks\n\n        Raises\n        ------\n        FaceswapError\n            If the requested mask type is not available an error is returned along with a list\n            of available masks\n        \"\"\"\n        if not self._config[\"penalized_mask_loss\"] and not self._config[\"learn_mask\"]:\n            return None\n\n        if not self._config[\"mask_type\"]:\n            logger.debug(\"No mask selected. Not validating\")\n            return None\n\n        if self._config[\"mask_type\"] not in detected_face.mask:\n            exist_masks = list(detected_face.mask)\n            msg = \"No masks exist for this face\"\n            if exist_masks:\n                msg = f\"The masks that exist for this face are: {exist_masks}\"\n            raise FaceswapError(\n                f\"You have selected the mask type '{self._config['mask_type']}' but at least one \"\n                \"face does not contain the selected mask.\\n\"\n                f\"The face that failed was: '{filename}'\\n{msg}\")\n\n        mask = detected_face.mask[str(self._config[\"mask_type\"])]\n        assert isinstance(self._config[\"mask_dilation\"], float)\n        assert isinstance(self._config[\"mask_blur_kernel\"], int)\n        assert isinstance(self._config[\"mask_threshold\"], int)\n        mask.set_dilation(self._config[\"mask_dilation\"])\n        mask.set_blur_and_threshold(blur_kernel=self._config[\"mask_blur_kernel\"],\n                                    threshold=self._config[\"mask_threshold\"])\n\n        pose = detected_face.aligned.pose\n        mask.set_sub_crop(pose.offset[mask.stored_centering],\n                          pose.offset[self._centering],\n                          self._centering,\n                          self._coverage_ratio)\n        face_mask = mask.mask\n        if self._size != face_mask.shape[0]:\n            interpolator = cv2.INTER_CUBIC if mask.stored_size < self._size else cv2.INTER_AREA\n            face_mask = cv2.resize(face_mask,\n                                   (self._size, self._size),\n                                   interpolation=interpolator)[..., None]\n\n        logger.trace(\"Obtained face mask for: %s %s\", filename, face_mask.shape)  # type: ignore\n        return face_mask\n\n    def _get_localized_mask(self,\n                            filename: str,\n                            detected_face: DetectedFace,\n                            area: T.Literal[\"eye\", \"mouth\"]) -> np.ndarray | None:\n        \"\"\" Obtain a localized mask for the given area if it is required for training.\n\n        Parameters\n        ----------\n        filename: str\n            The file path for the current image\n        detected_face: :class:`lib.align.DetectedFace`\n            The detected face object that holds the masks\n        area: str\n            `\"eye\"` or `\"mouth\"`. The area of the face to obtain the mask for\n        \"\"\"\n        multiplier = self._config[f\"{area}_multiplier\"]\n        assert isinstance(multiplier, int)\n        if not self._config[\"penalized_mask_loss\"] or multiplier <= 1:\n            return None\n        try:\n            mask = detected_face.get_landmark_mask(area, self._size // 16, 2.5)\n        except FaceswapError as err:\n            logger.error(str(err))\n            raise FaceswapError(\"Eye/Mouth multiplier masks could not be generated due to missing \"\n                                f\"landmark data. The file that failed was: '{filename}'\") from err\n        logger.trace(\"Caching localized '%s' mask for: %s %s\",  # type: ignore\n                     area, filename, mask.shape)\n        return mask\n\n\nclass RingBuffer():\n    \"\"\" Rolling buffer for holding training/preview batches\n\n    Parameters\n    ----------\n    batch_size: int\n        The batch size to create the buffer for\n    image_shape: tuple\n        The height/width/channels shape of a single image in the batch\n    buffer_size: int, optional\n        The number of arrays to hold in the rolling buffer. Default: `2`\n    dtype: str, optional\n        The datatype to create the buffer as. Default: `\"uint8\"`\n    \"\"\"\n    def __init__(self,\n                 batch_size: int,\n                 image_shape: tuple[int, int, int],\n                 buffer_size: int = 2,\n                 dtype: str = \"uint8\") -> None:\n        logger.debug(\"Initializing: %s (batch_size: %s, image_shape: %s, buffer_size: %s, \"\n                     \"dtype: %s\", self.__class__.__name__, batch_size, image_shape, buffer_size,\n                     dtype)\n        self._max_index = buffer_size - 1\n        self._index = 0\n        self._buffer = [np.empty((batch_size, *image_shape), dtype=dtype)\n                        for _ in range(buffer_size)]\n        logger.debug(\"Initialized: %s\", self.__class__.__name__)  # type: ignore\n\n    def __call__(self) -> np.ndarray:\n        \"\"\" Obtain the next array from the ring buffer\n\n        Returns\n        -------\n        :class:`np.ndarray`\n            A pre-allocated numpy array from the buffer\n        \"\"\"\n        retval = self._buffer[self._index]\n        self._index += 1 if self._index < self._max_index else -self._max_index\n        return retval\n", "lib/training/lr_finder.py": "#!/usr/bin/env python3\n\"\"\" Learning Rate Finder for faceswap.py. \"\"\"\nfrom __future__ import annotations\nimport logging\nimport os\nimport shutil\nimport typing as T\nfrom datetime import datetime\nfrom enum import Enum\n\nimport tensorflow as tf\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tqdm import tqdm\n\nif T.TYPE_CHECKING:\n    from lib.config import ConfigValueType\n    from lib.training import Feeder\n    from plugins.train.model._base import ModelBase\n\nkeras = tf.keras\nK = keras.backend\n\nlogger = logging.getLogger(__name__)\n\n\nclass LRStrength(Enum):\n    \"\"\" Enum for how aggressively to set the optimal learning rate \"\"\"\n    DEFAULT = 10\n    AGGRESSIVE = 5\n    EXTREME = 2.5\n\n\nclass LearningRateFinder:\n    \"\"\" Learning Rate Finder\n\n    Parameters\n    ----------\n    model: :class:`tensorflow.keras.models.Model`\n        The keras model to find the optimal learning rate for\n    config: dict\n        The configuration options for the model\n    feeder: :class:`~lib.training.generator.Feeder`\n        The feeder for training the model\n    stop_factor: int\n        When to stop finding the optimal learning rate\n    beta: float\n        Amount to smooth loss by, for graphing purposes\n    \"\"\"\n    def __init__(self,\n                 model: ModelBase,\n                 config: dict[str, ConfigValueType],\n                 feeder: Feeder,\n                 stop_factor: int = 4,\n                 beta: float = 0.98) -> None:\n        logger.debug(\"Initializing %s: (model: %s, config: %s, feeder: %s, stop_factor: %s, \"\n                     \"beta: %s)\",\n                     self.__class__.__name__, model, config, feeder, stop_factor, beta)\n\n        self._iterations = T.cast(int, config[\"lr_finder_iterations\"])\n        self._save_graph = config[\"lr_finder_mode\"] in (\"graph_and_set\", \"graph_and_exit\")\n        self._strength = LRStrength[T.cast(str, config[\"lr_finder_strength\"]).upper()].value\n        self._config = config\n\n        self._start_lr = 1e-10\n        end_lr = 1e+1\n\n        self._model = model\n        self._feeder = feeder\n        self._stop_factor = stop_factor\n        self._beta = beta\n        self._lr_multiplier: float = (end_lr / self._start_lr) ** (1.0 / self._iterations)\n\n        self._metrics: dict[T.Literal[\"learning_rates\", \"losses\"], list[float]] = {\n            \"learning_rates\": [],\n            \"losses\": []}\n        self._loss: dict[T.Literal[\"avg\", \"best\"], float] = {\"avg\": 0.0, \"best\": 1e9}\n\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    def _on_batch_end(self, iteration: int, loss: float) -> None:\n        \"\"\" Learning rate actions to perform at the end of a batch\n\n        Parameters\n        ----------\n        iteration: int\n            The current iteration\n        loss: float\n            The loss value for the current batch\n        \"\"\"\n        learning_rate = K.get_value(self._model.model.optimizer.lr)\n        self._metrics[\"learning_rates\"].append(learning_rate)\n\n        self._loss[\"avg\"] = (self._beta * self._loss[\"avg\"]) + ((1 - self._beta) * loss)\n        smoothed = self._loss[\"avg\"] / (1 - (self._beta ** iteration))\n        self._metrics[\"losses\"].append(smoothed)\n\n        stop_loss = self._stop_factor * self._loss[\"best\"]\n\n        if iteration > 1 and smoothed > stop_loss:\n            self._model.model.stop_training = True\n            return\n\n        if iteration == 1 or smoothed < self._loss[\"best\"]:\n            self._loss[\"best\"] = smoothed\n\n        learning_rate *= self._lr_multiplier\n\n        K.set_value(self._model.model.optimizer.lr, learning_rate)\n\n    def _update_description(self, progress_bar: tqdm) -> None:\n        \"\"\" Update the description of the progress bar for the current iteration\n\n        Parameters\n        ----------\n        progress_bar: :class:`tqdm.tqdm`\n            The learning rate finder progress bar to update\n        \"\"\"\n        current = self._metrics['learning_rates'][-1]\n        best_idx = self._metrics[\"losses\"].index(self._loss[\"best\"])\n        best = self._metrics[\"learning_rates\"][best_idx] / self._strength\n        progress_bar.set_description(f\"Current: {current:.1e}  Best: {best:.1e}\")\n\n    def _train(self) -> None:\n        \"\"\" Train the model for the given number of iterations to find the optimal\n        learning rate and show progress\"\"\"\n        logger.info(\"Finding optimal learning rate...\")\n        pbar = tqdm(range(1, self._iterations + 1),\n                    desc=\"Current: N/A      Best: N/A    \",\n                    leave=False)\n        for idx in pbar:\n            model_inputs, model_targets = self._feeder.get_batch()\n            loss: list[float] = self._model.model.train_on_batch(model_inputs, y=model_targets)\n            if np.isnan(loss[0]):\n                break\n            self._on_batch_end(idx, loss[0])\n            self._update_description(pbar)\n\n    def _reset_model(self, original_lr: float, new_lr: float) -> None:\n        \"\"\" Reset the model's weights to initial values, reset the model's optimizer and set the\n        learning rate\n\n        Parameters\n        ----------\n        original_lr: float\n            The model's original learning rate\n        new_lr: float\n            The discovered optimal learning rate\n        \"\"\"\n        self._model.state.update_session_config(\"learning_rate\", new_lr)\n        self._model.state.save()\n\n        logger.debug(\"Loading initial weights\")\n        self._model.model.load_weights(self._model.io.filename)\n\n        if self._config[\"lr_finder_mode\"] == \"graph_and_exit\":\n            return\n\n        opt_conf = self._model.model.optimizer.get_config()\n        logger.debug(\"Recompiling model to reset optimizer state. Optimizer config: %s\", opt_conf)\n        new_opt = self._model.model.optimizer.__class__(**opt_conf)\n        self._model.model.compile(optimizer=new_opt, loss=self._model.model.loss)\n\n        logger.info(\"Updating Learning Rate from %s to %s\", f\"{original_lr:.1e}\", f\"{new_lr:.1e}\")\n        K.set_value(self._model.model.optimizer.lr, new_lr)\n\n    def find(self) -> bool:\n        \"\"\" Find the optimal learning rate\n\n        Returns\n        -------\n        bool\n            ``True`` if the learning rate was succesfully discovered otherwise ``False``\n        \"\"\"\n        if not self._model.io.model_exists:\n            self._model.io.save()\n\n        original_lr = K.get_value(self._model.model.optimizer.lr)\n        K.set_value(self._model.model.optimizer.lr, self._start_lr)\n\n        self._train()\n        print()\n\n        best_idx = self._metrics[\"losses\"].index(self._loss[\"best\"])\n        new_lr = self._metrics[\"learning_rates\"][best_idx] / self._strength\n        if new_lr < 1e-9:\n            logger.error(\"The optimal learning rate could not be found. This is most likely \"\n                         \"because you did not run the finder for enough iterations.\")\n            shutil.rmtree(self._model.io.model_dir)\n            return False\n\n        self._plot_loss()\n        self._reset_model(original_lr, new_lr)\n        return True\n\n    def _plot_loss(self, skip_begin: int = 10, skip_end: int = 1) -> None:\n        \"\"\" Plot a graph of loss vs learning rate and save to the training folder\n\n        Parameters\n        ----------\n        skip_begin: int, optional\n            Number of iterations to skip at the start. Default: `10`\n        skip_end: int, optional\n            Number of iterations to skip at the end. Default: `1`\n        \"\"\"\n        if not self._save_graph:\n            return\n\n        matplotlib.use(\"Agg\")\n        lrs = self._metrics[\"learning_rates\"][skip_begin:-skip_end]\n        losses = self._metrics[\"losses\"][skip_begin:-skip_end]\n        plt.plot(lrs, losses, label=\"Learning Rate\")\n        best_idx = self._metrics[\"losses\"].index(self._loss[\"best\"])\n        best_lr = self._metrics[\"learning_rates\"][best_idx]\n        for val, color in zip(LRStrength, (\"g\", \"y\", \"r\")):\n            l_r = best_lr / val.value\n            idx = lrs.index(next(r for r in lrs if r >= l_r))\n            plt.plot(l_r, losses[idx],\n                     f\"{color}o\",\n                     label=f\"{val.name.title()}: {l_r:.1e}\")\n\n        plt.xscale(\"log\")\n        plt.xlabel(\"Learning Rate (Log Scale)\")\n        plt.ylabel(\"Loss\")\n        plt.title(\"Learning Rate Finder\")\n        plt.legend()\n\n        now = datetime.now().strftime(\"%Y-%m-%d_%H.%M.%S\")\n        output = os.path.join(self._model.io.model_dir, f\"learning_rate_finder_{now}.png\")\n        logger.info(\"Saving Learning Rate Finder graph to: '%s'\", output)\n        plt.savefig(output)\n", "lib/training/__init__.py": "#!/usr/bin/env python3\n\"\"\" Package for handling alignments files, detected faces and aligned faces along with their\nassociated objects. \"\"\"\nfrom __future__ import annotations\nimport typing as T\n\nfrom .augmentation import ImageAugmentation\nfrom .generator import Feeder\nfrom .lr_finder import LearningRateFinder\nfrom .preview_cv import PreviewBuffer, TriggerType\n\nif T.TYPE_CHECKING:\n    from .preview_cv import PreviewBase\n    Preview: type[PreviewBase]\n\ntry:\n    from .preview_tk import PreviewTk as Preview\nexcept ImportError:\n    from .preview_cv import PreviewCV as Preview\n", "lib/gui/menu.py": "#!/usr/bin python3\n\"\"\" The Menu Bars for faceswap GUI \"\"\"\nfrom __future__ import annotations\nimport gettext\nimport logging\nimport os\nimport tkinter as tk\nimport typing as T\nfrom tkinter import ttk\nimport webbrowser\n\nfrom lib.git import git\nfrom lib.multithreading import MultiThread\nfrom lib.serializer import get_serializer, Serializer\nfrom lib.utils import FaceswapError\nimport update_deps\n\nfrom .popup_configure import open_popup\nfrom .custom_widgets import Tooltip\nfrom .utils import get_config, get_images\n\nif T.TYPE_CHECKING:\n    from scripts.gui import FaceswapGui\n\nlogger = logging.getLogger(__name__)\n\n# LOCALES\n_LANG = gettext.translation(\"gui.menu\", localedir=\"locales\", fallback=True)\n_ = _LANG.gettext\n\n_RESOURCES: list[tuple[str, str]] = [\n    (_(\"faceswap.dev - Guides and Forum\"), \"https://www.faceswap.dev\"),\n    (_(\"Patreon - Support this project\"), \"https://www.patreon.com/faceswap\"),\n    (_(\"Discord - The FaceSwap Discord server\"), \"https://discord.gg/VasFUAy\"),\n    (_(\"Github - Our Source Code\"), \"https://github.com/deepfakes/faceswap\")]\n\n\nclass MainMenuBar(tk.Menu):  # pylint:disable=too-many-ancestors\n    \"\"\" GUI Main Menu Bar\n\n    Parameters\n    ----------\n    master: :class:`tkinter.Tk`\n        The root tkinter object\n    \"\"\"\n    def __init__(self, master: FaceswapGui) -> None:\n        logger.debug(\"Initializing %s\", self.__class__.__name__)\n        super().__init__(master)\n        self.root = master\n\n        self.file_menu = FileMenu(self)\n        self.settings_menu = SettingsMenu(self)\n        self.help_menu = HelpMenu(self)\n\n        self.add_cascade(label=_(\"File\"), menu=self.file_menu, underline=0)\n        self.add_cascade(label=_(\"Settings\"), menu=self.settings_menu, underline=0)\n        self.add_cascade(label=_(\"Help\"), menu=self.help_menu, underline=0)\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n\nclass SettingsMenu(tk.Menu):  # pylint:disable=too-many-ancestors\n    \"\"\" Settings menu items and functions\n\n    Parameters\n    ----------\n    parent: :class:`tkinter.Menu`\n        The main menu bar to hold this menu item\n    \"\"\"\n    def __init__(self, parent: MainMenuBar) -> None:\n        logger.debug(\"Initializing %s\", self.__class__.__name__)\n        super().__init__(parent, tearoff=0)\n        self.root = parent.root\n        self._build()\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    def _build(self) -> None:\n        \"\"\" Add the settings menu to the menu bar \"\"\"\n        # pylint:disable=cell-var-from-loop\n        logger.debug(\"Building settings menu\")\n        self.add_command(label=_(\"Configure Settings...\"),\n                         underline=0,\n                         command=open_popup)\n        logger.debug(\"Built settings menu\")\n\n\nclass FileMenu(tk.Menu):  # pylint:disable=too-many-ancestors\n    \"\"\" File menu items and functions\n\n    Parameters\n    ----------\n    parent: :class:`tkinter.Menu`\n        The main menu bar to hold this menu item\n    \"\"\"\n    def __init__(self, parent: MainMenuBar) -> None:\n        logger.debug(\"Initializing %s\", self.__class__.__name__)\n        super().__init__(parent, tearoff=0)\n        self.root = parent.root\n        self._config = get_config()\n        self.recent_menu = tk.Menu(self, tearoff=0, postcommand=self._refresh_recent_menu)\n        self._build()\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    def _refresh_recent_menu(self) -> None:\n        \"\"\" Refresh recent menu on save/load of files \"\"\"\n        self.recent_menu.delete(0, \"end\")\n        self._build_recent_menu()\n\n    def _build(self) -> None:\n        \"\"\" Add the file menu to the menu bar \"\"\"\n        logger.debug(\"Building File menu\")\n        self.add_command(label=_(\"New Project...\"),\n                         underline=0,\n                         accelerator=\"Ctrl+N\",\n                         command=self._config.project.new)\n        self.root.bind_all(\"<Control-n>\", self._config.project.new)\n        self.add_command(label=_(\"Open Project...\"),\n                         underline=0,\n                         accelerator=\"Ctrl+O\",\n                         command=self._config.project.load)\n        self.root.bind_all(\"<Control-o>\", self._config.project.load)\n        self.add_command(label=_(\"Save Project\"),\n                         underline=0,\n                         accelerator=\"Ctrl+S\",\n                         command=lambda: self._config.project.save(save_as=False))\n        self.root.bind_all(\"<Control-s>\", lambda e: self._config.project.save(e, save_as=False))\n        self.add_command(label=_(\"Save Project as...\"),\n                         underline=13,\n                         accelerator=\"Ctrl+Alt+S\",\n                         command=lambda: self._config.project.save(save_as=True))\n        self.root.bind_all(\"<Control-Alt-s>\", lambda e: self._config.project.save(e, save_as=True))\n        self.add_command(label=_(\"Reload Project from Disk\"),\n                         underline=0,\n                         accelerator=\"F5\",\n                         command=self._config.project.reload)\n        self.root.bind_all(\"<F5>\", self._config.project.reload)\n        self.add_command(label=_(\"Close Project\"),\n                         underline=0,\n                         accelerator=\"Ctrl+W\",\n                         command=self._config.project.close)\n        self.root.bind_all(\"<Control-w>\", self._config.project.close)\n        self.add_separator()\n        self.add_command(label=_(\"Open Task...\"),\n                         underline=5,\n                         accelerator=\"Ctrl+Alt+T\",\n                         command=lambda: self._config.tasks.load(current_tab=False))\n        self.root.bind_all(\"<Control-Alt-t>\",\n                           lambda e: self._config.tasks.load(e, current_tab=False))\n        self.add_separator()\n        self.add_cascade(label=_(\"Open recent\"), underline=6, menu=self.recent_menu)\n        self.add_separator()\n        self.add_command(label=_(\"Quit\"),\n                         underline=0,\n                         accelerator=\"Alt+F4\",\n                         command=self.root.close_app)\n        self.root.bind_all(\"<Alt-F4>\", self.root.close_app)\n        logger.debug(\"Built File menu\")\n\n    @classmethod\n    def _clear_recent_files(cls, serializer: Serializer, menu_file: str) -> None:\n        \"\"\" Creates or clears recent file list\n\n        Parameters\n        ----------\n        serializer: :class:`~lib.serializer.Serializer`\n            The serializer to use for storing files\n        menu_file: str\n            The file name holding the recent files\n        \"\"\"\n        logger.debug(\"clearing recent files list: '%s'\", menu_file)\n        serializer.save(menu_file, [])\n\n    def _build_recent_menu(self) -> None:\n        \"\"\" Load recent files into menu bar \"\"\"\n        logger.debug(\"Building Recent Files menu\")\n        serializer = get_serializer(\"json\")\n        menu_file = os.path.join(self._config.pathcache, \".recent.json\")\n        if not os.path.isfile(menu_file) or os.path.getsize(menu_file) == 0:\n            self._clear_recent_files(serializer, menu_file)\n        try:\n            recent_files = serializer.load(menu_file)\n        except FaceswapError as err:\n            if \"Error unserializing data for type\" in str(err):\n                # Some reports of corruption breaking menus\n                logger.warning(\"There was an error opening the recent files list so it has been \"\n                               \"reset.\")\n                self._clear_recent_files(serializer, menu_file)\n                recent_files = []\n\n        logger.debug(\"Loaded recent files: %s\", recent_files)\n        removed_files = []\n        for recent_item in recent_files:\n            filename, command = recent_item\n            if not os.path.isfile(filename):\n                logger.debug(\"File does not exist. Flagging for removal: '%s'\", filename)\n                removed_files.append(recent_item)\n                continue\n            # Legacy project files didn't have a command stored\n            command = command if command else \"project\"\n            logger.debug(\"processing: ('%s', %s)\", filename, command)\n            if command.lower() == \"project\":\n                load_func = self._config.project.load\n                lbl = command\n                kwargs = {\"filename\": filename}\n            else:\n                load_func = self._config.tasks.load  # type:ignore\n                lbl = _(\"{} Task\").format(command)\n                kwargs = {\"filename\": filename, \"current_tab\": False}\n            self.recent_menu.add_command(\n                label=f\"{filename} ({lbl.title()})\",\n                command=lambda kw=kwargs, fn=load_func: fn(**kw))  # type:ignore\n        if removed_files:\n            for recent_item in removed_files:\n                logger.debug(\"Removing from recent files: `%s`\", recent_item[0])\n                recent_files.remove(recent_item)\n            serializer.save(menu_file, recent_files)\n        self.recent_menu.add_separator()\n        self.recent_menu.add_command(\n            label=_(\"Clear recent files\"),\n            underline=0,\n            command=lambda srl=serializer, mnu=menu_file: self._clear_recent_files(  # type:ignore\n                srl, mnu))\n\n        logger.debug(\"Built Recent Files menu\")\n\n\nclass HelpMenu(tk.Menu):  # pylint:disable=too-many-ancestors\n    \"\"\" Help menu items and functions\n\n    Parameters\n    ----------\n    parent: :class:`tkinter.Menu`\n        The main menu bar to hold this menu item\n    \"\"\"\n    def __init__(self, parent: MainMenuBar) -> None:\n        logger.debug(\"Initializing %s\", self.__class__.__name__)\n        super().__init__(parent, tearoff=0)\n        self.root = parent.root\n        self.recources_menu = tk.Menu(self, tearoff=0)\n        self._branches_menu = tk.Menu(self, tearoff=0)\n        self._build()\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    def _in_thread(self, action: str):\n        \"\"\" Perform selected action inside a thread\n\n        Parameters\n        ----------\n        action: str\n            The action to be performed. The action corresponds to the function name to be called\n        \"\"\"\n        logger.debug(\"Performing help action: %s\", action)\n        thread = MultiThread(getattr(self, action), thread_count=1)\n        thread.start()\n        logger.debug(\"Performed help action: %s\", action)\n\n    def _output_sysinfo(self):\n        \"\"\" Output system information to console \"\"\"\n        logger.debug(\"Obtaining system information\")\n        self.root.config(cursor=\"watch\")\n        self._clear_console()\n        try:\n            from lib.sysinfo import sysinfo  # pylint:disable=import-outside-toplevel\n            info = sysinfo\n        except Exception as err:  # pylint:disable=broad-except\n            info = f\"Error obtaining system info: {str(err)}\"\n        self._clear_console()\n        logger.debug(\"Obtained system information: %s\", info)\n        print(info)\n        self.root.config(cursor=\"\")\n\n    @classmethod\n    def _process_status_output(cls, status: list[str]) -> bool:\n        \"\"\" Process the output of a git status call and output information\n\n        Parameters\n        ----------\n        status : list[str]\n            The lines returned from a git status call\n\n        Returns\n        -------\n        bool\n            ``True`` if the repo can be updated otherwise ``False``\n        \"\"\"\n        for line in status:\n            if line.lower().startswith(\"your branch is ahead\"):\n                logger.warning(\"Your branch is ahead of the remote repo. Not updating\")\n                return False\n            if line.lower().startswith(\"your branch is up to date\"):\n                logger.info(\"Faceswap is up to date.\")\n                return False\n            if \"have diverged\" in line.lower():\n                logger.warning(\"Your branch has diverged from the remote repo. Not updating\")\n                return False\n            if line.lower().startswith(\"your branch is behind\"):\n                return True\n\n        logger.warning(\"Unable to retrieve status of branch\")\n        return False\n\n    def _check_for_updates(self, check: bool = False) -> bool:\n        \"\"\" Check whether an update is required\n\n        Parameters\n        ----------\n        check: bool\n            ``True`` if we are just checking for updates ``False`` if a check and update is to be\n            performed. Default: ``False``\n\n        Returns\n        -------\n        bool\n            ``True`` if an update is required\n        \"\"\"\n        # Do the check\n        logger.info(\"Checking for updates...\")\n        msg = (\"Git is not installed or you are not running a cloned repo. \"\n               \"Unable to check for updates\")\n\n        sync = git.update_remote()\n        if not sync:\n            logger.warning(msg)\n            return False\n\n        status = git.status\n        if not status:\n            logger.warning(msg)\n            return False\n\n        retval = self._process_status_output(status)\n        if retval and check:\n            logger.info(\"There are updates available\")\n        return retval\n\n    def _check(self) -> None:\n        \"\"\" Check for updates and clone repository \"\"\"\n        logger.debug(\"Checking for updates...\")\n        self.root.config(cursor=\"watch\")\n        self._check_for_updates(check=True)\n        self.root.config(cursor=\"\")\n\n    def _do_update(self) -> bool:\n        \"\"\" Update Faceswap\n\n        Returns\n        -------\n        bool\n            ``True`` if update was successful\n        \"\"\"\n        logger.info(\"A new version is available. Updating...\")\n        success = git.pull()\n        if not success:\n            logger.info(\"An error occurred during update\")\n        return success\n\n    def _update(self) -> None:\n        \"\"\" Check for updates and clone repository \"\"\"\n        logger.debug(\"Updating Faceswap...\")\n        self.root.config(cursor=\"watch\")\n        success = False\n        if self._check_for_updates():\n            success = self._do_update()\n        update_deps.main(is_gui=True)\n        if success:\n            logger.info(\"Please restart Faceswap to complete the update.\")\n        self.root.config(cursor=\"\")\n\n    def _build(self) -> None:\n        \"\"\" Build the help menu \"\"\"\n        logger.debug(\"Building Help menu\")\n\n        self.add_command(label=_(\"Check for updates...\"),\n                         underline=0,\n                         command=lambda action=\"_check\": self._in_thread(action))  # type:ignore\n        self.add_command(label=_(\"Update Faceswap...\"),\n                         underline=0,\n                         command=lambda action=\"_update\": self._in_thread(action))  # type:ignore\n        if self._build_branches_menu():\n            self.add_cascade(label=_(\"Switch Branch\"), underline=7, menu=self._branches_menu)\n        self.add_separator()\n        self._build_recources_menu()\n        self.add_cascade(label=_(\"Resources\"), underline=0, menu=self.recources_menu)\n        self.add_separator()\n        self.add_command(\n            label=_(\"Output System Information\"),\n            underline=0,\n            command=lambda action=\"_output_sysinfo\": self._in_thread(action))  # type:ignore\n        logger.debug(\"Built help menu\")\n\n    def _build_branches_menu(self) -> bool:\n        \"\"\" Build branch selection menu.\n\n        Queries git for available branches and builds a menu based on output.\n\n        Returns\n        -------\n        bool\n            ``True`` if menu was successfully built otherwise ``False``\n        \"\"\"\n        branches = git.branches\n        if not branches:\n            return False\n\n        branches = self._filter_branches(branches)\n        if not branches:\n            return False\n\n        for branch in branches:\n            self._branches_menu.add_command(\n                label=branch,\n                command=lambda b=branch: self._switch_branch(b))  # type:ignore\n        return True\n\n    @classmethod\n    def _filter_branches(cls, branches: list[str]) -> list[str]:\n        \"\"\" Filter the branches, remove any non-local branches\n\n        Parameters\n        ----------\n        branches: list[str]\n            list of available git branches\n\n        Returns\n        -------\n        list[str]\n            Unique list of available branches sorted in alphabetical order\n        \"\"\"\n        current = None\n        unique = set()\n        for line in branches:\n            branch = line.strip()\n            if branch.startswith(\"remotes\"):\n                continue\n            if branch.startswith(\"*\"):\n                branch = branch.replace(\"*\", \"\").strip()\n                current = branch\n                continue\n            unique.add(branch)\n        logger.debug(\"Found branches: %s\", unique)\n        if current in unique:\n            logger.debug(\"Removing current branch from output: %s\", current)\n            unique.remove(current)\n\n        retval = sorted(list(unique), key=str.casefold)\n        logger.debug(\"Final branches: %s\", retval)\n        return retval\n\n    @classmethod\n    def _switch_branch(cls, branch: str) -> None:\n        \"\"\" Change the currently checked out branch, and return a notification.\n\n        Parameters\n        ----------\n        str\n            The branch to switch to\n        \"\"\"\n        logger.info(\"Switching branch to '%s'...\", branch)\n        if not git.checkout(branch):\n            logger.error(\"Unable to switch branch to '%s'\", branch)\n            return\n        logger.info(\"Succesfully switched to '%s'. You may want to check for updates to make sure \"\n                    \"that you have the latest code.\", branch)\n        logger.info(\"Please restart Faceswap to complete the switch.\")\n\n    def _build_recources_menu(self) -> None:\n        \"\"\" Build resources menu \"\"\"\n        # pylint:disable=cell-var-from-loop\n        logger.debug(\"Building Resources Files menu\")\n        for resource in _RESOURCES:\n            self.recources_menu.add_command(\n                label=resource[0],\n                command=lambda link=resource[1]: webbrowser.open_new(link))  # type:ignore\n        logger.debug(\"Built resources menu\")\n\n    @classmethod\n    def _clear_console(cls) -> None:\n        \"\"\" Clear the console window \"\"\"\n        get_config().tk_vars.console_clear.set(True)\n\n\nclass TaskBar(ttk.Frame):  # pylint:disable=too-many-ancestors\n    \"\"\" Task bar buttons\n\n    Parameters\n    ----------\n    parent: :class:`tkinter.ttk.Frame`\n        The frame that holds the task bar\n    \"\"\"\n    def __init__(self, parent: ttk.Frame) -> None:\n        super().__init__(parent)\n        self._config = get_config()\n        self.pack(side=tk.TOP, anchor=tk.W, fill=tk.X, expand=False)\n        self._btn_frame = ttk.Frame(self)\n        self._btn_frame.pack(side=tk.TOP, pady=2, anchor=tk.W, fill=tk.X, expand=False)\n\n        self._project_btns()\n        self._group_separator()\n        self._task_btns()\n        self._group_separator()\n        self._settings_btns()\n        self._section_separator()\n\n    @classmethod\n    def _loader_and_kwargs(cls, btntype: str) -> tuple[str, dict[str, bool]]:\n        \"\"\" Get the loader name and key word arguments for the given button type\n\n        Parameters\n        ----------\n        btntype: str\n            The button type to obtain the information for\n\n        Returns\n        -------\n        loader: str\n            The name of the loader to use for the given button type\n        kwargs: dict[str, bool]\n            The keyword arguments to use for the returned loader\n        \"\"\"\n        if btntype == \"save\":\n            loader = btntype\n            kwargs = {\"save_as\": False}\n        elif btntype == \"save_as\":\n            loader = \"save\"\n            kwargs = {\"save_as\": True}\n        else:\n            loader = btntype\n            kwargs = {}\n        logger.debug(\"btntype: %s, loader: %s, kwargs: %s\", btntype, loader, kwargs)\n        return loader, kwargs\n\n    @classmethod\n    def _set_help(cls, btntype: str) -> str:\n        \"\"\" Set the helptext for option buttons\n\n        Parameters\n        ----------\n        btntype: str\n            The button type to set the help text for\n        \"\"\"\n        logger.debug(\"Setting help\")\n        hlp = \"\"\n        task = _(\"currently selected Task\") if btntype[-1] == \"2\" else _(\"Project\")\n        if btntype.startswith(\"reload\"):\n            hlp = _(\"Reload {} from disk\").format(task)\n        if btntype == \"new\":\n            hlp = _(\"Create a new {}...\").format(task)\n        if btntype.startswith(\"clear\"):\n            hlp = _(\"Reset {} to default\").format(task)\n        elif btntype.startswith(\"save\") and \"_\" not in btntype:\n            hlp = _(\"Save {}\").format(task)\n        elif btntype.startswith(\"save_as\"):\n            hlp = _(\"Save {} as...\").format(task)\n        elif btntype.startswith(\"load\"):\n            msg = task\n            if msg.endswith(\"Task\"):\n                msg += _(\" from a task or project file\")\n            hlp = _(\"Load {}...\").format(msg)\n        return hlp\n\n    def _project_btns(self) -> None:\n        \"\"\" Place the project buttons \"\"\"\n        frame = ttk.Frame(self._btn_frame)\n        frame.pack(side=tk.LEFT, anchor=tk.W, expand=False, padx=2)\n\n        for btntype in (\"new\", \"load\", \"save\", \"save_as\", \"reload\"):\n            logger.debug(\"Adding button: '%s'\", btntype)\n\n            loader, kwargs = self._loader_and_kwargs(btntype)\n            cmd = getattr(self._config.project, loader)\n            btn = ttk.Button(frame,\n                             image=get_images().icons[btntype],\n                             command=lambda fn=cmd, kw=kwargs: fn(**kw))  # type:ignore\n            btn.pack(side=tk.LEFT, anchor=tk.W)\n            hlp = self._set_help(btntype)\n            Tooltip(btn, text=hlp, wrap_length=200)\n\n    def _task_btns(self) -> None:\n        \"\"\" Place the task buttons \"\"\"\n        frame = ttk.Frame(self._btn_frame)\n        frame.pack(side=tk.LEFT, anchor=tk.W, expand=False, padx=2)\n\n        for loadtype in (\"load\", \"save\", \"save_as\", \"clear\", \"reload\"):\n            btntype = f\"{loadtype}2\"\n            logger.debug(\"Adding button: '%s'\", btntype)\n\n            loader, kwargs = self._loader_and_kwargs(loadtype)\n            if loadtype == \"load\":\n                kwargs[\"current_tab\"] = True\n            cmd = getattr(self._config.tasks, loader)\n            btn = ttk.Button(\n                frame,\n                image=get_images().icons[btntype],\n                command=lambda fn=cmd, kw=kwargs: fn(**kw))  # type:ignore\n            btn.pack(side=tk.LEFT, anchor=tk.W)\n            hlp = self._set_help(btntype)\n            Tooltip(btn, text=hlp, wrap_length=200)\n\n    def _settings_btns(self) -> None:\n        \"\"\" Place the settings buttons \"\"\"\n        # pylint:disable=cell-var-from-loop\n        frame = ttk.Frame(self._btn_frame)\n        frame.pack(side=tk.LEFT, anchor=tk.W, expand=False, padx=2)\n        for name in (\"extract\", \"train\", \"convert\"):\n            btntype = f\"settings_{name}\"\n            btntype = btntype if btntype in get_images().icons else \"settings\"\n            logger.debug(\"Adding button: '%s'\", btntype)\n            btn = ttk.Button(\n                frame,\n                image=get_images().icons[btntype],\n                command=lambda n=name: open_popup(name=n))  # type:ignore\n            btn.pack(side=tk.LEFT, anchor=tk.W)\n            hlp = _(\"Configure {} settings...\").format(name.title())\n            Tooltip(btn, text=hlp, wrap_length=200)\n\n    def _group_separator(self) -> None:\n        \"\"\" Place a group separator \"\"\"\n        separator = ttk.Separator(self._btn_frame, orient=\"vertical\")\n        separator.pack(padx=(2, 1), fill=tk.Y, side=tk.LEFT)\n\n    def _section_separator(self) -> None:\n        \"\"\" Place a section separator \"\"\"\n        frame = ttk.Frame(self)\n        frame.pack(side=tk.BOTTOM, fill=tk.X)\n        separator = ttk.Separator(frame, orient=\"horizontal\")\n        separator.pack(fill=tk.X, side=tk.LEFT, expand=True)\n", "lib/gui/display.py": "#!/usr/bin python3\n\"\"\" Display Frame of the Faceswap GUI\n\nThis is the large right hand area of the GUI. At default, the Analysis tab is always displayed\nhere. Further optional tabs will also be displayed depending on the currently executing Faceswap\ntask. \"\"\"\n\nimport logging\nimport gettext\nimport tkinter as tk\nfrom tkinter import ttk\n\nfrom lib.logger import parse_class_init\n\nfrom .display_analysis import Analysis\nfrom .display_command import GraphDisplay, PreviewExtract, PreviewTrain\nfrom .utils import get_config\n\nlogger = logging.getLogger(__name__)\n\n# LOCALES\n_LANG = gettext.translation(\"gui.tooltips\", localedir=\"locales\", fallback=True)\n_ = _LANG.gettext\n\n\nclass DisplayNotebook(ttk.Notebook):  # pylint:disable=too-many-ancestors\n    \"\"\" The tkinter Notebook that holds the display items.\n\n    Parameters\n    ----------\n    parent: :class:`tk.PanedWindow`\n        The paned window that holds the Display Notebook\n    \"\"\"\n\n    def __init__(self, parent):\n        logger.debug(parse_class_init(locals()))\n        super().__init__(parent)\n        parent.add(self)\n        tk_vars = get_config().tk_vars\n        self._wrapper_var = tk_vars.display\n        self._running_task = tk_vars.running_task\n\n        self._set_wrapper_var_trace()\n        self._add_static_tabs()\n        # pylint:disable=unnecessary-comprehension\n        self._static_tabs = [child for child in self.tabs()]\n        self.bind(\"<<NotebookTabChanged>>\", self._on_tab_change)\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    @property\n    def running_task(self):\n        \"\"\" :class:`tkinter.BooleanVar`: The global tkinter variable that indicates whether a\n        Faceswap task is currently running or not. \"\"\"\n        return self._running_task\n\n    def _set_wrapper_var_trace(self):\n        \"\"\" Sets the trigger to update the displayed notebook's pages when the global tkinter\n        variable `display` is updated in the :class:`~lib.gui.wrapper.ProcessWrapper`. \"\"\"\n        logger.debug(\"Setting wrapper var trace\")\n        self._wrapper_var.trace(\"w\", self._update_displaybook)\n\n    def _add_static_tabs(self):\n        \"\"\" Add the tabs to the Display Notebook that are permanently displayed.\n\n        Currently this is just the `Analysis` tab.\n        \"\"\"\n        logger.debug(\"Adding static tabs\")\n        for tab in (\"job queue\", \"analysis\"):\n            if tab == \"job queue\":\n                continue    # Not yet implemented\n            if tab == \"analysis\":\n                helptext = {\"stats\":\n                            _(\"Summary statistics for each training session\")}\n                frame = Analysis(self, tab, helptext)\n            else:\n                frame = self._add_frame()\n                self.add(frame, text=tab.title())\n\n    def _add_frame(self):\n        \"\"\" Add a single frame for holding a static tab's contents.\n\n        Returns\n        -------\n        ttk.Frame\n            The frame, packed into position\n        \"\"\"\n        logger.debug(\"Adding frame\")\n        frame = ttk.Frame(self)\n        frame.pack(side=tk.LEFT, fill=tk.BOTH, expand=True, padx=5, pady=5)\n        return frame\n\n    def _command_display(self, command):\n        \"\"\" Build the relevant command specific tabs based on the incoming Faceswap command.\n\n        Parameters\n        ----------\n        command: str\n            The Faceswap command that is being executed\n        \"\"\"\n        build_tabs = getattr(self, f\"_{command}_tabs\")\n        build_tabs()\n\n    def _extract_tabs(self, command=\"extract\"):\n        \"\"\" Build the display tabs that are used for Faceswap extract and convert tasks.\n\n        Notes\n        -----\n        The same display tabs are used for both convert and extract tasks.\n\n        command: [`\"extract\"`, `\"convert\"`], optional\n            The command that the display tabs are being built for. Default: `\"extract\"`\n\n        \"\"\"\n        logger.debug(\"Build extract tabs\")\n        helptext = _(\"Preview updates every 5 seconds\")\n        PreviewExtract(self, \"preview\", helptext, 5000, command)\n        logger.debug(\"Built extract tabs\")\n\n    def _train_tabs(self):\n        \"\"\" Build the display tabs that are used for the Faceswap train task.\"\"\"\n        logger.debug(\"Build train tabs\")\n        for tab in (\"graph\", \"preview\"):\n            if tab == \"graph\":\n                helptext = _(\"Graph showing Loss vs Iterations\")\n                GraphDisplay(self, \"graph\", helptext, 5000)\n            elif tab == \"preview\":\n                helptext = _(\"Training preview. Updated on every save iteration\")\n                PreviewTrain(self, \"preview\", helptext, 1000)\n        logger.debug(\"Built train tabs\")\n\n    def _convert_tabs(self):\n        \"\"\" Build the display tabs that are used for the Faceswap convert task.\n\n        Notes\n        -----\n        The tabs displayed are the same as used for extract, so :func:`_extract_tabs` is called.\n        \"\"\"\n        logger.debug(\"Build convert tabs\")\n        self._extract_tabs(command=\"convert\")\n        logger.debug(\"Built convert tabs\")\n\n    def _remove_tabs(self):\n        \"\"\" Remove all optional displayed command specific tabs from the notebook. \"\"\"\n        for child in self.tabs():\n            if child in self._static_tabs:\n                continue\n            logger.debug(\"removing child: %s\", child)\n            child_name = child.split(\".\")[-1]\n            child_object = self.children.get(child_name)  # returns the OptionalDisplayPage object\n            if not child_object:\n                continue\n            child_object.close()  # Call the OptionalDisplayPage close() method\n            self.forget(child)\n\n    def _update_displaybook(self, *args):  # pylint:disable=unused-argument\n        \"\"\" Callback to be executed when the global tkinter variable `display`\n        (:attr:`wrapper_var`) is updated when a Faceswap task is executed.\n\n        Currently only updates when a core faceswap task (extract, train or convert) is executed.\n\n        Parameters\n        ----------\n        args: tuple\n            Required for tkinter callback events, but unused.\n\n        \"\"\"\n        command = self._wrapper_var.get()\n        self._remove_tabs()\n        if not command or command not in (\"extract\", \"train\", \"convert\"):\n            return\n        self._command_display(command)\n\n    def _on_tab_change(self, event):  # pylint:disable=unused-argument\n        \"\"\" Event trigger for tab change events.\n\n        Calls the selected tabs :func:`on_tab_select` method, if it exists, otherwise returns.\n\n        Parameters\n        ----------\n        event: tkinter callback event\n            Required, but unused\n        \"\"\"\n        selected = self.select().split(\".\")[-1]\n        logger.debug(\"Selected tab: %s\", selected)\n        selected_object = self.children[selected]\n        if hasattr(selected_object, \"on_tab_select\"):\n            logger.debug(\"Calling on_tab_select for '%s'\", selected_object)\n            selected_object.on_tab_select()\n        else:\n            logger.debug(\"Object does not have on_tab_select method. Returning: '%s'\",\n                         selected_object)\n", "lib/gui/options.py": "#!/usr/bin python3\n\"\"\" Cli Options for the GUI \"\"\"\nfrom __future__ import annotations\n\nimport inspect\nfrom argparse import SUPPRESS\nfrom dataclasses import dataclass\nfrom importlib import import_module\nimport logging\nimport os\nimport re\nimport sys\nimport typing as T\n\nfrom lib.cli import actions\nfrom .utils import get_images\nfrom .control_helper import ControlPanelOption\n\nif T.TYPE_CHECKING:\n    from tkinter import Variable\n    from types import ModuleType\n    from lib.cli.args import FaceSwapArgs\n\nlogger = logging.getLogger(__name__)\n\n\n@dataclass\nclass CliOption:\n    \"\"\" A parsed command line option\n\n    Parameters\n    ----------\n    cpanel_option: :class:`~lib.gui.control_helper.ControlPanelOption`:\n        Object to hold information of a command line item for displaying in a GUI\n        :class:`~lib.gui.control_helper.ControlPanel`\n    opts: tuple[str, ...]:\n        The short switch and long name (if exists) of the command line option\n    nargs: Literal[\"+\"] | None:\n        ``None`` for not used. \"+\" for at least 1 argument required with values to be contained\n        in a list\n    \"\"\"\n    cpanel_option: ControlPanelOption\n    \"\"\":class:`~lib.gui.control_helper.ControlPanelOption`: Object to hold information of a command\n    line item for displaying in a GUI :class:`~lib.gui.control_helper.ControlPanel`\"\"\"\n    opts: tuple[str, ...]\n    \"\"\"tuple[str, ...]: The short switch and long name (if exists) of cli option \"\"\"\n    nargs: T.Literal[\"+\"] | None\n    \"\"\"Literal[\"+\"] | None: ``None`` for not used. \"+\" for at least 1 argument required with\n    values to be contained in a list \"\"\"\n\n\nclass CliOptions():\n    \"\"\" Class and methods for the command line options \"\"\"\n    def __init__(self) -> None:\n        logger.debug(\"Initializing %s\", self.__class__.__name__)\n        self._base_path = os.path.realpath(os.path.dirname(sys.argv[0]))\n        self._commands: dict[T.Literal[\"faceswap\", \"tools\"], list[str]] = {\"faceswap\": [],\n                                                                           \"tools\": []}\n        self._opts: dict[str, dict[str, CliOption | str]] = {}\n        self._build_options()\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    @property\n    def categories(self) -> tuple[T.Literal[\"faceswap\", \"tools\"], ...]:\n        \"\"\"tuple[str, str] The categories for faceswap's GUI \"\"\"\n        return tuple(self._commands)\n\n    @property\n    def commands(self) -> dict[T.Literal[\"faceswap\", \"tools\"], list[str]]:\n        \"\"\"dict[str, ]\"\"\"\n        return self._commands\n\n    @property\n    def opts(self) -> dict[str, dict[str, CliOption | str]]:\n        \"\"\"dict[str, dict[str, CliOption | str]] The command line options collected from faceswap's\n        cli files \"\"\"\n        return self._opts\n\n    def _get_modules_tools(self) -> list[ModuleType]:\n        \"\"\" Parse the tools cli python files for the modules that contain the command line\n        arguments\n\n        Returns\n        -------\n        list[`types.ModuleType`]\n            The modules for each faceswap tool that exists in the project\n        \"\"\"\n        tools_dir = os.path.join(self._base_path, \"tools\")\n        logger.debug(\"Scanning '%s' for cli files\", tools_dir)\n        retval: list[ModuleType] = []\n        for tool_name in sorted(os.listdir(tools_dir)):\n            cli_file = os.path.join(tools_dir, tool_name, \"cli.py\")\n            if not os.path.exists(cli_file):\n                logger.debug(\"File does not exist. Skipping: '%s'\", cli_file)\n                continue\n\n            mod = \".\".join((\"tools\", tool_name, \"cli\"))\n            retval.append(import_module(mod))\n            logger.debug(\"Collected: %s\", retval[-1])\n        return retval\n\n    def _get_modules_faceswap(self) -> list[ModuleType]:\n        \"\"\" Parse the faceswap cli python files for the modules that contain the command line\n        arguments\n\n        Returns\n        -------\n        list[`types.ModuleType`]\n            The modules for each faceswap command line argument file that exists in the project\n        \"\"\"\n        base_dir = [\"lib\", \"cli\"]\n        cli_dir = os.path.join(self._base_path, *base_dir)\n        logger.debug(\"Scanning '%s' for cli files\", cli_dir)\n        retval: list[ModuleType] = []\n\n        for fname in os.listdir(cli_dir):\n            if not fname.startswith(\"args\"):\n                logger.debug(\"Skipping file '%s'\", fname)\n                continue\n            mod = \".\".join((*base_dir, os.path.splitext(fname)[0]))\n            retval.append(import_module(mod))\n            logger.debug(\"Collected: '%s\", retval[-1])\n        return retval\n\n    def _get_modules(self, category: T.Literal[\"faceswap\", \"tools\"]) -> list[ModuleType]:\n        \"\"\" Parse the cli files for faceswap and tools and return the imported module\n\n        Parameters\n        ----------\n        category: Literal[\"faceswap\", \"tools\"]\n            The faceswap category to obtain the cli modules\n\n        Returns\n        -------\n        list[`types.ModuleType`]\n            The modules for each faceswap command/tool that exists in the project for the given\n            category\n        \"\"\"\n        logger.debug(\"Getting '%s' cli modules\", category)\n        if category == \"tools\":\n            return self._get_modules_tools()\n        return self._get_modules_faceswap()\n\n    @classmethod\n    def _get_classes(cls, module: ModuleType) -> list[T.Type[FaceSwapArgs]]:\n        \"\"\" Obtain the classes from the given module that contain the command line\n        arguments\n\n        Parameters\n        ----------\n        module: :class:`types.ModuleType`\n            The imported module to parse for command line argument classes\n\n        Returns\n        -------\n        list[:class:`~lib.cli.args.FaceswapArgs`]\n            The command line argument class objects that exist in the module\n        \"\"\"\n        retval = []\n        for name, obj in inspect.getmembers(module):\n            if not inspect.isclass(obj) or not name.lower().endswith(\"args\"):\n                logger.debug(\"Skipping non-cli class object '%s'\", name)\n                continue\n            if name.lower() in ((\"faceswapargs\", \"extractconvertargs\", \"guiargs\")):\n                logger.debug(\"Skipping uneeded object '%s'\", name)\n                continue\n            logger.debug(\"Collecting %s\", obj)\n            retval.append(obj)\n        logger.debug(\"Collected from '%s': %s\", module.__name__, [c.__name__ for c in retval])\n        return retval\n\n    def _get_all_classes(self, modules: list[ModuleType]) -> list[T.Type[FaceSwapArgs]]:\n        \"\"\"Obtain the  the command line options classes for the given modules\n\n        Parameters\n        ----------\n        modules : list[:class:`types.ModuleType`]\n            The imported modules to extract the command line argument classes from\n\n        Returns\n        -------\n        list[:class:`~lib.cli.args.FaceSwapArgs`]\n            The valid command line class objects for the given modules\n        \"\"\"\n        retval = []\n        for module in modules:\n            mod_classes = self._get_classes(module)\n            if not mod_classes:\n                logger.debug(\"module '%s' contains no cli classes. Skipping\", module)\n                continue\n            retval.extend(mod_classes)\n        logger.debug(\"Obtained %s cli classes from %s modules\", len(retval), len(modules))\n        return retval\n\n    @classmethod\n    def _class_name_to_command(cls, class_name: str) -> str:\n        \"\"\" Format a FaceSwapArgs class name to a standardized command name\n\n        Parameters\n        ----------\n        class_name: str\n            The name of the class to convert to a command name\n\n        Returns\n        -------\n        str\n            The formatted command name\n        \"\"\"\n        return class_name.lower()[:-4]\n\n    def _store_commands(self,\n                        category: T.Literal[\"faceswap\", \"tools\"],\n                        classes: list[T.Type[FaceSwapArgs]]) -> None:\n        \"\"\" Format classes into command names and sort. Store in :attr:`commands`.\n        Sorting is in specific workflow order for faceswap and alphabetical for all others\n\n        Parameters\n        ----------\n        category: Literal[\"faceswap\", \"tools\"]\n            The category to store the command names for\n        classes: list[:class:`~lib.cli.args.FaceSwapArgs`]\n            The valid command line class objects for the category\n        \"\"\"\n        class_names = [c.__name__ for c in classes]\n        commands = sorted(self._class_name_to_command(n) for n in class_names)\n\n        if category == \"faceswap\":\n            ordered = [\"extract\", \"train\", \"convert\"]\n            commands = ordered + [command for command in commands\n                                  if command not in ordered]\n        self._commands[category].extend(commands)\n        logger.debug(\"Set '%s' commands: %s\", category, self._commands[category])\n\n    @classmethod\n    def _get_cli_arguments(cls,\n                           arg_class: T.Type[FaceSwapArgs],\n                           command: str) -> tuple[str, list[dict[str, T.Any]]]:\n        \"\"\" Extract the command line options from the given cli class\n\n        Parameters\n        ----------\n        arg_class: :class:`~lib.cli.args.FaceSwapArgs`\n            The class to extract the options from\n        command: str\n            The command name to extract the options for\n\n        Returns\n        -------\n        info: str\n            The helptext information for given command\n        options: list[dict. str, Any]\n            The command line options for the given command\n        \"\"\"\n        args = arg_class(None, command)\n        arg_list = args.argument_list + args.optional_arguments + args.global_arguments\n        logger.debug(\"Obtain options for '%s'. Info: '%s', options: %s\",\n                     command, args.info, len(arg_list))\n        return args.info, arg_list\n\n    @classmethod\n    def _set_control_title(cls, opts: tuple[str, ...]) -> str:\n        \"\"\" Take the option switch and format it nicely\n\n        Parameters\n        ----------\n        opts: tuple[str, ...]\n            The option switch for a command line option\n\n        Returns\n        -------\n        str\n            The option switch formatted for display\n        \"\"\"\n        ctltitle = opts[1] if len(opts) == 2 else opts[0]\n        retval = ctltitle.replace(\"-\", \" \").replace(\"_\", \" \").strip().title()\n        logger.debug(\"Formatted '%s' to '%s'\",  ctltitle, retval)\n        return retval\n\n    @classmethod\n    def _get_data_type(cls, opt: dict[str, T.Any]) -> type:\n        \"\"\" Return a data type for passing into control_helper.py to get the correct control\n\n        Parameters\n        ----------\n        option: dict[str, Any]\n            The option to extract the data type from\n\n        Returns\n        -------\n        :class:`type`\n            The Python type for the option\n        \"\"\"\n        type_ = opt.get(\"type\")\n        if type_ is not None and isinstance(opt[\"type\"], type):\n            retval = type_\n        elif opt.get(\"action\", \"\") in (\"store_true\", \"store_false\"):\n            retval = bool\n        else:\n            retval = str\n        logger.debug(\"Setting type to %s for %s\", retval, type_)\n        return retval\n\n    @classmethod\n    def _get_rounding(cls, opt: dict[str, T.Any]) -> int | None:\n        \"\"\" Return rounding for the given option\n\n        Parameters\n        ----------\n        option: dict[str, Any]\n            The option to extract the rounding from\n\n        Returns\n        -------\n        int | None\n            int if the data type supports rounding otherwise ``None``\n        \"\"\"\n        dtype = opt.get(\"type\")\n        if dtype == float:\n            retval = opt.get(\"rounding\", 2)\n        elif dtype == int:\n            retval = opt.get(\"rounding\", 1)\n        else:\n            retval = None\n        logger.debug(\"Setting rounding to %s for type %s\", retval, dtype)\n        return retval\n\n    @classmethod\n    def _expand_action_option(cls,\n                              option: dict[str, T.Any],\n                              options: list[dict[str, T.Any]]) -> None:\n        \"\"\" Expand the action option to the full command name\n\n        Parameters\n        ----------\n        option: dict[str, Any]\n            The option to expand the action for\n        options: list[dict[str, Any]]\n            The full list of options for the command\n        \"\"\"\n        opts = {opt[\"opts\"][0]: opt[\"opts\"][-1]\n                for opt in options}\n        old_val = option[\"action_option\"]\n        new_val = opts[old_val]\n        logger.debug(\"Updating action option from '%s' to '%s'\", old_val, new_val)\n        option[\"action_option\"] = new_val\n\n    def _get_sysbrowser(self,\n                        option: dict[str, T.Any],\n                        options: list[dict[str, T.Any]],\n                        command: str) -> dict[T.Literal[\"filetypes\",\n                                                        \"browser\",\n                                                        \"command\",\n                                                        \"destination\",\n                                                        \"action_option\"], str | list[str]] | None:\n        \"\"\" Return the system file browser and file types if required\n\n        Parameters\n        ----------\n        option: dict[str, Any]\n            The option to obtain the system browser for\n        options: list[dict[str, Any]]\n            The full list of options for the command\n        command: str\n            The command that the options belong to\n\n        Returns\n        -------\n        dict[Literal[\"filetypes\", \"browser\", \"command\",\n                     \"destination\", \"action_option\"], list[str]] | None\n            The browser information, if valid, or ``None`` if browser not required\n        \"\"\"\n        action = option.get(\"action\", None)\n        if action not in (actions.DirFullPaths,\n                          actions.FileFullPaths,\n                          actions.FilesFullPaths,\n                          actions.DirOrFileFullPaths,\n                          actions.DirOrFilesFullPaths,\n                          actions.SaveFileFullPaths,\n                          actions.ContextFullPaths):\n            return None\n\n        retval: dict[T.Literal[\"filetypes\",\n                               \"browser\",\n                               \"command\",\n                               \"destination\",\n                               \"action_option\"], str | list[str]] = {}\n        action_option = None\n        if option.get(\"action_option\", None) is not None:\n            self._expand_action_option(option, options)\n            action_option = option[\"action_option\"]\n        retval[\"filetypes\"] = option.get(\"filetypes\", \"default\")\n        if action == actions.FileFullPaths:\n            retval[\"browser\"] = [\"load\"]\n        elif action == actions.FilesFullPaths:\n            retval[\"browser\"] = [\"multi_load\"]\n        elif action == actions.SaveFileFullPaths:\n            retval[\"browser\"] = [\"save\"]\n        elif action == actions.DirOrFileFullPaths:\n            retval[\"browser\"] = [\"folder\", \"load\"]\n        elif action == actions.DirOrFilesFullPaths:\n            retval[\"browser\"] = [\"folder\", \"multi_load\"]\n        elif action == actions.ContextFullPaths and action_option:\n            retval[\"browser\"] = [\"context\"]\n            retval[\"command\"] = command\n            retval[\"action_option\"] = action_option\n            retval[\"destination\"] = option.get(\"dest\", option[\"opts\"][1].replace(\"--\", \"\"))\n        else:\n            retval[\"browser\"] = [\"folder\"]\n        logger.debug(retval)\n        return retval\n\n    def _process_options(self, command_options: list[dict[str, T.Any]], command: str\n                         ) -> dict[str, CliOption]:\n        \"\"\" Process the options for a single command\n\n        Parameters\n        ----------\n        command_options: list[dict. str, Any]\n            The command line options for the given command\n        command: str\n            The command name to process\n\n        Returns\n        -------\n        dict[str, :class:`CliOption`]\n            The collected command line options for handling by the GUI\n        \"\"\"\n        retval: dict[str, CliOption] = {}\n        for opt in command_options:\n            logger.debug(\"Processing: cli option: %s\", opt[\"opts\"])\n            if opt.get(\"help\", \"\") == SUPPRESS:\n                logger.debug(\"Skipping suppressed option: %s\", opt)\n                continue\n            title = self._set_control_title(opt[\"opts\"])\n            cpanel_option = ControlPanelOption(\n                title,\n                self._get_data_type(opt),\n                group=opt.get(\"group\", None),\n                default=opt.get(\"default\", None),\n                choices=opt.get(\"choices\", None),\n                is_radio=opt.get(\"action\", \"\") == actions.Radio,\n                is_multi_option=opt.get(\"action\", \"\") == actions.MultiOption,\n                rounding=self._get_rounding(opt),\n                min_max=opt.get(\"min_max\", None),\n                sysbrowser=self._get_sysbrowser(opt, command_options, command),\n                helptext=opt[\"help\"],\n                track_modified=True,\n                command=command)\n            retval[title] = CliOption(cpanel_option=cpanel_option,\n                                      opts=opt[\"opts\"],\n                                      nargs=opt.get(\"nargs\"))\n            logger.debug(\"Processed: %s\", retval)\n        return retval\n\n    def _extract_options(self, arguments: list[T.Type[FaceSwapArgs]]):\n        \"\"\" Extract the collected command line FaceSwapArg options into master options\n        :attr:`opts` dictionary\n\n        Parameters\n        ----------\n        arguments: list[:class:`~lib.cli.args.FaceSwapArgs`]\n            The command line class objects to process\n        \"\"\"\n        retval = {}\n        for arg_class in arguments:\n            logger.debug(\"Processing: '%s'\", arg_class.__name__)\n            command = self._class_name_to_command(arg_class.__name__)\n            info, options = self._get_cli_arguments(arg_class, command)\n            opts = T.cast(dict[str, CliOption | str], self._process_options(options, command))\n            opts[\"helptext\"] = info\n            retval[command] = opts\n        self._opts.update(retval)\n\n    def _build_options(self) -> None:\n        \"\"\" Parse the command line argument modules and populate :attr:`commands` and :attr:`opts`\n        for each category \"\"\"\n        for category in self.categories:\n            modules = self._get_modules(category)\n            classes = self._get_all_classes(modules)\n            self._store_commands(category, classes)\n            self._extract_options(classes)\n            logger.debug(\"Built '%s'\", category)\n\n    def _gen_command_options(self, command: str\n                             ) -> T.Generator[tuple[str, CliOption], None, None]:\n        \"\"\" Yield each option for specified command\n\n        Parameters\n        ----------\n        command: str\n            The faceswap command to generate the options for\n\n        Yields\n        ------\n        str\n            The option name for display\n        :class:`CliOption`:\n            The option object\n        \"\"\"\n        for key, val in self._opts.get(command, {}).items():\n            if not isinstance(val, CliOption):\n                continue\n            yield key, val\n\n    def _options_to_process(self, command: str | None = None) -> list[CliOption]:\n        \"\"\" Return a consistent object for processing regardless of whether processing all commands\n        or just one command for reset and clear. Removes helptext from return value\n\n        Parameters\n        ----------\n        command: str | None, optional\n            The command to return the options for. ``None`` for all commands. Default ``None``\n\n        Returns\n        -------\n        list[:class:`CliOption`]\n            The options to be processed\n        \"\"\"\n        if command is None:\n            return [opt for opts in self._opts.values()\n                    for opt in opts if isinstance(opt, CliOption)]\n        return [opt for opt in self._opts[command] if isinstance(opt, CliOption)]\n\n    def reset(self, command: str | None = None) -> None:\n        \"\"\" Reset the options for all or passed command back to default value\n\n        Parameters\n        ----------\n        command: str | None, optional\n            The command to reset the options for. ``None`` to reset for all commands.\n            Default: ``None``\n        \"\"\"\n        logger.debug(\"Resetting options to default. (command: '%s'\", command)\n        for option in self._options_to_process(command):\n            cp_opt = option.cpanel_option\n            default = \"\" if cp_opt.default is None else cp_opt.default\n            if option.nargs is not None and isinstance(default, (list, tuple)):\n                default = ' '.join(str(val) for val in default)\n            cp_opt.set(default)\n\n    def clear(self, command: str | None = None) -> None:\n        \"\"\" Clear the options values for all or passed commands\n\n        Parameters\n        ----------\n        command: str | None, optional\n            The command to clear the options for. ``None`` to clear options for all commands.\n            Default: ``None``\n        \"\"\"\n        logger.debug(\"Clearing options. (command: '%s'\", command)\n        for option in self._options_to_process(command):\n            cp_opt = option.cpanel_option\n            if isinstance(cp_opt.get(), bool):\n                cp_opt.set(False)\n            elif isinstance(cp_opt.get(), (int, float)):\n                cp_opt.set(0)\n            else:\n                cp_opt.set(\"\")\n\n    def get_option_values(self, command: str | None = None\n                          ) -> dict[str, dict[str, bool | int | float | str]]:\n        \"\"\" Return all or single command control titles with the associated tk_var value\n\n        Parameters\n        ----------\n        command: str | None, optional\n            The command to get the option values for. ``None`` to get all option values.\n            Default: ``None``\n\n        Returns\n        -------\n        dict[str, dict[str, bool | int | float | str]]\n            option values in the format {command: {option_name: option_value}}\n        \"\"\"\n        ctl_dict: dict[str, dict[str, bool | int | float | str]] = {}\n        for cmd, opts in self._opts.items():\n            if command and command != cmd:\n                continue\n            cmd_dict: dict[str, bool | int | float | str] = {}\n            for key, val in opts.items():\n                if not isinstance(val, CliOption):\n                    continue\n                cmd_dict[key] = val.cpanel_option.get()\n            ctl_dict[cmd] = cmd_dict\n        logger.debug(\"command: '%s', ctl_dict: %s\", command, ctl_dict)\n        return ctl_dict\n\n    def get_one_option_variable(self, command: str, title: str) -> Variable | None:\n        \"\"\" Return a single :class:`tkinter.Variable` tk_var for the specified command and\n        control_title\n\n        Parameters\n        ----------\n        command: str\n            The command to return the variable from\n        title: str\n            The option title to return the variable for\n\n        Returns\n        -------\n        :class:`tkinter.Variable` | None\n            The requested tkinter variable, or ``None`` if it could not be found\n        \"\"\"\n        for opt_title, option in self._gen_command_options(command):\n            if opt_title == title:\n                return option.cpanel_option.tk_var\n        return None\n\n    def gen_cli_arguments(self, command: str) -> T.Generator[tuple[str, ...], None, None]:\n        \"\"\" Yield the generated cli arguments for the selected command\n\n        Parameters\n        ----------\n        command: str\n            The command to generate the command line arguments for\n\n        Yields\n        ------\n        tuple[str, ...]\n            The generated command line arguments\n        \"\"\"\n        output_dir = None\n        for _, option in self._gen_command_options(command):\n            str_val = str(option.cpanel_option.get())\n            switch = option.opts[0]\n            batch_mode = command == \"extract\" and switch == \"-b\"  # Check for batch mode\n            if command in (\"extract\", \"convert\") and switch == \"-o\":  # Output location for preview\n                output_dir = str_val\n\n            if str_val in (\"False\", \"\"):  # skip no value opts\n                continue\n\n            if str_val == \"True\":  # store_true just output the switch\n                yield (switch, )\n                continue\n\n            if option.nargs is not None:\n                if \"\\\"\" in str_val:\n                    val = [arg[1:-1] for arg in re.findall(r\"\\\".+?\\\"\", str_val)]\n                else:\n                    val = str_val.split(\" \")\n                retval = (switch, *val)\n            else:\n                retval = (switch, str_val)\n            yield retval\n\n        if command in (\"extract\", \"convert\") and output_dir is not None:\n            get_images().preview_extract.set_faceswap_output_path(output_dir,\n                                                                  batch_mode=batch_mode)\n", "lib/gui/command.py": "#!/usr/bin python3\n\"\"\" The command frame for Faceswap GUI \"\"\"\n\nimport logging\nimport gettext\nimport tkinter as tk\nfrom tkinter import ttk\n\nfrom .control_helper import ControlPanel\nfrom .custom_widgets import Tooltip\nfrom .utils import get_images, get_config\nfrom .options import CliOption\n\nlogger = logging.getLogger(__name__)\n\n# LOCALES\n_LANG = gettext.translation(\"gui.tooltips\", localedir=\"locales\", fallback=True)\n_ = _LANG.gettext\n\n\nclass CommandNotebook(ttk.Notebook):  # pylint:disable=too-many-ancestors\n    \"\"\" Frame to hold each individual tab of the command notebook \"\"\"\n\n    def __init__(self, parent):\n        logger.debug(\"Initializing %s: (parent: %s)\", self.__class__.__name__, parent)\n        self.actionbtns = {}\n        super().__init__(parent)\n        parent.add(self)\n\n        self.tools_notebook = ToolsNotebook(self)\n        self.set_running_task_trace()\n        self.build_tabs()\n        self.modified_vars = self._set_modified_vars()\n        get_config().set_command_notebook(self)\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    @property\n    def tab_names(self):\n        \"\"\" dict: Command tab titles with their IDs \"\"\"\n        return {self.tab(tab_id, \"text\").lower(): tab_id\n                for tab_id in range(0, self.index(\"end\"))}\n\n    @property\n    def tools_tab_names(self):\n        \"\"\" dict: Tools tab titles with their IDs \"\"\"\n        return {self.tools_notebook.tab(tab_id, \"text\").lower(): tab_id\n                for tab_id in range(0, self.tools_notebook.index(\"end\"))}\n\n    def set_running_task_trace(self):\n        \"\"\" Set trigger action for the running task\n            to change the action buttons text and command \"\"\"\n        logger.debug(\"Set running trace\")\n        tk_vars = get_config().tk_vars\n        tk_vars.running_task.trace(\"w\", self.change_action_button)\n\n    def build_tabs(self):\n        \"\"\" Build the tabs for the relevant command \"\"\"\n        logger.debug(\"Build Tabs\")\n        cli_opts = get_config().cli_opts\n        for category in cli_opts.categories:\n            book = self.tools_notebook if category == \"tools\" else self\n            cmdlist = cli_opts.commands[category]\n            for command in cmdlist:\n                title = command.title()\n                commandtab = CommandTab(book, category, command)\n                book.add(commandtab, text=title)\n        self.add(self.tools_notebook, text=\"Tools\")\n        logger.debug(\"Built Tabs\")\n\n    def change_action_button(self, *args):\n        \"\"\" Change the action button to relevant control \"\"\"\n        logger.debug(\"Update Action Buttons: (args: %s\", args)\n        tk_vars = get_config().tk_vars\n\n        for cmd, action in self.actionbtns.items():\n            btnact = action\n            if tk_vars.running_task.get():\n                ttl = \" Stop\"\n                img = get_images().icons[\"stop\"]\n                hlp = \"Exit the running process\"\n            else:\n                ttl = f\" {cmd.title()}\"\n                img = get_images().icons[\"start\"]\n                hlp = f\"Run the {cmd.title()} script\"\n            logger.debug(\"Updated Action Button: '%s'\", ttl)\n            btnact.config(text=ttl, image=img)\n            Tooltip(btnact, text=hlp, wrap_length=200)\n\n    def _set_modified_vars(self):\n        \"\"\" Set the tkinter variable for each tab to indicate whether contents\n        have been modified \"\"\"\n        tkvars = {}\n        for tab in self.tab_names:\n            if tab == \"tools\":\n                for ttab in self.tools_tab_names:\n                    var = tk.BooleanVar()\n                    var.set(False)\n                    tkvars[ttab] = var\n                continue\n            var = tk.BooleanVar()\n            var.set(False)\n            tkvars[tab] = var\n        logger.debug(\"Set modified vars: %s\", tkvars)\n        return tkvars\n\n\nclass ToolsNotebook(ttk.Notebook):  # pylint:disable=too-many-ancestors\n    \"\"\" Tools sit in their own tab, but need to inherit objects from the main command notebook \"\"\"\n    def __init__(self, parent):\n        super().__init__(parent)\n        self.actionbtns = parent.actionbtns\n\n\nclass CommandTab(ttk.Frame):  # pylint:disable=too-many-ancestors\n    \"\"\" Frame to hold each individual tab of the command notebook \"\"\"\n\n    def __init__(self, parent, category, command):\n        logger.debug(\"Initializing %s: (category: '%s', command: '%s')\",\n                     self.__class__.__name__, category, command)\n        super().__init__(parent, name=f\"tab_{command.lower()}\")\n\n        self.category = category\n        self.actionbtns = parent.actionbtns\n        self.command = command\n\n        self.build_tab()\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    def build_tab(self):\n        \"\"\" Build the tab \"\"\"\n        logger.debug(\"Build Tab: '%s'\", self.command)\n        options = get_config().cli_opts.opts[self.command]\n        cp_opts = [val.cpanel_option for val in options.values() if isinstance(val, CliOption)]\n        ControlPanel(self,\n                     cp_opts,\n                     label_width=16,\n                     option_columns=3,\n                     columns=1,\n                     header_text=options.get(\"helptext\", None),\n                     style=\"CPanel\")\n        self.add_frame_separator()\n        ActionFrame(self)\n        logger.debug(\"Built Tab: '%s'\", self.command)\n\n    def add_frame_separator(self):\n        \"\"\" Add a separator between top and bottom frames \"\"\"\n        logger.debug(\"Add frame seperator\")\n        sep = ttk.Frame(self, height=2, relief=tk.RIDGE)\n        sep.pack(fill=tk.X, pady=(5, 0), side=tk.TOP)\n        logger.debug(\"Added frame seperator\")\n\n\nclass ActionFrame(ttk.Frame):  # pylint:disable=too-many-ancestors\n    \"\"\"Action Frame - Displays action controls for the command tab \"\"\"\n\n    def __init__(self, parent):\n        logger.debug(\"Initializing %s: (command: '%s')\", self.__class__.__name__, parent.command)\n        super().__init__(parent)\n        self.pack(fill=tk.BOTH, padx=5, pady=5, side=tk.BOTTOM, anchor=tk.N)\n\n        self.command = parent.command\n        self.title = self.command.title()\n\n        self.add_action_button(parent.category,\n                               parent.actionbtns)\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    def add_action_button(self, category, actionbtns):\n        \"\"\" Add the action buttons for page \"\"\"\n        logger.debug(\"Add action buttons: '%s'\", self.title)\n        actframe = ttk.Frame(self)\n        actframe.pack(fill=tk.X, side=tk.RIGHT)\n\n        tk_vars = get_config().tk_vars\n        var_value = f\"{category},{self.command}\"\n\n        btngen = ttk.Button(actframe,\n                            image=get_images().icons[\"generate\"],\n                            text=\" Generate\",\n                            compound=tk.LEFT,\n                            width=14,\n                            command=lambda: tk_vars.generate_command.set(var_value))\n        btngen.pack(side=tk.LEFT, padx=5)\n        Tooltip(btngen,\n                text=_(\"Output command line options to the console\"),\n                wrap_length=200)\n\n        btnact = ttk.Button(actframe,\n                            image=get_images().icons[\"start\"],\n                            text=f\" {self.title}\",\n                            compound=tk.LEFT,\n                            width=14,\n                            command=lambda: tk_vars.action_command.set(var_value))\n        btnact.pack(side=tk.LEFT, fill=tk.X, expand=True)\n        Tooltip(btnact,\n                text=_(\"Run the {} script\").format(self.title),\n                wrap_length=200)\n        actionbtns[self.command] = btnact\n\n        logger.debug(\"Added action buttons: '%s'\", self.title)\n", "lib/gui/display_command.py": "#!/usr/bin python3\n\"\"\" Command specific tabs of Display Frame of the Faceswap GUI \"\"\"\nimport datetime\nimport gettext\nimport logging\nimport os\nimport tkinter as tk\nimport typing as T\n\nfrom tkinter import ttk\n\nfrom lib.logger import parse_class_init\nfrom lib.training.preview_tk import PreviewTk\n\nfrom .display_graph import TrainingGraph\nfrom .display_page import DisplayOptionalPage\nfrom .custom_widgets import Tooltip\nfrom .analysis import Calculations, Session\nfrom .control_helper import set_slider_rounding\nfrom .utils import FileHandler, get_config, get_images, preview_trigger\n\nlogger = logging.getLogger(__name__)\n\n# LOCALES\n_LANG = gettext.translation(\"gui.tooltips\", localedir=\"locales\", fallback=True)\n_ = _LANG.gettext\n\n\nclass PreviewExtract(DisplayOptionalPage):  # pylint:disable=too-many-ancestors\n    \"\"\" Tab to display output preview images for extract and convert \"\"\"\n    def __init__(self, *args, **kwargs) -> None:\n        logger.debug(parse_class_init(locals()))\n        self._preview = get_images().preview_extract\n        super().__init__(*args, **kwargs)\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    def display_item_set(self) -> None:\n        \"\"\" Load the latest preview if available \"\"\"\n        logger.trace(\"Loading latest preview\")  # type:ignore[attr-defined]\n        size = int(256 if self.command == \"convert\" else 128 * get_config().scaling_factor)\n        if not self._preview.load_latest_preview(thumbnail_size=size,\n                                                 frame_dims=(self.winfo_width(),\n                                                             self.winfo_height())):\n            logger.trace(\"Preview not updated\")  # type:ignore[attr-defined]\n            return\n        logger.debug(\"Preview loaded\")\n        self.display_item = True\n\n    def display_item_process(self) -> None:\n        \"\"\" Display the preview \"\"\"\n        logger.trace(\"Displaying preview\")  # type:ignore[attr-defined]\n        if not self.subnotebook.children:\n            self.add_child()\n        else:\n            self.update_child()\n\n    def add_child(self) -> None:\n        \"\"\" Add the preview label child \"\"\"\n        logger.debug(\"Adding child\")\n        preview = self.subnotebook_add_page(self.tabname, widget=None)\n        lblpreview = ttk.Label(preview, image=self._preview.image)\n        lblpreview.pack(side=tk.TOP, anchor=tk.NW)\n        Tooltip(lblpreview, text=self.helptext, wrap_length=200)\n\n    def update_child(self) -> None:\n        \"\"\" Update the preview image on the label \"\"\"\n        logger.trace(\"Updating preview\")  # type:ignore[attr-defined]\n        for widget in self.subnotebook_get_widgets():\n            widget.configure(image=self._preview.image)\n\n    def save_items(self) -> None:\n        \"\"\" Open save dialogue and save preview \"\"\"\n        location = FileHandler(\"dir\", None).return_file\n        if not location:\n            return\n        filename = \"extract_convert_preview\"\n        now = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        filename = os.path.join(location, f\"{filename}_{now}.png\")\n        self._preview.save(filename)\n        print(f\"Saved preview to {filename}\")\n\n\nclass PreviewTrain(DisplayOptionalPage):  # pylint:disable=too-many-ancestors\n    \"\"\" Training preview image(s) \"\"\"\n    def __init__(self, *args, **kwargs) -> None:\n        logger.debug(parse_class_init(locals()))\n        self._preview = get_images().preview_train\n        self._display: PreviewTk | None = None\n        super().__init__(*args, **kwargs)\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    def add_options(self) -> None:\n        \"\"\" Add the additional options \"\"\"\n        self._add_option_refresh()\n        self._add_option_mask_toggle()\n        super().add_options()\n\n    def subnotebook_hide(self) -> None:\n        \"\"\" Override default subnotebook hide action to also remove the embedded option bar\n        control and reset the training image buffer \"\"\"\n        if self.subnotebook and self.subnotebook.winfo_ismapped():\n            logger.debug(\"Removing preview controls from options bar\")\n            if self._display is not None:\n                self._display.remove_option_controls()\n            super().subnotebook_hide()\n            del self._display\n            self._display = None\n            self._preview.reset()\n\n    def _add_option_refresh(self) -> None:\n        \"\"\" Add refresh button to refresh preview immediately \"\"\"\n        logger.debug(\"Adding refresh option\")\n        btnrefresh = ttk.Button(self.optsframe,\n                                image=get_images().icons[\"reload\"],\n                                command=lambda x=\"update\": preview_trigger().set(x))  # type:ignore\n        btnrefresh.pack(padx=2, side=tk.RIGHT)\n        Tooltip(btnrefresh,\n                text=_(\"Preview updates at every model save. Click to refresh now.\"),\n                wrap_length=200)\n        logger.debug(\"Added refresh option\")\n\n    def _add_option_mask_toggle(self) -> None:\n        \"\"\" Add button to toggle mask display on and off \"\"\"\n        logger.debug(\"Adding mask toggle option\")\n        btntoggle = ttk.Button(\n            self.optsframe,\n            image=get_images().icons[\"mask2\"],\n            command=lambda x=\"mask_toggle\": preview_trigger().set(x))  # type:ignore\n        btntoggle.pack(padx=2, side=tk.RIGHT)\n        Tooltip(btntoggle,\n                text=_(\"Click to toggle mask overlay on and off.\"),\n                wrap_length=200)\n        logger.debug(\"Added mask toggle option\")\n\n    def display_item_set(self) -> None:\n        \"\"\" Load the latest preview if available \"\"\"\n        # TODO This seems to be triggering faster than the waittime\n        logger.trace(\"Loading latest preview\")  # type:ignore[attr-defined]\n        if not self._preview.load():\n            logger.trace(\"Preview not updated\")  # type:ignore[attr-defined]\n            return\n        logger.debug(\"Preview loaded\")\n        self.display_item = True\n\n    def display_item_process(self) -> None:\n        \"\"\" Display the preview(s) resized as appropriate \"\"\"\n        if self.subnotebook.children:\n            return\n\n        logger.debug(\"Displaying preview\")\n        self._display = PreviewTk(self._preview.buffer, self.subnotebook, self.optsframe, None)\n        self.subnotebook_add_page(self.tabname, widget=self._display.master_frame)\n\n    def save_items(self) -> None:\n        \"\"\" Open save dialogue and save preview \"\"\"\n        if self._display is None:\n            return\n\n        location = FileHandler(\"dir\", None).return_file\n        if not location:\n            return\n\n        self._display.save(location)\n\n\nclass GraphDisplay(DisplayOptionalPage):  # pylint:disable=too-many-ancestors\n    \"\"\" The Graph Tab of the Display section \"\"\"\n    def __init__(self,\n                 parent: ttk.Notebook,\n                 tab_name: str,\n                 helptext: str,\n                 wait_time: int,\n                 command: str | None = None) -> None:\n        logger.debug(parse_class_init(locals()))\n        self._trace_vars: dict[T.Literal[\"smoothgraph\", \"display_iterations\"],\n                               tuple[tk.BooleanVar, str]] = {}\n        super().__init__(parent, tab_name, helptext, wait_time, command)\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    def set_vars(self) -> None:\n        \"\"\" Add graphing specific variables to the default variables.\n\n        Overrides original method.\n\n        Returns\n        -------\n        dict\n            The variable names with their corresponding tkinter variable\n        \"\"\"\n        tk_vars = super().set_vars()\n\n        smoothgraph = tk.DoubleVar()\n        smoothgraph.set(0.900)\n        tk_vars[\"smoothgraph\"] = smoothgraph\n\n        raw_var = tk.BooleanVar()\n        raw_var.set(True)\n        tk_vars[\"raw_data\"] = raw_var\n\n        smooth_var = tk.BooleanVar()\n        smooth_var.set(True)\n        tk_vars[\"smooth_data\"] = smooth_var\n\n        iterations_var = tk.IntVar()\n        iterations_var.set(10000)\n        tk_vars[\"display_iterations\"] = iterations_var\n\n        logger.debug(tk_vars)\n        return tk_vars\n\n    def on_tab_select(self) -> None:\n        \"\"\" Callback for when the graph tab is selected.\n\n        Pull latest data and run the tab's update code when the tab is selected.\n        \"\"\"\n        logger.debug(\"Callback received for '%s' tab (display_item: %s)\",\n                     self.tabname, self.display_item)\n        if self.display_item is not None:\n            get_config().tk_vars.refresh_graph.set(True)\n        self._update_page()\n\n    def add_options(self) -> None:\n        \"\"\" Add the additional options \"\"\"\n        self._add_option_refresh()\n        super().add_options()\n        self._add_option_raw()\n        self._add_option_smoothed()\n        self._add_option_smoothing()\n        self._add_option_iterations()\n\n    def _add_option_refresh(self) -> None:\n        \"\"\" Add refresh button to refresh graph immediately \"\"\"\n        logger.debug(\"Adding refresh option\")\n        tk_var = get_config().tk_vars.refresh_graph\n        btnrefresh = ttk.Button(self.optsframe,\n                                image=get_images().icons[\"reload\"],\n                                command=lambda: tk_var.set(True))\n        btnrefresh.pack(padx=2, side=tk.RIGHT)\n        Tooltip(btnrefresh,\n                text=_(\"Graph updates at every model save. Click to refresh now.\"),\n                wrap_length=200)\n        logger.debug(\"Added refresh option\")\n\n    def _add_option_raw(self) -> None:\n        \"\"\" Add check-button to hide/display raw data \"\"\"\n        logger.debug(\"Adding display raw option\")\n        tk_var = self.vars[\"raw_data\"]\n        chkbtn = ttk.Checkbutton(\n            self.optsframe,\n            variable=tk_var,\n            text=\"Raw\",\n            command=lambda v=tk_var: self._display_data_callback(\"raw\", v))  # type:ignore\n        chkbtn.pack(side=tk.RIGHT, padx=5, anchor=tk.W)\n        Tooltip(chkbtn, text=_(\"Display the raw loss data\"), wrap_length=200)\n\n    def _add_option_smoothed(self) -> None:\n        \"\"\" Add check-button to hide/display smoothed data \"\"\"\n        logger.debug(\"Adding display smoothed option\")\n        tk_var = self.vars[\"smooth_data\"]\n        chkbtn = ttk.Checkbutton(\n            self.optsframe,\n            variable=tk_var,\n            text=\"Smoothed\",\n            command=lambda v=tk_var: self._display_data_callback(\"smoothed\", v))  # type:ignore\n        chkbtn.pack(side=tk.RIGHT, padx=5, anchor=tk.W)\n        Tooltip(chkbtn, text=_(\"Display the smoothed loss data\"), wrap_length=200)\n\n    def _add_option_smoothing(self) -> None:\n        \"\"\" Add a slider to adjust the smoothing amount \"\"\"\n        logger.debug(\"Adding Smoothing Slider\")\n        tk_var = self.vars[\"smoothgraph\"]\n        min_max = (0, 0.999)\n        hlp = _(\"Set the smoothing amount. 0 is no smoothing, 0.99 is maximum smoothing.\")\n\n        ctl_frame = ttk.Frame(self.optsframe)\n        ctl_frame.pack(padx=2, side=tk.RIGHT)\n\n        lbl = ttk.Label(ctl_frame, text=\"Smoothing:\", anchor=tk.W)\n        lbl.pack(pady=5, side=tk.LEFT, anchor=tk.N, expand=True)\n\n        tbox = ttk.Entry(ctl_frame, width=6, textvariable=tk_var, justify=tk.RIGHT)\n        tbox.pack(padx=(0, 5), side=tk.RIGHT)\n\n        ctl = ttk.Scale(\n            ctl_frame,\n            variable=tk_var,\n            command=lambda val, var=tk_var, dt=float, rn=3, mm=min_max:  # type:ignore\n            set_slider_rounding(val, var, dt, rn, mm))\n        ctl[\"from_\"] = min_max[0]\n        ctl[\"to\"] = min_max[1]\n        ctl.pack(padx=5, pady=5, fill=tk.X, expand=True)\n        for item in (tbox, ctl):\n            Tooltip(item,\n                    text=hlp,\n                    wrap_length=200)\n        logger.debug(\"Added Smoothing Slider\")\n\n    def _add_option_iterations(self) -> None:\n        \"\"\" Add a slider to adjust the amount if iterations to display \"\"\"\n        logger.debug(\"Adding Iterations Slider\")\n        tk_var = self.vars[\"display_iterations\"]\n        min_max = (0, 100000)\n        hlp = _(\"Set the number of iterations to display. 0 displays the full session.\")\n\n        ctl_frame = ttk.Frame(self.optsframe)\n        ctl_frame.pack(padx=2, side=tk.RIGHT)\n\n        lbl = ttk.Label(ctl_frame, text=\"Iterations:\", anchor=tk.W)\n        lbl.pack(pady=5, side=tk.LEFT, anchor=tk.N, expand=True)\n\n        tbox = ttk.Entry(ctl_frame, width=6, textvariable=tk_var, justify=tk.RIGHT)\n        tbox.pack(padx=(0, 5), side=tk.RIGHT)\n\n        ctl = ttk.Scale(\n            ctl_frame,\n            variable=tk_var,\n            command=lambda val, var=tk_var, dt=int, rn=1000, mm=min_max:  # type:ignore\n            set_slider_rounding(val, var, dt, rn, mm))\n        ctl[\"from_\"] = min_max[0]\n        ctl[\"to\"] = min_max[1]\n        ctl.pack(padx=5, pady=5, fill=tk.X, expand=True)\n        for item in (tbox, ctl):\n            Tooltip(item,\n                    text=hlp,\n                    wrap_length=200)\n        logger.debug(\"Added Iterations Slider\")\n\n    def display_item_set(self) -> None:\n        \"\"\" Load the graph(s) if available \"\"\"\n        if Session.is_training and Session.logging_disabled:\n            logger.trace(\"Logs disabled. Hiding graph\")  # type:ignore[attr-defined]\n            self.set_info(\"Graph is disabled as 'no-logs' has been selected\")\n            self.display_item = None\n            self._clear_trace_variables()\n        elif Session.is_training and self.display_item is None:\n            logger.trace(\"Loading graph\")  # type:ignore[attr-defined]\n            self.display_item = Session\n            self._add_trace_variables()\n        elif Session.is_training and self.display_item is not None:\n            logger.trace(\"Graph already displayed. Nothing to do.\")  # type:ignore[attr-defined]\n        else:\n            logger.trace(\"Clearing graph\")  # type:ignore[attr-defined]\n            self.display_item = None\n            self._clear_trace_variables()\n\n    def display_item_process(self) -> None:\n        \"\"\" Add a single graph to the graph window \"\"\"\n        if not Session.is_training:\n            logger.debug(\"Waiting for Session Data to become available to graph\")\n            self.after(1000, self.display_item_process)\n            return\n\n        existing = list(self.subnotebook_get_titles_ids().keys())\n\n        loss_keys = self.display_item.get_loss_keys(Session.session_ids[-1])\n        if not loss_keys:\n            # Reload if we attempt to get loss keys before data is written\n            logger.debug(\"Waiting for Session Data to become available to graph\")\n            self.after(1000, self.display_item_process)\n            return\n\n        loss_keys = [key for key in loss_keys if key != \"total\"]\n        display_tabs = sorted(set(key[:-1].rstrip(\"_\") for key in loss_keys))\n\n        for loss_key in display_tabs:\n            tabname = loss_key.replace(\"_\", \" \").title()\n            if tabname in existing:\n                continue\n            logger.debug(\"Adding graph '%s'\", tabname)\n\n            display_keys = [key for key in loss_keys if key.startswith(loss_key)]\n            data = Calculations(session_id=Session.session_ids[-1],\n                                display=\"loss\",\n                                loss_keys=display_keys,\n                                selections=[\"raw\", \"smoothed\"],\n                                smooth_amount=self.vars[\"smoothgraph\"].get())\n            self.add_child(tabname, data)\n\n    def _smooth_amount_callback(self, *args) -> None:\n        \"\"\" Update each graph's smooth amount on variable change \"\"\"\n        try:\n            smooth_amount = self.vars[\"smoothgraph\"].get()\n        except tk.TclError:\n            # Don't update when there is no value in the variable\n            return\n        logger.debug(\"Updating graph smooth_amount: (new_value: %s, args: %s)\",\n                     smooth_amount, args)\n        for graph in self.subnotebook.children.values():\n            graph.calcs.set_smooth_amount(smooth_amount)\n\n    def _iteration_limit_callback(self, *args) -> None:\n        \"\"\" Limit the amount of data displayed in the live graph on a iteration slider\n        variable change. \"\"\"\n        try:\n            limit = self.vars[\"display_iterations\"].get()\n        except tk.TclError:\n            # Don't update when there is no value in the variable\n            return\n        logger.debug(\"Updating graph iteration limit: (new_value: %s, args: %s)\",\n                     limit, args)\n        for graph in self.subnotebook.children.values():\n            graph.calcs.set_iterations_limit(limit)\n\n    def _display_data_callback(self, line: str, variable: tk.BooleanVar) -> None:\n        \"\"\" Update the displayed graph lines based on option check button selection.\n\n        Parameters\n        ----------\n        line: str\n            The line to hide or display\n        variable: :class:`tkinter.BooleanVar`\n            The tkinter variable containing the ``True`` or ``False`` data for this display item\n        \"\"\"\n        var = variable.get()\n        logger.debug(\"Updating display %s to %s\", line, var)\n        for graph in self.subnotebook.children.values():\n            graph.calcs.update_selections(line, var)\n\n    def add_child(self, name: str, data: Calculations) -> None:\n        \"\"\" Add the graph for the selected keys.\n\n        Parameters\n        ----------\n        name: str\n            The name of the graph to add to the notebook\n        data: :class:`~lib.gui.analysis.stats.Calculations`\n            The object holding the data to be graphed\n        \"\"\"\n        logger.debug(\"Adding child: %s\", name)\n        graph = TrainingGraph(self.subnotebook, data, \"Loss\")\n        graph.build()\n        graph = self.subnotebook_add_page(name, widget=graph)\n        Tooltip(graph, text=self.helptext, wrap_length=200)\n\n    def save_items(self) -> None:\n        \"\"\" Open save dialogue and save graphs \"\"\"\n        graphlocation = FileHandler(\"dir\", None).return_file\n        if not graphlocation:\n            return\n        for graph in self.subnotebook.children.values():\n            graph.save_fig(graphlocation)\n\n    def _add_trace_variables(self) -> None:\n        \"\"\" Add tracing for when the option sliders are updated, for updating the graph. \"\"\"\n        for name, action in zip(T.get_args(T.Literal[\"smoothgraph\", \"display_iterations\"]),\n                                (self._smooth_amount_callback, self._iteration_limit_callback)):\n            var = self.vars[name]\n            if name not in self._trace_vars:\n                self._trace_vars[name] = (var, var.trace(\"w\", action))\n\n    def _clear_trace_variables(self) -> None:\n        \"\"\" Clear all of the trace variables from :attr:`_trace_vars` and reset the dictionary. \"\"\"\n        if self._trace_vars:\n            for name, (var, trace) in self._trace_vars.items():\n                logger.debug(\"Clearing trace from variable: %s\", name)\n                var.trace_vdelete(\"w\", trace)\n            self._trace_vars = {}\n\n    def close(self) -> None:\n        \"\"\" Clear the plots from RAM \"\"\"\n        self._clear_trace_variables()\n        if self.subnotebook is None:\n            logger.debug(\"No graphs to clear. Returning\")\n            return\n\n        for name, graph in self.subnotebook.children.items():\n            logger.debug(\"Clearing: %s\", name)\n            graph.clear()\n        super().close()\n", "lib/gui/display_analysis.py": "#!/usr/bin python3\n\"\"\" Analysis tab of Display Frame of the Faceswap GUI \"\"\"\n\nimport csv\nimport gettext\nimport logging\nimport os\nimport tkinter as tk\nfrom tkinter import ttk\n\nfrom lib.logger import parse_class_init\n\nfrom .custom_widgets import Tooltip\nfrom .display_page import DisplayPage\nfrom .popup_session import SessionPopUp\nfrom .analysis import Session\nfrom .utils import FileHandler, get_config, get_images, LongRunningTask\n\nlogger = logging.getLogger(__name__)\n\n# LOCALES\n_LANG = gettext.translation(\"gui.tooltips\", localedir=\"locales\", fallback=True)\n_ = _LANG.gettext\n\n\nclass Analysis(DisplayPage):  # pylint:disable=too-many-ancestors\n    \"\"\" Session Analysis Tab.\n\n    The area of the GUI that holds the session summary stats for model training sessions.\n\n    Parameters\n    ----------\n    parent: :class:`lib.gui.display.DisplayNotebook`\n        The :class:`ttk.Notebook` that holds this session summary statistics page\n    tab_name: str\n        The name of the tab to be displayed in the notebook\n    helptext: str\n        The help text to display for the summary statistics page\n    \"\"\"\n    def __init__(self, parent, tab_name, helptext):\n        logger.debug(parse_class_init(locals()))\n        super().__init__(parent, tab_name, helptext)\n        self._summary = None\n\n        self._reset_session_info()\n        _Options(self)\n        self._stats = self._get_main_frame()\n\n        self._thread = None  # Thread for compiling stats data in background\n        self._set_callbacks()\n        logger.debug(\"Initialized: %s\", self.__class__.__name__)\n\n    def set_vars(self):\n        \"\"\" Set the analysis specific tkinter variables to :attr:`vars`.\n\n        The tracked variables are the global variables that:\n            * Trigger when a graph refresh has been requested.\n            * Trigger training is commenced or halted\n            * The variable holding the location of the current Tensorboard log folder.\n\n        Returns\n        -------\n        dict\n            The dictionary of variable names to tkinter variables\n        \"\"\"\n        return {\"selected_id\": tk.StringVar(),\n                \"refresh_graph\": get_config().tk_vars.refresh_graph,\n                \"is_training\": get_config().tk_vars.is_training,\n                \"analysis_folder\": get_config().tk_vars.analysis_folder}\n\n    def on_tab_select(self):\n        \"\"\" Callback for when the analysis tab is selected.\n\n        If Faceswap is currently training a model, then update the statistics with the latest\n        values.\n        \"\"\"\n        if not self.vars[\"is_training\"].get():\n            return\n        logger.debug(\"Analysis update callback received\")\n        self._reset_session()\n\n    def _get_main_frame(self):\n        \"\"\" Get the main frame to the sub-notebook to hold stats and session data.\n\n        Returns\n        -------\n        :class:`StatsData`\n            The frame that holds the analysis statistics for the Analysis notebook page\n        \"\"\"\n        logger.debug(\"Getting main stats frame\")\n        mainframe = self.subnotebook_add_page(\"stats\")\n        retval = StatsData(mainframe, self.vars[\"selected_id\"], self.helptext[\"stats\"])\n        logger.debug(\"got main frame: %s\", retval)\n        return retval\n\n    def _set_callbacks(self):\n        \"\"\" Adds callbacks to update the analysis summary statistics and add them to :attr:`vars`\n\n        Training graph refresh - Updates the stats for the current training session when the graph\n        has been updated.\n\n        When the analysis folder has been populated - Updates the stats from that folder.\n        \"\"\"\n        self.vars[\"refresh_graph\"].trace(\"w\", self._update_current_session)\n        self.vars[\"analysis_folder\"].trace(\"w\", self._populate_from_folder)\n\n    def _update_current_session(self, *args):  # pylint:disable=unused-argument\n        \"\"\" Update the currently training session data on a graph update callback. \"\"\"\n        if not self.vars[\"refresh_graph\"].get():\n            return\n        if not self._tab_is_active:\n            logger.debug(\"Analyis tab not selected. Not updating stats\")\n            return\n        logger.debug(\"Analysis update callback received\")\n        self._reset_session()\n\n    def _reset_session_info(self):\n        \"\"\" Reset the session info status to default \"\"\"\n        logger.debug(\"Resetting session info\")\n        self.set_info(\"No session data loaded\")\n\n    def _populate_from_folder(self, *args):  # pylint:disable=unused-argument\n        \"\"\" Populate the Analysis tab from a model folder.\n\n        Triggered when :attr:`vars` ``analysis_folder`` variable is is set.\n        \"\"\"\n        if Session.is_training:\n            return\n\n        folder = self.vars[\"analysis_folder\"].get()\n        if not folder or not os.path.isdir(folder):\n            logger.debug(\"Not a valid folder\")\n            self._clear_session()\n            return\n\n        state_files = [fname\n                       for fname in os.listdir(folder)\n                       if fname.endswith(\"_state.json\")]\n        if not state_files:\n            logger.debug(\"No state files found in folder: '%s'\", folder)\n            self._clear_session()\n            return\n\n        state_file = state_files[0]\n        if len(state_files) > 1:\n            logger.debug(\"Multiple models found. Selecting: '%s'\", state_file)\n\n        if self._thread is None:\n            self._load_session(full_path=os.path.join(folder, state_file))\n\n    @classmethod\n    def _get_model_name(cls, model_dir, state_file):\n        \"\"\" Obtain the model name from a state file's file name.\n\n        Parameters\n        ----------\n        model_dir: str\n            The folder that the model's state file resides in\n        state_file: str\n            The filename of the model's state file\n\n        Returns\n        -------\n        str or ``None``\n            The name of the model extracted from the state file's file name or ``None`` if no\n            log folders were found in the model folder\n        \"\"\"\n        logger.debug(\"Getting model name\")\n        model_name = state_file.replace(\"_state.json\", \"\")\n        logger.debug(\"model_name: %s\", model_name)\n        logs_dir = os.path.join(model_dir, f\"{model_name}_logs\")\n        if not os.path.isdir(logs_dir):\n            logger.warning(\"No logs folder found in folder: '%s'\", logs_dir)\n            return None\n        return model_name\n\n    def _set_session_summary(self, message):\n        \"\"\" Set the summary data and info message.\n\n        Parameters\n        ----------\n        message: str\n            The information message to set\n        \"\"\"\n        if self._thread is None:\n            logger.debug(\"Setting session summary. (message: '%s')\", message)\n            self._thread = LongRunningTask(target=self._summarise_data,\n                                           args=(Session, ),\n                                           widget=self)\n            self._thread.start()\n            self.after(1000, lambda msg=message: self._set_session_summary(msg))\n        elif not self._thread.complete.is_set():\n            logger.debug(\"Data not yet available\")\n            self.after(1000, lambda msg=message: self._set_session_summary(msg))\n        else:\n            logger.debug(\"Retrieving data from thread\")\n            result = self._thread.get_result()\n            if result is None:\n                logger.debug(\"No result from session summary. Clearing analysis view\")\n                self._clear_session()\n                return\n            self._summary = result\n            self._thread = None\n            self.set_info(f\"Session: {message}\")\n            self._stats.tree_insert_data(self._summary)\n\n    @classmethod\n    def _summarise_data(cls, session):\n        \"\"\" Summarize data in a LongRunningThread as it can take a while.\n\n        Parameters\n        ----------\n        session: :class:`lib.gui.analysis.Session`\n            The session object to generate the summary for\n        \"\"\"\n        return session.full_summary\n\n    def _clear_session(self):\n        \"\"\" Clear the currently displayed analysis data from the Tree-View. \"\"\"\n        logger.debug(\"Clearing session\")\n        if not Session.is_loaded:\n            logger.trace(\"No session loaded. Returning\")\n            return\n        self._summary = None\n        self._stats.tree_clear()\n        if not Session.is_training:\n            self._reset_session_info()\n            Session.clear()\n\n    def _load_session(self, full_path=None):\n        \"\"\" Load the session statistics from a model's state file into the Analysis tab of the GUI\n        display window.\n\n        If a model's log files cannot be found within the model folder then the session is cleared.\n\n        Parameters\n        ----------\n        full_path: str, optional\n            The path to the state file to load session information from. If this is ``None`` then\n            a file dialog is popped to enable the user to choose a state file. Default: ``None``\n         \"\"\"\n        logger.debug(\"Loading session\")\n        if full_path is None:\n            full_path = FileHandler(\"filename\", \"state\").return_file\n            if not full_path:\n                return\n        self._clear_session()\n        logger.debug(\"state_file: '%s'\", full_path)\n        model_dir, state_file = os.path.split(full_path)\n        logger.debug(\"model_dir: '%s'\", model_dir)\n        model_name = self._get_model_name(model_dir, state_file)\n        if not model_name:\n            return\n        Session.initialize_session(model_dir, model_name, is_training=False)\n        msg = full_path\n        if len(msg) > 70:\n            msg = f\"...{msg[-70:]}\"\n        self._set_session_summary(msg)\n\n    def _reset_session(self):\n        \"\"\" Reset currently training sessions. Clears the current session and loads in the latest\n        data. \"\"\"\n        logger.debug(\"Reset current training session\")\n        if not Session.is_training:\n            logger.debug(\"Training not running\")\n            return\n        if Session.logging_disabled:\n            logger.trace(\"Logging disabled. Not triggering analysis update\")\n            return\n        self._clear_session()\n        self._set_session_summary(\"Currently running training session\")\n\n    def _save_session(self):\n        \"\"\" Launch a file dialog pop-up to save the current analysis data to a CSV file. \"\"\"\n        logger.debug(\"Saving session\")\n        if not self._summary:\n            logger.debug(\"No summary data loaded. Nothing to save\")\n            print(\"No summary data loaded. Nothing to save\")\n            return\n        savefile = FileHandler(\"save\", \"csv\").return_file\n        if not savefile:\n            logger.debug(\"No save file. Returning\")\n            return\n\n        logger.debug(\"Saving to: '%s'\", savefile)\n        fieldnames = sorted(key for key in self._summary[0].keys())\n        with savefile as outfile:\n            csvout = csv.DictWriter(outfile, fieldnames)\n            csvout.writeheader()\n            for row in self._summary:\n                csvout.writerow(row)\n\n\nclass _Options():  # pylint:disable=too-few-public-methods\n    \"\"\" Options buttons for the Analysis tab.\n\n    Parameters\n    ----------\n    parent: :class:`Analysis`\n        The Analysis Display Tab that holds the options buttons\n    \"\"\"\n    def __init__(self, parent):\n        logger.debug(parse_class_init(locals()))\n        self._parent = parent\n        self._buttons = self._add_buttons()\n        self._add_training_callback()\n        logger.debug(\"Initialized: %s\", self.__class__.__name__)\n\n    def _add_buttons(self):\n        \"\"\" Add the option buttons.\n\n        Returns\n        -------\n        dict\n            The button names to button objects\n        \"\"\"\n        buttons = {}\n        for btntype in (\"clear\", \"save\", \"load\"):\n            logger.debug(\"Adding button: '%s'\", btntype)\n            cmd = getattr(self._parent, f\"_{btntype}_session\")\n            btn = ttk.Button(self._parent.optsframe,\n                             image=get_images().icons[btntype],\n                             command=cmd)\n            btn.pack(padx=2, side=tk.RIGHT)\n            hlp = self._set_help(btntype)\n            Tooltip(btn, text=hlp, wrap_length=200)\n            buttons[btntype] = btn\n        logger.debug(\"buttons: %s\", buttons)\n        return buttons\n\n    @classmethod\n    def _set_help(cls, button_type):\n        \"\"\" Set the help text for option buttons.\n\n        Parameters\n        ----------\n        button_type: {\"reload\", \"clear\", \"save\", \"load\"}\n            The type of button to set the help text for\n        \"\"\"\n        logger.debug(\"Setting help\")\n        hlp = \"\"\n        if button_type == \"reload\":\n            hlp = _(\"Load/Refresh stats for the currently training session\")\n        elif button_type == \"clear\":\n            hlp = _(\"Clear currently displayed session stats\")\n        elif button_type == \"save\":\n            hlp = _(\"Save session stats to csv\")\n        elif button_type == \"load\":\n            hlp = _(\"Load saved session stats\")\n        return hlp\n\n    def _add_training_callback(self):\n        \"\"\" Add a callback to the training tkinter variable to disable save and clear buttons\n        when a model is training. \"\"\"\n        var = self._parent.vars[\"is_training\"]\n        var.trace(\"w\", self._set_buttons_state)\n\n    def _set_buttons_state(self, *args):  # pylint:disable=unused-argument\n        \"\"\" Callback to enable/disable button when training is commenced and stopped. \"\"\"\n        is_training = self._parent.vars[\"is_training\"].get()\n        state = \"disabled\" if is_training else \"!disabled\"\n        for name, button in self._buttons.items():\n            if name not in (\"load\", \"clear\"):\n                continue\n            logger.debug(\"Setting %s button state to %s\", name, state)\n            button.state([state])\n\n\nclass StatsData(ttk.Frame):  # pylint:disable=too-many-ancestors\n    \"\"\" Stats frame of analysis tab.\n\n    Holds the tree-view containing the summarized session statistics in the Analysis tab.\n\n    Parameters\n    ----------\n    parent: :class:`tkinter.Frame`\n        The frame within the Analysis Notebook that will hold the statistics\n    selected_id: :class:`tkinter.IntVar`\n        The tkinter variable that holds the currently selected session ID\n    helptext: str\n        The help text to display for the summary statistics page\n    \"\"\"\n    def __init__(self, parent, selected_id, helptext):\n        logger.debug(parse_class_init(locals()))\n        super().__init__(parent)\n        self._selected_id = selected_id\n\n        self._canvas = tk.Canvas(self, bd=0, highlightthickness=0)\n        tree_frame = ttk.Frame(self._canvas)\n        self._tree_canvas = self._canvas.create_window((0, 0), window=tree_frame, anchor=tk.NW)\n        self._sub_frame = ttk.Frame(tree_frame)\n\n        self._add_label()\n\n        self._tree = ttk.Treeview(self._sub_frame, height=1, selectmode=tk.BROWSE)\n        self._scrollbar = ttk.Scrollbar(tree_frame, orient=\"vertical\", command=self._tree.yview)\n\n        self._columns = self._tree_configure(helptext)\n        self._canvas.bind(\"<Configure>\", self._resize_frame)\n\n        self._scrollbar.pack(side=tk.RIGHT, fill=tk.Y)\n        self._tree.pack(side=tk.TOP, fill=tk.X)\n        self._sub_frame.pack(side=tk.LEFT, fill=tk.X, anchor=tk.N, expand=True)\n        self._canvas.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)\n        self.pack(side=tk.TOP, padx=5, pady=5, fill=tk.BOTH, expand=True)\n\n        logger.debug(\"Initialized: %s\", self.__class__.__name__)\n\n    def _add_label(self):\n        \"\"\" Add the title above the tree-view. \"\"\"\n        logger.debug(\"Adding Treeview title\")\n        lbl = ttk.Label(self._sub_frame, text=\"Session Stats\", anchor=tk.CENTER)\n        lbl.pack(side=tk.TOP, fill=tk.X, padx=5, pady=5)\n\n    def _resize_frame(self, event):\n        \"\"\" Resize the options frame to fit the canvas.\n\n        Parameters\n        ----------\n        event: `tkinter.Event`\n            The tkinter resize event\n        \"\"\"\n        logger.debug(\"Resize Analysis Frame\")\n        canvas_width = event.width\n        canvas_height = event.height\n        self._canvas.itemconfig(self._tree_canvas, width=canvas_width, height=canvas_height)\n        logger.debug(\"Resized Analysis Frame\")\n\n    def _tree_configure(self, helptext):\n        \"\"\" Build a tree-view widget to hold the sessions stats.\n\n        Parameters\n        ----------\n        helptext: str\n            The helptext to display when the mouse is over the tree-view\n\n        Returns\n        -------\n        list\n            The list of tree-view columns\n        \"\"\"\n        logger.debug(\"Configuring Treeview\")\n        self._tree.configure(yscrollcommand=self._scrollbar.set)\n        self._tree.tag_configure(\"total\", background=\"black\", foreground=\"white\")\n        self._tree.bind(\"<ButtonRelease-1>\", self._select_item)\n        Tooltip(self._tree, text=helptext, wrap_length=200)\n        return self._tree_columns()\n\n    def _tree_columns(self):\n        \"\"\" Add the columns to the totals tree-view.\n\n        Returns\n        -------\n        list\n            The list of tree-view columns\n        \"\"\"\n        logger.debug(\"Adding Treeview columns\")\n        columns = ((\"session\", 40, \"#\"),\n                   (\"start\", 130, None),\n                   (\"end\", 130, None),\n                   (\"elapsed\", 90, None),\n                   (\"batch\", 50, None),\n                   (\"iterations\", 90, None),\n                   (\"rate\", 60, \"EGs/sec\"))\n        self._tree[\"columns\"] = [column[0] for column in columns]\n\n        for column in columns:\n            text = column[2] if column[2] else column[0].title()\n            logger.debug(\"Adding heading: '%s'\", text)\n            self._tree.heading(column[0], text=text)\n            self._tree.column(column[0], width=column[1], anchor=tk.E, minwidth=40)\n        self._tree.column(\"#0\", width=40)\n        self._tree.heading(\"#0\", text=\"Graphs\")\n\n        return [column[0] for column in columns]\n\n    def tree_insert_data(self, sessions_summary):\n        \"\"\" Insert the summary data into the statistics tree-view.\n\n        Parameters\n        ----------\n        sessions_summary: list\n            List of session summary dicts for populating into the tree-view\n        \"\"\"\n        logger.debug(\"Inserting treeview data\")\n        self._tree.configure(height=len(sessions_summary))\n\n        for item in sessions_summary:\n            values = [item[column] for column in self._columns]\n            kwargs = {\"values\": values}\n            if self._check_valid_data(values):\n                # Don't show graph icon for non-existent sessions\n                kwargs[\"image\"] = get_images().icons[\"graph\"]\n            if values[0] == \"Total\":\n                kwargs[\"tags\"] = \"total\"\n            self._tree.insert(\"\", \"end\", **kwargs)\n\n    def tree_clear(self):\n        \"\"\" Clear all of the summary data from the tree-view. \"\"\"\n        logger.debug(\"Clearing treeview data\")\n        try:\n            self._tree.delete(* self._tree.get_children())\n            self._tree.configure(height=1)\n        except tk.TclError:\n            # Catch non-existent tree view when rebuilding the GUI\n            pass\n\n    def _select_item(self, event):\n        \"\"\" Update the session summary info with the selected item or launch graph.\n\n        If the mouse is clicked on the graph icon, then the session summary pop-up graph is\n        launched. Otherwise the selected ID is stored.\n\n        Parameters\n        ----------\n        event: :class:`tkinter.Event`\n            The tkinter mouse button release event\n        \"\"\"\n        region = self._tree.identify(\"region\", event.x, event.y)\n        selection = self._tree.focus()\n        values = self._tree.item(selection, \"values\")\n        if values:\n            logger.debug(\"Selected values: %s\", values)\n            self._selected_id.set(values[0])\n            if region == \"tree\" and self._check_valid_data(values):\n                data_points = int(values[self._columns.index(\"iterations\")])\n                self._data_popup(data_points)\n\n    def _check_valid_data(self, values):\n        \"\"\" Check there is valid data available for popping up a graph.\n\n        Parameters\n        ----------\n        values: list\n            The values that exist for a single session that are to be validated\n        \"\"\"\n        col_indices = [self._columns.index(\"batch\"), self._columns.index(\"iterations\")]\n        for idx in col_indices:\n            if (isinstance(values[idx], int) or values[idx].isdigit()) and int(values[idx]) == 0:\n                logger.warning(\"No data to graph for selected session\")\n                return False\n        return True\n\n    def _data_popup(self, data_points):\n        \"\"\" Pop up a window and control it's position\n\n        The default view is rolling average over 500 points. If there are fewer data points than\n        this, switch the default to smoothed,\n\n        Parameters\n        ----------\n        data_points: int\n            The number of iterations that are to be plotted\n        \"\"\"\n        logger.debug(\"Popping up data window\")\n        scaling_factor = get_config().scaling_factor\n        toplevel = SessionPopUp(self._selected_id.get(),\n                                data_points)\n        toplevel.title(self._data_popup_title())\n        toplevel.tk.call(\n            'wm',\n            'iconphoto',\n            toplevel._w, get_images().icons[\"favicon\"])  # pylint:disable=protected-access\n\n        root = get_config().root\n        offset = (root.winfo_x() + 20, root.winfo_y() + 20)\n        height = int(900 * scaling_factor)\n        width = int(480 * scaling_factor)\n        toplevel.geometry(f\"{height}x{width}+{offset[0]}+{offset[1]}\")\n\n        toplevel.update()\n\n    def _data_popup_title(self):\n        \"\"\" Get the summary graph popup title.\n\n        Returns\n        -------\n        str\n            The title to display at the top of the pop-up graph window\n        \"\"\"\n        logger.debug(\"Setting poup title\")\n        selected_id = self._selected_id.get()\n        model_dir, model_name = os.path.split(Session.model_filename)\n        title = \"All Sessions\"\n        if selected_id != \"Total\":\n            title = f\"{model_name.title()} Model: Session #{selected_id}\"\n        logger.debug(\"Title: '%s'\", title)\n        return f\"{title} - {model_dir}\"\n", "lib/gui/control_helper.py": "#!/usr/bin/env python3\n\"\"\" Helper functions and classes for GUI controls \"\"\"\nimport gettext\nimport logging\nimport re\n\nimport tkinter as tk\nimport typing as T\nfrom tkinter import colorchooser, ttk\nfrom itertools import zip_longest\nfrom functools import partial\n\nfrom _tkinter import Tcl_Obj, TclError\n\nfrom .custom_widgets import ContextMenu, MultiOption, ToggledFrame, Tooltip\nfrom .utils import FileHandler, get_config, get_images\n\nlogger = logging.getLogger(__name__)\n\n# LOCALES\n_LANG = gettext.translation(\"gui.tooltips\", localedir=\"locales\", fallback=True)\n_ = _LANG.gettext\n\n# We store Tooltips, ContextMenus and Commands globally when they are created\n# Because we need to add them back to newly cloned widgets (they are not easily accessible from\n# original config or are prone to getting destroyed when the original widget is destroyed)\n_RECREATE_OBJECTS: dict[str, dict[str, T.Any]] = {\"tooltips\": {},\n                                                  \"commands\": {},\n                                                  \"contextmenus\": {}}\n\n\ndef _get_tooltip(widget, text=None, text_variable=None):\n    \"\"\" Store the tooltip layout and widget id in _TOOLTIPS and return a tooltip.\n\n    Auto adjust tooltip width based on amount of text.\n\n    \"\"\"\n    _RECREATE_OBJECTS[\"tooltips\"][str(widget)] = {\"text\": text,\n                                                  \"text_variable\": text_variable}\n    logger.debug(\"Adding to tooltips dict: (widget: %s. text: '%s')\", widget, text)\n\n    wrap_length = 400\n    if text is not None:\n        while True:\n            if len(text) < wrap_length * 5:\n                break\n            if wrap_length > 800:\n                break\n            wrap_length = int(wrap_length * 1.10)\n\n    return Tooltip(widget, text=text, text_variable=text_variable, wrap_length=wrap_length)\n\n\ndef _get_contextmenu(widget):\n    \"\"\" Create a context menu, store its mapping and return \"\"\"\n    rc_menu = ContextMenu(widget)\n    _RECREATE_OBJECTS[\"contextmenus\"][str(widget)] = rc_menu\n    logger.debug(\"Adding to Context menu: (widget: %s. rc_menu: %s)\",\n                 widget, rc_menu)\n    return rc_menu\n\n\ndef _add_command(name, func):\n    \"\"\" For controls that execute commands, the command must be added to the _COMMAND list so that\n        it can be added back to the widget during cloning \"\"\"\n    logger.debug(\"Adding to commands: %s - %s\", name, func)\n    _RECREATE_OBJECTS[\"commands\"][str(name)] = func\n\n\ndef set_slider_rounding(value, var, d_type, round_to, min_max):\n    \"\"\" Set the value of sliders underlying variable based on their datatype,\n    rounding value and min/max.\n\n    Parameters\n    ----------\n    var: tkinter.Var\n        The variable to set the value for\n    d_type: [:class:`int`, :class:`float`]\n        The type of value that is stored in :attr:`var`\n    round_to: int or list\n        If :attr:`d_type` is :class:`float` then this is the decimal place rounding for\n        :attr:`var`. If :attr:`d_type` is :class:`int` then this is the number of steps between\n        each increment for :attr:`var`. If a list is provided, then this must be a list of\n        discreet values that are of the correct :attr:`d_type`.\n    min_max: tuple (`int`, `int`)\n        The (``min``, ``max``) values that this slider accepts\n    \"\"\"\n    if isinstance(round_to, list):\n        # Lock to nearest item\n        var.set(min(round_to, key=lambda x: abs(x-float(value))))\n    elif d_type == float:\n        var.set(round(float(value), round_to))\n    else:\n        steps = range(min_max[0], min_max[1] + round_to, round_to)\n        value = min(steps, key=lambda x: abs(x - int(float(value))))\n        var.set(value)\n\n\nclass ControlPanelOption():\n    \"\"\"\n    A class to hold a control panel option. A list of these is expected\n    to be passed to the ControlPanel object.\n\n    Parameters\n    ----------\n    title: str\n        Title of the control. Will be used for label text and control naming\n    dtype: datatype object\n        Datatype of the control.\n    group: str, optional\n        The group that this control should sit with. If provided, all controls in the same\n        group will be placed together. Default: None\n    subgroup: str, optional\n        The subgroup that this option belongs to. If provided, will group options in the same\n        subgroups together for the same layout as option/check boxes. Default: ``None``\n    default: str, optional\n        Default value for the control. If None is provided, then action will be dictated by\n        whether \"blank_nones\" is set in ControlPanel\n    initial_value: str, optional\n        Initial value for the control. If None, default will be used\n    choices: list or tuple, object\n        Used for combo boxes and radio control option setting. Set to `\"colorchooser\"` for a color\n        selection dialog.\n    is_radio: bool, optional\n        Specifies to use a Radio control instead of combobox if choices are passed\n    is_multi_option:\n        Specifies to use a Multi Check Button option group for the specified control\n    rounding: int or float, optional\n        For slider controls. Sets the stepping\n    min_max: int or float, optional\n        For slider controls. Sets the min and max values\n    sysbrowser: dict, optional\n        Adds Filesystem browser buttons to ttk.Entry options.\n        Expects a dict: {sysbrowser: str, filetypes: str}\n    helptext: str, optional\n        Sets the tooltip text\n    track_modified: bool, optional\n        Set whether to set a callback trace indicating that the parameter has been modified.\n        Default: False\n    command: str, optional\n        Required if tracking modified. The command that this option belongs to. Default: None\n    \"\"\"\n\n    def __init__(self, title, dtype,  # pylint:disable=too-many-arguments\n                 group=None, subgroup=None, default=None, initial_value=None, choices=None,\n                 is_radio=False, is_multi_option=False, rounding=None, min_max=None,\n                 sysbrowser=None, helptext=None, track_modified=False, command=None):\n        logger.debug(\"Initializing %s: (title: '%s', dtype: %s, group: %s, subgroup: %s, \"\n                     \"default: %s, initial_value: %s, choices: %s, is_radio: %s, \"\n                     \"is_multi_option: %s, rounding: %s, min_max: %s, sysbrowser: %s, \"\n                     \"helptext: '%s', track_modified: %s, command: '%s')\", self.__class__.__name__,\n                     title, dtype, group, subgroup, default, initial_value, choices, is_radio,\n                     is_multi_option, rounding, min_max, sysbrowser, helptext, track_modified,\n                     command)\n\n        self.dtype = dtype\n        self.sysbrowser = sysbrowser\n        self._command = command\n        self._options = {\"title\": title,\n                         \"subgroup\": subgroup,\n                         \"group\": group,\n                         \"default\": default,\n                         \"initial_value\": initial_value,\n                         \"choices\": choices,\n                         \"is_radio\": is_radio,\n                         \"is_multi_option\": is_multi_option,\n                         \"rounding\": rounding,\n                         \"min_max\": min_max,\n                         \"helptext\": helptext}\n        self.control = self.get_control()\n        self.tk_var = self.get_tk_var(initial_value, track_modified)\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    @property\n    def name(self):\n        \"\"\" Lowered title for naming \"\"\"\n        return self._options[\"title\"].lower()\n\n    @property\n    def title(self):\n        \"\"\" Title case title for naming with underscores removed \"\"\"\n        return self._options[\"title\"].replace(\"_\", \" \").title()\n\n    @property\n    def group(self):\n        \"\"\" Return group or _master if no group set \"\"\"\n        group = self._options[\"group\"]\n        group = \"_master\" if group is None else group\n        return group\n\n    @property\n    def subgroup(self):\n        \"\"\" str: The subgroup for the option, or ``None`` if none provided. \"\"\"\n        return self._options[\"subgroup\"]\n\n    @property\n    def default(self):\n        \"\"\" Return either selected value or default \"\"\"\n        return self._options[\"default\"]\n\n    @property\n    def value(self):\n        \"\"\" Return either initial value or default \"\"\"\n        val = self._options[\"initial_value\"]\n        val = self.default if val is None else val\n        return val\n\n    @property\n    def choices(self):\n        \"\"\" Return choices \"\"\"\n        return self._options[\"choices\"]\n\n    @property\n    def is_radio(self):\n        \"\"\" Return is_radio \"\"\"\n        return self._options[\"is_radio\"]\n\n    @property\n    def is_multi_option(self):\n        \"\"\" bool: ``True`` if the control should be contained in a multi check button group,\n        otherwise ``False``. \"\"\"\n        return self._options[\"is_multi_option\"]\n\n    @property\n    def rounding(self):\n        \"\"\" Return rounding \"\"\"\n        return self._options[\"rounding\"]\n\n    @property\n    def min_max(self):\n        \"\"\" Return min_max \"\"\"\n        return self._options[\"min_max\"]\n\n    @property\n    def helptext(self):\n        \"\"\" Format and return help text for tooltips \"\"\"\n        helptext = self._options[\"helptext\"]\n        if helptext is None:\n            return helptext\n        logger.debug(\"Format control help: '%s'\", self.name)\n        if helptext.startswith(\"R|\"):\n            helptext = helptext[2:].replace(\"\\nL|\", \"\\n - \").replace(\"\\n\", \"\\n\\n\")\n        else:\n            helptext = helptext.replace(\"\\n\\t\", \"\\n - \").replace(\"%%\", \"%\")\n        helptext = self.title + \" - \" + helptext\n        logger.debug(\"Formatted control help: (name: '%s', help: '%s'\", self.name, helptext)\n        return helptext\n\n    def get(self):\n        \"\"\" Return the value from the tk_var\n\n        Notes\n        -----\n        tk variables don't like empty values if it's not a stringVar. This seems to be pretty\n        much the only reason that a get() call would fail, so replace any numerical variable\n        with it's numerical zero equivalent on a TCL Error. Only impacts variables linked\n        to Entry widgets.\n        \"\"\"\n        try:\n            val = self.tk_var.get()\n        except TclError:\n            if isinstance(self.tk_var, tk.IntVar):\n                val = 0\n            elif isinstance(self.tk_var, tk.DoubleVar):\n                val = 0.0\n            else:\n                raise\n        return val\n\n    def set(self, value):\n        \"\"\" Set the tk_var to a new value \"\"\"\n        self.tk_var.set(value)\n\n    def set_initial_value(self, value):\n        \"\"\" Set the initial_value to the given value\n\n        Parameters\n        ----------\n        value: varies\n            The value to set the initial value attribute to\n        \"\"\"\n        logger.debug(\"Setting inital value for %s to %s\", self.name, value)\n        self._options[\"initial_value\"] = value\n\n    def get_control(self):\n        \"\"\" Set the correct control type based on the datatype or for this option \"\"\"\n        if self.choices and self.is_radio:\n            control = \"radio\"\n        elif self.choices and self.is_multi_option:\n            control = \"multi\"\n        elif self.choices and self.choices == \"colorchooser\":\n            control = \"colorchooser\"\n        elif self.choices:\n            control = ttk.Combobox\n        elif self.dtype == bool:\n            control = ttk.Checkbutton\n        elif self.dtype in (int, float):\n            control = \"scale\"\n        else:\n            control = tk.Entry\n        logger.debug(\"Setting control '%s' to %s\", self.title, control)\n        return control\n\n    def get_tk_var(self, initial_value, track_modified):\n        \"\"\" Correct variable type for control \"\"\"\n        if self.dtype == bool:\n            var = tk.BooleanVar()\n        elif self.dtype == int:\n            var = tk.IntVar()\n        elif self.dtype == float:\n            var = tk.DoubleVar()\n        else:\n            var = tk.StringVar()\n        if initial_value is not None:\n            var.set(initial_value)\n        logger.debug(\"Setting tk variable: (name: '%s', dtype: %s, tk_var: %s, initial_value: %s)\",\n                     self.name, self.dtype, var, initial_value)\n        if track_modified and self._command is not None:\n            logger.debug(\"Tracking variable modification: %s\", self.name)\n            var.trace(\"w\",\n                      lambda name, index, mode, cmd=self._command: self._modified_callback(cmd))\n\n        if track_modified and self._command == \"train\" and self.title == \"Model Dir\":\n            var.trace(\"w\", lambda name, index, mode, v=var: self._model_callback(v))\n\n        return var\n\n    @staticmethod\n    def _modified_callback(command):\n        \"\"\" Set the modified variable for this tab to TRUE\n\n        On initial setup the notebook won't yet exist, and we don't want to track the changes\n        for initial variables anyway, so make sure notebook exists prior to performing the callback\n        \"\"\"\n        config = get_config()\n        if config.command_notebook is None:\n            return\n        config.set_modified_true(command)\n\n    @staticmethod\n    def _model_callback(var):\n        \"\"\" Set a callback to load model stats for existing models when a model\n        folder is selected \"\"\"\n        config = get_config()\n        if not config.user_config_dict[\"auto_load_model_stats\"]:\n            logger.debug(\"Session updating disabled by user config\")\n            return\n        if config.tk_vars.running_task.get():\n            logger.debug(\"Task running. Not updating session\")\n            return\n        folder = var.get()\n        logger.debug(\"Setting analysis model folder callback: '%s'\", folder)\n        get_config().tk_vars.analysis_folder.set(folder)\n\n\nclass ControlPanel(ttk.Frame):  # pylint:disable=too-many-ancestors\n    \"\"\"\n    A Control Panel to hold control panel options.\n    This class handles all of the formatting, placing and TK_Variables\n    in a consistent manner.\n\n    It can also provide dynamic columns for resizing widgets\n\n    Parameters\n    ----------\n    parent: tkinter object\n        Parent widget that should hold this control panel\n    options: list of  ControlPanelOptions objects\n        The list of controls that are to be built into this control panel\n    label_width: int, optional\n        The width that labels for controls should be set to.\n        Defaults to 20\n    columns: int, optional\n        The initial number of columns to set the layout for. Default: 1\n    max_columns: int, optional\n        The maximum number of columns that this control panel should be able\n        to accommodate. Setting to 1 means that there will only be 1 column\n        regardless of how wide the control panel is. Higher numbers will\n        dynamically fill extra columns if space permits. Defaults to 4\n    option_columns: int, optional\n        For check-button and radio-button containers, how many options should\n        be displayed on each row. Defaults to 4\n    header_text: str, optional\n        If provided, will place an information box at the top of the control\n        panel with these contents.\n    style: str, optional\n        The name of the style to use for the control panel. Styles are configured when TkInter\n        initializes. The style name is the common prefix prior to the widget name. Default:\n        ``None`` (use the OS style)\n    blank_nones: bool, optional\n        How the control panel should handle None values. If set to True then None values will be\n        converted to empty strings. Default: False\n    scrollbar: bool, optional\n        ``True`` if a scrollbar should be added to the control panel, otherwise ``False``.\n        Default: ``True``\n    \"\"\"\n\n    def __init__(self, parent, options,  # pylint:disable=too-many-arguments\n                 label_width=20, columns=1, max_columns=4, option_columns=4, header_text=None,\n                 style=None, blank_nones=True, scrollbar=True):\n        logger.debug(\"Initializing %s: (parent: '%s', options: %s, label_width: %s, columns: %s, \"\n                     \"max_columns: %s, option_columns: %s, header_text: %s, style: %s, \"\n                     \"blank_nones: %s, scrollbar: %s)\",\n                     self.__class__.__name__, parent, options, label_width, columns, max_columns,\n                     option_columns, header_text, style, blank_nones, scrollbar)\n        self._style = \"\" if style is None else f\"{style}.\"\n        super().__init__(parent, style=f\"{self._style}.Group.TFrame\")\n\n        self.pack(side=tk.TOP, fill=tk.BOTH, expand=True)\n\n        self.options = options\n        self.controls = []\n        self.label_width = label_width\n        self.columns = columns\n        self.max_columns = max_columns\n        self.option_columns = option_columns\n\n        self.header_text = header_text\n        self._theme = get_config().user_theme[\"group_panel\"]\n        if self._style.startswith(\"SPanel\"):\n            self._theme = {**self._theme, **get_config().user_theme[\"group_settings\"]}\n\n        self.group_frames = {}\n        self._sub_group_frames = {}\n\n        canvas_kwargs = {\"bd\": 0, \"highlightthickness\": 0, \"bg\": self._theme[\"panel_background\"]}\n\n        self._canvas = tk.Canvas(self, **canvas_kwargs)\n        self._canvas.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)\n\n        self.mainframe, self.optsframe = self.get_opts_frame()\n        self._optscanvas = self._canvas.create_window((0, 0), window=self.mainframe, anchor=tk.NW)\n        self.build_panel(blank_nones, scrollbar)\n\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    @staticmethod\n    def _adjust_wraplength(event):\n        \"\"\" dynamically adjust the wrap length of a label on event \"\"\"\n        label = event.widget\n        label.configure(wraplength=event.width - 1)\n\n    def get_opts_frame(self):\n        \"\"\" Return an auto-fill container for the options inside a main frame \"\"\"\n        style = f\"{self._style}Holder.\"\n        mainframe = ttk.Frame(self._canvas, style=f\"{style}TFrame\")\n        if self.header_text is not None:\n            self.add_info(mainframe)\n        optsframe = ttk.Frame(mainframe, name=\"opts_frame\", style=f\"{style}TFrame\")\n        optsframe.pack(expand=True, fill=tk.BOTH)\n        holder = AutoFillContainer(optsframe, self.columns, self.max_columns, style=style)\n        logger.debug(\"Opts frames: '%s'\", holder)\n        return mainframe, holder\n\n    def add_info(self, frame):\n        \"\"\" Plugin information \"\"\"\n        info_frame = ttk.Frame(frame, style=f\"{self._style}InfoHeader.TFrame\")\n        info_frame.pack(fill=tk.X, side=tk.TOP, expand=True, padx=10, pady=(10, 0))\n        label_frame = ttk.Frame(info_frame, style=f\"{self._style}InfoHeader.TFrame\")\n        label_frame.pack(padx=5, pady=5, fill=tk.X, expand=True)\n        for idx, line in enumerate(self.header_text.splitlines()):\n            if not line:\n                continue\n            style = f\"{self._style}InfoHeader\" if idx == 0 else f\"{self._style}InfoBody\"\n            info = ttk.Label(label_frame, text=line, style=f\"{style}.TLabel\", anchor=tk.W)\n            info.bind(\"<Configure>\", self._adjust_wraplength)\n            info.pack(fill=tk.X, padx=0, pady=0, expand=True, side=tk.TOP)\n\n    def build_panel(self, blank_nones, scrollbar):\n        \"\"\" Build the options frame for this command \"\"\"\n        logger.debug(\"Add Config Frame\")\n        if scrollbar:\n            self.add_scrollbar()\n        self._canvas.bind(\"<Configure>\", self.resize_frame)\n\n        for option in self.options:\n            group_frame = self.get_group_frame(option.group)\n            sub_group_frame = self._get_subgroup_frame(group_frame[\"frame\"], option.subgroup)\n            frame = group_frame[\"frame\"] if sub_group_frame is None else sub_group_frame.subframe\n\n            ctl = ControlBuilder(frame,\n                                 option,\n                                 label_width=self.label_width,\n                                 checkbuttons_frame=group_frame[\"chkbtns\"],\n                                 option_columns=self.option_columns,\n                                 style=self._style,\n                                 blank_nones=blank_nones)\n            if group_frame[\"chkbtns\"].items > 0:\n                group_frame[\"chkbtns\"].parent.pack(side=tk.BOTTOM, fill=tk.X, anchor=tk.NW)\n\n            self.controls.append(ctl)\n        for control in self.controls:\n            filebrowser = control.filebrowser\n            if filebrowser is not None:\n                filebrowser.set_context_action_option(self.options)\n        logger.debug(\"Added Config Frame\")\n\n    def get_group_frame(self, group):\n        \"\"\" Return a group frame.\n\n        If a group frame has already been created for the given group, then it will be returned,\n        otherwise it will be created and returned.\n\n        Parameters\n        ----------\n        group: str\n            The name of the group to obtain the group frame for\n\n        Returns\n        -------\n        :class:`ttk.Frame` or :class:`ToggledFrame`\n            If this is a 'master' group frame then returns a standard frame. If this is any\n            other group, then will return the ToggledFrame for that group\n        \"\"\"\n        group = group.lower()\n\n        if self.group_frames.get(group, None) is None:\n            logger.debug(\"Creating new group frame for: %s\", group)\n            is_master = group == \"_master\"\n            opts_frame = self.optsframe.subframe\n            if is_master:\n                group_frame = ttk.Frame(opts_frame, style=f\"{self._style}.Group.TFrame\")\n                retval = group_frame\n            else:\n                group_frame = ToggledFrame(opts_frame, text=group.title(), theme=self._style)\n                retval = group_frame.sub_frame\n\n            group_frame.pack(side=tk.TOP, fill=tk.X, padx=5, pady=5, anchor=tk.NW)\n\n            self.group_frames[group] = {\"frame\": retval,\n                                        \"chkbtns\": self.checkbuttons_frame(retval)}\n        group_frame = self.group_frames[group]\n        return group_frame\n\n    def add_scrollbar(self):\n        \"\"\" Add a scrollbar to the options frame \"\"\"\n        logger.debug(\"Add Config Scrollbar\")\n        scrollbar = ttk.Scrollbar(self,\n                                  command=self._canvas.yview,\n                                  style=f\"{self._style}Vertical.TScrollbar\")\n        scrollbar.pack(side=tk.RIGHT, fill=tk.Y)\n        self._canvas.config(yscrollcommand=scrollbar.set)\n        self.mainframe.bind(\"<Configure>\", self.update_scrollbar)\n        logger.debug(\"Added Config Scrollbar\")\n\n    def update_scrollbar(self, event):  # pylint:disable=unused-argument\n        \"\"\" Update the options frame scrollbar \"\"\"\n        self._canvas.configure(scrollregion=self._canvas.bbox(\"all\"))\n\n    def resize_frame(self, event):\n        \"\"\" Resize the options frame to fit the canvas \"\"\"\n        logger.debug(\"Resize Config Frame\")\n        canvas_width = event.width\n        self._canvas.itemconfig(self._optscanvas, width=canvas_width)\n        self.optsframe.rearrange_columns(canvas_width)\n        logger.debug(\"Resized Config Frame\")\n\n    def checkbuttons_frame(self, frame):\n        \"\"\" Build and format frame for holding the check buttons\n            if is_master then check buttons will be placed in a LabelFrame\n            otherwise in a standard frame \"\"\"\n        logger.debug(\"Add Options CheckButtons Frame\")\n        chk_frame = ttk.Frame(frame, name=\"chkbuttons\", style=f\"{self._style}Group.TFrame\")\n        holder = AutoFillContainer(chk_frame,\n                                   self.option_columns,\n                                   self.option_columns,\n                                   style=f\"{self._style}Group.\")\n        logger.debug(\"Added Options CheckButtons Frame\")\n        return holder\n\n    def _get_subgroup_frame(self, parent, subgroup):\n        if subgroup is None:\n            return subgroup\n        if subgroup not in self._sub_group_frames:\n            sub_frame = ttk.Frame(parent, style=f\"{self._style}Group.TFrame\")\n            self._sub_group_frames[subgroup] = AutoFillContainer(sub_frame,\n                                                                 self.option_columns,\n                                                                 self.option_columns,\n                                                                 style=f\"{self._style}Group.\")\n            sub_frame.pack(anchor=tk.W, expand=True, fill=tk.X)\n            logger.debug(\"Added Subgroup Frame: %s\", subgroup)\n        return self._sub_group_frames[subgroup]\n\n\nclass AutoFillContainer():\n    \"\"\" A container object that auto-fills columns.\n\n    Parameters\n    ----------\n    parent: :class:`ttk.Frame`\n        The parent widget that holds this container\n    initial_columns: int\n        The initial number of columns that this container should display\n    max_columns: int\n        The maximum number of column that this container is permitted to display\n    style: str, optional\n        The name of the style to use for the control panel. Styles are configured when TkInter\n        initializes. The style name is the common prefix prior to the widget name. Default:\n        empty string (use the OS style)\n    \"\"\"\n    def __init__(self, parent, initial_columns, max_columns, style=\"\"):\n        logger.debug(\"Initializing: %s: (parent: %s, initial_columns: %s, max_columns: %s)\",\n                     self.__class__.__name__, parent, initial_columns, max_columns)\n        self.max_columns = max_columns\n        self.columns = initial_columns\n        self.parent = parent\n        self._style = style\n#        self.columns = min(columns, self.max_columns)\n        self.single_column_width = self.scale_column_width(288, 9)\n        self.max_width = self.max_columns * self.single_column_width\n        self._items = 0\n        self._idx = 0\n        self._widget_config = []  # Master list of all children in order\n        self.subframes = self.set_subframes()\n        logger.debug(\"Initialized: %s\", self.__class__.__name__)\n\n    @staticmethod\n    def scale_column_width(original_size, original_fontsize):\n        \"\"\" Scale the column width based on selected font size \"\"\"\n        font_size = get_config().user_config_dict[\"font_size\"]\n        if font_size == original_fontsize:\n            return original_size\n        scale = 1 + (((font_size / original_fontsize) - 1) / 2)\n        retval = round(original_size * scale)\n        logger.debug(\"scaled column width: (old_width: %s, scale: %s, new_width:%s)\",\n                     original_size, scale, retval)\n        return retval\n\n    @property\n    def items(self):\n        \"\"\" Returns the number of items held in this container \"\"\"\n        return self._items\n\n    @property\n    def subframe(self):\n        \"\"\" Returns the next sub-frame to be populated \"\"\"\n        frame = self.subframes[self._idx]\n        next_idx = self._idx + 1 if self._idx + 1 < self.columns else 0\n        logger.debug(\"current_idx: %s, next_idx: %s\", self._idx, next_idx)\n        self._idx = next_idx\n        self._items += 1\n        return frame\n\n    def set_subframes(self):\n        \"\"\" Set a sub-frame for each possible column \"\"\"\n        subframes = []\n        for idx in range(self.max_columns):\n            name = f\"af_subframe_{idx}\"\n            subframe = ttk.Frame(self.parent, name=name, style=f\"{self._style}TFrame\")\n            if idx < self.columns:\n                # Only pack visible columns\n                subframe.pack(padx=5, pady=5, side=tk.LEFT, anchor=tk.N, expand=True, fill=tk.X)\n            subframes.append(subframe)\n            logger.debug(\"Added subframe: %s\", name)\n        return subframes\n\n    def rearrange_columns(self, width):\n        \"\"\" On column number change redistribute widgets \"\"\"\n        if not self.validate(width):\n            return\n\n        new_columns = min(self.max_columns, max(1, width // self.single_column_width))\n        logger.debug(\"Rearranging columns: (width: %s, old_columns: %s, new_columns: %s)\",\n                     width, self.columns, new_columns)\n        self.columns = new_columns\n        if not self._widget_config:\n            self.compile_widget_config()\n        self.destroy_children()\n        self.repack_columns()\n        # Reset counters\n        self._items = 0\n        self._idx = 0\n        self.pack_widget_clones(self._widget_config)\n\n    def validate(self, width):\n        \"\"\" Validate that passed in width should trigger column re-arranging \"\"\"\n        if ((width < self.single_column_width and self.columns == 1) or\n                (width > self.max_width and self.columns == self.max_columns)):\n            logger.debug(\"width outside min/max thresholds: (min: %s, width: %s, max: %s)\",\n                         self.single_column_width, width, self.max_width)\n            return False\n        range_min = self.columns * self.single_column_width\n        range_max = (self.columns + 1) * self.single_column_width\n        if range_min < width < range_max:\n            logger.debug(\"width outside next step refresh threshold: (step down: %s, width: %s,\"\n                         \"step up: %s)\", range_min, width, range_max)\n            return False\n        return True\n\n    def compile_widget_config(self):\n        \"\"\" Compile all children recursively in correct order if not already compiled and add\n        to :attr:`_widget_config` \"\"\"\n        zipped = zip_longest(*(subframe.winfo_children() for subframe in self.subframes))\n        children = [child for group in zipped for child in group if child is not None]\n        self._widget_config = [{\"class\": child.__class__,\n                                \"id\": str(child),\n                                \"tooltip\": _RECREATE_OBJECTS[\"tooltips\"].get(str(child), None),\n                                \"rc_menu\": _RECREATE_OBJECTS[\"contextmenus\"].get(str(child), None),\n                                \"pack_info\": self.pack_config_cleaner(child),\n                                \"name\": child.winfo_name(),\n                                \"config\": self.config_cleaner(child),\n                                \"children\": self.get_all_children_config(child, []),\n                                # Some children have custom kwargs, so keep dicts in sync\n                                \"custom_kwargs\": self._custom_kwargs(child)}\n                               for idx, child in enumerate(children)]\n        logger.debug(\"Compiled AutoFillContainer children: %s\", self._widget_config)\n\n    @classmethod\n    def _custom_kwargs(cls, widget):\n        \"\"\" For custom widgets some custom arguments need to be passed from the old widget to the\n        newly created widget.\n\n        Parameters\n        ----------\n        widget: tkinter widget\n            The widget to be checked for custom keyword arguments\n\n        Returns\n        -------\n        dict\n            The custom keyword arguments required for recreating the given widget\n        \"\"\"\n        retval = {}\n        if widget.__class__.__name__ == \"MultiOption\":\n            retval = {\"value\": widget._value,  # pylint:disable=protected-access\n                      \"variable\": widget._master_variable}  # pylint:disable=protected-access\n        elif widget.__class__.__name__ == \"ToggledFrame\":\n            # Toggled Frames need to have their variable tracked\n            retval = {\"text\": widget._text,  # pylint:disable=protected-access\n                      \"toggle_var\": widget._toggle_var}  # pylint:disable=protected-access\n        return retval\n\n    def get_all_children_config(self, widget, child_list):\n        \"\"\" Return all children, recursively, of given widget.\n\n        Parameters\n        ----------\n        widget: tkinter widget\n            The widget to recursively obtain the configurations of each child\n        child_list: list\n            The list of child configurations already collected\n\n        Returns\n        -------\n        list\n            The list of configurations for all recursive children of the given widget\n         \"\"\"\n        unpack = set()\n        for child in widget.winfo_children():\n            # Hidden Toggle Frame boxes need to be mapped\n            if child.winfo_ismapped() or \"toggledframe_subframe\" in str(child):\n                not_mapped = not child.winfo_ismapped()\n                # ToggleFrame is a custom widget that creates it's own children and handles\n                # bindings on the headers, to auto-hide the contents. To ensure that all child\n                # information (specifically pack information) can be collected, we need to pack\n                # any hidden sub-frames. These are then hidden again once collected.\n                if not_mapped and (child.winfo_name() == \"toggledframe_subframe\" or\n                                   child.winfo_name() == \"chkbuttons\"):\n                    child.pack(fill=tk.X, expand=True)\n                    child.update_idletasks()  # Updates the packing info of children\n                    unpack.add(child)\n\n                if child.winfo_name().startswith(\"toggledframe_header\"):\n                    # Headers should be entirely handled by parent widget\n                    continue\n\n                child_list.append({\n                    \"class\": child.__class__,\n                    \"id\": str(child),\n                    \"tooltip\": _RECREATE_OBJECTS[\"tooltips\"].get(str(child), None),\n                    \"rc_menu\": _RECREATE_OBJECTS[\"contextmenus\"].get(str(child), None),\n                    \"pack_info\": self.pack_config_cleaner(child),\n                    \"name\": child.winfo_name(),\n                    \"config\": self.config_cleaner(child),\n                    \"parent\": child.winfo_parent(),\n                    \"custom_kwargs\": self._custom_kwargs(child)})\n            self.get_all_children_config(child, child_list)\n\n        # Re-hide any toggle frames that were expanded\n        for hide in unpack:\n            hide.pack_forget()\n            hide.update_idletasks()\n        return child_list\n\n    @staticmethod\n    def config_cleaner(widget):\n        \"\"\" Some options don't like to be copied, so this returns a cleaned\n            configuration from a widget\n            We use config() instead of configure() because some items (ttk Scale) do\n            not populate configure()\"\"\"\n        new_config = {}\n        for key in widget.config():\n            if key == \"class\":\n                continue\n            val = widget.cget(key)\n            # Some keys default to \"\" but tkinter doesn't like to set config to this value\n            # so skip them to use default value.\n            if key in (\"anchor\", \"justify\", \"compound\") and val == \"\":\n                continue\n            # Following keys cannot be defined after widget is created:\n            if key in (\"colormap\", \"container\", \"visual\"):\n                continue\n            val = str(val) if isinstance(val, Tcl_Obj) else val\n            # Return correct command from master command dict\n            val = _RECREATE_OBJECTS[\"commands\"][val] if key == \"command\" and val != \"\" else val\n            new_config[key] = val\n        return new_config\n\n    @staticmethod\n    def pack_config_cleaner(widget):\n        \"\"\" Some options don't like to be copied, so this returns a cleaned\n            configuration from a widget \"\"\"\n        return {key: val for key, val in widget.pack_info().items() if key != \"in\"}\n\n    def destroy_children(self):\n        \"\"\" Destroy the currently existing widgets \"\"\"\n        for subframe in self.subframes:\n            for child in subframe.winfo_children():\n                child.destroy()\n\n    def repack_columns(self):\n        \"\"\" Repack or unpack columns based on display columns \"\"\"\n        for idx, subframe in enumerate(self.subframes):\n            logger.trace(\"Processing subframe: %s\", subframe)\n            if idx < self.columns and not subframe.winfo_ismapped():\n                logger.trace(\"Packing subframe: %s\", subframe)\n                subframe.pack(padx=5, pady=5, side=tk.LEFT, anchor=tk.N, expand=True, fill=tk.X)\n            elif idx >= self.columns and subframe.winfo_ismapped():\n                logger.trace(\"Forgetting subframe: %s\", subframe)\n                subframe.pack_forget()\n\n    def pack_widget_clones(self, widget_dicts, old_children=None, new_children=None):\n        \"\"\" Recursively pass through the list of widgets creating clones and packing all\n        children.\n\n        Widgets cannot be given a new parent so we need to clone them and then pack the\n        new widgets.\n\n        Parameters\n        ----------\n        widget_dicts: list\n            List of dictionaries, in appearance order, of widget information for cloning widgets\n        old_childen: list, optional\n            Used for recursion. Leave at ``None``\n        new_childen: list, optional\n            Used for recursion. Leave at ``None``\n        \"\"\"\n        for widget_dict in widget_dicts:\n            logger.debug(\"Cloning widget: %s\", widget_dict)\n            old_children = [] if old_children is None else old_children\n            new_children = [] if new_children is None else new_children\n            if widget_dict.get(\"parent\", None) is not None:\n                parent = new_children[old_children.index(widget_dict[\"parent\"])]\n                logger.trace(\"old parent: '%s', new_parent: '%s'\", widget_dict[\"parent\"], parent)\n            else:\n                # Get the next sub-frame if this doesn't have a logged parent\n                parent = self.subframe\n            clone = widget_dict[\"class\"](parent,\n                                         name=widget_dict[\"name\"],\n                                         **widget_dict[\"custom_kwargs\"])\n            if widget_dict[\"config\"] is not None:\n                clone.configure(**widget_dict[\"config\"])\n            if widget_dict[\"tooltip\"] is not None:\n                Tooltip(clone, **widget_dict[\"tooltip\"])\n            rc_menu = widget_dict[\"rc_menu\"]\n            if rc_menu is not None:\n                # Re-initialize for new widget and bind\n                rc_menu.__init__(widget=clone)\n                rc_menu.cm_bind()\n            clone.pack(**widget_dict[\"pack_info\"])\n\n            # Handle ToggledFrame sub-frames. If the parent is not set to expanded, then we need to\n            # hide the sub-frame\n            if clone.winfo_name() == \"toggledframe_subframe\":\n                toggle_frame = clone.nametowidget(clone.winfo_parent())\n                if not toggle_frame.is_expanded:\n                    logger.debug(\"Hiding minimized toggle box: %s\", clone)\n                    clone.pack_forget()\n\n            old_children.append(widget_dict[\"id\"])\n            new_children.append(clone)\n            if widget_dict.get(\"children\", None) is not None:\n                self.pack_widget_clones(widget_dict[\"children\"], old_children, new_children)\n\n\nclass ControlBuilder():\n    \"\"\"\n    Builds and returns a frame containing a tkinter control with label\n    This should only be called from the ControlPanel class\n\n    Parameters\n    ----------\n    parent: tkinter object\n        Parent tkinter object\n    option: ControlPanelOption object\n        Holds all of the required option information\n    option_columns: int\n        Number of options to put on a single row for check-buttons/radio-buttons\n    label_width: int\n        Sets the width of the control label\n    checkbuttons_frame: tkinter.frame\n        If a check-button frame is passed in, then check-buttons will be placed in this frame\n        rather than the main options frame\n    style: str\n        The name of the style to use for the control panel. Styles are configured when TkInter\n        initializes. The style name is the common prefix prior to the widget name. Provide an empty\n        string to use the OS style\n    blank_nones: bool\n        Sets selected values to an empty string rather than None if this is true.\n    \"\"\"\n    def __init__(self, parent, option, option_columns,  # pylint:disable=too-many-arguments\n                 label_width, checkbuttons_frame, style, blank_nones):\n        logger.debug(\"Initializing %s: (parent: %s, option: %s, option_columns: %s, \"\n                     \"label_width: %s, checkbuttons_frame: %s, style: %s, blank_nones: %s)\",\n                     self.__class__.__name__, parent, option, option_columns, label_width,\n                     checkbuttons_frame, style, blank_nones)\n\n        self.option = option\n        self.option_columns = option_columns\n        self.helpset = False\n        self.label_width = label_width\n        self.filebrowser = None\n        # Default to Control Panel Style\n        self._style = style = style if style else \"CPanel.\"\n        self._theme = get_config().user_theme[\"group_panel\"]\n        if self._style.startswith(\"SPanel\"):\n            self._theme = {**self._theme, **get_config().user_theme[\"group_settings\"]}\n\n        self.frame = self.control_frame(parent)\n        self.chkbtns = checkbuttons_frame\n\n        self.set_tk_var(blank_nones)\n        self.build_control()\n        logger.debug(\"Initialized: %s\", self.__class__.__name__)\n\n    # Frame, control type and variable\n    def control_frame(self, parent):\n        \"\"\" Frame to hold control and it's label \"\"\"\n        logger.debug(\"Build control frame\")\n        frame = ttk.Frame(parent,\n                          name=f\"fr_{self.option.name}\",\n                          style=f\"{self._style}Group.TFrame\")\n        frame.pack(fill=tk.X)\n        logger.debug(\"Built control frame\")\n        return frame\n\n    def set_tk_var(self, blank_nones):\n        \"\"\" Correct variable type for control \"\"\"\n        val = \"\" if self.option.value is None and blank_nones else self.option.value\n        self.option.tk_var.set(val)\n        logger.debug(\"Set tk variable: (option: '%s', variable: %s, value: '%s')\",\n                     self.option.name, self.option.tk_var, val)\n\n    # Build the full control\n    def build_control(self):\n        \"\"\" Build the correct control type for the option passed through \"\"\"\n        logger.debug(\"Build config option control\")\n        if self.option.control not in (ttk.Checkbutton, \"radio\", \"multi\", \"colorchooser\"):\n            self.build_control_label()\n        self.build_one_control()\n        logger.debug(\"Built option control\")\n\n    def build_control_label(self):\n        \"\"\" Label for control \"\"\"\n        logger.debug(\"Build control label: (option: '%s')\", self.option.name)\n        lbl = ttk.Label(self.frame,\n                        text=self.option.title,\n                        width=self.label_width,\n                        anchor=tk.W,\n                        style=f\"{self._style}Group.TLabel\")\n        lbl.pack(padx=5, pady=5, side=tk.LEFT, anchor=tk.N)\n        if self.option.helptext is not None:\n            _get_tooltip(lbl, text=self.option.helptext)\n        logger.debug(\"Built control label: (widget: '%s', title: '%s'\",\n                     self.option.name, self.option.title)\n\n    def build_one_control(self):\n        \"\"\" Build and place the option controls \"\"\"\n        logger.debug(\"Build control: '%s')\", self.option.name)\n        if self.option.control == \"scale\":\n            ctl = self.slider_control()\n        elif self.option.control in (\"radio\", \"multi\"):\n            ctl = self._multi_option_control(self.option.control)\n        elif self.option.control == \"colorchooser\":\n            ctl = self._color_control()\n        elif self.option.control == ttk.Checkbutton:\n            ctl = self.control_to_checkframe()\n        else:\n            ctl = self.control_to_optionsframe()\n        if self.option.control != ttk.Checkbutton:\n            ctl.pack(padx=5, pady=5, fill=tk.X, expand=True)\n            if self.option.helptext is not None and not self.helpset:\n                tooltip_kwargs = {\"text\": self.option.helptext}\n                if self.option.sysbrowser is not None:\n                    tooltip_kwargs[\"text_variable\"] = self.option.tk_var\n                _get_tooltip(ctl, **tooltip_kwargs)\n\n        logger.debug(\"Built control: '%s'\", self.option.name)\n\n    def _multi_option_control(self, option_type):\n        \"\"\" Create a group of buttons for single or multi-select\n\n        Parameters\n        ----------\n        option_type: {\"radio\", \"multi\"}\n            The type of boxes that this control should hold. \"radio\" for single item select,\n            \"multi\" for multi item select.\n\n        \"\"\"\n        logger.debug(\"Adding %s group: %s\", option_type, self.option.name)\n        help_intro, help_items = self._get_multi_help_items(self.option.helptext)\n        ctl = ttk.LabelFrame(self.frame,\n                             text=self.option.title,\n                             name=f\"{option_type}_labelframe\",\n                             style=f\"{self._style}Group.TLabelframe\")\n        holder = AutoFillContainer(ctl,\n                                   self.option_columns,\n                                   self.option_columns,\n                                   style=f\"{self._style}Group.\")\n        for choice in self.option.choices:\n            if option_type == \"radio\":\n                ctl = ttk.Radiobutton\n                style = f\"{self._style}Group.TRadiobutton\"\n            else:\n                ctl = MultiOption\n                style = f\"{self._style}Group.TCheckbutton\"\n\n            ctl = ctl(holder.subframe,\n                      text=choice.replace(\"_\", \" \").title(),\n                      value=choice,\n                      variable=self.option.tk_var,\n                      style=style)\n            if choice.lower() in help_items:\n                self.helpset = True\n                helptext = help_items[choice.lower()]\n                helptext = f\"{helptext}\\n\\n - {help_intro}\"\n                _get_tooltip(ctl, text=helptext)\n            ctl.pack(anchor=tk.W, fill=tk.X)\n            logger.debug(\"Added %s option %s\", option_type, choice)\n        return holder.parent\n\n    @staticmethod\n    def _get_multi_help_items(helptext):\n        \"\"\" Split the help text up, for formatted help text, into the individual options\n        for multi/radio buttons.\n\n        Parameters\n        ----------\n        helptext: str\n            The raw help text for this cli. option\n\n        Returns\n        -------\n        tuple (`str`, `dict`)\n            The help text intro and a dictionary containing the help text split into separate\n            entries for each option choice\n        \"\"\"\n        logger.debug(\"raw help: %s\", helptext)\n        all_help = helptext.splitlines()\n        intro = \"\"\n        if any(line.startswith(\" - \") for line in all_help):\n            intro = all_help[0]\n        retval = (intro,\n                  {re.sub(r\"[^\\w\\-\\_]+\", \"\",\n                          line.split()[1].lower()): \" \".join(line.replace(\"_\", \" \").split()[1:])\n                   for line in all_help if line.startswith(\" - \")})\n        logger.debug(\"help items: %s\", retval)\n        return retval\n\n    def slider_control(self):\n        \"\"\" A slider control with corresponding Entry box \"\"\"\n        logger.debug(\"Add slider control to Options Frame: (widget: '%s', dtype: %s, \"\n                     \"rounding: %s, min_max: %s)\", self.option.name, self.option.dtype,\n                     self.option.rounding, self.option.min_max)\n        validate = self.slider_check_int if self.option.dtype == int else self.slider_check_float\n        vcmd = self.frame.register(validate)\n        tbox = tk.Entry(self.frame,\n                        width=8,\n                        textvariable=self.option.tk_var,\n                        justify=tk.RIGHT,\n                        font=get_config().default_font,\n                        validate=\"all\",\n                        validatecommand=(vcmd, \"%P\"),\n                        bg=self._theme[\"input_color\"],\n                        fg=self._theme[\"input_font\"],\n                        highlightbackground=self._theme[\"input_font\"],\n                        highlightthickness=1,\n                        bd=0)\n        tbox.pack(padx=(0, 5), side=tk.RIGHT)\n        cmd = partial(set_slider_rounding,\n                      var=self.option.tk_var,\n                      d_type=self.option.dtype,\n                      round_to=self.option.rounding,\n                      min_max=self.option.min_max)\n        ctl = ttk.Scale(self.frame,\n                        variable=self.option.tk_var,\n                        command=cmd,\n                        style=f\"{self._style}Horizontal.TScale\")\n        _add_command(ctl.cget(\"command\"), cmd)\n        rc_menu = _get_contextmenu(tbox)\n        rc_menu.cm_bind()\n        ctl[\"from_\"] = self.option.min_max[0]\n        ctl[\"to\"] = self.option.min_max[1]\n        logger.debug(\"Added slider control to Options Frame: %s\", self.option.name)\n        return ctl\n\n    @staticmethod\n    def slider_check_int(value):\n        \"\"\" Validate a slider's text entry box for integer values.\n\n        Parameters\n        ----------\n        value: str\n            The slider text entry value to validate\n        \"\"\"\n        if value.isdigit() or value == \"\":\n            return True\n        return False\n\n    @staticmethod\n    def slider_check_float(value):\n        \"\"\" Validate a slider's text entry box for float values.\n        Parameters\n        ----------\n        value: str\n            The slider text entry value to validate\n        \"\"\"\n        if value:\n            try:\n                float(value)\n            except ValueError:\n                return False\n        return True\n\n    def control_to_optionsframe(self):\n        \"\"\" Standard non-check buttons sit in the main options frame \"\"\"\n        logger.debug(\"Add control to Options Frame: (widget: '%s', control: %s, choices: %s)\",\n                     self.option.name, self.option.control, self.option.choices)\n        if self.option.sysbrowser is not None:\n            self.filebrowser = FileBrowser(self.option.name,\n                                           self.option.tk_var,\n                                           self.frame,\n                                           self.option.sysbrowser,\n                                           self._style)\n\n        if self.option.control == tk.Entry:\n            ctl = self.option.control(self.frame,\n                                      textvariable=self.option.tk_var,\n                                      font=get_config().default_font,\n                                      bg=self._theme[\"input_color\"],\n                                      fg=self._theme[\"input_font\"],\n                                      highlightbackground=self._theme[\"input_font\"],\n                                      highlightthickness=1,\n                                      bd=0)\n        else:  # Combobox\n            ctl = self.option.control(self.frame,\n                                      textvariable=self.option.tk_var,\n                                      font=get_config().default_font,\n                                      state=\"readonly\",\n                                      style=f\"{self._style}TCombobox\")\n\n            # Style for combo list boxes needs to be set directly on widget as no style parameter\n            cmd = f\"[ttk::combobox::PopdownWindow {ctl}].f.l configure -\"\n            ctl.tk.eval(f\"{cmd}foreground {self._theme['input_font']}\")\n            ctl.tk.eval(f\"{cmd}background {self._theme['input_color']}\")\n            ctl.tk.eval(f\"{cmd}selectforeground {self._theme['control_active']}\")\n            ctl.tk.eval(f\"{cmd}selectbackground {self._theme['control_disabled']}\")\n\n        rc_menu = _get_contextmenu(ctl)\n        rc_menu.cm_bind()\n\n        if self.option.choices:\n            logger.debug(\"Adding combo choices: %s\", self.option.choices)\n            ctl[\"values\"] = self.option.choices\n            ctl[\"state\"] = \"readonly\"\n        logger.debug(\"Added control to Options Frame: %s\", self.option.name)\n        return ctl\n\n    def _color_control(self):\n        \"\"\" Clickable label holding the currently selected color \"\"\"\n        logger.debug(\"Add control to Options Frame: (widget: '%s', control: %s, choices: %s)\",\n                     self.option.name, self.option.control, self.option.choices)\n        frame = ttk.Frame(self.frame, style=f\"{self._style}Group.TFrame\")\n        lbl = ttk.Label(frame,\n                        text=self.option.title,\n                        width=self.label_width,\n                        anchor=tk.W,\n                        style=f\"{self._style}Group.TLabel\")\n        ctl = tk.Frame(frame,\n                       bg=self.option.tk_var.get(),\n                       bd=2,\n                       cursor=\"hand2\",\n                       relief=tk.SUNKEN,\n                       width=round(int(20 * get_config().scaling_factor)),\n                       height=round(int(14 * get_config().scaling_factor)))\n        ctl.bind(\"<Button-1>\", lambda *e, c=ctl, t=self.option.title: self._ask_color(c, t))\n        lbl.pack(side=tk.LEFT, anchor=tk.N)\n        ctl.pack(side=tk.RIGHT, anchor=tk.W)\n        frame.pack(padx=5, side=tk.LEFT, anchor=tk.W)\n        if self.option.helptext is not None:\n            _get_tooltip(frame, text=self.option.helptext)\n        # Callback to set the color chooser background on an update (e.g. reset)\n        self.option.tk_var.trace(\"w\", lambda *e: ctl.config(bg=self.option.tk_var.get()))\n        logger.debug(\"Added control to Options Frame: %s\", self.option.name)\n        return ctl\n\n    def _ask_color(self, frame, title):\n        \"\"\" Pop ask color dialog set to variable and change frame color \"\"\"\n        color = self.option.tk_var.get()\n        chosen = colorchooser.askcolor(parent=frame, color=color, title=f\"{title} Color\")[1]\n        if chosen is None:\n            return\n        self.option.tk_var.set(chosen)\n\n    def control_to_checkframe(self):\n        \"\"\" Add check-buttons to the check-button frame \"\"\"\n        logger.debug(\"Add control checkframe: '%s'\", self.option.name)\n        chkframe = self.chkbtns.subframe\n        ctl = self.option.control(chkframe,\n                                  variable=self.option.tk_var,\n                                  text=self.option.title,\n                                  name=self.option.name,\n                                  style=f\"{self._style}Group.TCheckbutton\")\n        _get_tooltip(ctl, text=self.option.helptext)\n        ctl.pack(side=tk.TOP, anchor=tk.W, fill=tk.X)\n        logger.debug(\"Added control checkframe: '%s'\", self.option.name)\n        return ctl\n\n\nclass FileBrowser():\n    \"\"\" Add FileBrowser buttons to control and handle routing \"\"\"\n    def __init__(self, opt_name, tk_var, control_frame, sysbrowser_dict, style):\n        logger.debug(\"Initializing: %s: (tk_var: %s, control_frame: %s, sysbrowser_dict: %s, \"\n                     \"style: %s)\", self.__class__.__name__, tk_var, control_frame,\n                     sysbrowser_dict, style)\n        self._opt_name = opt_name\n        self.tk_var = tk_var\n        self.frame = control_frame\n        self._style = style\n        self.browser = sysbrowser_dict[\"browser\"]\n        self.filetypes = sysbrowser_dict[\"filetypes\"]\n        self.action_option = self.format_action_option(sysbrowser_dict.get(\"action_option\", None))\n        self.command = sysbrowser_dict.get(\"command\", None)\n        self.destination = sysbrowser_dict.get(\"destination\", None)\n        self.add_browser_buttons()\n        logger.debug(\"Initialized: %s\", self.__class__.__name__)\n\n    @property\n    def helptext(self):\n        \"\"\" Dict containing tooltip text for buttons \"\"\"\n        retval = {\"folder\": _(\"Select a folder...\"),\n                  \"load\": _(\"Select a file...\"),\n                  \"load2\": _(\"Select a file...\"),\n                  \"picture\": _(\"Select a folder of images...\"),\n                  \"video\": _(\"Select a video...\"),\n                  \"model\": _(\"Select a model folder...\"),\n                  \"multi_load\": _(\"Select one or more files...\"),\n                  \"context\": _(\"Select a file or folder...\"),\n                  \"save_as\": _(\"Select a save location...\")}\n        return retval\n\n    @staticmethod\n    def format_action_option(action_option):\n        \"\"\" Format the action option to remove any dashes at the start \"\"\"\n        if action_option is None:\n            return action_option\n        if action_option.startswith(\"--\"):\n            return action_option[2:]\n        if action_option.startswith(\"-\"):\n            return action_option[1:]\n        return action_option\n\n    def add_browser_buttons(self):\n        \"\"\" Add correct file browser button for control \"\"\"\n        logger.debug(\"Adding browser buttons: (sysbrowser: %s\", self.browser)\n        frame = ttk.Frame(self.frame, style=f\"{self._style}Group.TFrame\")\n        frame.pack(side=tk.RIGHT, padx=(0, 5))\n\n        for browser in self.browser:\n            if browser == \"save\":\n                lbl = \"save_as\"\n            elif browser == \"load\" and self.filetypes == \"video\":\n                lbl = self.filetypes\n            elif browser == \"load\":\n                lbl = \"load2\"\n            elif browser == \"folder\" and (self._opt_name.startswith((\"frames\", \"faces\"))\n                                          or \"input\" in self._opt_name):\n                lbl = \"picture\"\n            elif browser == \"folder\" and \"model\" in self._opt_name:\n                lbl = \"model\"\n            else:\n                lbl = browser\n            img = get_images().icons[lbl]\n            action = getattr(self, \"ask_\" + browser)\n            cmd = partial(action, filepath=self.tk_var, filetypes=self.filetypes)\n            fileopn = tk.Button(frame,\n                                image=img,\n                                command=cmd,\n                                relief=tk.SOLID,\n                                bd=1,\n                                bg=get_config().user_theme[\"group_panel\"][\"button_background\"],\n                                cursor=\"hand2\")\n            _add_command(fileopn.cget(\"command\"), cmd)\n            fileopn.pack(padx=1, side=tk.RIGHT)\n            _get_tooltip(fileopn, text=self.helptext[lbl])\n            logger.debug(\"Added browser buttons: (action: %s, filetypes: %s\",\n                         action, self.filetypes)\n\n    def set_context_action_option(self, options):\n        \"\"\" Set the tk_var for the source action option\n            that dictates the context sensitive file browser. \"\"\"\n        if self.browser != [\"context\"]:\n            return\n        actions = {opt.name: opt.tk_var for opt in options}\n        logger.debug(\"Settiong action option for opt %s\", self.action_option)\n        self.action_option = actions[self.action_option]\n\n    @staticmethod\n    def ask_folder(filepath, filetypes=None):\n        \"\"\" Pop-up to get path to a directory\n            :param filepath: tkinter StringVar object\n            that will store the path to a directory.\n            :param filetypes: Unused argument to allow\n            filetypes to be given in ask_load(). \"\"\"\n        dirname = FileHandler(\"dir\", filetypes).return_file\n        if dirname:\n            logger.debug(dirname)\n            filepath.set(dirname)\n\n    @staticmethod\n    def ask_load(filepath, filetypes):\n        \"\"\" Pop-up to get path to a file \"\"\"\n        filename = FileHandler(\"filename\", filetypes).return_file\n        if filename:\n            logger.debug(filename)\n            filepath.set(filename)\n\n    @staticmethod\n    def ask_multi_load(filepath, filetypes):\n        \"\"\" Pop-up to get path to a file \"\"\"\n        filenames = FileHandler(\"filename_multi\", filetypes).return_file\n        if filenames:\n            final_names = \" \".join(f\"\\\"{fname}\\\"\" for fname in filenames)\n            logger.debug(final_names)\n            filepath.set(final_names)\n\n    @staticmethod\n    def ask_save(filepath, filetypes=None):\n        \"\"\" Pop-up to get path to save a new file \"\"\"\n        filename = FileHandler(\"save_filename\", filetypes).return_file\n        if filename:\n            logger.debug(filename)\n            filepath.set(filename)\n\n    @staticmethod\n    def ask_nothing(filepath, filetypes=None):  # pylint:disable=unused-argument\n        \"\"\" Method that does nothing, used for disabling open/save pop up \"\"\"\n        return\n\n    def ask_context(self, filepath, filetypes):\n        \"\"\" Method to pop the correct dialog depending on context \"\"\"\n        logger.debug(\"Getting context filebrowser\")\n        selected_action = self.action_option.get()\n        selected_variable = self.destination\n        filename = FileHandler(\"context\",\n                               filetypes,\n                               command=self.command,\n                               action=selected_action,\n                               variable=selected_variable).return_file\n        if filename:\n            logger.debug(filename)\n            filepath.set(filename)\n", "lib/gui/display_page.py": "#!/usr/bin python3\n\"\"\" Display Page parent classes for display section of the Faceswap GUI \"\"\"\n\nimport gettext\nimport logging\nimport tkinter as tk\nfrom tkinter import ttk\n\nfrom .custom_widgets import Tooltip\nfrom .utils import get_images\n\nlogger = logging.getLogger(__name__)\n\n# LOCALES\n_LANG = gettext.translation(\"gui.tooltips\", localedir=\"locales\", fallback=True)\n_ = _LANG.gettext\n\n\nclass DisplayPage(ttk.Frame):  # pylint:disable=too-many-ancestors\n    \"\"\" Parent frame holder for each tab.\n        Defines uniform structure for each tab to inherit from \"\"\"\n    def __init__(self, parent, tab_name, helptext):\n        super().__init__(parent)\n\n        self._parent = parent\n        self.running_task = parent.running_task\n        self.helptext = helptext\n        self.tabname = tab_name\n\n        self.vars = {\"info\": tk.StringVar()}\n        self.add_optional_vars(self.set_vars())\n\n        self.subnotebook = self.add_subnotebook()\n        self.optsframe = self.add_options_frame()\n        self.add_options_info()\n\n        self.add_frame_separator()\n        self.set_mainframe_single_tab_style()\n\n        self.pack(fill=tk.BOTH, side=tk.TOP, anchor=tk.NW)\n        parent.add(self, text=self.tabname.title())\n\n    @property\n    def _tab_is_active(self):\n        \"\"\" bool: ``True`` if the tab currently has focus otherwise ``False`` \"\"\"\n        return self._parent.tab(self._parent.select(), \"text\").lower() == self.tabname.lower()\n\n    def add_optional_vars(self, varsdict):\n        \"\"\" Add page specific variables \"\"\"\n        if isinstance(varsdict, dict):\n            for key, val in varsdict.items():\n                logger.debug(\"Adding: (%s: %s)\", key, val)\n                self.vars[key] = val\n\n    def set_vars(self):\n        \"\"\" Override to return a dict of page specific variables \"\"\"\n        return {}\n\n    def on_tab_select(self):\n        \"\"\" Override for specific actions when the current tab is selected \"\"\"\n        logger.debug(\"Returning as 'on_tab_select' not implemented for %s\",\n                     self.__class__.__name__)\n\n    def add_subnotebook(self):\n        \"\"\" Add the main frame notebook \"\"\"\n        logger.debug(\"Adding subnotebook\")\n        notebook = ttk.Notebook(self)\n        notebook.pack(side=tk.TOP, anchor=tk.NW, fill=tk.BOTH, expand=True)\n        return notebook\n\n    def add_options_frame(self):\n        \"\"\" Add the display tab options \"\"\"\n        logger.debug(\"Adding options frame\")\n        optsframe = ttk.Frame(self)\n        optsframe.pack(side=tk.BOTTOM, padx=5, pady=5, fill=tk.X)\n        return optsframe\n\n    def add_options_info(self):\n        \"\"\" Add the info bar \"\"\"\n        logger.debug(\"Adding options info\")\n        lblinfo = ttk.Label(self.optsframe,\n                            textvariable=self.vars[\"info\"],\n                            anchor=tk.W)\n        lblinfo.pack(side=tk.LEFT, expand=True, padx=5, pady=5, anchor=tk.W)\n\n    def set_info(self, msg):\n        \"\"\" Set the info message \"\"\"\n        logger.debug(\"Setting info: %s\", msg)\n        self.vars[\"info\"].set(msg)\n\n    def add_frame_separator(self):\n        \"\"\" Add a separator between top and bottom frames \"\"\"\n        logger.debug(\"Adding frame seperator\")\n        sep = ttk.Frame(self, height=2, relief=tk.RIDGE)\n        sep.pack(fill=tk.X, pady=(5, 0), side=tk.BOTTOM)\n\n    @staticmethod\n    def set_mainframe_single_tab_style():\n        \"\"\" Configure ttk notebook style to represent a single frame \"\"\"\n        logger.debug(\"Setting main frame single tab style\")\n        nbstyle = ttk.Style()\n        nbstyle.configure(\"single.TNotebook\", borderwidth=0)\n        nbstyle.layout(\"single.TNotebook.Tab\", [])\n\n    def subnotebook_add_page(self, tabtitle, widget=None):\n        \"\"\" Add a page to the sub notebook \"\"\"\n        logger.debug(\"Adding subnotebook page: %s\", tabtitle)\n        frame = widget if widget else ttk.Frame(self.subnotebook)\n        frame.pack(padx=5, pady=5, fill=tk.BOTH, expand=True)\n        self.subnotebook.add(frame, text=tabtitle)\n        self.subnotebook_configure()\n        return frame\n\n    def subnotebook_configure(self):\n        \"\"\" Configure notebook to display or hide tabs \"\"\"\n        if len(self.subnotebook.children) == 1:\n            logger.debug(\"Setting single page style\")\n            self.subnotebook.configure(style=\"single.TNotebook\")\n        else:\n            logger.debug(\"Setting multi page style\")\n            self.subnotebook.configure(style=\"TNotebook\")\n\n    def subnotebook_hide(self):\n        \"\"\" Hide the subnotebook. Used for hiding\n            Optional displays \"\"\"\n        if self.subnotebook and self.subnotebook.winfo_ismapped():\n            logger.debug(\"Hiding subnotebook\")\n            self.subnotebook.pack_forget()\n            self.subnotebook.destroy()\n            self.subnotebook = None\n\n    def subnotebook_show(self):\n        \"\"\" Show subnotebook. Used for displaying\n            Optional displays  \"\"\"\n        if not self.subnotebook:\n            logger.debug(\"Showing subnotebook\")\n            self.subnotebook = self.add_subnotebook()\n\n    def subnotebook_get_widgets(self):\n        \"\"\" Return each widget that sits within each\n            subnotebook frame \"\"\"\n        logger.debug(\"Getting subnotebook widgets\")\n        for child in self.subnotebook.winfo_children():\n            for widget in child.winfo_children():\n                yield widget\n\n    def subnotebook_get_titles_ids(self):\n        \"\"\" Return tabs ids and titles \"\"\"\n        tabs = {}\n        for tab_id in range(0, self.subnotebook.index(\"end\")):\n            tabs[self.subnotebook.tab(tab_id, \"text\")] = tab_id\n        logger.debug(tabs)\n        return tabs\n\n    def subnotebook_page_from_id(self, tab_id):\n        \"\"\" Return subnotebook tab widget from it's ID \"\"\"\n        tab_name = self.subnotebook.tabs()[tab_id].split(\".\")[-1]\n        logger.debug(tab_name)\n        return self.subnotebook.children[tab_name]\n\n\nclass DisplayOptionalPage(DisplayPage):  # pylint:disable=too-many-ancestors\n    \"\"\" Parent Context Sensitive Display Tab \"\"\"\n\n    def __init__(self, parent, tab_name, helptext, wait_time, command=None):\n        super().__init__(parent, tab_name, helptext)\n\n        self._waittime = wait_time\n        self.command = command\n        self.display_item = None\n\n        self.set_info_text()\n        self.add_options()\n        parent.select(self)\n\n        self.update_idletasks()\n        self._update_page()\n\n    def set_vars(self):\n        \"\"\" Analysis specific vars \"\"\"\n        enabled = tk.BooleanVar()\n        enabled.set(True)\n\n        ready = tk.BooleanVar()\n        ready.set(False)\n\n        tk_vars = {\"enabled\": enabled,\n                   \"ready\": ready}\n        logger.debug(tk_vars)\n        return tk_vars\n\n    def on_tab_select(self):\n        \"\"\" Callback for when the optional tab is selected.\n\n        Run the tab's update code when the tab is selected.\n        \"\"\"\n        logger.debug(\"Callback received for '%s' tab\", self.tabname)\n        self._update_page()\n\n    # INFO LABEL\n    def set_info_text(self):\n        \"\"\" Set waiting for display text \"\"\"\n        if not self.vars[\"enabled\"].get():\n            msg = f\"{self.tabname.title()} disabled\"\n        elif self.vars[\"enabled\"].get() and not self.vars[\"ready\"].get():\n            msg = f\"Waiting for {self.tabname}...\"\n        else:\n            msg = f\"Displaying {self.tabname}\"\n        logger.debug(msg)\n        self.set_info(msg)\n\n    # DISPLAY OPTIONS BAR\n    def add_options(self):\n        \"\"\" Add the additional options \"\"\"\n        self.add_option_save()\n        self.add_option_enable()\n\n    def add_option_save(self):\n        \"\"\" Add save button to save page output to file \"\"\"\n        logger.debug(\"Adding save option\")\n        btnsave = ttk.Button(self.optsframe,\n                             image=get_images().icons[\"save\"],\n                             command=self.save_items)\n        btnsave.pack(padx=2, side=tk.RIGHT)\n        Tooltip(btnsave,\n                text=_(f\"Save {self.tabname}(s) to file\"),\n                wrap_length=200)\n\n    def add_option_enable(self):\n        \"\"\" Add check-button to enable/disable page \"\"\"\n        logger.debug(\"Adding enable option\")\n        chkenable = ttk.Checkbutton(self.optsframe,\n                                    variable=self.vars[\"enabled\"],\n                                    text=f\"Enable {self.tabname}\",\n                                    command=self.on_chkenable_change)\n        chkenable.pack(side=tk.RIGHT, padx=5, anchor=tk.W)\n        Tooltip(chkenable,\n                text=_(f\"Enable or disable {self.tabname} display\"),\n                wrap_length=200)\n\n    def save_items(self):\n        \"\"\" Save items. Override for display specific saving \"\"\"\n        raise NotImplementedError()\n\n    def on_chkenable_change(self):\n        \"\"\" Update the display immediately on a check-button change \"\"\"\n        logger.debug(\"Enabled checkbox changed\")\n        if self.vars[\"enabled\"].get():\n            self.subnotebook_show()\n        else:\n            self.subnotebook_hide()\n        self.set_info_text()\n\n    def _update_page(self):\n        \"\"\" Update the latest preview item \"\"\"\n        if not self.running_task.get() or not self._tab_is_active:\n            return\n        if self.vars[\"enabled\"].get():\n            logger.trace(\"Updating page: %s\", self.__class__.__name__)\n            self.display_item_set()\n            self.load_display()\n        self.after(self._waittime, self._update_page)\n\n    def display_item_set(self):\n        \"\"\" Override for display specific loading \"\"\"\n        raise NotImplementedError()\n\n    def load_display(self):\n        \"\"\" Load the display \"\"\"\n        if not self.display_item or not self._tab_is_active:\n            return\n        logger.debug(\"Loading display for tab: %s\", self.tabname)\n        self.display_item_process()\n        self.vars[\"ready\"].set(True)\n        self.set_info_text()\n\n    def display_item_process(self):\n        \"\"\" Override for display specific loading \"\"\"\n        raise NotImplementedError()\n\n    def close(self):\n        \"\"\" Called when the parent notebook is shutting down\n            Children must be destroyed as forget only hides display\n            Override for page specific shutdown \"\"\"\n        for child in self.winfo_children():\n            logger.debug(\"Destroying child: %s\", child)\n            child.destroy()\n", "lib/gui/theme.py": "#!/usr/bin/env python3\n\"\"\" functions for implementing themes in Faceswap's GUI \"\"\"\nimport logging\nimport os\nimport tkinter as tk\nfrom tkinter import ttk\n\nimport numpy as np\n\nfrom lib.serializer import get_serializer\nfrom lib.utils import FaceswapError\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass Style():\n    \"\"\" Set the overarching theme and customize widgets.\n\n    Parameters\n    ----------\n    default_font: tuple\n        The name and size of the default font\n    root: :class:`tkinter.Tk`\n        The root tkinter object\n    path_cache: str\n        The path to the GUI's cache\n    \"\"\"\n    def __init__(self, default_font, root, path_cache):\n        self._root = root\n        self._font = default_font\n        default = os.path.join(path_cache, \"themes\", \"default.json\")\n        self._user_theme = get_serializer(\"json\").load(default)\n        self._style = ttk.Style()\n        self._widgets = _Widgets(self._style)\n        self._set_styles()\n\n    @property\n    def user_theme(self):\n        \"\"\" dict: The currently selected user theme. \"\"\"\n        return self._user_theme\n\n    def _set_styles(self):\n        \"\"\" Configure widget theme and styles \"\"\"\n        self._config_settings_group()\n        # Command page\n        theme = self._user_theme[\"command_tabs\"]\n        self._widgets.notebook(\"CPanel\",\n                               theme[\"frame_border\"],\n                               theme[\"tab_color\"],\n                               theme[\"tab_selected\"],\n                               theme[\"tab_hover\"])\n\n        # Settings Popup\n        self._style.configure(\"SPanel.Header1.TLabel\",\n                              font=(self._font[0], self._font[1] + 4, \"bold\"))\n        self._style.configure(\"SPanel.Header2.TLabel\",\n                              font=(self._font[0], self._font[1] + 2, \"bold\"))\n        # Console\n        theme = self._user_theme[\"console\"]\n        console_sbar = tuple(tuple(theme[f\"scrollbar_{area}_{state}\"]\n                                   for state in (\"normal\", \"disabled\", \"active\"))\n                             for area in (\"background\", \"foreground\", \"border\"))\n        self._widgets.scrollbar(\"Console\",\n                                theme[\"scrollbar_trough\"],\n                                theme[\"scrollbar_border\"],\n                                *console_sbar)\n        self._widgets.frame(\"Console\",\n                            theme[\"background_color\"],\n                            theme[\"border_color\"],\n                            borderwidth=1)\n\n    def _config_settings_group(self):\n        \"\"\" Configures the style of the control panel entry boxes. Used for inputting Faceswap\n        options or controlling plugin settings. \"\"\"\n        theme = self._user_theme[\"group_panel\"]\n        for panel_type in (\"CPanel\", \"SPanel\"):\n            if panel_type == \"SPanel\":  # Merge in Settings Panel overrides\n                theme = {**theme, **self._user_theme[\"group_settings\"]}\n            self._style.configure(f\"{panel_type}.Holder.TFrame\",\n                                  background=theme[\"panel_background\"])\n            # Header Colors on option/group controls\n            self._style.configure(f\"{panel_type}.Group.TLabelframe.Label\",\n                                  foreground=theme[\"header_color\"])\n            self._style.configure(f\"{panel_type}.Groupheader.TLabel\",\n                                  background=theme[\"header_color\"],\n                                  foreground=theme[\"header_font\"],\n                                  font=(self._font[0], self._font[1], \"bold\"))\n            # Widgets and specific areas\n            self._group_panel_widgets(panel_type, theme)\n            self._group_panel_infoheader(panel_type, theme)\n            self._widgets.slider(panel_type,\n                                 theme[\"control_color\"],\n                                 theme[\"control_active\"],\n                                 self._user_theme[\"group_panel\"][\"group_background\"])\n            backgrounds = (theme[\"control_color\"],\n                           theme[\"control_disabled\"],\n                           theme[\"control_active\"])\n            foregrounds = (theme[\"control_disabled\"],\n                           theme[\"control_color\"],\n                           theme[\"control_disabled\"])\n            borders = (theme[\"header_color\"], theme[\"control_color\"], theme[\"header_color\"])\n            self._widgets.scrollbar(panel_type,\n                                    theme[\"scrollbar_trough\"],\n                                    theme[\"scrollbar_border\"],\n                                    backgrounds,\n                                    foregrounds,\n                                    borders)\n            self._widgets.combobox(panel_type,\n                                   theme[\"control_color\"],\n                                   theme[\"control_active\"],\n                                   theme[\"control_disabled\"],\n                                   theme[\"header_color\"],\n                                   theme[\"group_background\"],\n                                   theme[\"group_font\"])\n\n    def _group_panel_infoheader(self, key, theme):\n        \"\"\" Set the theme for the information header box that appears at the top of each group\n        panel\n\n        Parameters\n        ----------\n        key: str\n            The section that the slider will belong to\n        theme: dict\n            The user configuration theme options\n        \"\"\"\n        self._widgets.frame(f\"{key}.InfoHeader\",\n                            theme[\"info_color\"],\n                            theme[\"info_border\"],\n                            borderwidth=1)\n\n        self._style.configure(f\"{key}.InfoHeader.TLabel\",\n                              background=theme[\"info_color\"],\n                              foreground=theme[\"info_font\"],\n                              font=(self._font[0], self._font[1], \"bold\"))\n        self._style.configure(f\"{key}.InfoBody.TLabel\",\n                              background=theme[\"info_color\"],\n                              foreground=theme[\"info_font\"])\n\n    def _group_panel_widgets(self, key, theme):\n        \"\"\" Configure the foreground and background colors of common widgets.\n\n        Parameters\n        ----------\n        key: str\n            The section that the slider will belong to\n        theme: dict\n            The user configuration theme options\n        \"\"\"\n        # Put a border on a group's sub-frame\n        self._widgets.frame(f\"{key}.Subframe.Group\",\n                            theme[\"group_background\"],\n                            theme[\"group_border\"],\n                            borderwidth=1)\n\n        # Background and Foreground of widgets and labels\n        for lbl in [\"TLabel\", \"TFrame\", \"TLabelframe\", \"TCheckbutton\", \"TRadiobutton\",\n                    \"TLabelframe.Label\"]:\n            self._style.configure(f\"{key}.Group.{lbl}\",\n                                  background=theme[\"group_background\"],\n                                  foreground=theme[\"group_font\"])\n\n\nclass _Widgets():\n    \"\"\" Create custom ttk widget layouts for themed widgets.\n\n    Parameters\n    ----------\n    style: :class:`ttk.Style`\n        The master style object\n    \"\"\"\n    def __init__(self, style):\n        self._images = _TkImage()\n        self._style = style\n\n    def combobox(self, key, control_color, active_color, arrow_color, control_border, field_color,\n                 field_border):\n        \"\"\" Combo-boxes are fairly complex to style.\n\n        Parameters\n        ----------\n        key: str\n            The section that the slider will belong to\n        control_color: str\n            The color of inactive combo pull down button\n        active_color: str\n            The color of combo pull down button when it is hovered or pressed\n        arrow_color: str\n            The color of the combo pull down arrow\n        control_border: str\n            The color of the combo pull down button border\n        field_color: str\n            The color of the input field's background\n        field_border: str\n            The color of the input field's border\n        \"\"\"\n        # All the stock down arrow images are bad\n        images = {}\n        for state in (\"active\", \"normal\"):\n            images[f\"arrow_{state}\"] = self._images.get_image(\n                (20, 20),\n                control_color if state == \"normal\" else active_color,\n                foreground=arrow_color,\n                pattern=\"arrow\",\n                thickness=2,\n                border_width=1,\n                border_color=control_border)\n\n        self._style.element_create(f\"{key}.Combobox.downarrow\",\n                                   \"image\",\n                                   images[\"arrow_normal\"],\n                                   (\"active\", images[\"arrow_active\"]),\n                                   (\"pressed\", images[\"arrow_active\"]),\n                                   sticky=\"e\",\n                                   width=20)\n\n        # None of the themes give us the border control we need, so create an image\n        box = self._images.get_image((16, 16),\n                                     field_color,\n                                     border_width=1,\n                                     border_color=field_border)\n        self._style.element_create(f\"{key}.Combobox.field\",\n                                   \"image\",\n                                   box,\n                                   border=1,\n                                   padding=(6, 0, 0, 0))\n\n        # Set a layout so we can access required params\n        self._style.layout(f\"{key}.TCombobox\", [\n            (f\"{key}.Combobox.field\", {\n                \"children\": [\n                    (f\"{key}.Combobox.downarrow\", {\"side\": \"right\", \"sticky\": \"ns\"}),\n                    (f\"{key}.Combobox.padding\", {\n                        \"expand\": \"1\",\n                        \"sticky\": \"nswe\",\n                        \"children\": [(f\"{key}.Combobox.focus\", {\n                            \"expand\": \"1\",\n                            \"sticky\": \"nswe\",\n                            \"children\": [(f\"{key}.Combobox.textarea\", {\"sticky\": \"nswe\"})]})]})],\n                \"sticky\": \"nswe\"})])\n\n    def frame(self, key, background, border, borderwidth=1):\n        \"\"\" Create a custom frame widget for controlling background and border colors.\n\n        Parameters\n        ----------\n        key: str\n            The section that the Frame will belong to\n        background: str\n            The hex code for the background of the frame\n        border: str\n            The hex code for the border of the frame\n        \"\"\"\n        self._style.element_create(f\"{key}.Frame.border\", \"from\", \"alt\")\n        self._style.layout(f\"{key}.TFrame\",\n                           [(f\"{key}.Frame.border\", {\"sticky\": \"nswe\"})])\n        self._style.configure(f\"{key}.TFrame\",\n                              background=background,\n                              relief=tk.SOLID,\n                              borderwidth=borderwidth,\n                              bordercolor=border)\n\n    def notebook(self, key, frame_border, tab_color, tab_selected, tab_hover):\n        \"\"\" Create a custom notebook widget so we can control the colors.\n\n        Parameters\n        ----------\n        key: str\n            The section that the scrollbar will belong to\n        frame_border: str\n            The border color around the tab's contents\n        tab_color: str\n            The color of non selected tabs\n        tab_selected: str\n            The color of selected tabs\n        tab_hover: str\n            The color of hovered tabs\n        \"\"\"\n        # TODO This lags out the GUI, so need to test where this is failing prior to implementing\n        client = self._images.get_image((8, 8), frame_border)\n        self._style.element_create(f\"{key}.Notebook.client\", \"image\", client, border=1)\n\n        tabs = [self._images.get_image((8, 8), color)\n                for color in (tab_color, tab_selected, tab_hover)]\n\n        self._style.element_create(f\"{key}.Notebook.tab\",\n                                   \"image\",\n                                   tabs[0],\n                                   (\"selected\", tabs[1]),\n                                   (\"active\", tabs[2]),\n                                   padding=(0, 2, 0, 0),\n                                   border=3)\n\n        self._style.layout(f\"{key}.TNotebook\", [(f\"{key}.Notebook.client\", {\"sticky\": \"nswe\"})])\n        self._style.layout(f\"{key}.TNotebook.Tab\", [\n            (f\"{key}.Notebook.tab\", {\n                \"sticky\": \"nswe\",\n                \"children\": [\n                    (\"Notebook.padding\", {\n                        \"side\": \"top\",\n                        \"sticky\": \"nswe\",\n                        \"children\": [\n                            (\"Notebook.focus\", {\n                                \"side\": \"top\",\n                                \"sticky\": \"nswe\",\n                                \"children\": [(\"Notebook.label\", {\"side\": \"top\", \"sticky\": \"\"})]\n                            })]\n                    })]\n            })])\n\n        self._style.configure(f\"{key}.TNotebook\", tabmargins=(0, 2, 0, 0))\n        self._style.configure(f\"{key}.TNotebook.Tab\", padding=(6, 2, 6, 2), expand=(0, 0, 2))\n        self._style.configure(f\"{key}.TNotebook.Tab\", expand=(\"selected\", (1, 2, 4, 2)))\n\n    def scrollbar(self, key, trough_color, border_color, control_backgrounds, control_foregrounds,\n                  control_borders):\n        \"\"\" Create a custom scroll bar widget so we can control the colors.\n\n        Parameters\n        ----------\n        key: str\n            The section that the scrollbar will belong to\n        theme: dict\n            The theme options for a scroll bar. The dict should contain the keys: `background`,\n            `foreground`, `border`, with each item containing a tuple of the colors for the states\n            `normal`, `disabled` and `active` respectively\n        trough_color: str\n            The hex code for the scrollbar trough color\n        border_color: str\n            The hex code for the scrollbar border color\n        control_backgrounds: tuple\n            Tuple of length 3 for the button and slider colors for the states `normal`,\n            `disabled`, `active`\n        control_foregrounds: tuple\n            Tuple of length 3 for the button arrow colors for the states `normal`,\n            `disabled`, `active`\n        control_borders: tuple\n            Tuple of length 3 for the borders of the buttons and slider for the states `normal`,\n            `disabled`, `active`\n        \"\"\"\n        logger.debug(\"Creating scrollbar: (key: %s, trough_color: %s, border_color: %s, \"\n                     \"control_backgrounds: %s, control_foregrounds: %s, control_borders: %s)\",\n                     key, trough_color, border_color, control_backgrounds, control_foregrounds,\n                     control_borders)\n        images = {}\n        for idx, state in enumerate((\"normal\", \"disabled\", \"active\")):\n            # Create arrow and slider widgets for each state\n            img_args = ((16, 16), control_backgrounds[idx])\n            for dir_ in (\"up\", \"down\"):\n                images[f\"img_{dir_}_{state}\"] = self._images.get_image(\n                    *img_args,\n                    foreground=control_foregrounds[idx],\n                    pattern=\"arrow\",\n                    direction=dir_,\n                    thickness=4,\n                    border_width=1,\n                    border_color=control_borders[idx])\n            images[f\"img_thumb_{state}\"] = self._images.get_image(\n                *img_args,\n                border_width=1,\n                border_color=control_borders[idx])\n\n        for element in (\"thumb\", \"uparrow\", \"downarrow\"):\n            # Create the elements with the new images\n            lookup = element.replace(\"arrow\", \"\")\n            args = (f\"{key}.Vertical.Scrollbar.{element}\",\n                    \"image\",\n                    images[f\"img_{lookup}_normal\"],\n                    (\"disabled\", images[f\"img_{lookup}_disabled\"]),\n                    (\"pressed !disabled\", images[f\"img_{lookup}_active\"]),\n                    (\"active !disabled\", images[f\"img_{lookup}_active\"]))\n            kwargs = {\"border\": 1, \"sticky\": \"ns\"} if element == \"thumb\" else {}\n            self._style.element_create(*args, **kwargs)\n\n        # Get a configurable trough\n        self._style.element_create(f\"{key}.Vertical.Scrollbar.trough\", \"from\", \"clam\")\n\n        self._style.layout(\n            f\"{key}.Vertical.TScrollbar\",\n            [(f\"{key}.Vertical.Scrollbar.trough\", {\n                \"sticky\": \"ns\",\n                \"children\": [\n                    (f\"{key}.Vertical.Scrollbar.uparrow\", {\"side\": \"top\", \"sticky\": \"\"}),\n                    (f\"{key}.Vertical.Scrollbar.downarrow\", {\"side\": \"bottom\", \"sticky\": \"\"}),\n                    (f\"{key}.Vertical.Scrollbar.thumb\", {\"expand\": \"1\", \"sticky\": \"nswe\"})\n                ]\n            })])\n        self._style.configure(f\"{key}.Vertical.TScrollbar\",\n                              troughcolor=trough_color,\n                              bordercolor=border_color,\n                              troughrelief=tk.SOLID,\n                              troughborderwidth=1)\n\n    def slider(self, key, control_color, active_color, trough_color):\n        \"\"\" Take a copy of the default ttk.Scale widget and replace the slider element with a\n        version we can control the color and shape of.\n\n        Parameters\n        ----------\n        key: str\n            The section that the slider will belong to\n        control_color: str\n            The color of inactive slider and up down buttons\n        active_color: str\n            The color of slider and up down buttons when they are hovered or pressed\n        trough_color: str\n            The color of the scroll bar's trough\n        \"\"\"\n        img_slider = self._images.get_image((10, 25), control_color)\n        img_slider_alt = self._images.get_image((10, 25), active_color)\n\n        self._style.element_create(f\"{key}.Horizontal.Scale.trough\", \"from\", \"alt\")\n        self._style.element_create(f\"{key}.Horizontal.Scale.slider\",\n                                   \"image\",\n                                   img_slider,\n                                   (\"active\", img_slider_alt))\n\n        self._style.layout(\n            f\"{key}.Horizontal.TScale\",\n            [(f\"{key}.Scale.focus\", {\n                \"expand\": \"1\",\n                \"sticky\": \"nswe\",\n                \"children\": [\n                    (f\"{key}.Horizontal.Scale.trough\", {\n                        \"expand\": \"1\",\n                        \"sticky\": \"nswe\",\n                        \"children\": [\n                            (f\"{key}.Horizontal.Scale.track\", {\"sticky\": \"we\"}),\n                            (f\"{key}.Horizontal.Scale.slider\", {\"side\": \"left\", \"sticky\": \"\"})\n                            ]\n                        })\n                ]\n            })])\n\n        self._style.configure(f\"{key}.Horizontal.TScale\",\n                              background=trough_color,\n                              groovewidth=4,\n                              troughcolor=trough_color)\n\n\nclass _TkImage():\n    \"\"\" Create a tk image for a given pattern and shape.\n    \"\"\"\n    def __init__(self):\n        self._cache = []  # We need to keep a reference to every image created\n\n    # Numpy array patterns\n    @classmethod\n    def _get_solid(cls, dimensions):\n        \"\"\" Return a solid background color pattern.\n\n        Parameters\n        ----------\n        dimensions: tuple\n            The (`width`, `height`) of the desired tk image\n\n        Returns\n        -------\n        :class:`numpy.ndarray`\n            A 2D, UINT8 array of shape (height, width) of all zeros\n        \"\"\"\n        return np.zeros((dimensions[1], dimensions[0]), dtype=\"uint8\")\n\n    @classmethod\n    def _get_arrow(cls, dimensions, thickness, direction):\n        \"\"\" Return a background color with a \"v\" arrow in foreground color\n\n        Parameters\n        ----------\n        dimensions: tuple\n            The (`width`, `height`) of the desired tk image\n        thickness: int\n            The thickness of the pattern to be drawn\n        direction: [\"left\", \"up\", \"right\", \"down\"]\n            The direction that the pattern should be facing\n\n        Returns\n        -------\n        :class:`numpy.ndarray`\n            A 2D, UINT8 array of shape (height, width) of all zeros\n        \"\"\"\n        square_size = min(dimensions[1], dimensions[0])\n        if square_size < 16 or any(dim % 2 != 0 for dim in dimensions):\n            raise FaceswapError(\"For arrow image, the minimum size across any axis must be 8 and \"\n                                \"dimensions must all be divisible by 2\")\n        crop_size = (square_size // 16) * 16\n        draw_rows = int(6 * crop_size / 16)\n        start_row = dimensions[1] // 2 - draw_rows // 2\n        initial_indent = 2 * (crop_size // 16) + (dimensions[0] - crop_size) // 2\n\n        retval = np.zeros((dimensions[1], dimensions[0]), dtype=\"uint8\")\n        for i in range(start_row, start_row + draw_rows):\n            indent = initial_indent + i - start_row\n            join = (min(indent + thickness, dimensions[0] // 2),\n                    max(dimensions[0] - indent - thickness, dimensions[0] // 2))\n            retval[i, np.r_[indent:join[0], join[1]:dimensions[0] - indent]] = 1\n        if direction in (\"right\", \"left\"):\n            retval = np.rot90(retval)\n        if direction in (\"up\", \"left\"):\n            retval = np.flip(retval)\n        return retval\n\n    def get_image(self,\n                  dimensions,\n                  background,\n                  foreground=None,\n                  pattern=\"solid\",\n                  border_width=0,\n                  border_color=None,\n                  thickness=2,\n                  direction=\"down\"):\n        \"\"\" Obtain a tk image.\n\n        Generates the requested image and stores in cache.\n\n        Parameters\n        ----------\n        dimensions: tuple\n            The (`width`, `height`) of the desired tk image\n        background: str\n            The hex code for the background (main) color\n        foreground: str, optional\n            The hex code for the background (secondary) color. If ``None`` is provided then a\n            solid background color image will be returned. Default: ``None``\n        pattern: [\"solid\", \"arrow\"], optional\n            The pattern to generate for the tk image. Default: `\"solid\"`\n        border_width: int, optional\n            The thickness of foreground border to apply. Default: 0\n        border_color: int, optional\n            The color of the border, if one is to be created. Default: ``None`` (use foreground\n            color)\n        thickness: int, optional\n            The thickness of the pattern to be drawn. Default: `2`\n        direction: [\"left\", \"up\", \"right\", \"down\"], optional\n            The direction that the pattern should be facing. Default: `\"down\"`\n        \"\"\"\n        foreground = foreground if foreground else background\n        border_color = border_color if border_color else foreground\n\n        args = [dimensions]\n        if pattern.lower() == \"arrow\":\n            args.extend([thickness, direction])\n        if pattern.lower() == \"border\":\n            args.extend([thickness])\n        pattern = getattr(self, f\"_get_{pattern.lower()}\")(*args)\n\n        if border_width > 0:\n            border = np.ones_like(pattern) + 1\n            border[border_width:-border_width,\n                   border_width:-border_width] = pattern[border_width:-border_width,\n                                                         border_width:-border_width]\n            pattern = border\n\n        return self._create_photoimage(background, foreground, border_color, pattern)\n\n    def _create_photoimage(self, background, foreground, border, pattern):\n        \"\"\" Create a tkinter PhotoImage and populate it with the requested color pattern.\n\n        Parameters\n        ----------\n        background: str\n            The hex code for the background (main) color\n        foreground: str\n            The hex code for the foreground (secondary) color\n        border: str\n            The hex code for the border color\n        pattern: class:`numpy.ndarray`\n            The pattern for the final image with background colors marked as 0 and foreground\n            colors marked as 1\n        \"\"\"\n        image = tk.PhotoImage(width=pattern.shape[1], height=pattern.shape[0])\n        self._cache.append(image)\n\n        pixels = \"} {\".join(\" \".join(foreground\n                                     if pxl == 1 else border if pxl == 2 else background\n                                     for pxl in row)\n                            for row in pattern)\n        image.put(\"{\" + pixels + \"}\")\n        return image\n", "lib/gui/_config.py": "#!/usr/bin/env python3\n\"\"\" Default configurations for models \"\"\"\n\nimport logging\nimport sys\nimport os\nfrom tkinter import font as tk_font\nfrom matplotlib import font_manager\n\nfrom lib.config import FaceswapConfig\n\nlogger = logging.getLogger(__name__)\n\n\nclass Config(FaceswapConfig):\n    \"\"\" Config File for GUI \"\"\"\n    # pylint:disable=too-many-statements\n    def set_defaults(self):\n        \"\"\" Set the default values for config \"\"\"\n        logger.debug(\"Setting defaults\")\n        self.set_globals()\n\n    def set_globals(self):\n        \"\"\"\n        Set the global options for GUI\n        \"\"\"\n        logger.debug(\"Setting global config\")\n        section = \"global\"\n        self.add_section(section,\n                         \"Faceswap GUI Options.\\nConfigure the appearance and behaviour of \"\n                         \"the GUI\")\n        self.add_item(\n            section=section, title=\"fullscreen\", datatype=bool, default=False, group=\"startup\",\n            info=\"Start Faceswap maximized.\")\n        self.add_item(\n            section=section, title=\"tab\", datatype=str, default=\"extract\", group=\"startup\",\n            choices=get_commands(),\n            info=\"Start Faceswap in this tab.\")\n        self.add_item(\n            section=section, title=\"options_panel_width\", datatype=int, default=30,\n            min_max=(10, 90), rounding=1, group=\"layout\",\n            info=\"How wide the lefthand option panel is as a percentage of GUI width at startup.\")\n        self.add_item(\n            section=section, title=\"console_panel_height\", datatype=int, default=20,\n            min_max=(10, 90), rounding=1, group=\"layout\",\n            info=\"How tall the bottom console panel is as a percentage of GUI height at startup.\")\n        self.add_item(\n            section=section, title=\"icon_size\", datatype=int, default=14,\n            min_max=(10, 20), rounding=1, group=\"layout\",\n            info=\"Pixel size for icons. NB: Size is scaled by DPI.\")\n        self.add_item(\n            section=section, title=\"font\", datatype=str,\n            choices=get_clean_fonts(),\n            default=\"default\", group=\"font\", info=\"Global font\")\n        self.add_item(\n            section=section, title=\"font_size\", datatype=int, default=9,\n            min_max=(6, 12), rounding=1, group=\"font\",\n            info=\"Global font size.\")\n        self.add_item(\n            section=section, title=\"autosave_last_session\", datatype=str, default=\"prompt\",\n            choices=[\"never\", \"prompt\", \"always\"], group=\"startup\", gui_radio=True,\n            info=\"Automatically save the current settings on close and reload on startup\"\n                 \"\\n\\tnever - Don't autosave session\"\n                 \"\\n\\tprompt - Prompt to reload last session on launch\"\n                 \"\\n\\talways - Always load last session on launch\")\n        self.add_item(\n            section=section, title=\"timeout\", datatype=int, default=120,\n            min_max=(10, 600), rounding=10, group=\"behaviour\",\n            info=\"Training can take some time to save and shutdown. Set the timeout in seconds \"\n                 \"before giving up and force quitting.\")\n        self.add_item(\n            section=section, title=\"auto_load_model_stats\", datatype=bool, default=True,\n            group=\"behaviour\",\n            info=\"Auto load model statistics into the Analysis tab when selecting a model \"\n                 \"in Train or Convert tabs.\")\n\n\ndef get_commands():\n    \"\"\" Return commands formatted for GUI \"\"\"\n    root_path = os.path.abspath(os.path.dirname(sys.argv[0]))\n    command_path = os.path.join(root_path, \"scripts\")\n    tools_path = os.path.join(root_path, \"tools\")\n    commands = [os.path.splitext(item)[0] for item in os.listdir(command_path)\n                if os.path.splitext(item)[1] == \".py\"\n                and os.path.splitext(item)[0] not in (\"gui\", \"fsmedia\")\n                and not os.path.splitext(item)[0].startswith(\"_\")]\n    tools = [os.path.splitext(item)[0] for item in os.listdir(tools_path)\n             if os.path.splitext(item)[1] == \".py\"\n             and os.path.splitext(item)[0] not in (\"gui\", \"cli\")\n             and not os.path.splitext(item)[0].startswith(\"_\")]\n    return commands + tools\n\n\ndef get_clean_fonts():\n    \"\"\" Return a sane list of fonts for the system that has both regular and bold variants.\n\n    Pre-pend \"default\" to the beginning of the list.\n\n    Returns\n    -------\n    list:\n        A list of valid fonts for the system\n    \"\"\"\n    fmanager = font_manager.FontManager()\n    fonts = {}\n    for font in fmanager.ttflist:\n        if str(font.weight) in (\"400\", \"normal\", \"regular\"):\n            fonts.setdefault(font.name, {})[\"regular\"] = True\n        if str(font.weight) in (\"700\", \"bold\"):\n            fonts.setdefault(font.name, {})[\"bold\"] = True\n    valid_fonts = {key for key, val in fonts.items() if len(val) == 2}\n    retval = sorted(list(valid_fonts.intersection(tk_font.families())))\n    if not retval:\n        # Return the font list with any @prefixed or non-Unicode characters stripped and default\n        # prefixed\n        logger.debug(\"No bold/regular fonts found. Running simple filter\")\n        retval = sorted([fnt for fnt in tk_font.families()\n                         if not fnt.startswith(\"@\") and not any(ord(c) > 127 for c in fnt)])\n    return [\"default\"] + retval\n", "lib/gui/custom_widgets.py": "#!/usr/bin/env python3\n\"\"\" Custom widgets for Faceswap GUI \"\"\"\n\nimport logging\nimport platform\nimport re\nimport sys\nimport typing as T\nimport tkinter as tk\nfrom tkinter import ttk, TclError\n\nimport numpy as np\n\nfrom .utils import get_config\n\nlogger = logging.getLogger(__name__)\n\n\nclass ContextMenu(tk.Menu):  # pylint:disable=too-many-ancestors\n    \"\"\" A Pop up menu to be triggered when right clicking on widgets that this menu has been\n    applied to.\n\n    This widget provides a simple right click pop up menu to the widget passed in with `Cut`,\n    `Copy`, `Paste` and `Select all` menu items.\n\n    Parameters\n    ----------\n    widget: tkinter object\n        The widget to apply the :class:`ContextMenu` to\n\n    Example\n    -------\n    >>> text_box = ttk.Entry(parent)\n    >>> text_box.pack()\n    >>> right_click_menu = ContextMenu(text_box)\n    >>> right_click_menu.cm_bind()\n    \"\"\"\n    def __init__(self, widget):\n        logger.debug(\"Initializing %s: (widget_class: '%s')\",\n                     self.__class__.__name__, widget.winfo_class())\n        super().__init__(tearoff=0)\n        self._widget = widget\n        self._standard_actions()\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    def _standard_actions(self):\n        \"\"\" Standard menu actions \"\"\"\n        self.add_command(label=\"Cut\", command=lambda: self._widget.event_generate(\"<<Cut>>\"))\n        self.add_command(label=\"Copy\", command=lambda: self._widget.event_generate(\"<<Copy>>\"))\n        self.add_command(label=\"Paste\", command=lambda: self._widget.event_generate(\"<<Paste>>\"))\n        self.add_separator()\n        self.add_command(label=\"Select all\", command=self._select_all)\n\n    def cm_bind(self):\n        \"\"\" Bind the menu to the given widgets Right Click event\n\n        After associating a widget with this :class:`ContextMenu` this function should be called\n        to bind it to the right click button\n        \"\"\"\n        button = \"<Button-2>\" if platform.system() == \"Darwin\" else \"<Button-3>\"\n        logger.debug(\"Binding '%s' to '%s'\", button, self._widget.winfo_class())\n        self._widget.bind(button, lambda event: self.tk_popup(event.x_root, event.y_root))\n\n    def _select_all(self):\n        \"\"\" Select all for Text or Entry widgets \"\"\"\n        logger.debug(\"Selecting all for '%s'\", self._widget.winfo_class())\n        if self._widget.winfo_class() == \"Text\":\n            self._widget.focus_force()\n            self._widget.tag_add(\"sel\", \"1.0\", \"end\")\n        else:\n            self._widget.focus_force()\n            self._widget.select_range(0, tk.END)\n\n\nclass RightClickMenu(tk.Menu):  # pylint:disable=too-many-ancestors\n    \"\"\" A Pop up menu that can be bound to a right click mouse event to bring up a context menu\n\n    Parameters\n    ----------\n    labels: list\n        A list of label titles that will appear in the right click menu\n    actions: list\n        A list of python functions that are called when the corresponding label is clicked on\n    hotkeys: list, optional\n        The hotkeys corresponding to the labels. If using hotkeys, then there must be an entry in\n        the list for every label even if they don't all use hotkeys. Labels without a hotkey can be\n        an empty string or ``None``. Passing ``None`` instead of a list means that no actions will\n        be given hotkeys. NB: The hotkey is not bound by this class, that needs to be done in code.\n        Giving hotkeys here means that they will be displayed in the menu though. Default: ``None``\n    \"\"\"\n    # TODO This should probably be merged with Context Menu\n    def __init__(self, labels, actions, hotkeys=None):\n        logger.debug(\"Initializing %s: (labels: %s, actions: %s)\", self.__class__.__name__, labels,\n                     actions)\n        super().__init__(tearoff=0)\n        self._labels = labels\n        self._actions = actions\n        self._hotkeys = hotkeys\n        self._create_menu()\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    def _create_menu(self):\n        \"\"\" Create the menu based on :attr:`_labels` and :attr:`_actions`. \"\"\"\n        for idx, (label, action) in enumerate(zip(self._labels, self._actions)):\n            kwargs = {\"label\": label, \"command\": action}\n            if isinstance(self._hotkeys, (list, tuple)) and self._hotkeys[idx]:\n                kwargs[\"accelerator\"] = self._hotkeys[idx]\n            self.add_command(**kwargs)\n\n    def popup(self, event):\n        \"\"\" Pop up the right click menu.\n\n        Parameters\n        ----------\n        event: class:`tkinter.Event`\n            The tkinter mouse event calling this popup\n        \"\"\"\n        self.tk_popup(event.x_root, event.y_root)\n\n\nclass ConsoleOut(ttk.Frame):  # pylint:disable=too-many-ancestors\n    \"\"\" The Console out section of the GUI.\n\n    A Read only text box for displaying the output from stdout/stderr.\n\n    All handling is internal to this method. To clear the console, the stored tkinter variable in\n    :attr:`~lib.gui.Config.tk_vars` ``console_clear`` should be triggered.\n\n    Parameters\n    ----------\n    parent: tkinter object\n        The Console's parent widget\n    debug: bool\n        ``True`` if console output should not be directed to this widget otherwise ``False``\n    \"\"\"\n\n    def __init__(self, parent, debug):\n        logger.debug(\"Initializing %s: (parent: %s, debug: %s)\",\n                     self.__class__.__name__, parent, debug)\n        super().__init__(parent, relief=tk.SOLID, padding=1, style=\"Console.TFrame\")\n        self._theme = get_config().user_theme[\"console\"]\n        self._console = _ReadOnlyText(self, relief=tk.FLAT)\n        rc_menu = ContextMenu(self._console)\n        rc_menu.cm_bind()\n        self._console_clear = get_config().tk_vars.console_clear\n        self._set_console_clear_var_trace()\n        self._debug = debug\n        self._build_console()\n        self._add_tags()\n        self.pack(side=tk.TOP, anchor=tk.W, padx=10, pady=(2, 0),\n                  fill=tk.BOTH, expand=True)\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    def _set_console_clear_var_trace(self):\n        \"\"\" Set a trace on the console clear tkinter variable to trigger :func:`_clear` \"\"\"\n        logger.debug(\"Set clear trace\")\n        self._console_clear.trace(\"w\", self._clear)\n\n    def _build_console(self):\n        \"\"\" Build and place the console  and add stdout/stderr redirection \"\"\"\n        logger.debug(\"Build console\")\n        self._console.config(width=100,\n                             height=6,\n                             bg=self._theme[\"background_color\"],\n                             fg=self._theme[\"stdout_color\"])\n\n        scrollbar = ttk.Scrollbar(self,\n                                  command=self._console.yview,\n                                  style=\"Console.Vertical.TScrollbar\")\n        self._console.configure(yscrollcommand=scrollbar.set)\n\n        scrollbar.pack(side=tk.RIGHT, fill=\"y\")\n        self._console.pack(side=tk.LEFT, anchor=tk.N, fill=tk.BOTH, expand=True)\n        self._redirect_console()\n        logger.debug(\"Built console\")\n\n    def _add_tags(self):\n        \"\"\" Add tags to text widget to color based on output \"\"\"\n        logger.debug(\"Adding text color tags\")\n        self._console.tag_config(\"default\", foreground=self._theme[\"stdout_color\"])\n        self._console.tag_config(\"stderr\", foreground=self._theme[\"stderr_color\"])\n        self._console.tag_config(\"info\", foreground=self._theme[\"info_color\"])\n        self._console.tag_config(\"verbose\", foreground=self._theme[\"verbose_color\"])\n        self._console.tag_config(\"warning\", foreground=self._theme[\"warning_color\"])\n        self._console.tag_config(\"critical\", foreground=self._theme[\"critical_color\"])\n        self._console.tag_config(\"error\", foreground=self._theme[\"error_color\"])\n\n    def _redirect_console(self):\n        \"\"\" Redirect stdout/stderr to console Text Box \"\"\"\n        logger.debug(\"Redirect console\")\n        if self._debug:\n            logger.info(\"Console debug activated. Outputting to main terminal\")\n        else:\n            sys.stdout = _SysOutRouter(self._console, \"stdout\")\n            sys.stderr = _SysOutRouter(self._console, \"stderr\")\n        logger.debug(\"Redirected console\")\n\n    def _clear(self, *args):  # pylint:disable=unused-argument\n        \"\"\" Clear the console output screen \"\"\"\n        logger.debug(\"Clear console\")\n        if not self._console_clear.get():\n            logger.debug(\"Console not set for clearing. Skipping\")\n            return\n        self._console.delete(1.0, tk.END)\n        self._console_clear.set(False)\n        logger.debug(\"Cleared console\")\n\n\nclass _ReadOnlyText(tk.Text):  # pylint:disable=too-many-ancestors\n    \"\"\" A read only text widget.\n\n    Standard tkinter Text widgets are read/write by default. As we want to make the console\n    display writable by the Faceswap process but not the user, we need to redirect its insert and\n    delete attributes.\n\n    Source: https://stackoverflow.com/questions/3842155\n    \"\"\"\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.redirector = _WidgetRedirector(self)\n        self.insert = self.redirector.register(\"insert\", lambda *args, **kw: \"break\")\n        self.delete = self.redirector.register(\"delete\", lambda *args, **kw: \"break\")\n\n\nclass _SysOutRouter():\n    \"\"\" Route stdout/stderr to the given text box.\n\n    Parameters\n    ----------\n    console: tkinter Object\n        The widget that will receive the output from stderr/stdout\n    out_type: ['stdout', 'stderr']\n        The output type to redirect\n    \"\"\"\n\n    def __init__(self, console, out_type):\n        logger.debug(\"Initializing %s: (console: %s, out_type: '%s')\",\n                     self.__class__.__name__, console, out_type)\n        self._console = console\n        self._out_type = out_type\n        self._recolor = re.compile(r\".+?(\\s\\d+:\\d+:\\d+\\s)(?P<lvl>[A-Z]+)\\s\")\n        self._ansi_escape = re.compile(r\"\\x1B(?:[@-Z\\\\-_]|\\[[0-?]*[ -/]*[@-~])\")\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    def _get_tag(self, string):\n        \"\"\" Set the tag based on regex of log output \"\"\"\n        if self._out_type == \"stderr\":\n            # Output all stderr in red\n            return self._out_type\n\n        output = self._recolor.match(string)\n        if not output:\n            return \"default\"\n        tag = output.groupdict()[\"lvl\"].strip().lower()\n        return tag\n\n    def write(self, string):\n        \"\"\" Capture stdout/stderr \"\"\"\n        string = self._ansi_escape.sub(\"\", string)\n        self._console.insert(tk.END, string, self._get_tag(string))\n        self._console.see(tk.END)\n\n    @staticmethod\n    def flush():\n        \"\"\" If flush is forced, send it to normal terminal \"\"\"\n        sys.__stdout__.flush()\n\n\nclass _WidgetRedirector:\n    \"\"\"Support for redirecting arbitrary widget sub-commands.\n\n    Some Tk operations don't normally pass through tkinter.  For example, if a\n    character is inserted into a Text widget by pressing a key, a default Tk\n    binding to the widget's 'insert' operation is activated, and the Tk library\n    processes the insert without calling back into tkinter.\n\n    Although a binding to <Key> could be made via tkinter, what we really want\n    to do is to hook the Tk 'insert' operation itself.  For one thing, we want\n    a text.insert call in idle code to have the same effect as a key press.\n\n    When a widget is instantiated, a Tcl command is created whose name is the\n    same as the path name widget._w.  This command is used to invoke the various\n    widget operations, e.g. insert (for a Text widget). We are going to hook\n    this command and provide a facility ('register') to intercept the widget\n    operation.  We will also intercept method calls on the tkinter class\n    instance that represents the tk widget.\n\n    In IDLE, WidgetRedirector is used in Percolator to intercept Text\n    commands.  The function being registered provides access to the top\n    of a Percolator chain.  At the bottom of the chain is a call to the\n    original Tk widget operation.\n\n    Attributes\n    -----------\n    _operations: dict\n        Dictionary mapping operation name to new function. widget: the widget whose tcl command\n        is to be intercepted.\n    tk: widget.tk\n        A convenience attribute, probably not needed.\n    orig: str\n        new name of the original tcl command.\n\n    Notes\n    -----\n    Since renaming to orig fails with TclError when orig already exists, only one\n    WidgetDirector can exist for a given widget.\n    \"\"\"\n    def __init__(self, widget):\n        self._operations = {}\n        self.widget = widget                                # widget instance\n        self.tk_ = tk_ = widget.tk                          # widget's root\n        wgt = widget._w  # pylint:disable=protected-access  # widget's (full) Tk pathname\n        self.orig = wgt + \"_orig\"\n        # Rename the Tcl command within Tcl:\n        tk_.call(\"rename\", wgt, self.orig)\n        # Create a new Tcl command whose name is the widget's path name, and\n        # whose action is to dispatch on the operation passed to the widget:\n        tk_.createcommand(wgt, self.dispatch)\n\n    def __repr__(self):\n        return (f\"{self.__class__.__name__}({self.widget.__class__.__name__}\"\n                f\"<{self.widget._w}>)\")  # pylint:disable=protected-access\n\n    def close(self):\n        \"de-register operations and revert redirection created by .__init__.\"\n        for operation in list(self._operations):\n            self.unregister(operation)\n        widget = self.widget\n        tk_ = widget.tk\n        wgt = widget._w  # pylint:disable=protected-access\n        # Restore the original widget Tcl command.\n        tk_.deletecommand(wgt)\n        tk_.call(\"rename\", self.orig, wgt)\n        del self.widget, self.tk_  # Should not be needed\n        # if instance is deleted after close, as in Percolator.\n\n    def register(self, operation, function):\n        \"\"\"Return _OriginalCommand(operation) after registering function.\n\n        Registration adds an operation: function pair to ._operations.\n        It also adds a widget function attribute that masks the tkinter\n        class instance method.  Method masking operates independently\n        from command dispatch.\n\n        If a second function is registered for the same operation, the\n        first function is replaced in both places.\n        \"\"\"\n        self._operations[operation] = function\n        setattr(self.widget, operation, function)\n        return _OriginalCommand(self, operation)\n\n    def unregister(self, operation):\n        \"\"\"Return the function for the operation, or None.\n\n        Deleting the instance attribute unmasks the class attribute.\n        \"\"\"\n        if operation in self._operations:\n            function = self._operations[operation]\n            del self._operations[operation]\n            try:\n                delattr(self.widget, operation)\n            except AttributeError:\n                pass\n            return function\n        return None\n\n    def dispatch(self, operation, *args):\n        \"\"\"Callback from Tcl which runs when the widget is referenced.\n\n        If an operation has been registered in self._operations, apply the\n        associated function to the args passed into Tcl. Otherwise, pass the\n        operation through to Tk via the original Tcl function.\n\n        Note that if a registered function is called, the operation is not\n        passed through to Tk.  Apply the function returned by self.register()\n        to *args to accomplish that.\n\n        \"\"\"\n        op_ = self._operations.get(operation)\n        try:\n            if op_:\n                return op_(*args)\n            return self.tk_.call((self.orig, operation) + args)\n        except TclError:\n            return \"\"\n\n\nclass _OriginalCommand:\n    \"\"\"Callable for original tk command that has been redirected.\n\n    Returned by .register; can be used in the function registered.\n    redirect = WidgetRedirector(text)\n    def my_insert(*args):\n        print(\"insert\", args)\n        original_insert(*args)\n    original_insert = redirect.register(\"insert\", my_insert)\n    \"\"\"\n\n    def __init__(self, redirect, operation):\n        \"\"\"Create .tk_call and .orig_and_operation for .__call__ method.\n\n        .redirect and .operation store the input args for __repr__.\n        .tk and .orig copy attributes of .redirect (probably not needed).\n        \"\"\"\n        self.redirect = redirect\n        self.operation = operation\n        self.tk_ = redirect.tk_  # redundant with self.redirect\n        self.orig = redirect.orig  # redundant with self.redirect\n        # These two could be deleted after checking recipient code.\n        self.tk_call = redirect.tk_.call\n        self.orig_and_operation = (redirect.orig, operation)\n\n    def __repr__(self):\n        return f\"{self.__class__.__name__}({self.redirect}, {self.operation})\"\n\n    def __call__(self, *args):\n        return self.tk_call(self.orig_and_operation + args)\n\n\nclass StatusBar(ttk.Frame):  # pylint:disable=too-many-ancestors\n    \"\"\" Status Bar for displaying the Status Message and  Progress Bar at the bottom of the GUI.\n\n    Parameters\n    ----------\n    parent: tkinter object\n        The parent tkinter widget that will hold the status bar\n    hide_status: bool, optional\n        ``True`` to hide the status message that appears at the far left hand side of the status\n        frame otherwise ``False``. Default: ``False``\n    \"\"\"\n\n    def __init__(self, parent: ttk.Frame, hide_status: bool = False) -> None:\n        super().__init__(parent)\n        self._frame = ttk.Frame(self)\n        self._message = tk.StringVar()\n        self._pbar_message = tk.StringVar()\n        self._pbar_position = tk.IntVar()\n        self._mode: T.Literal[\"indeterminate\", \"determinate\"] = \"determinate\"\n\n        self._message.set(\"Ready\")\n\n        self._status(hide_status)\n        self._pbar = self._progress_bar()\n        self.pack(side=tk.BOTTOM, fill=tk.X, expand=False)\n        self._frame.pack(padx=10, pady=2, fill=tk.X, expand=False)\n\n    @property\n    def message(self) -> tk.StringVar:\n        \"\"\":class:`tkinter.StringVar`: The variable to hold the status bar message on the left\n        hand side of the status bar. \"\"\"\n        return self._message\n\n    def _status(self, hide_status: bool) -> None:\n        \"\"\" Place Status label into left of the status bar.\n\n        Parameters\n        ----------\n        hide_status: bool, optional\n            ``True`` to hide the status message that appears at the far left hand side of the\n            status frame otherwise ``False``\n        \"\"\"\n        if hide_status:\n            return\n\n        statusframe = ttk.Frame(self._frame)\n        statusframe.pack(side=tk.LEFT, anchor=tk.W, fill=tk.X, expand=False)\n\n        lbltitle = ttk.Label(statusframe, text=\"Status:\", width=6, anchor=tk.W)\n        lbltitle.pack(side=tk.LEFT, expand=False)\n\n        lblstatus = ttk.Label(statusframe,\n                              width=40,\n                              textvariable=self._message,\n                              anchor=tk.W)\n        lblstatus.pack(side=tk.LEFT, anchor=tk.W, fill=tk.X, expand=True)\n\n    def _progress_bar(self) -> ttk.Progressbar:\n        \"\"\" Place progress bar into right of the status bar.\n\n        Returns\n        -------\n        :class:`tkinter.ttk.Progressbar`\n            The progress bar object\n        \"\"\"\n        progressframe = ttk.Frame(self._frame)\n        progressframe.pack(side=tk.RIGHT, anchor=tk.E, fill=tk.X)\n\n        lblmessage = ttk.Label(progressframe, textvariable=self._pbar_message)\n        lblmessage.pack(side=tk.LEFT, padx=3, fill=tk.X, expand=True)\n\n        pbar = ttk.Progressbar(progressframe,\n                               length=200,\n                               variable=self._pbar_position,\n                               maximum=100,\n                               mode=self._mode)\n        pbar.pack(side=tk.LEFT, padx=2, fill=tk.X, expand=True)\n        pbar.pack_forget()\n        return pbar\n\n    def start(self, mode: T.Literal[\"indeterminate\", \"determinate\"]) -> None:\n        \"\"\" Set progress bar mode and display,\n\n        Parameters\n        ----------\n        mode: [\"indeterminate\", \"determinate\"]\n            The mode that the progress bar should be executed in\n        \"\"\"\n        self._set_mode(mode)\n        self._pbar.pack()\n\n    def stop(self) -> None:\n        \"\"\" Reset progress bar and hide \"\"\"\n        self._pbar_message.set(\"\")\n        self._pbar_position.set(0)\n        self._mode = \"determinate\"\n        self._set_mode(self._mode)\n        self._pbar.pack_forget()\n\n    def _set_mode(self, mode: T.Literal[\"indeterminate\", \"determinate\"]) -> None:\n        \"\"\" Set the progress bar mode\n\n        Parameters\n        ----------\n        mode: [\"indeterminate\", \"determinate\"]\n            The mode that the progress bar should be executed in\n        \"\"\"\n        self._mode = mode\n        self._pbar.config(mode=self._mode)\n        if mode == \"indeterminate\":\n            self._pbar.config(maximum=100)\n            self._pbar.start()\n        else:\n            self._pbar.stop()\n            self._pbar.config(maximum=100)\n\n    def set_mode(self, mode: T.Literal[\"indeterminate\", \"determinate\"]) -> None:\n        \"\"\" Set the mode of a currently displayed progress bar and reset position to 0.\n\n        If the given mode is the same as the currently configured mode, returns without performing\n        any action.\n\n        Parameters\n        ----------\n        mode: [\"indeterminate\", \"determinate\"]\n            The mode that the progress bar should be set to\n        \"\"\"\n        if mode == self._mode:\n            return\n        self.stop()\n        self.start(mode)\n\n    def progress_update(self, message: str, position: int, update_position: bool = True) -> None:\n        \"\"\" Update the GUIs progress bar and position.\n\n        Parameters\n        ----------\n        message: str\n            The message to display next to the progress bar\n        position: int\n            The position that the progress bar should be set to\n        update_position: bool, optional\n            If ``True`` then the progress bar will be updated to the position given in\n            :attr:`position`. If ``False`` the progress bar will not be updates. Default: ``True``\n        \"\"\"\n        self._pbar_message.set(message)\n        if update_position:\n            self._pbar_position.set(position)\n\n\nclass Tooltip:  # pylint:disable=too-few-public-methods\n    \"\"\" Create a tooltip for a given widget as the mouse goes on it.\n\n    Parameters\n    ----------\n    widget: tkinter object\n        The widget to apply the tool-tip to\n    pad: tuple, optional\n        (left, top, right, bottom) padding for the tool-tip. Default: (5, 3, 5, 3)\n    text: str, optional\n        The text to be displayed in the tool-tip. Default: 'widget info'\n    text_variable: :class:`tkinter.strVar`, optional\n        The text variable to use for dynamic help text. Appended after the contents of :attr:`text`\n        if provided. Default: ``None``\n    wait_time: int, optional\n        The time in milliseconds to wait before showing the tool-tip. Default: 400\n    wrap_length: int, optional\n        The text length for each line before wrapping. Default: 250\n\n    Example\n    -------\n    >>> button = ttk.Button(parent, text=\"Exit\")\n    >>> Tooltip(button, text=\"Click to exit\")\n    >>> button.pack()\n\n    Notes\n    -----\n    Adapted from StackOverflow: http://stackoverflow.com/questions/3221956 and\n    http://www.daniweb.com/programming/software-development/code/484591/a-tooltip-class-for-tkinter\n    \"\"\"\n    def __init__(self, widget, *, pad=(5, 3, 5, 3), text=\"widget info\",\n                 text_variable=None, wait_time=400, wrap_length=250):\n\n        self._waittime = wait_time  # in milliseconds, originally 500\n        self.wrap_length = wrap_length  # in pixels, originally 180\n        self._widget = widget\n        self._text = text\n        self._text_variable = text_variable\n        self._widget.bind(\"<Enter>\", self._on_enter)\n        self._widget.bind(\"<Leave>\", self._on_leave)\n        self._widget.bind(\"<ButtonPress>\", self._on_leave)\n        self._theme = get_config().user_theme[\"tooltip\"]\n        self._pad = pad\n        self._ident = None\n        self._topwidget = None\n\n    def _on_enter(self, event=None):  # pylint:disable=unused-argument\n        \"\"\" Schedule on an enter event \"\"\"\n        self._schedule()\n\n    def _on_leave(self, event=None):  # pylint:disable=unused-argument\n        \"\"\" remove schedule on a leave event \"\"\"\n        self._unschedule()\n        self._hide()\n\n    def _schedule(self):\n        \"\"\" Show the tooltip after wait period \"\"\"\n        self._unschedule()\n        self._ident = self._widget.after(self._waittime, self._show)\n\n    def _unschedule(self):\n        \"\"\" Hide the tooltip \"\"\"\n        id_ = self._ident\n        self._ident = None\n        if id_:\n            self._widget.after_cancel(id_)\n\n    def _show(self):\n        \"\"\" Show the tooltip \"\"\"\n        def tip_pos_calculator(widget, label,\n                               *,\n                               tip_delta=(10, 5), pad=(5, 3, 5, 3)):\n            \"\"\" Calculate the tooltip position \"\"\"\n\n            s_width, s_height = widget.winfo_screenwidth(), widget.winfo_screenheight()\n\n            width, height = (pad[0] + label.winfo_reqwidth() + pad[2],\n                             pad[1] + label.winfo_reqheight() + pad[3])\n\n            mouse_x, mouse_y = widget.winfo_pointerxy()\n\n            x_1, y_1 = mouse_x + tip_delta[0], mouse_y + tip_delta[1]\n            x_2, y_2 = x_1 + width, y_1 + height\n\n            x_delta = max(x_2 - s_width, 0)\n            y_delta = max(y_2 - s_height, 0)\n\n            offscreen = (x_delta, y_delta) != (0, 0)\n\n            if offscreen:\n\n                if x_delta:\n                    x_1 = mouse_x - tip_delta[0] - width\n\n                if y_delta:\n                    y_1 = mouse_y - tip_delta[1] - height\n\n            offscreen_again = y_1 < 0  # out on the top\n\n            if offscreen_again:\n                # No further checks will be done.\n                # TIP:\n                # A further mod might auto-magically augment the wrap length when the tooltip is\n                # too high to be kept inside the screen.\n                y_1 = 0\n\n            return x_1, y_1\n\n        pad = self._pad\n        widget = self._widget\n\n        # Creates a top level window\n        self._topwidget = tk.Toplevel(widget)\n        if platform.system() == \"Darwin\":\n            # For Mac OS\n            self._topwidget.tk.call(\"::tk::unsupported::MacWindowStyle\",\n                                    \"style\", self._topwidget._w,  # pylint:disable=protected-access\n                                    \"help\", \"none\")\n\n        # Leaves only the label and removes the app window\n        self._topwidget.wm_overrideredirect(True)\n\n        win = tk.Frame(self._topwidget,\n                       background=self._theme[\"background_color\"],\n                       highlightbackground=self._theme[\"border_color\"],\n                       highlightcolor=self._theme[\"border_color\"],\n                       highlightthickness=1,\n                       borderwidth=0)\n\n        text = self._text\n        if self._text_variable and self._text_variable.get():\n            text += f\"\\n\\nCurrent value: '{self._text_variable.get()}'\"\n        label = tk.Label(win,\n                         text=text,\n                         justify=tk.LEFT,\n                         background=self._theme[\"background_color\"],\n                         foreground=self._theme[\"font_color\"],\n                         relief=tk.SOLID,\n                         borderwidth=0,\n                         wraplength=self.wrap_length)\n\n        label.grid(padx=(pad[0], pad[2]),\n                   pady=(pad[1], pad[3]),\n                   sticky=tk.NSEW)\n        win.grid()\n\n        xpos, ypos = tip_pos_calculator(widget, label)\n\n        self._topwidget.wm_geometry(f\"+{xpos}+{ypos}\")\n\n    def _hide(self):\n        \"\"\" Hide the tooltip \"\"\"\n        topwidget = self._topwidget\n        if topwidget:\n            topwidget.destroy()\n        self._topwidget = None\n\n\nclass MultiOption(ttk.Checkbutton):  # pylint:disable=too-many-ancestors\n    \"\"\" Similar to the standard :class:`ttk.Radio` widget, but with the ability to select\n    multiple pre-defined options. Selected options are generated as `nargs` for the argument\n    parser to consume.\n\n    Parameters\n    ----------\n    parent: :class:`ttk.Frame`\n        The tkinter parent widget for the check button\n    value: str\n        The raw option value for this check button\n    variable: :class:`tkinter.StingVar`\n        The master variable for the group of check buttons that this check button will belong to.\n        The output of this variable will be a string containing a space separated list of the\n        selected check button options\n    \"\"\"\n    def __init__(self, parent, value, variable, **kwargs):\n        self._tk_var = tk.BooleanVar()\n        self._tk_var.set(value in variable.get().split())\n        super().__init__(parent, variable=self._tk_var, **kwargs)\n        self._value = value\n        self._master_variable = variable\n        self._tk_var.trace(\"w\", self._on_update)\n        self._master_variable.trace(\"w\", self._on_master_update)\n\n    @property\n    def _master_list(self):\n        \"\"\" list: The contents of the check box group's :attr:`_master_variable` in list form.\n        Selected check boxes will appear in this list. \"\"\"\n        retval = self._master_variable.get().split()\n        logger.trace(retval)\n        return retval\n\n    @property\n    def _master_needs_update(self):\n        \"\"\" bool: ``True`` if :attr:`_master_variable` requires updating otherwise ``False``. \"\"\"\n        active = self._tk_var.get()\n        retval = ((active and self._value not in self._master_list) or\n                  (not active and self._value in self._master_list))\n        logger.trace(retval)\n        return retval\n\n    def _on_update(self, *args):  # pylint:disable=unused-argument\n        \"\"\" Update the master variable on a check button change.\n\n        The value for this checked option is added or removed from the :attr:`_master_variable`\n        on a ``True``, ``False`` change for this check button.\n\n        Parameters\n        ----------\n        args: tuple\n            Required for variable callback, but unused\n        \"\"\"\n        if not self._master_needs_update:\n            return\n        new_vals = self._master_list + [self._value] if self._tk_var.get() else [\n            val\n            for val in self._master_list\n            if val != self._value]\n        val = \" \".join(new_vals)\n        logger.trace(\"Setting master variable to: %s\", val)\n        self._master_variable.set(val)\n\n    def _on_master_update(self, *args):  # pylint:disable=unused-argument\n        \"\"\" Update the check button on a master variable change (e.g. load .fsw file in the GUI).\n\n        The value for this option is set to ``True`` or ``False`` depending on it's existence in\n        the :attr:`_master_variable`\n\n        Parameters\n        ----------\n        args: tuple\n            Required for variable callback, but unused\n        \"\"\"\n        if not self._master_needs_update:\n            return\n        state = self._value in self._master_list\n        logger.trace(\"Setting '%s' to %s\", self._value, state)\n        self._tk_var.set(state)\n\n\nclass PopupProgress(tk.Toplevel):\n    \"\"\" A simple pop up progress bar that appears of the center of the root window.\n\n    When this is called, the root will be disabled until the :func:`close` method is called.\n\n    Parameters\n    ----------\n    title: str\n        The title to appear above the progress bar\n    total: int or float\n        The total count of items for the progress bar\n\n    Example\n    -------\n    >>> total = 100\n    >>> progress = PopupProgress(\"My title...\", total)\n    >>> for i in range(total):\n    >>>     progress.update(1)\n    >>> progress.close()\n    \"\"\"\n    def __init__(self, title, total):\n        super().__init__()\n        self._total = total\n        if platform.system() == \"Darwin\":  # For Mac OS\n            self.tk.call(\"::tk::unsupported::MacWindowStyle\",\n                         \"style\", self._w,  # pylint:disable=protected-access\n                         \"help\", \"none\")\n        # Leaves only the label and removes the app window\n        self.wm_overrideredirect(True)\n        self.attributes('-topmost', 'true')\n        self.transient()\n\n        self._lbl_title = self._set_title(title)\n        self._progress_bar = self._get_progress_bar()\n\n        offset = np.array((self.master.winfo_rootx(), self.master.winfo_rooty()))\n        # TODO find way to get dimensions of the pop up without it flicking onto the screen\n        self.update_idletasks()\n        center = np.array((\n            (self.master.winfo_width() // 2) - (self.winfo_width() // 2),\n            (self.master.winfo_height() // 2) - (self.winfo_height() // 2))) + offset\n        self.wm_geometry(f\"+{center[0]}+{center[1]}\")\n        get_config().set_cursor_busy()\n        self.grab_set()\n\n    @property\n    def progress_bar(self):\n        \"\"\" :class:`tkinter.ttk.Progressbar`: The progress bar object within the pop up window. \"\"\"\n        return self._progress_bar\n\n    def _set_title(self, title):\n        \"\"\" Set the initial title of the pop up progress bar.\n\n        Parameters\n        ----------\n        title: str\n            The title to appear above the progress bar\n\n        Returns\n        -------\n        :class:`tkinter.ttk.Label`\n            The heading label for the progress bar\n        \"\"\"\n        frame = ttk.Frame(self)\n        frame.pack(side=tk.TOP, padx=5, pady=5)\n        lbl = ttk.Label(frame, text=title)\n        lbl.pack(side=tk.TOP, pady=(5, 0), expand=True, fill=tk.X)\n        return lbl\n\n    def _get_progress_bar(self):\n        \"\"\" Set up the progress bar with the supplied total.\n\n        Returns\n        -------\n        :class:`tkinter.ttk.Progressbar`\n            The configured progress bar for the pop up window\n        \"\"\"\n        frame = ttk.Frame(self)\n        frame.pack(side=tk.BOTTOM, padx=5, pady=(0, 5))\n        pbar = ttk.Progressbar(frame,\n                               length=400,\n                               maximum=self._total,\n                               mode=\"determinate\")\n        pbar.pack(side=tk.LEFT)\n        return pbar\n\n    def step(self, amount):\n        \"\"\" Increment the progress bar.\n\n        Parameters\n        ----------\n        amount: int or float\n            The amount to increment the progress bar by\n        \"\"\"\n        self._progress_bar.step(amount)\n        self._progress_bar.update_idletasks()\n\n    def stop(self):\n        \"\"\" Stop the progress bar, re-enable the root window and destroy the pop up window. \"\"\"\n        self._progress_bar.stop()\n        get_config().set_cursor_default()\n        self.grab_release()\n        self.destroy()\n\n    def update_title(self, title):\n        \"\"\" Update the title that displays above the progress bar.\n\n        Parameters\n        ----------\n        title: str\n            The title to appear above the progress bar\n        \"\"\"\n        self._lbl_title.config(text=title)\n        self._lbl_title.update_idletasks()\n\n\nclass ToggledFrame(ttk.Frame):  # pylint:disable=too-many-ancestors\n    \"\"\" A collapsible and expandable frame.\n\n    The frame contains a header given in the text argument, and adds an expand contract button.\n    Clicking on the header will expand and contract the sub-frame below\n\n    Parameters\n    ----------\n    text: str\n        The text to appear in the Toggle Frame header\n    theme: str, optional\n        The theme to use for the panel header. Default: `\"CPanel\"`\n    subframe_style: str, optional\n        The name of the ttk Style to use for the sub frame. Default: ``None``\n    toggle_var: :class:`tk.BooleanVar`, optional\n        If provided, this variable will control the expanded (``True``) and minimized (``False``)\n        state of the widget. Set to None to create the variable internally. Default: ``None``\n    \"\"\"\n    def __init__(self, parent, *args, text=\"\", theme=\"CPanel\", toggle_var=None, **kwargs):\n        logger.debug(\"Initializing %s: (parent: %s, text: %s, theme: %s, toggle_var: %s)\",\n                     self.__class__.__name__, parent, text, theme, toggle_var)\n\n        theme = \"CPanel\" if not theme else theme\n        theme = theme[:-1] if theme[-1] == \".\" else theme\n        super().__init__(parent, *args, style=f\"{theme}.Group.TFrame\", **kwargs)\n        self._text = text\n\n        if toggle_var:\n            self._toggle_var = toggle_var\n        else:\n            self._toggle_var = tk.BooleanVar()\n            self._toggle_var.set(1)\n        self._icon_var = tk.StringVar()\n        self._icon_var.set(\"-\" if self.is_expanded else \"+\")\n\n        self._build_header(theme)\n\n        self.sub_frame = ttk.Frame(self, style=f\"{theme}.Subframe.Group.TFrame\", padding=1)\n\n        if self.is_expanded:\n            self.sub_frame.pack(fill=tk.X, expand=True)\n\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    @property\n    def is_expanded(self):\n        \"\"\" bool: ``True`` if the Toggle Frame is expanded. ``False`` if it is minimized. \"\"\"\n        return self._toggle_var.get()\n\n    def _build_header(self, theme):\n        \"\"\" The Header row. Contains the title text and is made clickable to expand and contract\n        the sub-frame.\n\n        Parameters\n        theme: str\n            The theme to use for the panel header\n        \"\"\"\n        header_frame = ttk.Frame(self, name=\"toggledframe_header\")\n\n        text_label = ttk.Label(header_frame,\n                               name=\"toggledframe_headerlbl\",\n                               text=self._text,\n                               style=f\"{theme}.Groupheader.TLabel\",\n                               cursor=\"hand2\")\n        toggle_button = ttk.Label(header_frame,\n                                  name=\"toggledframe_headerbtn\",\n                                  textvariable=self._icon_var,\n                                  style=f\"{theme}.Groupheader.TLabel\",\n                                  cursor=\"hand2\",\n                                  width=2)\n        text_label.bind(\"<Button-1>\", self._toggle)\n        toggle_button.bind(\"<Button-1>\", self._toggle)\n\n        text_label.pack(side=tk.LEFT, fill=tk.X, expand=True)\n        toggle_button.pack(side=tk.RIGHT)\n        header_frame.pack(fill=tk.X, expand=True)\n\n    def _toggle(self, event):  # pylint:disable=unused-argument\n        \"\"\" Toggle the sub-frame between contracted or expanded, and update the toggle icon\n        appropriately.\n\n        Parameters\n        ----------\n        event: tkinter event\n            Required but unused\n         \"\"\"\n        if self.is_expanded:\n            self.sub_frame.forget()\n            self._icon_var.set(\"+\")\n            self._toggle_var.set(0)\n        else:\n            self.sub_frame.pack(fill=tk.X, expand=True)\n            self._icon_var.set(\"-\")\n            self._toggle_var.set(1)\n", "lib/gui/wrapper.py": "#!/usr/bin python3\n\"\"\" Process wrapper for underlying faceswap commands for the GUI \"\"\"\nfrom __future__ import annotations\nimport os\nimport logging\nimport re\nimport signal\nimport sys\nimport typing as T\n\nfrom subprocess import PIPE, Popen\nfrom threading import Thread\nfrom time import time\n\nimport psutil\n\nfrom .analysis import Session\nfrom .utils import get_config, get_images, LongRunningTask, preview_trigger\n\nif os.name == \"nt\":\n    import win32console  # pylint:disable=import-error\n\nlogger = logging.getLogger(__name__)\n\n\nclass ProcessWrapper():\n    \"\"\" Builds command, launches and terminates the underlying\n        faceswap process. Updates GUI display depending on state \"\"\"\n\n    def __init__(self) -> None:\n        logger.debug(\"Initializing %s\", self.__class__.__name__)\n        self._tk_vars = get_config().tk_vars\n        self._set_callbacks()\n        self._command: str | None = None\n        \"\"\" str | None: The currently executing command, when process running or ``None`` \"\"\"\n\n        self._statusbar = get_config().statusbar\n        self._training_session_location: dict[T.Literal[\"model_name\", \"model_folder\"], str] = {}\n        self._task = FaceswapControl(self)\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    @property\n    def task(self) -> FaceswapControl:\n        \"\"\" :class:`FaceswapControl`: The object that controls the underlying faceswap process \"\"\"\n        return self._task\n\n    def _set_callbacks(self) -> None:\n        \"\"\" Set the tkinter variable callbacks for performing an action or generating a command \"\"\"\n        logger.debug(\"Setting tk variable traces\")\n        self._tk_vars.action_command.trace(\"w\", self._action_command)\n        self._tk_vars.generate_command.trace(\"w\", self._generate_command)\n\n    def _action_command(self, *args: tuple[str, str, str]):  # pylint:disable=unused-argument\n        \"\"\" Callback for when the Action button is pressed. Process command line options and\n        launches the action\n\n        Parameters\n        ----------\n        args:\n            tuple[str, str, str]\n                Tkinter variable callback args. Required but unused\n        \"\"\"\n        if not self._tk_vars.action_command.get():\n            return\n        category, command = self._tk_vars.action_command.get().split(\",\")\n\n        if self._tk_vars.running_task.get():\n            self._task.terminate()\n        else:\n            self._command = command\n            fs_args = self._prepare(T.cast(T.Literal[\"faceswap\", \"tools\"], category))\n            self._task.execute_script(command, fs_args)\n        self._tk_vars.action_command.set(\"\")\n\n    def _generate_command(self,  # pylint:disable=unused-argument\n                          *args: tuple[str, str, str]) -> None:\n        \"\"\" Callback for when the Generate button is pressed. Process command line options and\n        output the cli command\n\n        Parameters\n        ----------\n        args:\n            tuple[str, str, str]\n                Tkinter variable callback args. Required but unused\n        \"\"\"\n        if not self._tk_vars.generate_command.get():\n            return\n        category, command = self._tk_vars.generate_command.get().split(\",\")\n        fs_args = self._build_args(category, command=command, generate=True)\n        self._tk_vars.console_clear.set(True)\n        logger.debug(\" \".join(fs_args))\n        print(\" \".join(fs_args))\n        self._tk_vars.generate_command.set(\"\")\n\n    def _prepare(self, category: T.Literal[\"faceswap\", \"tools\"]) -> list[str]:\n        \"\"\" Prepare the environment for execution, Sets the 'running task' and 'console clear'\n        global tkinter variables. If training, sets the 'is training' variable\n\n        Parameters\n        ----------\n        category: str, [\"faceswap\", \"tools\"]\n            The script that is executing the command\n\n        Returns\n        -------\n        list[str]\n            The command line arguments to execute for the faceswap job\n        \"\"\"\n        logger.debug(\"Preparing for execution\")\n        assert self._command is not None\n        self._tk_vars.running_task.set(True)\n        self._tk_vars.console_clear.set(True)\n        if self._command == \"train\":\n            self._tk_vars.is_training.set(True)\n        print(\"Loading...\")\n\n        self._statusbar.message.set(f\"Executing - {self._command}.py\")\n        mode: T.Literal[\"indeterminate\",\n                        \"determinate\"] = (\"indeterminate\" if self._command in (\"effmpeg\", \"train\")\n                                          else \"determinate\")\n        self._statusbar.start(mode)\n\n        args = self._build_args(category)\n        self._tk_vars.display.set(self._command)\n        logger.debug(\"Prepared for execution\")\n        return args\n\n    def _build_args(self,\n                    category: str,\n                    command: str | None = None,\n                    generate: bool = False) -> list[str]:\n        \"\"\" Build the faceswap command and arguments list.\n\n        If training, pass the model folder and name to the training\n        :class:`lib.gui.analysis.Session` for the GUI.\n\n        Parameters\n        ----------\n        category: str, [\"faceswap\", \"tools\"]\n            The script that is executing the command\n        command: str, optional\n            The main faceswap command to execute, if provided. The currently running task if\n            ``None``. Default: ``None``\n        generate: bool, optional\n            ``True`` if the command is just to be generated for display. ``False`` if the command\n            is to be executed\n\n        Returns\n        -------\n        list[str]\n            The full faceswap command to be executed or displayed\n        \"\"\"\n        logger.debug(\"Build cli arguments: (category: %s, command: %s, generate: %s)\",\n                     category, command, generate)\n        command = self._command if not command else command\n        assert command is not None\n        script = f\"{category}.py\"\n        pathexecscript = os.path.join(os.path.realpath(os.path.dirname(sys.argv[0])), script)\n\n        args = [sys.executable] if generate else [sys.executable, \"-u\"]\n        args.extend([pathexecscript, command])\n\n        cli_opts = get_config().cli_opts\n        for cliopt in cli_opts.gen_cli_arguments(command):\n            args.extend(cliopt)\n            if command == \"train\" and not generate:\n                self._get_training_session_info(cliopt)\n\n        if not generate:\n            args.append(\"-gui\")  # Indicate to Faceswap that we are running the GUI\n        if generate:\n            # Delimit args with spaces\n            args = [f'\"{arg}\"' if \" \" in arg and not arg.startswith((\"[\", \"(\"))\n                    and not arg.endswith((\"]\", \")\")) else arg\n                    for arg in args]\n        logger.debug(\"Built cli arguments: (%s)\", args)\n        return args\n\n    def _get_training_session_info(self, cli_option: list[str]) -> None:\n        \"\"\" Set the model folder and model name to :`attr:_training_session_location` so the global\n        session picks them up for logging to the graph and analysis tab.\n\n        Parameters\n        ----------\n        cli_option: list[str]\n            The command line option to be checked for model folder or name\n        \"\"\"\n        if cli_option[0] == \"-t\":\n            self._training_session_location[\"model_name\"] = cli_option[1].lower().replace(\"-\", \"_\")\n            logger.debug(\"model_name: '%s'\", self._training_session_location[\"model_name\"])\n        if cli_option[0] == \"-m\":\n            self._training_session_location[\"model_folder\"] = cli_option[1]\n            logger.debug(\"model_folder: '%s'\", self._training_session_location[\"model_folder\"])\n\n    def terminate(self, message: str) -> None:\n        \"\"\" Finalize wrapper when process has exited. Stops the progress bar, sets the status\n        message. If the terminating task is 'train', then triggers the training close down actions\n\n        Parameters\n        ----------\n        message: str\n            The message to display in the status bar\n        \"\"\"\n        logger.debug(\"Terminating Faceswap processes\")\n        self._tk_vars.running_task.set(False)\n        if self._task.command == \"train\":\n            self._tk_vars.is_training.set(False)\n            Session.stop_training()\n        self._statusbar.stop()\n        self._statusbar.message.set(message)\n        self._tk_vars.display.set(\"\")\n        get_images().delete_preview()\n        preview_trigger().clear(trigger_type=None)\n        self._command = None\n        logger.debug(\"Terminated Faceswap processes\")\n        print(\"Process exited.\")\n\n\nclass FaceswapControl():\n    \"\"\" Control the underlying Faceswap tasks.\n\n    wrapper: :class:`ProcessWrapper`\n        The object responsible for managing this faceswap task\n    \"\"\"\n    def __init__(self, wrapper: ProcessWrapper) -> None:\n        logger.debug(\"Initializing %s (wrapper: %s)\", self.__class__.__name__, wrapper)\n        self._wrapper = wrapper\n        self._session_info = wrapper._training_session_location\n        self._config = get_config()\n        self._statusbar = self._config.statusbar\n        self._command: str | None = None\n        self._process: Popen | None = None\n        self._thread: LongRunningTask | None = None\n        self._train_stats: dict[T.Literal[\"iterations\", \"timestamp\"],\n                                int | float | None] = {\"iterations\": 0, \"timestamp\": None}\n        self._consoleregex: dict[T.Literal[\"loss\", \"tqdm\", \"ffmpeg\"], re.Pattern] = {\n            \"loss\": re.compile(r\"[\\W]+(\\d+)?[\\W]+([a-zA-Z\\s]*)[\\W]+?(\\d+\\.\\d+)\"),\n            \"tqdm\": re.compile(r\"(?P<dsc>.*?)(?P<pct>\\d+%).*?(?P<itm>\\S+/\\S+)\\W\\[\"\n                               r\"(?P<tme>[\\d+:]+<.*),\\W(?P<rte>.*)[a-zA-Z/]*\\]\"),\n            \"ffmpeg\": re.compile(r\"([a-zA-Z]+)=\\s*(-?[\\d|N/A]\\S+)\")}\n        self._first_loss_seen = False\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    @property\n    def command(self) -> str | None:\n        \"\"\" str | None: The currently executing command, when process running or ``None`` \"\"\"\n        return self._command\n\n    def execute_script(self, command: str, args: list[str]) -> None:\n        \"\"\" Execute the requested Faceswap Script\n\n        Parameters\n        ----------\n        command: str\n            The faceswap command that is to be run\n        args: list[str]\n            The full command line arguments to be executed\n        \"\"\"\n        logger.debug(\"Executing Faceswap: (command: '%s', args: %s)\", command, args)\n        self._thread = None\n        self._command = command\n\n        proc = Popen(args,  # pylint:disable=consider-using-with\n                     stdout=PIPE,\n                     stderr=PIPE,\n                     bufsize=1,\n                     text=True,\n                     stdin=PIPE,\n                     errors=\"backslashreplace\")\n        self._process = proc\n        self._thread_stdout()\n        self._thread_stderr()\n        logger.debug(\"Executed Faceswap\")\n\n    def _process_training_determinate_function(self, output: str) -> bool:\n        \"\"\" Process an stdout/stderr message to check for determinate TQDM output when training\n\n        Parameters\n        ----------\n        output: str\n            The stdout/stderr string to test\n\n        Returns\n        -------\n        bool\n            ``True`` if a determinate TQDM line was parsed when training otherwise ``False``\n        \"\"\"\n        if self._command == \"train\" and not self._first_loss_seen and self._capture_tqdm(output):\n            self._statusbar.set_mode(\"determinate\")\n            return True\n        return False\n\n    def _process_progress_stdout(self, output: str) -> bool:\n        \"\"\" Process stdout for any faceswap processes that update the status/progress bar(s)\n\n        Parameters\n        ----------\n        output: str\n            The output line read from stdout\n\n        Returns\n        -------\n        bool\n            ``True`` if all actions have been completed on the output line otherwise ``False``\n        \"\"\"\n        if self._process_training_determinate_function(output):\n            return True\n\n        if self._command == \"train\" and self._capture_loss(output):\n            return True\n\n        if self._command == \"effmpeg\" and self._capture_ffmpeg(output):\n            return True\n\n        if self._command not in (\"train\", \"effmpeg\") and self._capture_tqdm(output):\n            return True\n\n        return False\n\n    def _process_training_stdout(self, output: str) -> None:\n        \"\"\" Process any triggers that are required to update the GUI when Faceswap is running a\n        training session.\n\n        Parameters\n        ----------\n        output: str\n            The output line read from stdout\n        \"\"\"\n        tk_vars = get_config().tk_vars\n        if self._command != \"train\" or not tk_vars.is_training.get():\n            return\n\n        t_output = output.strip().lower()\n        if \"[saved model]\" not in t_output or t_output.endswith(\"[saved model]\"):\n            # Not a saved model line or saving the model for a reason other than standard saving\n            return\n\n        logger.debug(\"Trigger GUI Training update\")\n        logger.trace(\"tk_vars: %s\", {itm: var.get()  # type:ignore[attr-defined]\n                                     for itm, var in tk_vars.__dict__.items()})\n        if not Session.is_training:\n            # Don't initialize session until after the first save as state file must exist first\n            logger.debug(\"Initializing curret training session\")\n            Session.initialize_session(self._session_info[\"model_folder\"],\n                                       self._session_info[\"model_name\"],\n                                       is_training=True)\n        tk_vars.refresh_graph.set(True)\n\n    def _read_stdout(self) -> None:\n        \"\"\" Read stdout from the subprocess. \"\"\"\n        logger.debug(\"Opening stdout reader\")\n        assert self._process is not None\n        while True:\n            try:\n                buff = self._process.stdout\n                assert buff is not None\n                output: str = buff.readline()\n            except ValueError as err:\n                if str(err).lower().startswith(\"i/o operation on closed file\"):\n                    break\n                raise\n\n            if output == \"\" and self._process.poll() is not None:\n                break\n\n            if output and self._process_progress_stdout(output):\n                continue\n\n            if output:\n                self._process_training_stdout(output)\n                print(output.rstrip())\n\n        returncode = self._process.poll()\n        assert returncode is not None\n        self._first_loss_seen = False\n        message = self._set_final_status(returncode)\n        self._wrapper.terminate(message)\n        logger.debug(\"Terminated stdout reader. returncode: %s\", returncode)\n\n    def _read_stderr(self) -> None:\n        \"\"\" Read stdout from the subprocess. If training, pass the loss\n        values to Queue \"\"\"\n        logger.debug(\"Opening stderr reader\")\n        assert self._process is not None\n        while True:\n            try:\n                buff = self._process.stderr\n                assert buff is not None\n                output: str = buff.readline()\n            except ValueError as err:\n                if str(err).lower().startswith(\"i/o operation on closed file\"):\n                    break\n                raise\n            if output == \"\" and self._process.poll() is not None:\n                break\n            if output:\n                if self._command != \"train\" and self._capture_tqdm(output):\n                    continue\n                if self._process_training_determinate_function(output):\n                    continue\n                if os.name == \"nt\" and \"Call to CreateProcess failed. Error code: 2\" in output:\n                    # Suppress ptxas errors on Tensorflow for Windows\n                    logger.debug(\"Suppressed call to subprocess error: '%s'\", output)\n                    continue\n                print(output.strip(), file=sys.stderr)\n        logger.debug(\"Terminated stderr reader\")\n\n    def _thread_stdout(self) -> None:\n        \"\"\" Put the subprocess stdout so that it can be read without blocking \"\"\"\n        logger.debug(\"Threading stdout\")\n        thread = Thread(target=self._read_stdout)\n        thread.daemon = True\n        thread.start()\n        logger.debug(\"Threaded stdout\")\n\n    def _thread_stderr(self) -> None:\n        \"\"\" Put the subprocess stderr so that it can be read without blocking \"\"\"\n        logger.debug(\"Threading stderr\")\n        thread = Thread(target=self._read_stderr)\n        thread.daemon = True\n        thread.start()\n        logger.debug(\"Threaded stderr\")\n\n    def _capture_loss(self, string: str) -> bool:\n        \"\"\" Capture loss values from stdout\n\n        Parameters\n        ----------\n        string: str\n            An output line read from stdout\n\n        Returns\n        -------\n        bool\n            ``True`` if a loss line was captured from stdout, otherwise ``False``\n        \"\"\"\n        logger.trace(\"Capturing loss\")  # type:ignore[attr-defined]\n        if not str.startswith(string, \"[\"):\n            logger.trace(\"Not loss message. Returning False\")  # type:ignore[attr-defined]\n            return False\n\n        loss = self._consoleregex[\"loss\"].findall(string)\n        if len(loss) != 2 or not all(len(itm) == 3 for itm in loss):\n            logger.trace(\"Not loss message. Returning False\")  # type:ignore[attr-defined]\n            return False\n\n        message = f\"Total Iterations: {int(loss[0][0])} | \"\n        message += \"  \".join([f\"{itm[1]}: {itm[2]}\" for itm in loss])\n        if not message:\n            logger.trace(  # type:ignore[attr-defined]\n                \"Error creating loss message. Returning False\")\n            return False\n\n        iterations = self._train_stats[\"iterations\"]\n        assert isinstance(iterations, int)\n\n        if iterations == 0:\n            # Set initial timestamp\n            self._train_stats[\"timestamp\"] = time()\n\n        iterations += 1\n        self._train_stats[\"iterations\"] = iterations\n\n        elapsed = self._calculate_elapsed()\n        message = (f\"Elapsed: {elapsed} | \"\n                   f\"Session Iterations: {self._train_stats['iterations']}  {message}\")\n\n        if not self._first_loss_seen:\n            self._statusbar.set_mode(\"indeterminate\")\n            self._first_loss_seen = True\n\n        self._statusbar.progress_update(message, 0, False)\n        logger.trace(\"Succesfully captured loss: %s\", message)  # type:ignore[attr-defined]\n        return True\n\n    def _calculate_elapsed(self) -> str:\n        \"\"\" Calculate and format time since training started\n\n        Returns\n        -------\n        str\n            The amount of time elapsed since training started in HH:mm:ss format\n        \"\"\"\n        now = time()\n        timestamp = self._train_stats[\"timestamp\"]\n        assert isinstance(timestamp, float)\n        elapsed_time = now - timestamp\n        try:\n            i_hrs = int(elapsed_time // 3600)\n            hrs = f\"{i_hrs:02d}\" if i_hrs < 10 else str(i_hrs)\n            mins = f\"{(int(elapsed_time % 3600) // 60):02d}\"\n            secs = f\"{(int(elapsed_time % 3600) % 60):02d}\"\n        except ZeroDivisionError:\n            hrs = mins = secs = \"00\"\n        return f\"{hrs}:{mins}:{secs}\"\n\n    def _capture_tqdm(self, string: str) -> bool:\n        \"\"\" Capture tqdm output for progress bar\n\n        Parameters\n        ----------\n        string: str\n            An output line read from stdout\n\n        Returns\n        -------\n        bool\n            ``True`` if a tqdm line was captured from stdout, otherwise ``False``\n        \"\"\"\n        logger.trace(\"Capturing tqdm\")  # type:ignore[attr-defined]\n        mtqdm = self._consoleregex[\"tqdm\"].match(string)\n        if not mtqdm:\n            return False\n        tqdm = mtqdm.groupdict()\n        if any(\"?\" in val for val in tqdm.values()):\n            logger.trace(\"tqdm initializing. Skipping\")  # type:ignore[attr-defined]\n            return True\n        description = tqdm[\"dsc\"].strip()\n        description = description if description == \"\" else f\"{description[:-1]}  |  \"\n        processtime = (f\"Elapsed: {tqdm['tme'].split('<')[0]}  \"\n                       f\"Remaining: {tqdm['tme'].split('<')[1]}\")\n        msg = f\"{description}{processtime}  |  {tqdm['rte']}  |  {tqdm['itm']}  |  {tqdm['pct']}\"\n\n        position = tqdm[\"pct\"].replace(\"%\", \"\")\n        position = int(position) if position.isdigit() else 0\n\n        self._statusbar.progress_update(msg, position, True)\n        logger.trace(\"Succesfully captured tqdm message: %s\", msg)  # type:ignore[attr-defined]\n        return True\n\n    def _capture_ffmpeg(self, string: str) -> bool:\n        \"\"\" Capture ffmpeg output for progress bar\n\n        Parameters\n        ----------\n        string: str\n            An output line read from stdout\n\n        Returns\n        -------\n        bool\n            ``True`` if an ffmpeg line was captured from stdout, otherwise ``False``\n        \"\"\"\n        logger.trace(\"Capturing ffmpeg\")  # type:ignore[attr-defined]\n        ffmpeg = self._consoleregex[\"ffmpeg\"].findall(string)\n        if len(ffmpeg) < 7:\n            logger.trace(\"Not ffmpeg message. Returning False\")  # type:ignore[attr-defined]\n            return False\n\n        message = \"\"\n        for item in ffmpeg:\n            message += f\"{item[0]}: {item[1]}  \"\n        if not message:\n            logger.trace(  # type:ignore[attr-defined]\n                \"Error creating ffmpeg message. Returning False\")\n            return False\n\n        self._statusbar.progress_update(message, 0, False)\n        logger.trace(\"Succesfully captured ffmpeg message: %s\",  # type:ignore[attr-defined]\n                     message)\n        return True\n\n    def terminate(self) -> None:\n        \"\"\" Terminate the running process in a LongRunningTask so console can still be updated\n        console \"\"\"\n        if self._thread is None:\n            logger.debug(\"Terminating wrapper in LongRunningTask\")\n            self._thread = LongRunningTask(target=self._terminate_in_thread,\n                                           args=(self._command, self._process))\n            if self._command == \"train\":\n                get_config().tk_vars.is_training.set(False)\n            self._thread.start()\n            self._config.root.after(1000, self.terminate)\n        elif not self._thread.complete.is_set():\n            logger.debug(\"Not finished terminating\")\n            self._config.root.after(1000, self.terminate)\n        else:\n            logger.debug(\"Termination Complete. Cleaning up\")\n            _ = self._thread.get_result()  # Terminate the LongRunningTask object\n            self._thread = None\n\n    def _terminate_in_thread(self, command: str, process: Popen) -> bool:\n        \"\"\" Terminate the subprocess\n\n        Parameters\n        ----------\n        command: str\n            The command that is running\n\n        process: :class:`subprocess.Popen`\n            The running process\n\n        Returns\n        -------\n        bool\n            ``True`` when this function exits\n        \"\"\"\n        logger.debug(\"Terminating wrapper\")\n        if command == \"train\":\n            timeout = self._config.user_config_dict.get(\"timeout\", 120)\n            logger.debug(\"Sending Exit Signal\")\n            print(\"Sending Exit Signal\", flush=True)\n            now = time()\n            if os.name == \"nt\":\n                logger.debug(\"Sending carriage return to process\")\n                con_in = win32console.GetStdHandle(  # pylint:disable=c-extension-no-member\n                    win32console.STD_INPUT_HANDLE)  # pylint:disable=c-extension-no-member\n                keypress = self._generate_windows_keypress(\"\\n\")\n                con_in.WriteConsoleInput([keypress])\n            else:\n                logger.debug(\"Sending SIGINT to process\")\n                process.send_signal(signal.SIGINT)\n            while True:\n                timeelapsed = time() - now\n                if process.poll() is not None:\n                    break\n                if timeelapsed > timeout:\n                    logger.error(\"Timeout reached sending Exit Signal\")\n                    self._terminate_all_children()\n        else:\n            self._terminate_all_children()\n        return True\n\n    @classmethod\n    def _generate_windows_keypress(cls, character: str) -> bytes:\n        \"\"\" Generate a Windows keypress\n\n        Parameters\n        ----------\n        character: str\n            The caracter to generate the keypress for\n\n        Returns\n        -------\n        bytes\n            The generated Windows keypress\n        \"\"\"\n        buf = win32console.PyINPUT_RECORDType(  # pylint:disable=c-extension-no-member\n            win32console.KEY_EVENT)  # pylint:disable=c-extension-no-member\n        buf.KeyDown = 1\n        buf.RepeatCount = 1\n        buf.Char = character\n        return buf\n\n    @classmethod\n    def _terminate_all_children(cls) -> None:\n        \"\"\" Terminates all children \"\"\"\n        logger.debug(\"Terminating Process...\")\n        print(\"Terminating Process...\", flush=True)\n        children = psutil.Process().children(recursive=True)\n        for child in children:\n            child.terminate()\n        _, alive = psutil.wait_procs(children, timeout=10)\n        if not alive:\n            logger.debug(\"Terminated\")\n            print(\"Terminated\")\n            return\n\n        logger.debug(\"Termination timed out. Killing Process...\")\n        print(\"Termination timed out. Killing Process...\", flush=True)\n        for child in alive:\n            child.kill()\n        _, alive = psutil.wait_procs(alive, timeout=10)\n        if not alive:\n            logger.debug(\"Killed\")\n            print(\"Killed\")\n        else:\n            for child in alive:\n                msg = f\"Process {child} survived SIGKILL. Giving up\"\n                logger.debug(msg)\n                print(msg)\n\n    def _set_final_status(self, returncode: int) -> str:\n        \"\"\" Set the status bar output based on subprocess return code and reset training stats\n\n        Parameters\n        ----------\n        returncode: int\n            The returncode from the terminated process\n\n        Returns\n        -------\n        str\n            The final statusbar text\n        \"\"\"\n        logger.debug(\"Setting final status. returncode: %s\", returncode)\n        self._train_stats = {\"iterations\": 0, \"timestamp\": None}\n        if returncode in (0, 3221225786):\n            status = \"Ready\"\n        elif returncode == -15:\n            status = f\"Terminated - {self._command}.py\"\n        elif returncode == -9:\n            status = f\"Killed - {self._command}.py\"\n        elif returncode == -6:\n            status = f\"Aborted - {self._command}.py\"\n        else:\n            status = f\"Failed - {self._command}.py. Return Code: {returncode}\"\n        logger.debug(\"Set final status: %s\", status)\n        return status\n", "lib/gui/popup_session.py": "#!/usr/bin python3\n\"\"\" Pop-up Graph launched from the Analysis tab of the Faceswap GUI \"\"\"\n\nimport csv\nimport gettext\nimport logging\nimport tkinter as tk\n\nfrom dataclasses import dataclass, field\nfrom tkinter import ttk\n\nfrom .control_helper import ControlBuilder, ControlPanelOption\nfrom .custom_widgets import Tooltip\nfrom .display_graph import SessionGraph\nfrom .analysis import Calculations, Session\nfrom .utils import FileHandler, get_images, LongRunningTask\n\nlogger = logging.getLogger(__name__)\n\n# LOCALES\n_LANG = gettext.translation(\"gui.tooltips\", localedir=\"locales\", fallback=True)\n_ = _LANG.gettext\n\n\n@dataclass\nclass SessionTKVars:\n    \"\"\" Dataclass for holding the tk variables required for the session popup\n\n    Parameters\n    ----------\n    buildgraph: :class:`tkinter.BooleanVar`\n        Trigger variable to indicate the graph should be rebuilt\n    status: :class:`tkinter.StringVar`\n        The variable holding the current status of the popup window\n    display: :class:`tkinter.StringVar`\n        Variable indicating the type of information to be displayed\n    scale: :class:`tkinter.StringVar`\n        Variable indicating whether to display as log or linear data\n    raw: :class:`tkinter.BooleanVar`\n        Variable to indicate raw data should be displayed\n    trend: :class:`tkinter.BooleanVar`\n        Variable to indicate that trend data should be displayed\n    avg: :class:`tkinter.BooleanVar`\n        Variable to indicate that rolling average data should be displayed\n    smoothed: :class:`tkinter.BooleanVar`\n        Variable to indicate that smoothed data should be displayed\n    outliers: :class:`tkinter.BooleanVar`\n        Variable to indicate that outliers should be displayed\n    loss_keys: dict\n        Dictionary of names to :class:`tkinter.BooleanVar` indicating whether specific loss items\n        should be displayed\n    avgiterations: :class:`tkinter.IntVar`\n        The number of iterations to use for rolling average\n    smoothamount: :class:`tkinter.DoubleVar`\n        The amount of smoothing to apply for smoothed data\n    \"\"\"\n    buildgraph: tk.BooleanVar\n    status: tk.StringVar\n    display: tk.StringVar\n    scale: tk.StringVar\n    raw: tk.BooleanVar\n    trend: tk.BooleanVar\n    avg: tk.BooleanVar\n    smoothed: tk.BooleanVar\n    outliers: tk.BooleanVar\n    avgiterations: tk.IntVar\n    smoothamount: tk.DoubleVar\n    loss_keys: dict[str, tk.BooleanVar] = field(default_factory=dict)\n\n\nclass SessionPopUp(tk.Toplevel):\n    \"\"\" Pop up for detailed graph/stats for selected session.\n\n    session_id: int or `\"Total\"`\n        The session id number for the selected session from the Analysis tab. Should be the string\n        `\"Total\"` if all sessions are being graphed\n    data_points: int\n        The number of iterations in the selected session\n    \"\"\"\n    def __init__(self, session_id: int, data_points: int) -> None:\n        logger.debug(\"Initializing: %s: (session_id: %s, data_points: %s)\",\n                     self.__class__.__name__, session_id, data_points)\n        super().__init__()\n        self._thread: LongRunningTask | None = None  # Thread for loading data in background\n        self._default_view = \"avg\" if data_points > 1000 else \"smoothed\"\n        self._session_id = None if session_id == \"Total\" else int(session_id)\n\n        self._graph_frame = ttk.Frame(self)\n        self._graph: SessionGraph | None = None\n        self._display_data: Calculations | None = None\n\n        self._vars = self._set_vars()\n\n        self._graph_initialised = False\n\n        optsframe = self._layout_frames()\n        self._build_options(optsframe)\n\n        self._lbl_loading = ttk.Label(self._graph_frame, text=\"Loading Data...\", anchor=tk.CENTER)\n        self._lbl_loading.pack(fill=tk.BOTH, expand=True)\n        self.update_idletasks()\n\n        self._compile_display_data()\n\n        logger.debug(\"Initialized: %s\", self.__class__.__name__)\n\n    def _set_vars(self) -> SessionTKVars:\n        \"\"\" Set status tkinter String variable and tkinter Boolean variable to callback when the\n        graph is ready to build.\n\n        Returns\n        -------\n        :class:`SessionTKVars`\n            The tkinter Variables for the pop up graph\n        \"\"\"\n        logger.debug(\"Setting tk graph build variable and internal variables\")\n        retval = SessionTKVars(buildgraph=tk.BooleanVar(),\n                               status=tk.StringVar(),\n                               display=tk.StringVar(),\n                               scale=tk.StringVar(),\n                               raw=tk.BooleanVar(),\n                               trend=tk.BooleanVar(),\n                               avg=tk.BooleanVar(),\n                               smoothed=tk.BooleanVar(),\n                               outliers=tk.BooleanVar(),\n                               avgiterations=tk.IntVar(),\n                               smoothamount=tk.DoubleVar())\n        retval.buildgraph.set(False)\n        retval.buildgraph.trace(\"w\", self._graph_build)\n        return retval\n\n    def _layout_frames(self) -> ttk.Frame:\n        \"\"\" Top level container frames \"\"\"\n        logger.debug(\"Layout frames\")\n\n        leftframe = ttk.Frame(self)\n        sep = ttk.Frame(self, width=2, relief=tk.RIDGE)\n\n        self._graph_frame.pack(side=tk.RIGHT, fill=tk.BOTH, pady=5, expand=True)\n        sep.pack(fill=tk.Y, side=tk.LEFT)\n        leftframe.pack(side=tk.LEFT, expand=False, fill=tk.BOTH, pady=5)\n\n        logger.debug(\"Laid out frames\")\n\n        return leftframe\n\n    def _build_options(self, frame: ttk.Frame) -> None:\n        \"\"\" Build Options into the options frame.\n\n        Parameters\n        ----------\n        frame: :class:`tkinter.ttk.Frame`\n            The frame that the options reside in\n        \"\"\"\n        logger.debug(\"Building Options\")\n        self._opts_combobox(frame)\n        self._opts_checkbuttons(frame)\n        self._opts_loss_keys(frame)\n        self._opts_slider(frame)\n        self._opts_buttons(frame)\n        sep = ttk.Frame(frame, height=2, relief=tk.RIDGE)\n        sep.pack(fill=tk.X, pady=(5, 0), side=tk.BOTTOM)\n        logger.debug(\"Built Options\")\n\n    def _opts_combobox(self, frame: ttk.Frame) -> None:\n        \"\"\" Add the options combo boxes.\n\n        Parameters\n        ----------\n        frame: :class:`tkinter.ttk.Frame`\n            The frame that the options reside in\n        \"\"\"\n        logger.debug(\"Building Combo boxes\")\n        choices = {\"Display\": (\"Loss\", \"Rate\"), \"Scale\": (\"Linear\", \"Log\")}\n\n        for item in [\"Display\", \"Scale\"]:\n            var: tk.StringVar = getattr(self._vars, item.lower())\n\n            cmbframe = ttk.Frame(frame)\n            lblcmb = ttk.Label(cmbframe, text=f\"{item}:\", width=7, anchor=tk.W)\n            cmb = ttk.Combobox(cmbframe, textvariable=var, width=10)\n            cmb[\"values\"] = choices[item]\n            cmb.current(0)\n\n            cmd = self._option_button_reload if item == \"Display\" else self._graph_scale\n            var.trace(\"w\", cmd)\n            hlp = self._set_help(item)\n            Tooltip(cmbframe, text=hlp, wrap_length=200)\n\n            cmb.pack(fill=tk.X, side=tk.RIGHT)\n            lblcmb.pack(padx=(0, 2), side=tk.LEFT)\n            cmbframe.pack(fill=tk.X, pady=5, padx=5, side=tk.TOP)\n        logger.debug(\"Built Combo boxes\")\n\n    def _opts_checkbuttons(self, frame: ttk.Frame) -> None:\n        \"\"\" Add the options check buttons.\n\n        Parameters\n        ----------\n        frame: :class:`tkinter.ttk.Frame`\n            The frame that the options reside in\n        \"\"\"\n        logger.debug(\"Building Check Buttons\")\n        self._add_section(frame, \"Display\")\n        for item in (\"raw\", \"trend\", \"avg\", \"smoothed\", \"outliers\"):\n            if item == \"avg\":\n                text = \"Show Rolling Average\"\n            elif item == \"outliers\":\n                text = \"Flatten Outliers\"\n            else:\n                text = f\"Show {item.title()}\"\n\n            var: tk.BooleanVar = getattr(self._vars, item)\n            if item == self._default_view:\n                var.set(True)\n\n            ctl = ttk.Checkbutton(frame, variable=var, text=text)\n            hlp = self._set_help(item)\n            Tooltip(ctl, text=hlp, wrap_length=200)\n            ctl.pack(side=tk.TOP, padx=5, pady=5, anchor=tk.W)\n\n        logger.debug(\"Built Check Buttons\")\n\n    def _opts_loss_keys(self, frame: ttk.Frame) -> None:\n        \"\"\" Add loss key selections.\n\n        Parameters\n        ----------\n        frame: :class:`tkinter.ttk.Frame`\n            The frame that the options reside in\n        \"\"\"\n        logger.debug(\"Building Loss Key Check Buttons\")\n        loss_keys = Session.get_loss_keys(self._session_id)\n        lk_vars = {}\n        section_added = False\n        for loss_key in sorted(loss_keys):\n            if loss_key.startswith(\"total\"):\n                continue\n\n            text = loss_key.replace(\"_\", \" \").title()\n            helptext = _(\"Display {}\").format(text)\n\n            var = tk.BooleanVar()\n            var.set(True)\n            lk_vars[loss_key] = var\n\n            if len(loss_keys) == 1:\n                # Don't display if there's only one item\n                break\n\n            if not section_added:\n                self._add_section(frame, \"Keys\")\n                section_added = True\n\n            ctl = ttk.Checkbutton(frame, variable=var, text=text)\n            Tooltip(ctl, text=helptext, wrap_length=200)\n            ctl.pack(side=tk.TOP, padx=5, pady=5, anchor=tk.W)\n\n        self._vars.loss_keys = lk_vars\n        logger.debug(\"Built Loss Key Check Buttons\")\n\n    def _opts_slider(self, frame: ttk.Frame) -> None:\n        \"\"\" Add the options entry boxes.\n\n        Parameters\n        ----------\n        frame: :class:`tkinter.ttk.Frame`\n            The frame that the options reside in\n        \"\"\"\n\n        self._add_section(frame, \"Parameters\")\n        logger.debug(\"Building Slider Controls\")\n        for item in (\"avgiterations\", \"smoothamount\"):\n            if item == \"avgiterations\":\n                dtype: type[int] | type[float] = int\n                text = \"Iterations to Average:\"\n                default: int | float = 500\n                rounding = 25\n                min_max: tuple[int, int | float] = (25, 2500)\n            elif item == \"smoothamount\":\n                dtype = float\n                text = \"Smoothing Amount:\"\n                default = 0.90\n                rounding = 2\n                min_max = (0, 0.99)\n            slider = ControlPanelOption(text,\n                                        dtype,\n                                        default=default,\n                                        rounding=rounding,\n                                        min_max=min_max,\n                                        helptext=self._set_help(item))\n            setattr(self._vars, item, slider.tk_var)\n            ControlBuilder(frame, slider, 1, 19, None, \"Analysis.\", True)\n        logger.debug(\"Built Sliders\")\n\n    def _opts_buttons(self, frame: ttk.Frame) -> None:\n        \"\"\" Add the option buttons.\n\n        Parameters\n        ----------\n        frame: :class:`tkinter.ttk.Frame`\n            The frame that the options reside in\n        \"\"\"\n        logger.debug(\"Building Buttons\")\n        btnframe = ttk.Frame(frame)\n        lblstatus = ttk.Label(btnframe,\n                              width=40,\n                              textvariable=self._vars.status,\n                              anchor=tk.W)\n\n        for btntype in (\"reload\", \"save\"):\n            cmd = getattr(self, f\"_option_button_{btntype}\")\n            btn = ttk.Button(btnframe,\n                             image=get_images().icons[btntype],\n                             command=cmd)\n            hlp = self._set_help(btntype)\n            Tooltip(btn, text=hlp, wrap_length=200)\n            btn.pack(padx=2, side=tk.RIGHT)\n\n        lblstatus.pack(side=tk.LEFT, anchor=tk.W, fill=tk.X, expand=True)\n        btnframe.pack(fill=tk.X, pady=5, padx=5, side=tk.BOTTOM)\n        logger.debug(\"Built Buttons\")\n\n    @classmethod\n    def _add_section(cls, frame: ttk.Frame, title: str) -> None:\n        \"\"\" Add a separator and section title between options\n\n        Parameters\n        ----------\n        frame: :class:`tkinter.ttk.Frame`\n            The frame that the options reside in\n        title: str\n            The section title to display\n        \"\"\"\n        sep = ttk.Frame(frame, height=2, relief=tk.SOLID)\n        lbl = ttk.Label(frame, text=title)\n\n        lbl.pack(side=tk.TOP, padx=5, pady=0, anchor=tk.CENTER)\n        sep.pack(fill=tk.X, pady=(5, 0), side=tk.TOP)\n\n    def _option_button_save(self) -> None:\n        \"\"\" Action for save button press. \"\"\"\n        logger.debug(\"Saving File\")\n        savefile = FileHandler(\"save\", \"csv\").return_file\n        if not savefile:\n            logger.debug(\"Save Cancelled\")\n            return\n        logger.debug(\"Saving to: %s\", savefile)\n        assert self._display_data is not None\n        save_data = self._display_data.stats\n        fieldnames = sorted(key for key in save_data.keys())\n\n        with savefile as outfile:\n            csvout = csv.writer(outfile, delimiter=\",\")\n            csvout.writerow(fieldnames)\n            csvout.writerows(zip(*[save_data[key] for key in fieldnames]))\n\n    def _option_button_reload(self, *args) -> None:  # pylint:disable=unused-argument\n        \"\"\" Action for reset button press and checkbox changes.\n\n        Parameters\n        ----------\n        args: tuple\n            Required for TK Callback but unused\n        \"\"\"\n        logger.debug(\"Refreshing Graph\")\n        if not self._graph_initialised:\n            return\n        valid = self._compile_display_data()\n        if not valid:\n            logger.debug(\"Invalid data\")\n            return\n        assert self._graph is not None\n        self._graph.refresh(self._display_data,\n                            self._vars.display.get(),\n                            self._vars.scale.get())\n        logger.debug(\"Refreshed Graph\")\n\n    def _graph_scale(self, *args) -> None:  # pylint:disable=unused-argument\n        \"\"\" Action for changing graph scale.\n\n        Parameters\n        ----------\n        args: tuple\n            Required for TK Callback but unused\n        \"\"\"\n        assert self._graph is not None\n        if not self._graph_initialised:\n            return\n        self._graph.set_yscale_type(self._vars.scale.get())\n\n    @classmethod\n    def _set_help(cls, action: str) -> str:\n        \"\"\" Set the help text for option buttons.\n\n        Parameters\n        ----------\n        action: str\n            The action to get the help text for\n\n        Returns\n        -------\n        str\n            The help text for the given action\n        \"\"\"\n        lookup = {\n            \"reload\": _(\"Refresh graph\"),\n            \"save\": _(\"Save display data to csv\"),\n            \"avgiterations\": _(\"Number of data points to sample for rolling average\"),\n            \"smoothamount\": _(\"Set the smoothing amount. 0 is no smoothing, 0.99 is maximum \"\n                              \"smoothing\"),\n            \"outliers\": _(\"Flatten data points that fall more than 1 standard deviation from the \"\n                          \"mean to the mean value.\"),\n            \"avg\": _(\"Display rolling average of the data\"),\n            \"smoothed\": _(\"Smooth the data\"),\n            \"raw\": _(\"Display raw data\"),\n            \"trend\": _(\"Display polynormal data trend\"),\n            \"display\": _(\"Set the data to display\"),\n            \"scale\": _(\"Change y-axis scale\")}\n        return lookup.get(action.lower(), \"\")\n\n    def _compile_display_data(self) -> bool:\n        \"\"\" Compile the data to be displayed.\n\n        Returns\n        -------\n        bool\n            ``True`` if there is valid data to display, ``False`` if not\n        \"\"\"\n        if self._thread is None:\n            logger.debug(\"Compiling Display Data in background thread\")\n            loss_keys = [key for key, val in self._vars.loss_keys.items()\n                         if val.get()]\n            logger.debug(\"Selected loss_keys: %s\", loss_keys)\n\n            selections = self._selections_to_list()\n\n            if not self._check_valid_selection(loss_keys, selections):\n                logger.warning(\"No data to display. Not refreshing\")\n                return False\n            self._vars.status.set(\"Loading Data...\")\n\n            if self._graph is not None:\n                self._graph.pack_forget()\n            self._lbl_loading.pack(fill=tk.BOTH, expand=True)\n            self.update_idletasks()\n\n            kwargs = {\"session_id\": self._session_id,\n                      \"display\": self._vars.display.get(),\n                      \"loss_keys\": loss_keys,\n                      \"selections\": selections,\n                      \"avg_samples\": self._vars.avgiterations.get(),\n                      \"smooth_amount\": self._vars.smoothamount.get(),\n                      \"flatten_outliers\": self._vars.outliers.get()}\n            self._thread = LongRunningTask(target=self._get_display_data,\n                                           kwargs=kwargs,\n                                           widget=self)\n            self._thread.start()\n            self.after(1000, self._compile_display_data)\n            return True\n        if not self._thread.complete.is_set():\n            logger.debug(\"Popup Data not yet available\")\n            self.after(1000, self._compile_display_data)\n            return True\n\n        logger.debug(\"Getting Popup from background Thread\")\n        self._display_data = self._thread.get_result()\n        self._thread = None\n        if not self._check_valid_data():\n            logger.warning(\"No valid data to display. Not refreshing\")\n            self._vars.status.set(\"\")\n            return False\n        logger.debug(\"Compiled Display Data\")\n        self._vars.buildgraph.set(True)\n        return True\n\n    @classmethod\n    def _get_display_data(cls, **kwargs) -> Calculations:\n        \"\"\" Get the display data in a LongRunningTask.\n\n        Parameters\n        ----------\n        kwargs: dict\n            The keyword arguments to pass to `lib.gui.analysis.Calculations`\n\n        Returns\n        -------\n        :class:`lib.gui.analysis.Calculations`\n            The summarized results for the given session\n        \"\"\"\n        return Calculations(**kwargs)\n\n    def _check_valid_selection(self, loss_keys: list[str], selections: list[str]) -> bool:\n        \"\"\" Check that there will be data to display.\n\n        Parameters\n        ----------\n        loss_keys: list\n            The selected loss to display\n        selections: list\n            The selected checkbox options\n\n        Returns\n        -------\n        bool\n            ``True` if there is data to be displayed, otherwise ``False``\n        \"\"\"\n        display = self._vars.display.get().lower()\n        logger.debug(\"Validating selection. (loss_keys: %s, selections: %s, display: %s)\",\n                     loss_keys, selections, display)\n        if not selections or (display == \"loss\" and not loss_keys):\n            return False\n        return True\n\n    def _check_valid_data(self) -> bool:\n        \"\"\" Check that the selections holds valid data to display\n            NB: len-as-condition is used as data could be a list or a numpy array\n\n        Returns\n        -------\n        bool\n            ``True` if there is data to be displayed, otherwise ``False``\n        \"\"\"\n        assert self._display_data is not None\n        logger.debug(\"Validating data. %s\",\n                     {key: len(val) for key, val in self._display_data.stats.items()})\n        if any(len(val) == 0  # pylint:disable=len-as-condition\n               for val in self._display_data.stats.values()):\n            return False\n        return True\n\n    def _selections_to_list(self) -> list[str]:\n        \"\"\" Compile checkbox selections to a list.\n\n        Returns\n        -------\n        list\n            The selected options from the check-boxes\n        \"\"\"\n        logger.debug(\"Compiling selections to list\")\n        selections = []\n        for item in (\"raw\", \"trend\", \"avg\", \"smoothed\"):\n            var: tk.BooleanVar = getattr(self._vars, item)\n            if var.get():\n                selections.append(item)\n        logger.debug(\"Compiling selections to list: %s\", selections)\n        return selections\n\n    def _graph_build(self, *args) -> None:  # pylint:disable=unused-argument\n        \"\"\" Build the graph in the top right paned window\n\n        Parameters\n        ----------\n        args: tuple\n            Required for TK Callback but unused\n        \"\"\"\n        if not self._vars.buildgraph.get():\n            return\n        self._vars.status.set(\"Loading Data...\")\n        logger.debug(\"Building Graph\")\n        self._lbl_loading.pack_forget()\n        self.update_idletasks()\n        if self._graph is None:\n            graph = SessionGraph(self._graph_frame,\n                                 self._display_data,\n                                 self._vars.display.get(),\n                                 self._vars.scale.get())\n            graph.pack(expand=True, fill=tk.BOTH)\n            graph.build()\n            self._graph = graph\n            self._graph_initialised = True\n        else:\n            self._graph.refresh(self._display_data,\n                                self._vars.display.get(),\n                                self._vars.scale.get())\n            self._graph.pack(fill=tk.BOTH, expand=True)\n        self._vars.status.set(\"\")\n        self._vars.buildgraph.set(False)\n        logger.debug(\"Built Graph\")\n", "lib/gui/display_graph.py": "#!/usr/bin python3\n\"\"\" Graph functions for Display Frame area of the Faceswap GUI \"\"\"\nfrom __future__ import annotations\nimport datetime\nimport logging\nimport os\nimport tkinter as tk\nimport typing as T\n\nfrom tkinter import ttk\nfrom math import ceil, floor\n\nimport numpy as np\nimport matplotlib\nfrom matplotlib import style\nfrom matplotlib.figure import Figure\nfrom matplotlib.backends.backend_tkagg import (FigureCanvasTkAgg,\n                                               NavigationToolbar2Tk)\nfrom matplotlib.backend_bases import NavigationToolbar2\n\nfrom lib.logger import parse_class_init\n\nfrom .custom_widgets import Tooltip\nfrom .utils import get_config, get_images, LongRunningTask\n\nif T.TYPE_CHECKING:\n    from matplotlib.lines import Line2D\n\nlogger: logging.Logger = logging.getLogger(__name__)\n\n\nclass GraphBase(ttk.Frame):  # pylint:disable=too-many-ancestors\n    \"\"\" Base class for matplotlib line graphs.\n\n    Parameters\n    ----------\n    parent: :class:`tkinter.ttk.Frame`\n        The parent frame that holds the graph\n    data: :class:`lib.gui.analysis.stats.Calculations`\n        The statistics class that holds the data to be displayed\n    ylabel: str\n        The data label for the y-axis\n    \"\"\"\n    def __init__(self, parent: ttk.Frame, data, ylabel: str) -> None:\n        super().__init__(parent)\n        matplotlib.use(\"TkAgg\")  # Can't be at module level as breaks Github CI\n        style.use(\"ggplot\")\n\n        self._calcs = data\n        self._ylabel = ylabel\n        self._colourmaps = [\"Reds\", \"Blues\", \"Greens\", \"Purples\", \"Oranges\", \"Greys\", \"copper\",\n                            \"summer\", \"bone\", \"hot\", \"cool\", \"pink\", \"Wistia\", \"spring\", \"winter\"]\n        self._lines: list[Line2D] = []\n        self._toolbar: \"NavigationToolbar\" | None = None\n        self._fig = Figure(figsize=(4, 4), dpi=75)\n\n        self._ax1 = self._fig.add_subplot(1, 1, 1)\n        self._plotcanvas = FigureCanvasTkAgg(self._fig, self)\n\n        self._initiate_graph()\n        self._update_plot(initiate=True)\n\n    @property\n    def calcs(self):\n        \"\"\" :class:`lib.gui.analysis.stats.Calculations`. The calculated statistics associated with\n        this graph. \"\"\"\n        return self._calcs\n\n    def _initiate_graph(self) -> None:\n        \"\"\" Place the graph canvas \"\"\"\n        logger.debug(\"Setting plotcanvas\")\n        self._plotcanvas.get_tk_widget().pack(side=tk.TOP, padx=5, fill=tk.BOTH, expand=True)\n        self._fig.subplots_adjust(left=0.100,\n                                  bottom=0.100,\n                                  right=0.95,\n                                  top=0.95,\n                                  wspace=0.2,\n                                  hspace=0.2)\n        logger.debug(\"Set plotcanvas\")\n\n    def _update_plot(self, initiate: bool = True) -> None:\n        \"\"\" Update the plot with incoming data\n\n        Parameters\n        ----------\n        initiate: bool, Optional\n            Whether the graph should be initialized for the first time (``True``) or data is being\n            updated for an existing graph (``False``). Default: ``True``\n        \"\"\"\n        logger.trace(\"Updating plot\")  # type:ignore[attr-defined]\n        if initiate:\n            logger.debug(\"Initializing plot\")\n            self._lines = []\n            self._ax1.clear()\n            self._axes_labels_set()\n            logger.debug(\"Initialized plot\")\n\n        fulldata = list(self._calcs.stats.values())\n        self._axes_limits_set(fulldata)\n\n        if self._calcs.start_iteration > 0:\n            end_iteration = self._calcs.start_iteration + self._calcs.iterations\n            xrng = list(range(self._calcs.start_iteration, end_iteration))\n        else:\n            xrng = list(range(self._calcs.iterations))\n\n        keys = list(self._calcs.stats.keys())\n\n        for idx, item in enumerate(self._lines_sort(keys)):\n            if initiate:\n                self._lines.extend(self._ax1.plot(xrng, self._calcs.stats[item[0]],\n                                                  label=item[1], linewidth=item[2], color=item[3]))\n            else:\n                self._lines[idx].set_data(xrng, self._calcs.stats[item[0]])\n\n        if initiate:\n            self._legend_place()\n        logger.trace(\"Updated plot\")  # type:ignore[attr-defined]\n\n    def _axes_labels_set(self) -> None:\n        \"\"\" Set the X and Y axes labels. \"\"\"\n        logger.debug(\"Setting axes labels. y-label: '%s'\", self._ylabel)\n        self._ax1.set_xlabel(\"Iterations\")\n        self._ax1.set_ylabel(self._ylabel)\n\n    def _axes_limits_set_default(self) -> None:\n        \"\"\" Set the default axes limits for the X and Y axes. \"\"\"\n        logger.debug(\"Setting default axes ranges\")\n        self._ax1.set_ylim(0.00, 100.0)\n        self._ax1.set_xlim(0, 1)\n\n    def _axes_limits_set(self, data: list[float]) -> None:\n        \"\"\" Set the axes limits.\n\n        Parameters\n        ----------\n        data: list\n            The data points for the Y Axis\n        \"\"\"\n        xmin = self._calcs.start_iteration\n        if self._calcs.start_iteration > 0:\n            xmax = self._calcs.iterations + self._calcs.start_iteration\n        else:\n            xmax = self._calcs.iterations\n        xmax = max(1, xmax - 1)\n\n        if data:\n            ymin, ymax = self._axes_data_get_min_max(data)\n            self._ax1.set_ylim(ymin, ymax)\n            self._ax1.set_xlim(xmin, xmax)\n            logger.trace(\"axes ranges: (y: (%s, %s), x:(0, %s)\",  # type:ignore[attr-defined]\n                         ymin, ymax, xmax)\n        else:\n            self._axes_limits_set_default()\n\n    @staticmethod\n    def _axes_data_get_min_max(data: list[float]) -> tuple[float, float]:\n        \"\"\" Obtain the minimum and maximum values for the y-axis from the given data points.\n\n        Parameters\n        ----------\n        data: list\n            The data points for the Y Axis\n\n        Returns\n        -------\n        tuple\n            The minimum and maximum values for the y axis\n        \"\"\"\n        ymins, ymaxs = [], []\n\n        for item in data:  # TODO Handle as array not loop\n            ymins.append(np.nanmin(item) * 1000)\n            ymaxs.append(np.nanmax(item) * 1000)\n        ymin = floor(min(ymins)) / 1000\n        ymax = ceil(max(ymaxs)) / 1000\n        logger.trace(\"ymin: %s, ymax: %s\", ymin, ymax)  # type:ignore[attr-defined]\n        return ymin, ymax\n\n    def _axes_set_yscale(self, scale: str) -> None:\n        \"\"\" Set the Y-Scale to log or linear\n\n        Parameters\n        ----------\n        scale: str\n            Should be one of ``\"log\"`` or ``\"linear\"``\n        \"\"\"\n        logger.debug(\"yscale: '%s'\", scale)\n        self._ax1.set_yscale(scale)\n\n    def _lines_sort(self, keys: list[str]) -> list[list[str | int | tuple[float]]]:\n        \"\"\" Sort the data keys into consistent order and set line color map and line width.\n\n        Parameters\n        ----------\n        keys: list\n            The list of data point keys\n\n        Returns\n        -------\n        list\n            A list of loss keys with their corresponding line formatting and color information\n        \"\"\"\n        logger.trace(\"Sorting lines\")  # type:ignore[attr-defined]\n        raw_lines: list[list[str]] = []\n        sorted_lines: list[list[str]] = []\n        for key in sorted(keys):\n            title = key.replace(\"_\", \" \").title()\n            if key.startswith(\"raw\"):\n                raw_lines.append([key, title])\n            else:\n                sorted_lines.append([key, title])\n\n        groupsize = self._lines_groupsize(raw_lines, sorted_lines)\n        sorted_lines = raw_lines + sorted_lines\n        lines = self._lines_style(sorted_lines, groupsize)\n        return lines\n\n    @staticmethod\n    def _lines_groupsize(raw_lines: list[list[str]], sorted_lines: list[list[str]]) -> int:\n        \"\"\" Get the number of items in each group.\n\n        If raw data isn't selected, then check the length of remaining groups until something is\n        found.\n\n        Parameters\n        ----------\n        raw_lines: list\n            The list of keys for the raw data points\n        sorted_lines:\n            The list of sorted line keys to display on the graph\n\n        Returns\n        -------\n        int\n            The size of each group that exist within the data set.\n        \"\"\"\n        groupsize = 1\n        if raw_lines:\n            groupsize = len(raw_lines)\n        elif sorted_lines:\n            keys = [key[0][:key[0].find(\"_\")] for key in sorted_lines]\n            distinct_keys = set(keys)\n            groupsize = len(keys) // len(distinct_keys)\n        logger.trace(groupsize)  # type:ignore[attr-defined]\n        return groupsize\n\n    def _lines_style(self,\n                     lines: list[list[str]],\n                     groupsize: int) -> list[list[str | int | tuple[float]]]:\n        \"\"\" Obtain the color map and line width for each group.\n\n        Parameters\n        ----------\n        lines: list\n            The list of sorted line keys to display on the graph\n        groupsize: int\n            The size of each group to display in the graph\n\n        Returns\n        -------\n        list\n            A list of loss keys with their corresponding line formatting and color information\n        \"\"\"\n        logger.trace(\"Setting lines style\")  # type:ignore[attr-defined]\n        groups = int(len(lines) / groupsize)\n        colours = self._lines_create_colors(groupsize, groups)\n        widths = list(range(1, groups + 1))\n        retval = T.cast(list[list[str | int | tuple[float]]], lines)\n        for idx, item in enumerate(retval):\n            linewidth = widths[idx // groupsize]\n            item.extend((linewidth, colours[idx]))\n        return retval\n\n    def _lines_create_colors(self, groupsize: int, groups: int) -> list[tuple[float]]:\n        \"\"\" Create the color maps.\n\n        Parameters\n        ----------\n        groupsize: int\n            The size of each group to display in the graph\n        groups: int\n            The total number of groups to graph\n\n        Returns\n        -------\n        list\n            The colour map for each group\n        \"\"\"\n        colours = []\n        for i in range(1, groups + 1):\n            for colour in self._colourmaps[0:groupsize]:\n                cmap = matplotlib.cm.get_cmap(colour)\n                cpoint = 1 - (i / 5)\n                colours.append(cmap(cpoint))\n        logger.trace(colours)  # type:ignore[attr-defined]\n        return colours\n\n    def _legend_place(self) -> None:\n        \"\"\" Place and format the graph legend \"\"\"\n        logger.debug(\"Placing legend\")\n        self._ax1.legend(loc=\"upper right\", ncol=2)\n\n    def _toolbar_place(self, parent: ttk.Frame) -> None:\n        \"\"\" Add Graph Navigation toolbar.\n\n        Parameters\n        ----------\n        parent: ttk.Frame\n            The parent graph frame to place the toolbar onto\n        \"\"\"\n        logger.debug(\"Placing toolbar\")\n        self._toolbar = NavigationToolbar(self._plotcanvas, parent)\n        self._toolbar.pack(side=tk.BOTTOM)\n        self._toolbar.update()\n\n    def clear(self) -> None:\n        \"\"\" Clear the graph plots from RAM \"\"\"\n        logger.debug(\"Clearing graph from RAM: %s\", self)\n        self._fig.clf()\n        del self._fig\n\n\nclass TrainingGraph(GraphBase):  # pylint:disable=too-many-ancestors\n    \"\"\" Live graph to be displayed during training.\n\n    Parameters\n    ----------\n    parent: :class:`tkinter.ttk.Frame`\n        The parent frame that holds the graph\n    data: :class:`lib.gui.analysis.stats.Calculations`\n        The statistics class that holds the data to be displayed\n    ylabel: str\n        The data label for the y-axis\n    \"\"\"\n\n    def __init__(self, parent: ttk.Frame, data, ylabel: str) -> None:\n        logger.debug(parse_class_init(locals()))\n        super().__init__(parent, data, ylabel)\n        self._thread: LongRunningTask | None = None  # Thread for LongRunningTask\n        self._displayed_keys: list[str] = []\n        self._add_callback()\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    def _add_callback(self) -> None:\n        \"\"\" Add the variable trace to update graph on refresh button press or save iteration. \"\"\"\n        get_config().tk_vars.refresh_graph.trace(\"w\", self.refresh)  # type:ignore\n\n    def build(self) -> None:\n        \"\"\" Build the Training graph. \"\"\"\n        logger.debug(\"Building training graph\")\n        self._plotcanvas.draw()\n        logger.debug(\"Built training graph\")\n\n    def refresh(self, *args) -> None:  # pylint:disable=unused-argument\n        \"\"\" Read the latest loss data and apply to current graph \"\"\"\n        refresh_var = T.cast(tk.BooleanVar, get_config().tk_vars.refresh_graph)\n        if not refresh_var.get() and self._thread is None:\n            return\n\n        if self._thread is None:\n            logger.debug(\"Updating plot data\")\n            self._thread = LongRunningTask(target=self._calcs.refresh)\n            self._thread.start()\n            self.after(1000, self.refresh)\n        elif not self._thread.complete.is_set():\n            logger.debug(\"Graph Data not yet available\")\n            self.after(1000, self.refresh)\n        else:\n            logger.debug(\"Updating plot with data from background thread\")\n            self._calcs = self._thread.get_result()  # Terminate the LongRunningTask object\n            self._thread = None\n\n            dsp_keys = list(sorted(self._calcs.stats))\n            if dsp_keys != self._displayed_keys:\n                logger.debug(\"Reinitializing graph for keys change. Old keys: %s New keys: %s\",\n                             self._displayed_keys, dsp_keys)\n                initiate = True\n                self._displayed_keys = dsp_keys\n            else:\n                initiate = False\n\n            self._update_plot(initiate=initiate)\n            self._plotcanvas.draw()\n            refresh_var.set(False)\n\n    def save_fig(self, location: str) -> None:\n        \"\"\" Save the current graph to file\n\n        Parameters\n        ----------\n        location: str\n            The full path to the folder where the current graph should be saved\n        \"\"\"\n        logger.debug(\"Saving graph: '%s'\", location)\n        keys = sorted([key.replace(\"raw_\", \"\") for key in self._calcs.stats.keys()\n                       if key.startswith(\"raw_\")])\n        filename = \" - \".join(keys)\n        now = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        filename = os.path.join(location, f\"{filename}_{now}.png\")\n        self._fig.set_size_inches(16, 9)\n        self._fig.savefig(filename, bbox_inches=\"tight\", dpi=120)\n        print(f\"Saved graph to {filename}\")\n        logger.debug(\"Saved graph: '%s'\", filename)\n        self._resize_fig()\n\n    def _resize_fig(self) -> None:\n        \"\"\" Resize the figure to the current canvas size. \"\"\"\n        class Event():  # pylint:disable=too-few-public-methods\n            \"\"\" Event class that needs to be passed to plotcanvas.resize \"\"\"\n            pass  # pylint:disable=unnecessary-pass\n        setattr(Event, \"width\", self.winfo_width())\n        setattr(Event, \"height\", self.winfo_height())\n        self._plotcanvas.resize(Event)  # pylint:disable=no-value-for-parameter\n\n\nclass SessionGraph(GraphBase):  # pylint:disable=too-many-ancestors\n    \"\"\" Session Graph for session pop-up.\n\n    Parameters\n    ----------\n    parent: :class:`tkinter.ttk.Frame`\n        The parent frame that holds the graph\n    data: :class:`lib.gui.analysis.stats.Calculations`\n        The statistics class that holds the data to be displayed\n    ylabel: str\n        The data label for the y-axis\n    scale: str\n        Should be one of ``\"log\"`` or ``\"linear\"``\n    \"\"\"\n    def __init__(self, parent: ttk.Frame, data, ylabel: str, scale: str) -> None:\n        logger.debug(parse_class_init(locals()))\n        super().__init__(parent, data, ylabel)\n        self._scale = scale\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    def build(self) -> None:\n        \"\"\" Build the session graph \"\"\"\n        logger.debug(\"Building session graph\")\n        self._toolbar_place(self)\n        self._plotcanvas.draw()\n        logger.debug(\"Built session graph\")\n\n    def refresh(self, data, ylabel: str, scale: str) -> None:\n        \"\"\" Refresh the Session Graph's data.\n\n        Parameters\n        ----------\n        data: :class:`lib.gui.analysis.stats.Calculations`\n            The statistics class that holds the data to be displayed\n        ylabel: str\n            The data label for the y-axis\n        scale: str\n            Should be one of ``\"log\"`` or ``\"linear\"``\n        \"\"\"\n        logger.debug(\"Refreshing session graph: (ylabel: '%s', scale: '%s')\", ylabel, scale)\n        self._calcs = data\n        self._ylabel = ylabel\n        self.set_yscale_type(scale)\n        logger.debug(\"Refreshed session graph\")\n\n    def set_yscale_type(self, scale: str) -> None:\n        \"\"\" Set the scale type for the y-axis and redraw.\n\n        Parameters\n        ----------\n        scale: str\n            Should be one of ``\"log\"`` or ``\"linear\"``\n        \"\"\"\n        scale = scale.lower()\n        logger.debug(\"Updating scale type: '%s'\", scale)\n        self._scale = scale\n        self._update_plot(initiate=True)\n        self._axes_set_yscale(self._scale)\n        self._plotcanvas.draw()\n        logger.debug(\"Updated scale type\")\n\n\nclass NavigationToolbar(NavigationToolbar2Tk):  # pylint:disable=too-many-ancestors\n    \"\"\" Overrides the default Navigation Toolbar to provide only the buttons we require\n    and to layout the items in a consistent manner with the rest of the GUI for the Analysis\n    Session Graph pop up Window.\n\n    Parameters\n    ----------\n    canvas: :class:`matplotlib.backends.backend_tkagg.FigureCanvasTkAgg`\n        The canvas that holds the displayed graph and will hold the toolbar\n    window: :class:`~lib.gui.display_graph.SessionGraph`\n        The Session Graph canvas\n    pack_toolbar: bool, Optional\n        Whether to pack the Tool bar or not. Default: ``True``\n    \"\"\"\n    toolitems = [t for t in NavigationToolbar2Tk.toolitems if\n                 t[0] in (\"Home\", \"Pan\", \"Zoom\", \"Save\")]\n\n    def __init__(self,  # pylint:disable=super-init-not-called\n                 canvas: FigureCanvasTkAgg,\n                 window: ttk.Frame,\n                 *,\n                 pack_toolbar: bool = True) -> None:\n        logger.debug(parse_class_init(locals()))\n        # Avoid using self.window (prefer self.canvas.get_tk_widget().master),\n        # so that Tool implementations can reuse the methods.\n\n        ttk.Frame.__init__(self,  # pylint:disable=non-parent-init-called\n                           master=window,\n                           width=int(canvas.figure.bbox.width),\n                           height=50)\n\n        sep = ttk.Frame(self, height=2, relief=tk.RIDGE)\n        sep.pack(fill=tk.X, pady=(5, 0), side=tk.TOP)\n\n        btnframe = ttk.Frame(self)  # Add a button frame to consistently line up GUI\n        btnframe.pack(fill=tk.X, padx=5, pady=5, side=tk.RIGHT)\n\n        self._buttons = {}\n        for text, tooltip_text, image_file, callback in self.toolitems:\n            self._buttons[text] = button = self._Button(\n                btnframe,\n                text,\n                image_file,\n                toggle=callback in [\"zoom\", \"pan\"],\n                command=getattr(self, callback),\n            )\n            if tooltip_text is not None:\n                Tooltip(button, text=tooltip_text, wrap_length=200)\n\n        self.message = tk.StringVar(master=self)\n        self._message_label = ttk.Label(master=self, textvariable=self.message)\n        self._message_label.pack(side=tk.LEFT, padx=5)  # Additional left padding\n\n        NavigationToolbar2.__init__(self, canvas)  # pylint:disable=non-parent-init-called\n        if pack_toolbar:\n            self.pack(side=tk.BOTTOM, fill=tk.X)\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    @staticmethod\n    def _Button(frame: ttk.Frame,  # pylint:disable=arguments-differ,arguments-renamed\n                text: str,\n                image_file: str,\n                toggle: bool,\n                command) -> ttk.Button | ttk.Checkbutton:\n        \"\"\" Override the default button method to use our icons and ttk widgets for\n        consistent GUI layout.\n\n        Parameters\n        ----------\n        frame: :class:`tkinter.ttk.Frame`\n            The frame that holds the buttons\n        text: str\n            The display text for the button\n        image_file: str\n            The name of the image file to use\n        toggle: bool\n            Whether to use a checkbutton (``True``) or a regular button (``False``)\n        command: method\n            The Navigation Toolbar callback method\n\n        Returns\n        -------\n        :class:`tkinter.ttk.Button` or :class:`tkinter.ttk.Checkbutton`\n            The widger to use. A button if the option is not toggleable, a checkbutton if the\n            option is toggleable.\n        \"\"\"\n        iconmapping = {\"home\": \"reload\",\n                       \"filesave\": \"save\",\n                       \"zoom_to_rect\": \"zoom\"}\n        icon = iconmapping[image_file] if iconmapping.get(image_file, None) else image_file\n        img = get_images().icons[icon]\n\n        if not toggle:\n            btn: ttk.Button | ttk.Checkbutton = ttk.Button(frame,\n                                                           text=text,\n                                                           image=img,\n                                                           command=command)\n        else:\n            var = tk.IntVar(master=frame)\n            btn = ttk.Checkbutton(frame, text=text, image=img, command=command, variable=var)\n\n            # Original implementation uses tk Checkbuttons which have a select and deselect\n            # method. These aren't available in ttk Checkbuttons, so we monkey patch the methods\n            # to update the underlying variable.\n            setattr(btn, \"select\", lambda i=1: var.set(i))\n            setattr(btn, \"deselect\", lambda i=0: var.set(i))\n\n        btn.pack(side=tk.RIGHT, padx=2)\n        return btn\n", "lib/gui/__init__.py": "#!/usr/bin python3\n\"\"\" The Faceswap GUI \"\"\"\n\nfrom lib.gui.command import CommandNotebook\nfrom lib.gui.custom_widgets import ConsoleOut, StatusBar\nfrom lib.gui.display import DisplayNotebook\nfrom lib.gui.options import CliOptions\nfrom lib.gui.menu import MainMenuBar, TaskBar\nfrom lib.gui.project import LastSession\nfrom lib.gui.utils import (get_config, get_images, initialize_config, initialize_images,\n                           preview_trigger)\nfrom lib.gui.wrapper import ProcessWrapper\n", "lib/gui/project.py": "#!/usr/bin/env python3\n\"\"\" Handling of Faceswap GUI Projects, Tasks and Last Session \"\"\"\n\nimport logging\nimport os\nimport tkinter as tk\nfrom tkinter import messagebox\n\nfrom lib.serializer import get_serializer\n\nlogger = logging.getLogger(__name__)\n\n\nclass _GuiSession():  # pylint:disable=too-few-public-methods\n    \"\"\" Parent class for GUI Session Handlers.\n\n    Parameters\n    ----------\n    config: :class:`lib.gui.utils.Config`\n        The master GUI config\n    file_handler: :class:`lib.gui.utils.FileHandler`\n        A file handler object\n\n    \"\"\"\n    def __init__(self, config, file_handler=None):\n        # NB file_handler has to be passed in to avoid circular imports\n        logger.debug(\"Initializing: %s: (config: %s, file_handler: %s)\",\n                     self.__class__.__name__, config, file_handler)\n        self._serializer = get_serializer(\"json\")\n        self._config = config\n\n        self._options = None\n        self._file_handler = file_handler\n        self._filename = None\n        self._saved_tasks = None\n        self._modified = False\n        logger.debug(\"Initialized: %s\", self.__class__.__name__)\n\n    @property\n    def _active_tab(self):\n        \"\"\" str: The name of the currently selected :class:`lib.gui.command.CommandNotebook`\n        tab. \"\"\"\n        notebook = self._config.command_notebook\n        toolsbook = self._config.tools_notebook\n        command = notebook.tab(notebook.select(), \"text\").lower()\n        if command == \"tools\":\n            command = toolsbook.tab(toolsbook.select(), \"text\").lower()\n        logger.debug(\"Active tab: %s\", command)\n        return command\n\n    @property\n    def _modified_vars(self):\n        \"\"\" dict: The tkinter Boolean vars indicating the modified state for each tab. \"\"\"\n        return self._config.modified_vars\n\n    @property\n    def _file_exists(self):\n        \"\"\" bool: ``True`` if :attr:`_filename` exists otherwise ``False``. \"\"\"\n        return self._filename is not None and os.path.isfile(self._filename)\n\n    @property\n    def _cli_options(self):\n        \"\"\" dict: the raw cli options from :attr:`_options` with project fields removed. \"\"\"\n        return {key: val for key, val in self._options.items() if isinstance(val, dict)}\n\n    @property\n    def _default_options(self):\n        \"\"\" dict: The default options for all tabs \"\"\"\n        return self._config.default_options\n\n    @property\n    def _dirname(self):\n        \"\"\" str: The folder name that :attr:`_filename` resides in. Returns ``None`` if\n        filename is ``None``. \"\"\"\n        return os.path.dirname(self._filename) if self._filename is not None else None\n\n    @property\n    def _basename(self):\n        \"\"\" str: The base name of :attr:`_filename`. Returns ``None`` if filename is ``None``. \"\"\"\n        return os.path.basename(self._filename) if self._filename is not None else None\n\n    @property\n    def _stored_tab_name(self):\n        \"\"\"str: The tab_name stored in :attr:`_options` or ``None`` if it does not exist \"\"\"\n        if self._options is None:\n            return None\n        return self._options.get(\"tab_name\", None)\n\n    @property\n    def _selected_to_choices(self):\n        \"\"\" dict: The selected value and valid choices for multi-option, radio or combo options.\n        \"\"\"\n        valid_choices = {cmd: {opt: {\"choices\": val.cpanel_option.choices,\n                                     \"is_multi\": val.cpanel_option.is_multi_option}\n                               for opt, val in data.items()\n                               if hasattr(val, \"cpanel_option\")  # Filter out helptext\n                               and val.cpanel_option.choices is not None\n                               }\n                         for cmd, data in self._config.cli_opts.opts.items()}\n        logger.trace(\"valid_choices: %s\", valid_choices)\n        retval = {command: {option: {\"value\": value,\n                                     \"is_multi\": valid_choices[command][option][\"is_multi\"],\n                                     \"choices\":  valid_choices[command][option][\"choices\"]}\n                            for option, value in options.items()\n                            if value and command in valid_choices\n                            and option in valid_choices[command]}\n                  for command, options in self._options.items()\n                  if isinstance(options, dict)}\n        logger.trace(\"returning: %s\", retval)\n        return retval\n\n    def _current_gui_state(self, command=None):\n        \"\"\" The current state of the GUI.\n\n        Parameters\n        ----------\n        command: str, optional\n        If provided, returns the state of just the given tab command. If ``None`` returns options\n        for all tabs. Default ``None``\n\n        Returns\n        -------\n        dict: The options currently set in the GUI\n        \"\"\"\n        return self._config.cli_opts.get_option_values(command)\n\n    def _set_filename(self, filename=None, sess_type=\"project\"):\n        \"\"\" Set the :attr:`_filename` attribute.\n\n        :attr:`_filename` is set either from a given filename or the result from\n        a :attr:`_file_handler`.\n\n        Parameters\n        ----------\n        filename: str, optional\n            An optional filename. If given then this filename will be used otherwise it will be\n            collected by a :attr:`_file_handler`\n\n        sess_type: {all, project, task}, optional\n            The session type that the filename is being set for. Dictates the type of file handler\n            that is opened.\n\n        Returns\n        -------\n        bool: `True` if filename has been successfully set otherwise ``False``\n        \"\"\"\n        logger.debug(\"filename: '%s', sess_type: '%s'\", filename, sess_type)\n        handler = f\"config_{sess_type}\"\n\n        if filename is None:\n            logger.debug(\"Popping file handler\")\n            cfgfile = self._file_handler(\"open\", handler).return_file\n            if not cfgfile:\n                logger.debug(\"No filename given\")\n                return False\n            filename = cfgfile.name\n            cfgfile.close()\n\n        if not os.path.isfile(filename):\n            msg = f\"File does not exist: '{filename}'\"\n            logger.error(msg)\n            return False\n        ext = os.path.splitext(filename)[1]\n        if (sess_type == \"project\" and ext != \".fsw\") or (sess_type == \"task\" and ext != \".fst\"):\n            logger.debug(\"Invalid file extension for session type: (sess_type: '%s', \"\n                         \"extension: '%s')\", sess_type, ext)\n            return False\n        logger.debug(\"Setting filename: '%s'\", filename)\n        self._filename = filename\n        return True\n\n    # GUI STATE SETTING\n    def _set_options(self, command=None):\n        \"\"\" Set the GUI options based on the currently stored properties of :attr:`_options`\n        and sets the active tab.\n\n        Parameters\n        ----------\n        command: str, optional\n            The tab to set the options for. If None then sets options for all tabs.\n            Default: ``None``\n        \"\"\"\n        opts = self._get_options_for_command(command) if command else self._cli_options\n        logger.debug(\"command: %s, opts: %s\", command, opts)\n        if opts is None:\n            logger.debug(\"No options found. Returning\")\n            return\n        for cmd, opt in opts.items():\n            self._set_gui_state_for_command(cmd, opt)\n        tab_name = self._options.get(\"tab_name\", None) if command is None else command\n        tab_name = tab_name if tab_name is not None else \"extract\"\n        logger.debug(\"tab_name: %s\", tab_name)\n        self._config.set_active_tab_by_name(tab_name)\n\n    def _get_options_for_command(self, command):\n        \"\"\" Return a single command's options from :attr:`_options` formatted consistently with\n        an all options dict.\n\n        Parameters\n        ----------\n        command: str\n            The command to return the options for\n\n        Returns\n        -------\n        dict: The options for a single command in the format {command: options}. If the command\n        is not found then returns ``None``\n        \"\"\"\n        logger.debug(command)\n        opts = self._options.get(command, None)\n        retval = {command: opts}\n        if not opts:\n            self._config.tk_vars.console_clear.set(True)\n            logger.info(\"No  %s section found in file\", command)\n            retval = None\n        logger.debug(retval)\n        return retval\n\n    def _set_gui_state_for_command(self, command, options):\n        \"\"\" Set the GUI state for the given command.\n\n        Parameters\n        ----------\n        command: str\n            The tab to set the options for\n        options: dict\n            The option values to set the GUI to\n        \"\"\"\n        logger.debug(\"command: %s: options: %s\", command, options)\n        if not options:\n            logger.debug(\"No options provided, not updating GUI\")\n            return\n        for srcopt, srcval in options.items():\n            optvar = self._config.cli_opts.get_one_option_variable(command, srcopt)\n            if not optvar:\n                continue\n            logger.trace(\"setting option: (srcopt: %s, optvar: %s, srcval: %s)\",\n                         srcopt, optvar, srcval)\n            optvar.set(srcval)\n\n    def _reset_modified_var(self, command=None):\n        \"\"\" Reset :attr:`_modified_vars` variables back to unmodified (`False`) for all\n        commands or for the given command.\n\n        Parameters\n        ----------\n        command: str, optional\n            The command to reset the modified tkinter variable for. If ``None`` then all tkinter\n            modified variables are reset to `False`. Default: ``None``\n        \"\"\"\n        for key, tk_var in self._modified_vars.items():\n            if (command is None or command == key) and tk_var.get():\n                logger.debug(\"Reset modified state for: (command: %s key: %s)\", command, key)\n                tk_var.set(False)\n\n    # RECENT FILE HANDLING\n    def _add_to_recent(self, command=None):\n        \"\"\" Add the file for this session to the recent files list.\n\n        Parameters\n        ----------\n        command: str, optional\n            The command that this session relates to. If `None` then the whole project is added.\n            Default: ``None``\n        \"\"\"\n        logger.debug(command)\n        if self._filename is None:\n            logger.debug(\"No filename for selected file. Not adding to recent.\")\n            return\n        recent_filename = os.path.join(self._config.pathcache, \".recent.json\")\n        logger.debug(\"Adding to recent files '%s': (%s, %s)\",\n                     recent_filename, self._filename, command)\n        if not os.path.exists(recent_filename) or os.path.getsize(recent_filename) == 0:\n            logger.debug(\"Starting with empty recent_files list\")\n            recent_files = []\n        else:\n            logger.debug(\"loading recent_files list: %s\", recent_filename)\n            recent_files = self._serializer.load(recent_filename)\n        logger.debug(\"Initial recent files: %s\", recent_files)\n        recent_files = self._del_from_recent(self._filename, recent_files)\n        ftype = \"project\" if command is None else command\n        recent_files.insert(0, (self._filename, ftype))\n        recent_files = recent_files[:20]\n        logger.debug(\"Final recent files: %s\", recent_files)\n        self._serializer.save(recent_filename, recent_files)\n\n    def _del_from_recent(self, filename, recent_files=None, save=False):\n        \"\"\" Remove an item from the recent files list.\n\n        Parameters\n        ----------\n        filename: str\n            The filename to be removed from the recent files list\n        recent_files: list, optional\n            If the recent files list has already been loaded, it can be passed in to avoid\n            loading again. If ``None`` then load the recent files list from disk. Default: ``None``\n        save: bool, optional\n            Whether the recent files list should be saved after removing the file. ``True`` saves\n            the file, ``False`` does not. Default: ``False``\n        \"\"\"\n        recent_filename = os.path.join(self._config.pathcache, \".recent.json\")\n        if recent_files is None:\n            logger.debug(\"Loading file list from disk: %s\", recent_filename)\n            if not os.path.exists(recent_filename) or os.path.getsize(recent_filename) == 0:\n                logger.debug(\"No recent file list\")\n                return None\n            recent_files = self._serializer.load(recent_filename)\n        filenames = [recent[0] for recent in recent_files]\n        if filename in filenames:\n            idx = filenames.index(filename)\n            logger.debug(\"Removing from recent file list: %s\", filename)\n            del recent_files[idx]\n            if save:\n                logger.debug(\"Saving recent files list: %s\", recent_filename)\n                self._serializer.save(recent_filename, recent_files)\n        else:\n            logger.debug(\"Filename '%s' does not appear in recent file list\", filename)\n        return recent_files\n\n    def _get_lone_task(self):\n        \"\"\" Get the sole command name from :attr:`_options`.\n\n        Returns\n        -------\n        str: The only existing command name in the current :attr:`_options` dict or ``None`` if\n        there are multiple commands stored.\n        \"\"\"\n        command = None\n        if len(self._cli_options) == 1:\n            command = list(self._cli_options.keys())[0]\n        logger.debug(command)\n        return command\n\n    # DISK IO\n    def _load(self):\n        \"\"\" Load GUI options from :attr:`_filename` location and set to :attr:`_options`.\n\n        Returns\n        -------\n        bool: ``True`` if successfully loaded otherwise ``False``\n        \"\"\"\n        if self._file_exists:\n            logger.debug(\"Loading config\")\n            self._options = self._serializer.load(self._filename)\n            self._check_valid_choices()\n            retval = True\n        else:\n            logger.debug(\"File doesn't exist. Aborting\")\n            retval = False\n        return retval\n\n    def _check_valid_choices(self):\n        \"\"\" Check whether the loaded file has any selected combo/radio/multi-option values that are\n        no longer valid and remove them so that they are not passed into faceswap. \"\"\"\n        for command, options in self._selected_to_choices.items():\n            for option, data in options.items():\n                if ((data[\"is_multi\"] and all(v in data[\"choices\"] for v in data[\"value\"].split()))\n                        or not data[\"is_multi\"] and data[\"value\"] in data[\"choices\"]):\n                    continue\n                if data[\"is_multi\"]:\n                    val = \" \".join([v for v in data[\"value\"].split() if v in data[\"choices\"]])\n                else:\n                    val = \"\"\n                val = self._default_options[command][option] if not val else val\n                logger.debug(\"Updating invalid value to default: (command: '%s', option: '%s', \"\n                             \"original value: '%s', new value: '%s')\", command, option,\n                             self._options[command][option], val)\n                self._options[command][option] = val\n\n    def _save_as_to_filename(self, session_type):\n        \"\"\" Set :attr:`_filename` from a save as dialog.\n\n        Parameters\n        ----------\n        session_type: ['all', 'task', 'project']\n            The type of session to pop the save as dialog for. Limits the allowed filetypes\n\n        Returns\n        -------\n        bool:\n            True if :attr:`filename` successfully set otherwise ``False``\n        \"\"\"\n        logger.debug(\"Popping save as file handler. session_type: '%s'\", session_type)\n        title = f\"Save {f'{session_type.title()} ' if session_type != 'all' else ''}As...\"\n        cfgfile = self._file_handler(\"save\",\n                                     f\"config_{session_type}\",\n                                     title=title,\n                                     initial_folder=self._dirname).return_file\n        if not cfgfile:\n            logger.debug(\"No filename provided. session_type: '%s'\", session_type)\n            return False\n        self._filename = cfgfile.name\n        logger.debug(\"Set filename: (session_type: '%s', filename: '%s'\",\n                     session_type, self._filename)\n        cfgfile.close()\n        return True\n\n    def _save(self, command=None):\n        \"\"\" Collect the options in the current GUI state and save.\n\n        Obtains the current options set in the GUI with the selected tab and applies them to\n        :attr:`_options`. Saves :attr:`_options` to :attr:`_filename`. Resets :attr:_modified_vars\n        for either the given command or all commands,\n\n        Parameters\n        ----------\n        command: str, optional\n            The tab to collect the current state for. If ``None`` then collects the current\n            state for all tabs. Default: ``None``\n        \"\"\"\n        self._options = self._current_gui_state(command)\n        self._options[\"tab_name\"] = self._active_tab\n        logger.debug(\"Saving options: (filename: %s, options: %s\", self._filename, self._options)\n        self._serializer.save(self._filename, self._options)\n        self._reset_modified_var(command)\n        self._add_to_recent(command)\n\n\nclass Tasks(_GuiSession):\n    \"\"\" Faceswap ``.fst`` Task File handling.\n\n    Faceswap tasks handle the management of each individual task tab in the GUI. Unlike\n    :class:`Projects`, Tasks contains all the active tasks currently running, rather than an\n    individual task.\n\n    Parameters\n    ----------\n    config: :class:`lib.gui.utils.Config`\n        The master GUI config\n    file_handler: :class:`lib.gui.utils.FileHandler`\n        A file handler object\n    \"\"\"\n    def __init__(self, config, file_handler):\n        super().__init__(config, file_handler)\n        self._tasks = {}\n\n    @property\n    def _is_project(self):\n        \"\"\" str: ``True`` if all tasks are from an overarching session project else ``False``.\"\"\"\n        retval = False if not self._tasks else all(v.get(\"is_project\", False)\n                                                   for v in self._tasks.values())\n        return retval\n\n    @property\n    def _project_filename(self):\n        \"\"\" str: The overarching session project filename.\"\"\"\n        fname = None\n        if not self._is_project:\n            return fname\n\n        for val in self._tasks.values():\n            fname = val[\"filename\"]\n            break\n        return fname\n\n    def load(self, *args,  # pylint:disable=unused-argument\n             filename=None, current_tab=True):\n        \"\"\" Load a task into this :class:`Tasks` class.\n\n        Tasks can be loaded from project ``.fsw`` files or task ``.fst`` files, depending on where\n        this function is being called from.\n\n        Parameters\n        ----------\n        *args: tuple\n            Unused, but needs to be present for arguments passed by tkinter event handling\n        filename: str, optional\n            If a filename is passed in, This will be used, otherwise a file handler will be\n            launched to select the relevant file.\n        current_tab: bool, optional\n            ``True`` if the task to be loaded must be for the currently selected tab. ``False``\n            if loading a task into any tab. If current_tab is `True` then tasks can be loaded from\n            ``.fsw`` and ``.fst`` files, otherwise they can only be loaded from ``.fst`` files.\n            Default: ``True``\n        \"\"\"\n        logger.debug(\"Loading task config: (filename: '%s', current_tab: '%s')\",\n                     filename, current_tab)\n\n        # Option to load specific task from project files:\n        sess_type = \"all\" if current_tab else \"task\"\n\n        is_legacy = (not self._is_project and\n                     filename is not None and sess_type == \"task\" and\n                     os.path.splitext(filename)[1] == \".fsw\")\n        if is_legacy:\n            logger.debug(\"Legacy task found: '%s'\", filename)\n            filename = self._update_legacy_task(filename)\n\n        filename_set = self._set_filename(filename, sess_type=sess_type)\n        if not filename_set:\n            return\n        loaded = self._load()\n        if not loaded:\n            return\n\n        command = self._active_tab if current_tab else self._stored_tab_name\n        command = self._get_lone_task() if command is None else command\n        if command is None:\n            logger.error(\"Unable to determine task from the given file: '%s'\", filename)\n            return\n        if command not in self._options:\n            logger.error(\"No '%s' task in '%s'\", command, self._filename)\n            return\n\n        self._set_options(command)\n        self._add_to_recent(command)\n\n        if self._is_project:\n            self._filename = self._project_filename\n        elif self._filename.endswith(\".fsw\"):\n            self._filename = None\n\n        self._add_task(command)\n        if is_legacy:\n            self.save()\n\n        logger.debug(\"Loaded task config: (command: '%s', filename: '%s')\", command, filename)\n\n    def _update_legacy_task(self, filename):\n        \"\"\" Update legacy ``.fsw`` tasks to ``.fst`` tasks.\n\n        Tasks loaded from the recent files menu may be passed in with a ``.fsw`` extension.\n        This renames the file and removes it from the recent file list.\n\n        Parameters\n        ----------\n        filename: str\n            The filename of the `.fsw` file that needs converting\n\n        Returns\n        -------\n        str:\n            The new filename of the updated tasks file\n        \"\"\"\n        # TODO remove this code after a period of time. Implemented November 2019\n        logger.debug(\"original filename: '%s'\", filename)\n        fname, ext = os.path.splitext(filename)\n        if ext != \".fsw\":\n            logger.debug(\"Not a .fsw file: '%s'\", filename)\n            return filename\n\n        new_filename = f\"{fname}.fst\"\n        logger.debug(\"Renaming '%s' to '%s'\", filename, new_filename)\n        os.rename(filename, new_filename)\n        self._del_from_recent(filename, save=True)\n        logger.debug(\"new filename: '%s'\", new_filename)\n        return new_filename\n\n    def save(self, save_as=False):\n        \"\"\" Save the current GUI state for the active tab to a ``.fst`` faceswap task file.\n\n        Parameters\n        ----------\n        save_as: bool, optional\n            Whether to save to the stored filename, or pop open a file handler to ask for a\n            location. If there is no stored filename, then a file handler will automatically be\n            popped.\n        \"\"\"\n        logger.debug(\"Saving config...\")\n        self._set_active_task()\n        save_as = save_as or self._is_project or self._filename is None\n\n        if save_as and not self._save_as_to_filename(\"task\"):\n            return\n\n        command = self._active_tab\n        self._save(command=command)\n        self._add_task(command)\n        if not save_as:\n            logger.info(\"Saved project to: '%s'\", self._filename)\n        else:\n            logger.debug(\"Saved project to: '%s'\", self._filename)\n\n    def clear(self):\n        \"\"\" Reset all GUI options to their default values for the active tab. \"\"\"\n        self._config.cli_opts.reset(self._active_tab)\n\n    def reload(self):\n        \"\"\" Reset currently selected tab GUI options to their last saved state. \"\"\"\n        self._set_active_task()\n\n        if self._options is None:\n            logger.info(\"No active task to reload\")\n            return\n        logger.debug(\"Reloading task\")\n        self.load(filename=self._filename, current_tab=True)\n        if self._is_project:\n            self._reset_modified_var(self._active_tab)\n\n    def _add_task(self, command):\n        \"\"\" Add the currently active task to the internal :attr:`_tasks` dict.\n\n        If the currently stored task is from an overarching session project, then\n        only the options are updated. When resetting a tab to saved a project will always\n        be preferred to a task loaded into the project, so the original reference file name\n        stays with the project.\n\n        Parameters\n        ----------\n        command: str\n            The tab that pertains to the currently active task\n\n        \"\"\"\n        self._tasks[command] = {\"filename\": self._filename,\n                                \"options\": self._options,\n                                \"is_project\": self._is_project}\n\n    def clear_tasks(self):\n        \"\"\" Clears all of the stored tasks.\n\n        This is required when loading a task stored in a legacy project file, and is only to be\n        called by :class:`Project` when a project has been loaded which is in fact a task.\n        \"\"\"\n        logger.debug(\"Clearing stored tasks\")\n        self._tasks = {}\n\n    def add_project_task(self, filename, command, options):\n        \"\"\" Add an individual task from a loaded :class:`Project` to the internal :attr:`_tasks`\n        dict.\n\n        Project tasks take priority over any other tasks, so the individual tasks from a new\n        project must be placed in the _tasks dict.\n\n        Parameters\n        ----------\n        filename: str\n            The filename of the session project file\n        command: str\n            The tab that this task's options belong to\n        options: dict\n            The options for this task loaded from the project\n        \"\"\"\n        self._tasks[command] = {\"filename\": filename, \"options\": options, \"is_project\": True}\n\n    def _set_active_task(self, command=None):\n        \"\"\" Set the active :attr:`_filename` and :attr:`_options` to currently selected tab's\n        options.\n\n        Parameters\n        ----------\n        command: str, optional\n            If a command is passed in then set the given tab to active, If this is none set the tab\n            which currently has focus to active. Default: ``None``\n        \"\"\"\n        logger.debug(command)\n        command = self._active_tab if command is None else command\n        task = self._tasks.get(command, None)\n        if task is None:\n            self._filename, self._options = (None, None)\n        else:\n            self._filename, self._options = (task.get(\"filename\", None), task.get(\"options\", None))\n        logger.debug(\"tab: %s, filename: %s, options: %s\",\n                     self._active_tab, self._filename, self._options)\n\n\nclass Project(_GuiSession):\n    \"\"\" Faceswap ``.fsw`` Project File handling.\n\n    Faceswap projects handle the management of all task tabs in the GUI and updates\n    the main Faceswap title bar with the project name and modified state.\n\n    Parameters\n    ----------\n    config: :class:`lib.gui.utils.Config`\n        The master GUI config\n    file_handler: :class:`lib.gui.utils.FileHandler`\n        A file handler object\n    \"\"\"\n\n    def __init__(self, config, file_handler):\n        super().__init__(config, file_handler)\n        self._update_root_title()\n\n    @property\n    def filename(self):\n        \"\"\" str: The currently active project filename. \"\"\"\n        return self._filename\n\n    @property\n    def cli_options(self):\n        \"\"\" dict: the raw cli options from :attr:`_options` with project fields removed. \"\"\"\n        return self._cli_options\n\n    @property\n    def _project_modified(self):\n        \"\"\"bool: ``True`` if the project has been modified otherwise ``False``. \"\"\"\n        return any(var.get() for var in self._modified_vars.values())\n\n    @property\n    def _tasks(self):\n        \"\"\" :class:`Tasks`: The current session's :class:``Tasks``. \"\"\"\n        return self._config.tasks\n\n    def set_default_options(self):\n        \"\"\" Set the default options. The Default GUI options are stored on Faceswap startup.\n\n        Exposed as the :attr:`_default_options` for a project cannot be set until after the main\n        Command Tabs have been loaded.\n        \"\"\"\n        logger.debug(\"Setting options to default\")\n        self._options = self._default_options\n\n    # MODIFIED STATE CALLBACK\n    def set_modified_callback(self):\n        \"\"\" Adds a callback to each of the :attr:`_modified_vars` tkinter variables\n        When one of these variables is changed, triggers :func:`_modified_callback`\n        with the command that was changed.\n\n        This is exposed as the callback can only be added after the main Command Tabs have\n        been drawn, and their options' initial values have been set.\n\n        \"\"\"\n        for key, tkvar in self._modified_vars.items():\n            logger.debug(\"Adding callback for tab: %s\", key)\n            tkvar.trace(\"w\", self._modified_callback)\n\n    def _modified_callback(self, *args):  # pylint:disable=unused-argument\n        \"\"\" Update the project modified state on a GUI modification change and\n        update the Faceswap title bar. \"\"\"\n        if self._project_modified and self._current_gui_state() == self._cli_options:\n            logger.debug(\"Project is same as stored. Setting modified to False\")\n            self._reset_modified_var()\n\n        if self._modified != self._project_modified:\n            logger.debug(\"Updating project state from variable: (modified: %s)\",\n                         self._project_modified)\n            self._modified = self._project_modified\n            self._update_root_title()\n\n    def load(self, *args,  # pylint:disable=unused-argument\n             filename=None,  last_session=False):\n        \"\"\" Load a project from a saved ``.fsw`` project file.\n\n        Parameters\n        ----------\n        *args: tuple\n            Unused, but needs to be present for arguments passed by tkinter event handling\n        filename: str, optional\n            If a filename is passed in, This will be used, otherwise a file handler will be\n            launched to select the relevant file.\n        last_session: bool, optional\n            ``True`` if the project is being loaded from the last opened session ``False`` if the\n            project is being loaded directly from disk. Default: ``False``\n        \"\"\"\n        logger.debug(\"Loading project config: (filename: '%s', last_session: %s)\",\n                     filename, last_session)\n        filename_set = self._set_filename(filename, sess_type=\"project\")\n\n        if not filename_set:\n            logger.debug(\"No filename set\")\n            return\n        loaded = self._load()\n        if not loaded:\n            logger.debug(\"Options not loaded\")\n            return\n\n        # Legacy .fsw files could store projects or tasks. Check if this is a legacy file\n        # and hand off file to Tasks if necessary\n        command = self._get_lone_task()\n        legacy = command is not None\n        if legacy:\n            self._handoff_legacy_task()\n            return\n\n        if not last_session:\n            self._set_options()  # Options will be set by last session. Don't set now\n        self._update_tasks()\n        self._add_to_recent()\n        self._reset_modified_var()\n        self._update_root_title()\n        logger.debug(\"Loaded project config: (command: '%s', filename: '%s')\", command, filename)\n\n    def _handoff_legacy_task(self):\n        \"\"\" Update legacy tasks saved with the old file extension ``.fsw`` to tasks ``.fst``.\n\n        Hands off file handling to :class:`Tasks` and resets project to default.\n        \"\"\"\n        logger.debug(\"Updating legacy task '%s\", self._filename)\n        filename = self._filename\n        self._filename = None\n        self.set_default_options()\n        self._tasks.clear_tasks()\n        self._tasks.load(filename=filename, current_tab=False)\n        logger.debug(\"Updated legacy task and reset project\")\n\n    def _update_tasks(self):\n        \"\"\" Add the tasks from the loaded project to the :class:`Tasks` class. \"\"\"\n        for key, val in self._cli_options.items():\n            opts = {key: val}\n            opts[\"tab_name\"] = key\n            self._tasks.add_project_task(self._filename, key, opts)\n\n    def reload(self, *args):  # pylint:disable=unused-argument\n        \"\"\" Reset all GUI's option tabs to their last saved state.\n\n        Parameters\n        ----------\n        *args: tuple\n            Unused, but needs to be present for arguments passed by tkinter event handling\n        \"\"\"\n        if self._options is None:\n            logger.info(\"No active project to reload\")\n            return\n        logger.debug(\"Reloading project\")\n        self._set_options()\n        self._update_tasks()\n        self._reset_modified_var()\n        self._update_root_title()\n\n    def _update_root_title(self):\n        \"\"\" Update the root Window title with the project name. Add a asterisk\n        if the file is modified. \"\"\"\n        text = \"<untitled project>\" if self._basename is None else self._basename\n        text += \"*\" if self._modified else \"\"\n        self._config.set_root_title(text=text)\n\n    def save(self, *args, save_as=False):  # pylint:disable=unused-argument\n        \"\"\" Save the current GUI state to a ``.fsw`` project file.\n\n        Parameters\n        ----------\n        *args: tuple\n            Unused, but needs to be present for arguments passed by tkinter event handling\n        save_as: bool, optional\n            Whether to save to the stored filename, or pop open a file handler to ask for a\n            location. If there is no stored filename, then a file handler will automatically be\n            popped.\n        \"\"\"\n        logger.debug(\"Saving config as...\")\n\n        save_as = save_as or self._filename is None\n        if save_as and not self._save_as_to_filename(\"project\"):\n            return\n        self._save()\n        self._update_tasks()\n        self._update_root_title()\n        if not save_as:\n            logger.info(\"Saved project to: '%s'\", self._filename)\n        else:\n            logger.debug(\"Saved project to: '%s'\", self._filename)\n\n    def new(self, *args):  # pylint:disable=unused-argument\n        \"\"\" Create a new project with default options.\n\n        Pops a file handler to select location.\n\n        Parameters\n        ----------\n        *args: tuple\n            Unused, but needs to be present for arguments passed by tkinter event handling\n        \"\"\"\n        logger.debug(\"Creating new project\")\n        if not self.confirm_close():\n            logger.debug(\"Creating new project cancelled\")\n            return\n\n        cfgfile = self._file_handler(\"save\",\n                                     \"config_project\",\n                                     title=\"New Project...\",\n                                     initial_folder=self._basename).return_file\n        if not cfgfile:\n            logger.debug(\"No filename selected\")\n            return\n        self._filename = cfgfile.name\n        cfgfile.close()\n\n        self.set_default_options()\n        self._config.cli_opts.reset()\n        self._save()\n        self._update_root_title()\n\n    def close(self, *args):  # pylint:disable=unused-argument\n        \"\"\" Clear the current project and set all options to default.\n\n        Parameters\n        ----------\n        *args: tuple\n            Unused, but needs to be present for arguments passed by tkinter event handling\n        \"\"\"\n        logger.debug(\"Close requested\")\n        if not self.confirm_close():\n            logger.debug(\"Close cancelled\")\n            return\n        self._config.cli_opts.reset()\n        self._filename = None\n        self.set_default_options()\n        self._reset_modified_var()\n        self._update_root_title()\n        self._config.set_active_tab_by_name(self._config.user_config_dict[\"tab\"])\n\n    def confirm_close(self):\n        \"\"\" Pop a message box to get confirmation that an unsaved project should be closed\n\n        Returns\n        -------\n        bool: ``True`` if user confirms close, ``False`` if user cancels close\n        \"\"\"\n        if not self._modified:\n            logger.debug(\"Project is not modified\")\n            return True\n        confirmtxt = \"You have unsaved changes.\\n\\nAre you sure you want to close the project?\"\n        if messagebox.askokcancel(\"Close\", confirmtxt, default=\"cancel\", icon=\"warning\"):\n            logger.debug(\"Close Cancelled\")\n            return True\n        logger.debug(\"Close confirmed\")\n        return False\n\n\nclass LastSession(_GuiSession):\n    \"\"\" Faceswap Last Session handling.\n\n    Faceswap :class:`LastSession` handles saving the state of the Faceswap GUI at close and\n    reloading the state  at launch.\n\n    Last Session behavior can be configured in :file:`config.gui.ini`.\n\n    Parameters\n    ----------\n    config: :class:`lib.gui.utils.Config`\n        The master GUI config\n    \"\"\"\n\n    def __init__(self, config):\n        super().__init__(config)\n        self._filename = os.path.join(self._config.pathcache, \".last_session.json\")\n        if not self._enabled:\n            return\n\n        if self._save_option == \"prompt\":\n            self.ask_load()\n        elif self._save_option == \"always\":\n            self.load()\n\n    @property\n    def _save_option(self):\n        \"\"\" str: The user config autosave option. \"\"\"\n        return self._config.user_config_dict.get(\"autosave_last_session\", \"never\")\n\n    @property\n    def _enabled(self):\n        \"\"\" bool: ``True`` if autosave is enabled otherwise ``False``. \"\"\"\n        return self._save_option != \"never\"\n\n    def from_dict(self, options):\n        \"\"\" Set the :attr:`_options` property based on the given options dictionary\n        and update the GUI to use these values.\n\n        This function is required for reloading the GUI state when the GUI has been force\n        refreshed on a config change.\n\n        Parameters\n        ----------\n        options: dict\n            The options to set. Should be the output of :func:`to_dict`\n        \"\"\"\n        logger.debug(\"Setting options from dict: %s\", options)\n        self._options = options\n        self._set_options()\n\n    def to_dict(self):\n        \"\"\" Collect the current GUI options and place them in a dict for retrieval or storage.\n\n        This function is required for reloading the GUI state when the GUI has been force\n        refreshed on a config change.\n\n        Returns\n        -------\n        dict: The current cli options ready for saving or retrieval by :func:`from_dict`\n        \"\"\"\n        opts = self._current_gui_state()\n        logger.debug(\"Collected opts: %s\", opts)\n        if not opts or opts == self._default_options:\n            logger.debug(\"Default session, or no opts found. Not saving last session.\")\n            return None\n        opts[\"tab_name\"] = self._active_tab\n        opts[\"project\"] = self._config.project.filename\n        logger.debug(\"Added project items: %s\", {k: v for k, v in opts.items()\n                                                 if k in (\"tab_name\", \"project\")})\n        return opts\n\n    def ask_load(self):\n        \"\"\" Pop a message box to ask the user if they wish to load their last session.  \"\"\"\n        if not self._file_exists:\n            logger.debug(\"No last session file found\")\n        elif tk.messagebox.askyesno(\"Last Session\", \"Load last session?\"):\n            logger.debug(\"Loading last session at user request\")\n            self.load()\n        else:\n            logger.debug(\"Not loading last session at user request\")\n            logger.debug(\"Deleting LastSession file\")\n            os.remove(self._filename)\n\n    def load(self):\n        \"\"\" Load the last session.\n\n        Loads the last saved session options. Checks if a previous project was loaded\n        and whether there have been changes since the last saved version of the project.\n        Sets the display and :class:`Project` and :class:`Task` objects accordingly.\n        \"\"\"\n        loaded = self._load()\n        if not loaded:\n            return\n        self._set_project()\n        self._set_options()\n\n    def _set_project(self):\n        \"\"\" Set the :class:`Project` if session is resuming from one. \"\"\"\n        if self._options.get(\"project\", None) is None:\n            logger.debug(\"No project stored\")\n        else:\n            logger.debug(\"Loading stored project\")\n            self._config.project.load(filename=self._options[\"project\"], last_session=True)\n\n    def save(self):\n        \"\"\" Save a snapshot of currently set GUI config options.\n\n        Called on Faceswap shutdown.\n        \"\"\"\n        if not self._enabled:\n            logger.debug(\"LastSession not enabled\")\n            if os.path.exists(self._filename):\n                logger.debug(\"Deleting existing LastSession file\")\n                os.remove(self._filename)\n            return\n\n        opts = self.to_dict()\n        if opts is None and os.path.exists(self._filename):\n            logger.debug(\"Last session default or blank. Clearing saved last session.\")\n            os.remove(self._filename)\n        if opts is not None:\n            self._serializer.save(self._filename, opts)\n            logger.debug(\"Saved last session. (filename: '%s', opts: %s\", self._filename, opts)\n", "lib/gui/popup_configure.py": "#!/usr/bin python3\n\"\"\" The pop-up window of the Faceswap GUI for the setting of configuration options. \"\"\"\nfrom __future__ import annotations\nfrom collections import OrderedDict\nfrom configparser import ConfigParser\nimport gettext\nimport logging\nimport os\nimport sys\nimport tkinter as tk\nfrom tkinter import ttk\nimport typing as T\n\nfrom importlib import import_module\n\nfrom lib.serializer import get_serializer\n\nfrom .control_helper import ControlPanel, ControlPanelOption\nfrom .custom_widgets import Tooltip\nfrom .utils import FileHandler, get_config, get_images, PATHCACHE\n\nif T.TYPE_CHECKING:\n    from lib.config import FaceswapConfig\n\nlogger = logging.getLogger(__name__)\n\n# LOCALES\n_LANG = gettext.translation(\"gui.tooltips\", localedir=\"locales\", fallback=True)\n_ = _LANG.gettext\n\n\nclass _State():\n    \"\"\" Holds the existing config files and the current state of the popup window. \"\"\"\n    def __init__(self):\n        self._popup = None\n        # The GUI Config cannot be scanned until GUI is launched, so this is populated\n        # on the first call to load the settings\n        self._configs = {}\n\n    def open_popup(self, name=None):\n        \"\"\" Launch the popup, ensuring only one instance is ever open\n\n        Parameters\n        ----------\n        name: str, Optional\n            The name of the configuration file. Used for selecting the correct section if required.\n            Set to ``None`` if no initial section should be selected. Default: ``None``\n        \"\"\"\n        self._scan_for_configs()\n        logger.debug(\"name: %s\", name)\n        if self._popup is not None:\n            logger.debug(\"Restoring existing popup\")\n            self._popup.update()\n            self._popup.deiconify()\n            self._popup.lift()\n            return\n        self._popup = _ConfigurePlugins(name, self._configs)\n\n    def close_popup(self):\n        \"\"\" Destroy the open popup and remove it from tracking. \"\"\"\n        if self._popup is None:\n            logger.info(\"No popup to close. Returning\")\n            return\n        self._popup.destroy()\n        del self._popup\n        self._popup = None\n\n    def _scan_for_configs(self):\n        \"\"\" Scan the plugin folders for configuration settings. Add in the GUI configuration also.\n\n        Populates the attribute :attr:`_configs`.\n        \"\"\"\n        root_path = os.path.abspath(os.path.dirname(sys.argv[0]))\n        plugins_path = os.path.join(root_path, \"plugins\")\n        logger.debug(\"Scanning path: '%s'\", plugins_path)\n        for dirpath, _, filenames in os.walk(plugins_path):\n            if \"_config.py\" in filenames:\n                plugin_type = os.path.split(dirpath)[-1]\n                config = self._load_config(plugin_type)\n                self._configs[plugin_type] = config\n        self._configs[\"gui\"] = get_config().user_config\n        logger.debug(\"Configs loaded: %s\", sorted(list(self._configs.keys())))\n\n    @classmethod\n    def _load_config(cls, plugin_type):\n        \"\"\" Load the config from disk. If the file doesn't exist, then it will be generated.\n\n        Parameters\n        ----------\n        plugin_type: str\n            The plugin type (i.e. extract, train convert) that the config should be loaded for\n\n        Returns\n        -------\n        :class:`lib.config.FaceswapConfig`\n            The Configuration for the selected plugin\n        \"\"\"\n        # Load config to generate default if doesn't exist\n        mod = \".\".join((\"plugins\", plugin_type, \"_config\"))\n        module = import_module(mod)\n        config = module.Config(None)\n        logger.debug(\"Found '%s' config at '%s'\", plugin_type, config.configfile)\n        return config\n\n\n_STATE = _State()\nopen_popup = _STATE.open_popup  # pylint:disable=invalid-name\n\n\nclass _ConfigurePlugins(tk.Toplevel):\n    \"\"\" Pop-up window for the setting of Faceswap Configuration Options.\n\n    Parameters\n    ----------\n    name: str\n        The name of the section that is being navigated to. Used for opening on the correct\n        page in the Tree View.\n    configurations: dict\n        Dictionary containing the :class:`~lib.config.FaceswapConfig` object for each\n        configuration section for the requested pop-up window\n    \"\"\"\n    def __init__(self, name, configurations):\n        logger.debug(\"Initializing %s: (name: %s, configurations: %s)\",\n                     self.__class__.__name__, name, configurations)\n        super().__init__()\n        self._root = get_config().root\n        self._set_geometry()\n        self._tk_vars = {\"header\": tk.StringVar()}\n\n        theme = {**get_config().user_theme[\"group_panel\"],\n                 **get_config().user_theme[\"group_settings\"]}\n        header_frame = self._build_header()\n        content_frame = ttk.Frame(self)\n\n        self._tree = _Tree(content_frame, configurations, name, theme).tree\n        self._tree.bind(\"<ButtonRelease-1>\", self._select_item)\n\n        self._opts_frame = DisplayArea(self, content_frame, configurations, self._tree, theme)\n        self._opts_frame.pack(fill=tk.BOTH, expand=True, side=tk.RIGHT)\n        footer_frame = self._build_footer()\n\n        header_frame.pack(fill=tk.X, padx=5, pady=5, side=tk.TOP)\n        content_frame.pack(fill=tk.BOTH, padx=5, pady=(0, 5), expand=True, side=tk.TOP)\n        footer_frame.pack(fill=tk.X, padx=5, pady=(0, 5), side=tk.BOTTOM)\n\n        select = name if name else self._tree.get_children()[0]\n        self._tree.selection_set(select)\n        self._tree.focus(select)\n        self._select_item(0)\n\n        self.title(\"Configure Settings\")\n        self.tk.call('wm', 'iconphoto', self._w, get_images().icons[\"favicon\"])\n        self.protocol(\"WM_DELETE_WINDOW\", _STATE.close_popup)\n\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    def _set_geometry(self):\n        \"\"\" Set the geometry of the pop-up window \"\"\"\n        scaling_factor = get_config().scaling_factor\n        pos_x = self._root.winfo_x() + 80\n        pos_y = self._root.winfo_y() + 80\n        width = int(600 * scaling_factor)\n        height = int(536 * scaling_factor)\n        logger.debug(\"Pop up Geometry: %sx%s, %s+%s\", width, height, pos_x, pos_y)\n        self.geometry(f\"{width}x{height}+{pos_x}+{pos_y}\")\n\n    def _build_header(self):\n        \"\"\" Build the main header text and separator. \"\"\"\n        header_frame = ttk.Frame(self)\n        lbl_frame = ttk.Frame(header_frame)\n\n        self._tk_vars[\"header\"].set(\"Settings\")\n        lbl_header = ttk.Label(lbl_frame,\n                               textvariable=self._tk_vars[\"header\"],\n                               anchor=tk.W,\n                               style=\"SPanel.Header1.TLabel\")\n        lbl_header.pack(fill=tk.X, expand=True, side=tk.LEFT)\n\n        sep = ttk.Frame(header_frame, height=2, relief=tk.RIDGE)\n\n        lbl_frame.pack(fill=tk.X, expand=True, side=tk.TOP)\n        sep.pack(fill=tk.X, pady=(1, 0), side=tk.BOTTOM)\n        return header_frame\n\n    def _build_footer(self):\n        \"\"\" Build the main footer buttons and separator. \"\"\"\n        logger.debug(\"Adding action buttons\")\n        frame = ttk.Frame(self)\n        left_frame = ttk.Frame(frame)\n        right_frame = ttk.Frame(frame)\n\n        btn_saveall = ttk.Button(left_frame,\n                                 text=\"Save All\",\n                                 width=10,\n                                 command=self._opts_frame.save)\n        btn_rstall = ttk.Button(left_frame,\n                                text=\"Reset All\",\n                                width=10,\n                                command=self._opts_frame.reset)\n\n        btn_cls = ttk.Button(right_frame, text=\"Cancel\", width=10, command=_STATE.close_popup)\n        btn_save = ttk.Button(right_frame,\n                              text=\"Save\",\n                              width=10,\n                              command=lambda: self._opts_frame.save(page_only=True))\n        btn_rst = ttk.Button(right_frame,\n                             text=\"Reset\",\n                             width=10,\n                             command=lambda: self._opts_frame.reset(page_only=True))\n\n        Tooltip(btn_cls, text=_(\"Close without saving\"), wrap_length=720)\n        Tooltip(btn_save, text=_(\"Save this page's config\"), wrap_length=720)\n        Tooltip(btn_rst, text=_(\"Reset this page's config to default values\"), wrap_length=720)\n        Tooltip(btn_saveall,\n                text=_(\"Save all settings for the currently selected config\"),\n                wrap_length=720)\n        Tooltip(btn_rstall,\n                text=_(\"Reset all settings for the currently selected config to default values\"),\n                wrap_length=720)\n\n        btn_cls.pack(padx=2, side=tk.RIGHT)\n        btn_save.pack(padx=2, side=tk.RIGHT)\n        btn_rst.pack(padx=2, side=tk.RIGHT)\n        btn_saveall.pack(padx=2, side=tk.RIGHT)\n        btn_rstall.pack(padx=2, side=tk.RIGHT)\n\n        left_frame.pack(side=tk.LEFT)\n        right_frame.pack(side=tk.RIGHT)\n        logger.debug(\"Added action buttons\")\n        return frame\n\n    def _select_item(self, event):  # pylint:disable=unused-argument\n        \"\"\" Update the session summary info with the selected item or launch graph.\n\n        If the mouse is clicked on the graph icon, then the session summary pop-up graph is\n        launched. Otherwise the selected ID is stored.\n\n        Parameters\n        ----------\n        event: :class:`tkinter.Event`\n            The tkinter mouse button release event. Unused.\n        \"\"\"\n        selection = self._tree.focus()\n        section = selection.split(\"|\")[0]\n        subsections = selection.split(\"|\")[1:] if \"|\" in selection else []\n        self._tk_vars[\"header\"].set(f\"{section.title()} Settings\")\n        self._opts_frame.select_options(section, subsections)\n\n\nclass _Tree(ttk.Frame):  # pylint:disable=too-many-ancestors\n    \"\"\" Frame that holds the Tree View Navigator and scroll bar for the configuration pop-up.\n\n    Parameters\n    ----------\n    parent: :class:`tkinter.ttk.Frame`\n        The parent frame to the Tree View area\n    configurations: dict\n        Dictionary containing the :class:`~lib.config.FaceswapConfig` object for each\n        configuration section for the requested pop-up window\n    name: str\n        The name of the section that is being navigated to. Used for opening on the correct\n        page in the Tree View. ``None`` if no specific area is being navigated to\n    theme: dict\n        The color mapping for the settings pop-up theme\n    \"\"\"\n    def __init__(self, parent, configurations, name, theme):\n        super().__init__(parent)\n        self._fix_styles(theme)\n\n        frame = ttk.Frame(self, relief=tk.SOLID, borderwidth=1)\n        self._tree = self._build_tree(frame, configurations, name)\n        scrollbar = ttk.Scrollbar(frame, orient=\"vertical\", command=self._tree.yview)\n\n        scrollbar.pack(side=tk.RIGHT, fill=tk.Y)\n        self._tree.pack(fill=tk.Y, expand=True)\n        self._tree.configure(yscrollcommand=scrollbar.set)\n        frame.pack(expand=True, fill=tk.Y)\n        self.pack(side=tk.LEFT, fill=tk.Y)\n\n    @property\n    def tree(self):\n        \"\"\" :class:`tkinter.ttk.TreeView` The Tree View held within the frame \"\"\"\n        return self._tree\n\n    @classmethod\n    def _fix_styles(cls, theme):\n        \"\"\" Tkinter has a bug when setting the background style on certain OSes. This fixes the\n        issue so we can set different colored backgrounds.\n\n        We also set some default styles for our tree view.\n\n        Parameters\n        ----------\n        theme: dict\n            The color mapping for the settings pop-up theme\n        \"\"\"\n        style = ttk.Style()\n\n        # Fix a bug in Tree-view that doesn't show alternate foreground on selection\n        fix_map = lambda o: [elm for elm in style.map(\"Treeview\", query_opt=o)  # noqa[E731]  # pylint:disable=C3001\n                             if elm[:2] != (\"!disabled\", \"!selected\")]\n\n        # Remove the Borders\n        style.configure(\"ConfigNav.Treeview\", bd=0, background=\"#F0F0F0\")\n        style.layout(\"ConfigNav.Treeview\", [('ConfigNav.Treeview.treearea', {'sticky': 'nswe'})])\n\n        # Set colors\n        style.map(\"ConfigNav.Treeview\",\n                  foreground=fix_map(\"foreground\"),\n                  background=fix_map(\"background\"))\n        style.map('ConfigNav.Treeview', background=[('selected', theme[\"tree_select\"])])\n\n    def _build_tree(self, parent, configurations, name):\n        \"\"\" Build the configuration pop-up window.\n\n        Parameters\n        ----------\n        configurations: dict\n            Dictionary containing the :class:`~lib.config.FaceswapConfig` object for each\n            configuration section for the requested pop-up window\n        name: str\n            The name of the section that is being navigated to. Used for opening on the correct\n            page in the Tree View. ``None`` if no specific area is being navigated to\n\n        Returns\n        -------\n        :class:`tkinter.ttk.TreeView`\n            The populated tree view\n        \"\"\"\n        logger.debug(\"Building Tree View Navigator\")\n        tree = ttk.Treeview(parent, show=\"tree\", style=\"ConfigNav.Treeview\")\n        data = {category: [sect.split(\".\") for sect in sorted(conf.config.sections())]\n                for category, conf in configurations.items()}\n        ordered = sorted(list(data.keys()))\n        categories = [\"extract\", \"train\", \"convert\"]\n        categories += [x for x in ordered if x not in categories]\n\n        for cat in categories:\n            img = get_images().icons.get(f\"settings_{cat}\", \"\")\n            text = cat.replace(\"_\", \" \").title()\n            text = \" \" + text if img else text\n            is_open = tk.TRUE if name is None or name == cat else tk.FALSE\n            tree.insert(\"\", \"end\", cat, text=text, image=img, open=is_open, tags=\"category\")\n            self._process_sections(tree, data[cat], cat, name == cat)\n\n        tree.tag_configure('category', background='#DFDFDF')\n        tree.tag_configure('section', background='#E8E8E8')\n        tree.tag_configure('option', background='#F0F0F0')\n        logger.debug(\"Tree View Navigator\")\n        return tree\n\n    @classmethod\n    def _process_sections(cls, tree, sections, category, is_open):\n        \"\"\" Process the sections of a category's configuration.\n\n        Creates a category's sections, then the sub options for that category\n\n        Parameters\n        ----------\n        tree: :class:`tkinter.ttk.TreeView`\n            The tree view to insert sections into\n        sections: list\n            The sections to insert into the Tree View\n        category: str\n            The category node that these sections sit in\n        is_open: bool\n            ``True`` if the node should be created in \"open\" mode. ``False`` if it should be\n            closed.\n        \"\"\"\n        seen = set()\n        for section in sections:\n            if section[-1] == \"global\":  # Global categories get escalated to parent\n                continue\n            sect = section[0]\n            section_id = f\"{category}|{sect}\"\n            if sect not in seen:\n                seen.add(sect)\n                text = sect.replace(\"_\", \" \").title()\n                tree.insert(category, \"end\", section_id, text=text, open=is_open, tags=\"section\")\n            if len(section) == 2:\n                opt = section[-1]\n                opt_id = f\"{section_id}|{opt}\"\n                opt_text = opt.replace(\"_\", \" \").title()\n                tree.insert(section_id, \"end\", opt_id, text=opt_text, open=is_open, tags=\"option\")\n\n\nclass DisplayArea(ttk.Frame):  # pylint:disable=too-many-ancestors\n    \"\"\" The option configuration area of the pop up options.\n\n    Parameters\n    ----------\n    top_level: :class:``tk.Toplevel``\n        The tkinter Top Level widget\n    parent: :class:`tkinter.ttk.Frame`\n        The parent frame that holds the Display Area of the pop up configuration window\n    tree: :class:`tkinter.ttk.TreeView`\n        The Tree View navigator for the pop up configuration window\n    configurations: dict\n        Dictionary containing the :class:`~lib.config.FaceswapConfig` object for each\n        configuration section for the requested pop-up window\n    theme: dict\n        The color mapping for the settings pop-up theme\n    \"\"\"\n    def __init__(self, top_level, parent, configurations, tree, theme):\n        super().__init__(parent)\n        self._configs: dict[str, FaceswapConfig] = configurations\n        self._theme = theme\n        self._tree = tree\n        self._vars = {}\n        self._cache = {}\n        self._config_cpanel_dict = self._get_config()\n        self._displayed_frame = None\n        self._displayed_key = None\n\n        self._presets = _Presets(self, top_level)\n        self._build_header()\n\n    @property\n    def displayed_key(self):\n        \"\"\" str: The current display page's lookup key for configuration options. \"\"\"\n        return self._displayed_key\n\n    @property\n    def config_dict(self):\n        \"\"\" dict: The configuration dictionary for all display pages. \"\"\"\n        return self._config_cpanel_dict\n\n    def _get_config(self):\n        \"\"\" Format the configuration options stored in :attr:`_config` into a dict of\n        :class:`~lib.gui.control_helper.ControlPanelOption's for placement into option frames.\n\n        Returns\n        -------\n        dict\n            A dictionary of section names to :class:`~lib.gui.control_helper.ControlPanelOption`\n            objects\n        \"\"\"\n        logger.debug(\"Formatting Config for GUI\")\n        retval = {}\n        for plugin, conf in self._configs.items():\n            for section in conf.config.sections():\n                conf.section = section\n                category = section.split(\".\")[0]\n                sect = section.split(\".\")[-1]\n                # Elevate global to root\n                key = plugin if sect == \"global\" else f\"{plugin}|{category}|{sect}\"\n                retval[key] = {\"helptext\": None, \"options\": OrderedDict()}\n\n                retval[key][\"helptext\"] = conf.defaults[section].helptext\n                for option, params in conf.defaults[section].items.items():\n                    initial_value = conf.config_dict[option]\n                    initial_value = \"none\" if initial_value is None else initial_value\n                    if params.datatype == list and isinstance(initial_value, list):\n                        # Split multi-select lists into space separated strings for tk variables\n                        initial_value = \" \".join(initial_value)\n\n                    retval[key][\"options\"][option] = ControlPanelOption(\n                        title=option,\n                        dtype=params.datatype,\n                        group=params.group,\n                        default=params.default,\n                        initial_value=initial_value,\n                        choices=params.choices,\n                        is_radio=params.gui_radio,\n                        is_multi_option=params.datatype == list,\n                        rounding=params.rounding,\n                        min_max=params.min_max,\n                        helptext=params.helptext)\n        logger.debug(\"Formatted Config for GUI: %s\", retval)\n        return retval\n\n    def _build_header(self):\n        \"\"\" Build the dynamic header text. \"\"\"\n        header_frame = ttk.Frame(self)\n        lbl_frame = ttk.Frame(header_frame)\n\n        var = tk.StringVar()\n        lbl = ttk.Label(lbl_frame, textvariable=var, anchor=tk.W, style=\"SPanel.Header2.TLabel\")\n        lbl.pack(fill=tk.X, expand=True, side=tk.TOP)\n\n        self._build_presets_buttons(header_frame)\n        lbl_frame.pack(fill=tk.X, side=tk.LEFT, expand=True)\n        header_frame.pack(fill=tk.X, padx=5, pady=5, side=tk.TOP)\n        self._vars[\"header\"] = var\n\n    def _build_presets_buttons(self, frame):\n        \"\"\" Build the section that holds the preset load and save buttons.\n\n        Parameters\n        ----------\n        frame: :class:`ttk.Frame`\n            The frame that holds the preset buttons\n        \"\"\"\n        presets_frame = ttk.Frame(frame)\n        for lbl in (\"load\", \"save\"):\n            btn = ttk.Button(presets_frame,\n                             image=get_images().icons[lbl],\n                             command=getattr(self._presets, lbl))\n            Tooltip(btn, text=_(f\"{lbl.title()} preset for this plugin\"), wrap_length=720)\n            btn.pack(padx=2, side=tk.LEFT)\n        presets_frame.pack(side=tk.RIGHT)\n\n    def select_options(self, section, subsections):\n        \"\"\" Display the page for the given section and subsections.\n\n        Parameters\n        ----------\n        section: str\n            The main section to be navigated to (or root node)\n        subsections: list\n            The full list of subsections ending on the required node\n        \"\"\"\n        labels = [\"global\"] if not subsections else subsections\n        self._vars[\"header\"].set(\" - \".join(sect.replace(\"_\", \" \").title() for sect in labels))\n        self._set_display(section, subsections)\n\n    def _set_display(self, section, subsections):\n        \"\"\" Set the correct display page for the given section and subsections.\n\n        Parameters\n        ----------\n        section: str\n            The main section to be navigated to (or root node)\n        subsections: list\n            The full list of subsections ending on the required node\n        \"\"\"\n        key = \"|\".join([section] + subsections)\n        if self._displayed_frame is not None:\n            self._displayed_frame.pack_forget()\n\n        if key not in self._cache:\n            self._cache_page(key)\n\n        self._displayed_frame = self._cache[key]\n        self._displayed_key = key\n        self._displayed_frame.pack(side=tk.BOTTOM, fill=tk.BOTH, expand=True)\n\n    def _cache_page(self, key):\n        \"\"\" Create the control panel options for the requested configuration and cache.\n\n        Parameters\n        ----------\n        key: str\n            The lookup key to the settings cache\n        \"\"\"\n        info = self._config_cpanel_dict.get(key, None)\n        if info is None:\n            logger.debug(\"key '%s' does not exist in options. Creating links page.\", key)\n            self._cache[key] = self._create_links_page(key)\n        else:\n            self._cache[key] = ControlPanel(self,\n                                            list(info[\"options\"].values()),\n                                            header_text=info[\"helptext\"],\n                                            columns=1,\n                                            max_columns=1,\n                                            option_columns=4,\n                                            style=\"SPanel\",\n                                            blank_nones=False)\n\n    def _create_links_page(self, key):\n        \"\"\" For headings which don't have settings, build a links page to the subsections.\n\n        Parameters\n        ----------\n        key: str\n            The lookup key to set the links page for\n        \"\"\"\n        frame = ttk.Frame(self)\n        links = {item.replace(key, \"\")[1:].split(\"|\")[0]\n                 for item in self._config_cpanel_dict\n                 if item.startswith(key)}\n\n        if not links:\n            return frame\n\n        header_lbl = ttk.Label(frame, text=_(\"Select a plugin to configure:\"))\n        header_lbl.pack(side=tk.TOP, fill=tk.X, padx=5, pady=(5, 10))\n        for link in sorted(links):\n            lbl = ttk.Label(frame,\n                            text=link.replace(\"_\", \" \").title(),\n                            anchor=tk.W,\n                            foreground=self._theme[\"link_color\"],\n                            cursor=\"hand2\")\n            lbl.pack(side=tk.TOP, fill=tk.X, padx=10, pady=(0, 5))\n            bind = f\"{key}|{link}\"\n            lbl.bind(\"<Button-1>\", lambda e, x=bind: self._link_callback(x))\n\n        return frame\n\n    def _link_callback(self, identifier):\n        \"\"\" Set the tree view to the selected item and display the requested page on a link click.\n\n        Parameters\n        ----------\n        identifier: str\n            The identifier from the tree view for the page to display\n        \"\"\"\n        parent = \"|\".join(identifier.split(\"|\")[:-1])\n        self._tree.item(parent, open=True)\n        self._tree.selection_set(identifier)\n        self._tree.focus(identifier)\n        split = identifier.split(\"|\")\n        section = split[0]\n        subsections = split[1:] if len(split) > 1 else []\n        self.select_options(section, subsections)\n\n    def reset(self, page_only=False):\n        \"\"\" Reset all configuration options to their default values.\n\n        Parameters\n        ----------\n        page_only: bool, optional\n            ``True`` resets just the currently selected page's options to default, ``False`` resets\n            all plugins within the currently selected config to default. Default: ``False``\n        \"\"\"\n        logger.debug(\"Resetting config, page_only: %s\", page_only)\n        selection = self._tree.focus()\n        if page_only:\n            if selection not in self._config_cpanel_dict:\n                logger.info(\"No configuration options to reset for current page: %s\", selection)\n                return\n            items = list(self._config_cpanel_dict[selection][\"options\"].values())\n        else:\n            items = [opt\n                     for key, val in self._config_cpanel_dict.items()\n                     for opt in val[\"options\"].values()\n                     if key.startswith(selection.split(\"|\")[0])]\n        for item in items:\n            logger.debug(\"Resetting item '%s' from '%s' to default '%s'\",\n                         item.title, item.get(), item.default)\n            item.set(item.default)\n        logger.debug(\"Reset config\")\n\n    def _get_new_config(self,\n                        page_only: bool,\n                        config: FaceswapConfig,\n                        category: str,\n                        lookup: str) -> ConfigParser:\n        \"\"\" Obtain a new configuration file for saving\n\n        Parameters\n        ----------\n        page_only: bool\n            ``True`` saves just the currently selected page's options, ``False`` saves all the\n            plugins options within the currently selected config.\n        config: :class:`~lib.config.FaceswapConfig`\n            The original config that is to be addressed\n        category: str\n            The configuration category to update\n        lookup: str\n            The section of the configuration to update\n\n        Returns\n        -------\n        :class:`configparse.ConfigParser`\n            The newly created configuration object for saving\n        \"\"\"\n        new_config = ConfigParser(allow_no_value=True)\n        for section_name, section in config.defaults.items():\n            logger.debug(\"Adding section: '%s')\", section_name)\n            config.insert_config_section(section_name, section.helptext, config=new_config)\n            for item, options in section.items.items():\n                if item == \"helptext\":\n                    continue\n                if page_only and section_name != lookup:\n                    # Keep existing values for pages we are not updating\n                    new_opt = config.get(section_name, item)\n                    logger.debug(\"Retain existing value '%s' for %s\",\n                                 new_opt, \".\".join([section_name, item]))\n                else:\n                    # Get currently selected value\n                    key = category\n                    if section_name != \"global\":\n                        key += f\"|{section_name.replace('.', '|')}\"\n                    new_opt = self._config_cpanel_dict[key][\"options\"][item].get()\n                    logger.debug(\"Updating value to '%s' for %s\",\n                                 new_opt, \".\".join([section_name, item]))\n                helptext = config.format_help(options.helptext, is_section=False)\n                new_config.set(section_name, helptext)\n                if options.datatype == list:  # Comma separated multi select options\n                    assert isinstance(new_opt, (list, str))\n                    new_opt = \", \".join(new_opt if isinstance(new_opt, list) else new_opt.split())\n                new_config.set(section_name, item, str(new_opt))\n\n        return new_config\n\n    def save(self, page_only=False):\n        \"\"\" Save the configuration file to disk.\n\n        Parameters\n        ----------\n        page_only: bool, optional\n            ``True`` saves just the currently selected page's options, ``False`` saves all the\n            plugins options within the currently selected config. Default: ``False``\n        \"\"\"\n        logger.debug(\"Saving config\")\n        selection = self._tree.focus()\n        category = selection.split(\"|\")[0]\n        config = self._configs[category]\n        # Create a new config to pull through any defaults change\n\n        if \"|\" in selection:\n            lookup = \".\".join(selection.split(\"|\")[1:])\n        else:  # Expand global out from root node\n            lookup = \"global\"\n\n        if page_only and lookup not in config.config.sections():\n            logger.info(\"No settings to save for the current page\")\n            return\n\n        config.config = self._get_new_config(page_only, config, category, lookup)\n        config.save_config()\n        logger.info(\"Saved config: '%s'\", config.configfile)\n\n        if category == \"gui\":\n            if not get_config().tk_vars.running_task.get():\n                get_config().root.rebuild()\n            else:\n                logger.info(\"Can't redraw GUI whilst a task is running. GUI Settings will be \"\n                            \"applied at the next restart.\")\n        logger.debug(\"Saved config\")\n\n\nclass _Presets():\n    \"\"\" Handles the file dialog and loading and saving of plugin preset files.\n\n    Parameters\n    ----------\n    parent: :class:`ttk.Frame`\n        The parent display area frame\n    top_level: :class:`tkinter.Toplevel`\n        The top level pop up window\n    \"\"\"\n    def __init__(self, parent, top_level):\n        logger.debug(\"Initializing: %s (top_level: %s)\", self.__class__.__name__, top_level)\n        self._parent = parent\n        self._popup = top_level\n        self._base_path = os.path.join(PATHCACHE, \"presets\")\n        self._serializer = get_serializer(\"json\")\n        logger.debug(\"Initialized: %s\", self.__class__.__name__)\n\n    @property\n    def _preset_path(self):\n        \"\"\" str: The path to the default preset folder for the currently displayed plugin. \"\"\"\n        return os.path.join(self._base_path, self._parent.displayed_key.split(\"|\")[0])\n\n    @property\n    def _full_key(self):\n        \"\"\" str: The full extrapolated lookup key for the currently displayed page. \"\"\"\n        full_key = self._parent.displayed_key\n        return full_key if \"|\" in full_key else f\"{full_key}|global\"\n\n    def load(self):\n        \"\"\" Action to perform when load preset button is pressed.\n\n        Loads parameters from a saved json file and updates the displayed page.\n        \"\"\"\n        filename = self._get_filename(\"load\")\n        if not filename:\n            return\n\n        opts = self._serializer.load(filename)\n        if opts.get(\"__filetype\") != \"faceswap_preset\":\n            logger.warning(\"'%s' is not a valid plugin preset file\", filename)\n            return\n        if opts.get(\"__section\") != self._full_key:\n            logger.warning(\"You are attempting to load a preset for '%s' into '%s'. Aborted.\",\n                           opts.get(\"__section\", \"no section\"), self._full_key)\n            return\n\n        logger.debug(\"Loaded preset: %s\", opts)\n\n        exist = self._parent.config_dict[self._parent.displayed_key][\"options\"]\n        for key, val in opts.items():\n            if key.startswith(\"__\") or key not in exist:\n                logger.debug(\"Skipping non-existent item: '%s'\", key)\n                continue\n            logger.debug(\"Setting '%s' to '%s'\", key, val)\n            exist[key].set(val)\n        logger.info(\"Preset loaded from: '%s'\", os.path.basename(filename))\n\n    def save(self):\n        \"\"\" Action to perform when save preset button is pressed.\n\n        Compiles currently displayed configuration options into a json file and saves into selected\n        location.\n        \"\"\"\n        filename = self._get_filename(\"save\")\n        if not filename:\n            return\n\n        opts = self._parent.config_dict[self._parent.displayed_key][\"options\"]\n        preset = {opt: val.get() for opt, val in opts.items()}\n        preset[\"__filetype\"] = \"faceswap_preset\"\n        preset[\"__section\"] = self._full_key\n        self._serializer.save(filename, preset)\n        logger.info(\"Preset '%s' saved to: '%s'\", self._full_key, filename)\n\n    def _get_filename(self, action):\n        \"\"\" Obtain the filename for load and save preset actions.\n\n        Parameters\n        ----------\n        action: [\"load\", \"save\"]\n            The preset action that is being performed\n\n        Returns\n        -------\n        str: The requested preset filename\n        \"\"\"\n        if not self._parent.config_dict.get(self._parent.displayed_key):\n            logger.info(\"No settings to %s for the current page.\", action)\n            return None\n\n        args = (\"save_filename\", \"json\") if action == \"save\" else (\"filename\", \"json\")\n        kwargs = {\"title\": f\"{action.title()} Preset...\",\n                  \"initial_folder\": self._preset_path,\n                  \"parent\": self._parent}\n        if action == \"save\":\n            kwargs[\"initial_file\"] = self._get_initial_filename()\n\n        filename = FileHandler(*args, **kwargs).return_file\n        if not filename:\n            logger.debug(\"%s cancelled\", action.title())\n\n        self._raise_toplevel()\n        return filename\n\n    def _get_initial_filename(self):\n        \"\"\" Obtain the initial filename for saving a preset.\n\n        The name is based on the plugin's display key. A scan of the default presets folder is done\n        to ensure no filename clash. If a filename does clash, then an integer is added to the end.\n\n        Returns\n        -------\n        str\n            The initial preset filename\n        \"\"\"\n        _, key = self._full_key.split(\"|\", 1)\n        base_filename = f\"{key.replace('|', '_')}_preset\"\n\n        i = 0\n        filename = f\"{base_filename}.json\"\n        while True:\n            if not os.path.exists(os.path.join(self._preset_path, filename)):\n                break\n            logger.debug(\"File pre-exists: %s\", filename)\n            filename = f\"{base_filename}_{i}.json\"\n            i += 1\n        logger.debug(\"Initial filename: %s\", filename)\n        return filename\n\n    def _raise_toplevel(self):\n        \"\"\" Opening a file dialog tends to hide the top level pop up, so bring back to the\n        fore. \"\"\"\n        self._popup.update()\n        self._popup.deiconify()\n        self._popup.lift()\n", "lib/gui/analysis/event_reader.py": "#!/usr/bin/env python3\n\"\"\" Handles the loading and collation of events from Tensorflow event log files. \"\"\"\nfrom __future__ import annotations\nimport logging\nimport os\nimport re\nimport typing as T\nimport zlib\n\nfrom dataclasses import dataclass, field\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.core.util import event_pb2  # pylint:disable=no-name-in-module\nfrom tensorflow.python.framework import (  # pylint:disable=no-name-in-module\n    errors_impl as tf_errors)\n\nfrom lib.logger import parse_class_init\nfrom lib.serializer import get_serializer\n\nif T.TYPE_CHECKING:\n    from collections.abc import Generator, Iterator\n\nlogger = logging.getLogger(__name__)\n\n\n@dataclass\nclass EventData:\n    \"\"\" Holds data collected from Tensorflow Event Files\n\n    Parameters\n    ----------\n    timestamp: float\n        The timestamp of the event step (iteration)\n    loss: list[float]\n        The loss values collected for A and B sides for the event step\n    \"\"\"\n    timestamp: float = 0.0\n    loss: list[float] = field(default_factory=list)\n\n\nclass _LogFiles():\n    \"\"\" Holds the filenames of the Tensorflow Event logs that require parsing.\n\n    Parameters\n    ----------\n    logs_folder: str\n        The folder that contains the Tensorboard log files\n    \"\"\"\n    def __init__(self, logs_folder: str) -> None:\n        logger.debug(parse_class_init(locals()))\n        self._logs_folder = logs_folder\n        self._filenames = self._get_log_filenames()\n        logger.debug(\"Initialized: %s\", self.__class__.__name__)\n\n    @property\n    def session_ids(self) -> list[int]:\n        \"\"\" list[int]: Sorted list of `ints` of available session ids. \"\"\"\n        return list(sorted(self._filenames))\n\n    def _get_log_filenames(self) -> dict[int, str]:\n        \"\"\" Get the Tensorflow event filenames for all existing sessions.\n\n        Returns\n        -------\n        dict[int, str]\n            The full path of each log file for each training session id that has been run\n        \"\"\"\n        logger.debug(\"Loading log filenames. base_dir: '%s'\", self._logs_folder)\n        retval: dict[int, str] = {}\n        for dirpath, _, filenames in os.walk(self._logs_folder):\n            if not any(filename.startswith(\"events.out.tfevents\") for filename in filenames):\n                continue\n            session_id = self._get_session_id(dirpath)\n            if session_id is None:\n                logger.warning(\"Unable to load session data for model\")\n                return retval\n            retval[session_id] = self._get_log_filename(dirpath, filenames)\n        logger.debug(\"logfiles: %s\", retval)\n        return retval\n\n    @classmethod\n    def _get_session_id(cls, folder: str) -> int | None:\n        \"\"\" Obtain the session id for the given folder.\n\n        Parameters\n        ----------\n        folder: str\n            The full path to the folder that contains the session's Tensorflow Event Log\n\n        Returns\n        -------\n        int or ``None``\n            The session ID for the given folder. If no session id can be determined, return\n            ``None``\n        \"\"\"\n        session = os.path.split(os.path.split(folder)[0])[1]\n        session_id = session[session.rfind(\"_\") + 1:]\n        retval = None if not session_id.isdigit() else int(session_id)\n        logger.debug(\"folder: '%s', session_id: %s\", folder, retval)\n        return retval\n\n    @classmethod\n    def _get_log_filename(cls, folder: str, filenames: list[str]) -> str:\n        \"\"\" Obtain the session log file for the given folder. If multiple log files exist for the\n        given folder, then the most recent log file is used, as earlier files are assumed to be\n        obsolete.\n\n        Parameters\n        ----------\n        folder: str\n            The full path to the folder that contains the session's Tensorflow Event Log\n        filenames: list[str]\n            List of filenames that exist within the given folder\n\n        Returns\n        -------\n        str\n            The full path of the selected log file\n        \"\"\"\n        logfiles = [fname for fname in filenames if fname.startswith(\"events.out.tfevents\")]\n        retval = os.path.join(folder, sorted(logfiles)[-1])  # Take last item if multi matches\n        logger.debug(\"logfiles: %s, selected: '%s'\", logfiles, retval)\n        return retval\n\n    def refresh(self) -> None:\n        \"\"\" Refresh the list of log filenames. \"\"\"\n        logger.debug(\"Refreshing log filenames\")\n        self._filenames = self._get_log_filenames()\n\n    def get(self, session_id: int) -> str:\n        \"\"\" Obtain the log filename for the given session id.\n\n        Parameters\n        ----------\n        session_id: int\n            The session id to obtain the log filename for\n\n        Returns\n        -------\n        str\n            The full path to the log file for the requested session id\n        \"\"\"\n        retval = self._filenames.get(session_id, \"\")\n        logger.debug(\"session_id: %s, log_filename: '%s'\", session_id, retval)\n        return retval\n\n\nclass _CacheData():\n    \"\"\" Holds cached data that has been retrieved from Tensorflow Event Files and is compressed\n    in memory for a single or live training session\n\n    Parameters\n    ----------\n    labels: list[str]\n        The labels for the loss values\n    timestamps: :class:`np.ndarray`\n        The timestamp of the event step (iteration)\n    loss: :class:`np.ndarray`\n        The loss values collected for A and B sides for the session\n    \"\"\"\n    def __init__(self, labels: list[str], timestamps: np.ndarray, loss: np.ndarray) -> None:\n        self.labels = labels\n        self._loss = zlib.compress(T.cast(bytes, loss))\n        self._timestamps = zlib.compress(T.cast(bytes, timestamps))\n        self._timestamps_shape = timestamps.shape\n        self._loss_shape = loss.shape\n\n    @property\n    def loss(self) -> np.ndarray:\n        \"\"\" :class:`numpy.ndarray`: The loss values for this session \"\"\"\n        retval: np.ndarray = np.frombuffer(zlib.decompress(self._loss), dtype=\"float32\")\n        if len(self._loss_shape) > 1:\n            retval = retval.reshape(-1, *self._loss_shape[1:])\n        return retval\n\n    @property\n    def timestamps(self) -> np.ndarray:\n        \"\"\" :class:`numpy.ndarray`: The timestamps for this session \"\"\"\n        retval: np.ndarray = np.frombuffer(zlib.decompress(self._timestamps), dtype=\"float64\")\n        if len(self._timestamps_shape) > 1:\n            retval = retval.reshape(-1, *self._timestamps_shape[1:])\n        return retval\n\n    def add_live_data(self, timestamps: np.ndarray, loss: np.ndarray) -> None:\n        \"\"\" Add live data to the end of the stored data\n\n        loss: :class:`numpy.ndarray`\n            The latest loss values to add to the cache\n        timestamps: :class:`numpy.ndarray`\n            The latest timestamps  to add to the cache\n        \"\"\"\n        new_buffer: list[bytes] = []\n        new_shapes: list[tuple[int, ...]] = []\n        for data, buffer, dtype, shape in zip([timestamps, loss],\n                                              [self._timestamps, self._loss],\n                                              [\"float64\", \"float32\"],\n                                              [self._timestamps_shape, self._loss_shape]):\n\n            old = np.frombuffer(zlib.decompress(buffer), dtype=dtype)\n            if data.ndim > 1:\n                old = old.reshape(-1, *data.shape[1:])\n\n            new = np.concatenate((old, data))\n\n            logger.debug(\"old_shape: %s new_shape: %s\", shape, new.shape)\n            new_buffer.append(zlib.compress(new))\n            new_shapes.append(new.shape)\n            del old\n\n        self._timestamps = new_buffer[0]\n        self._loss = new_buffer[1]\n        self._timestamps_shape = new_shapes[0]\n        self._loss_shape = new_shapes[1]\n\n\nclass _Cache():\n    \"\"\" Holds parsed Tensorflow log event data in a compressed cache in memory. \"\"\"\n    def __init__(self) -> None:\n        logger.debug(parse_class_init(locals()))\n        self._data: dict[int, _CacheData] = {}\n        self._carry_over: dict[int, EventData] = {}\n        self._loss_labels: list[str] = []\n        logger.debug(\"Initialized: %s\", self.__class__.__name__)\n\n    def is_cached(self, session_id: int) -> bool:\n        \"\"\" Check if the given session_id's data is already cached\n\n        Parameters\n        ----------\n        session_id: int\n            The session ID to check\n\n        Returns\n        -------\n        bool\n            ``True`` if the data already exists in the cache otherwise ``False``.\n        \"\"\"\n        return self._data.get(session_id) is not None\n\n    def cache_data(self,\n                   session_id: int,\n                   data: dict[int, EventData],\n                   labels: list[str],\n                   is_live: bool = False) -> None:\n        \"\"\" Add a full session's worth of event data to :attr:`_data`.\n\n        Parameters\n        ----------\n        session_id: int\n            The session id to add the data for\n        data[int, :class:`EventData`]\n            The extracted event data dictionary generated from :class:`_EventParser`\n        labels: list[str]\n            List of `str` for the labels of each loss value output\n        is_live: bool, optional\n            ``True`` if the data to be cached is from a live training session otherwise ``False``.\n            Default: ``False``\n        \"\"\"\n        logger.debug(\"Caching event data: (session_id: %s, labels: %s, data points: %s, \"\n                     \"is_live: %s)\", session_id, labels, len(data), is_live)\n\n        if labels:\n            logger.debug(\"Setting loss labels: %s\", labels)\n            self._loss_labels = labels\n\n        if not data:\n            logger.debug(\"No data to cache\")\n            return\n\n        timestamps, loss = self._to_numpy(data, is_live)\n\n        if not is_live or (is_live and not self._data.get(session_id)):\n            self._data[session_id] = _CacheData(self._loss_labels, timestamps, loss)\n        else:\n            self._add_latest_live(session_id, loss, timestamps)\n\n    def _to_numpy(self,\n                  data: dict[int, EventData],\n                  is_live: bool) -> tuple[np.ndarray, np.ndarray]:\n        \"\"\" Extract each individual step data into separate numpy arrays for loss and timestamps.\n\n        Timestamps are stored float64 as the extra accuracy is needed for correct timings. Arrays\n        are returned at the length of the shortest available data (i.e. truncated records are\n        dropped)\n\n        Parameters\n        ----------\n        data: dict\n            The incoming tensorflow event data in dictionary form per step\n        is_live: bool, optional\n            ``True`` if the data to be cached is from a live training session otherwise ``False``.\n            Default: ``False``\n\n        Returns\n        -------\n        timestamps: :class:`numpy.ndarray`\n            float64 array of all iteration's timestamps\n        loss: :class:`numpy.ndarray`\n            float32 array of all iteration's loss\n        \"\"\"\n        if is_live and self._carry_over:\n            logger.debug(\"Processing carry over: %s\", self._carry_over)\n            self._collect_carry_over(data)\n\n        times, loss = self._process_data(data, is_live)\n\n        if is_live and not all(len(val) == len(self._loss_labels) for val in loss):\n            # TODO Many attempts have been made to fix this for live graph logging, and the issue\n            # of non-consistent loss record sizes keeps coming up. In the meantime we shall swallow\n            # any loss values that are of incorrect length so graph remains functional. This will,\n            # most likely, lead to a mismatch on iteration count so a proper fix should be\n            # implemented.\n\n            # Timestamps and loss appears to remain consistent with each other, but sometimes loss\n            # appears non-consistent. eg (lengths):\n            # [2, 2, 2, 2, 2, 2, 2, 0] - last loss collection has zero length\n            # [1, 2, 2, 2, 2, 2, 2, 2] - 1st loss collection has 1 length\n            # [2, 2, 2, 3, 2, 2, 2] - 4th loss collection has 3 length\n\n            logger.debug(\"Inconsistent loss found in collection: %s\", loss)\n            for idx in reversed(range(len(loss))):\n                if len(loss[idx]) != len(self._loss_labels):\n                    logger.debug(\"Removing loss/timestamps at position %s\", idx)\n                    del loss[idx]\n                    del times[idx]\n\n        n_times, n_loss = (np.array(times, dtype=\"float64\"), np.array(loss, dtype=\"float32\"))\n        logger.debug(\"Converted to numpy: (data points: %s, timestamps shape: %s, loss shape: %s)\",\n                     len(data), n_times.shape, n_loss.shape)\n\n        return n_times, n_loss\n\n    def _collect_carry_over(self, data: dict[int, EventData]) -> None:\n        \"\"\" For live data, collect carried over data from the previous update and merge into the\n        current data dictionary.\n\n        Parameters\n        ----------\n        data: dict[int, :class:`EventData`]\n            The latest raw data dictionary\n        \"\"\"\n        logger.debug(\"Carry over keys: %s, data keys: %s\", list(self._carry_over), list(data))\n        for key in list(self._carry_over):\n            if key not in data:\n                logger.debug(\"Carry over found for item %s which does not exist in current \"\n                             \"data: %s. Skipping.\", key, list(data))\n                continue\n            carry_over = self._carry_over.pop(key)\n            update = data[key]\n            logger.debug(\"Merging carry over data: %s in to %s\", carry_over, update)\n            timestamp = update.timestamp\n            update.timestamp = carry_over.timestamp if not timestamp else timestamp\n            update.loss = carry_over.loss + update.loss\n            logger.debug(\"Merged carry over data: %s\", update)\n\n    def _process_data(self,\n                      data: dict[int, EventData],\n                      is_live: bool) -> tuple[list[float], list[list[float]]]:\n        \"\"\" Process live update data.\n\n        Live data requires different processing as often we will only have partial data for the\n        current step, so we need to cache carried over partial data to be picked up at the next\n        query. In addition to this, if training is unexpectedly interrupted, there may also be\n        partial data which needs to be cleansed prior to creating a numpy array\n\n        Parameters\n        ----------\n        data: dict\n            The incoming tensorflow event data in dictionary form per step\n        is_live: bool\n            ``True`` if the data to be cached is from a live training session otherwise ``False``.\n\n        Returns\n        -------\n        timestamps: tuple\n            Cleaned list of complete timestamps for the latest live query\n        loss: list\n            Cleaned list of complete loss for the latest live query\n        \"\"\"\n        timestamps, loss = zip(*[(data[idx].timestamp, data[idx].loss)\n                               for idx in sorted(data)])\n\n        l_loss: list[list[float]] = list(loss)\n        l_timestamps: list[float] = list(timestamps)\n\n        if len(l_loss[-1]) != len(self._loss_labels):\n            logger.debug(\"Truncated loss found. loss count: %s\", len(l_loss))\n            idx = sorted(data)[-1]\n            if is_live:\n                logger.debug(\"Setting carried over data: %s\", data[idx])\n                self._carry_over[idx] = data[idx]\n            logger.debug(\"Removing truncated loss: (timestamp: %s, loss: %s)\",\n                         l_timestamps[-1], loss[-1])\n            del l_loss[-1]\n            del l_timestamps[-1]\n\n        return l_timestamps, l_loss\n\n    def _add_latest_live(self, session_id: int, loss: np.ndarray, timestamps: np.ndarray) -> None:\n        \"\"\" Append the latest received live training data to the cached data.\n\n        Parameters\n        ----------\n        session_id: int\n            The training session ID to update the cache for\n        loss: :class:`numpy.ndarray`\n            The latest loss values returned from the iterator\n        timestamps: :class:`numpy.ndarray`\n            The latest time stamps returned from the iterator\n        \"\"\"\n        logger.debug(\"Adding live data to cache: (session_id: %s, loss: %s, timestamps: %s)\",\n                     session_id, loss.shape, timestamps.shape)\n        if not np.any(loss) and not np.any(timestamps):\n            return\n\n        self._data[session_id].add_live_data(timestamps, loss)\n\n    def get_data(self, session_id: int, metric: T.Literal[\"loss\", \"timestamps\"]\n                 ) -> dict[int, dict[str, np.ndarray | list[str]]] | None:\n        \"\"\" Retrieve the decompressed cached data from the cache for the given session id.\n\n        Parameters\n        ----------\n        session_id: int or ``None``\n            If session_id is provided, then the cached data for that session is returned. If\n            session_id is ``None`` then the cached data for all sessions is returned\n        metric: ['loss', 'timestamps']\n            The metric to return the data for.\n\n        Returns\n        -------\n        dict or ``None``\n            The `session_id`(s) as key, the values are a dictionary containing the requested\n            metric information for each session returned. ``None`` if no data is stored for the\n            given session_id\n        \"\"\"\n        if session_id is None:\n            raw = self._data\n        else:\n            data = self._data.get(session_id)\n            if not data:\n                return None\n            raw = {session_id: data}\n\n        retval: dict[int, dict[str, np.ndarray | list[str]]] = {}\n        for idx, data in raw.items():\n            array = data.loss if metric == \"loss\" else data.timestamps\n            val: dict[str, np.ndarray | list[str]] = {str(metric): array}\n            if metric == \"loss\":\n                val[\"labels\"] = data.labels\n            retval[idx] = val\n\n        logger.debug(\"Obtained cached data: %s\",\n                     {session_id: {k: v.shape if isinstance(v, np.ndarray) else v\n                                   for k, v in data.items()}\n                      for session_id, data in retval.items()})\n        return retval\n\n\nclass TensorBoardLogs():\n    \"\"\" Parse data from TensorBoard logs.\n\n    Process the input logs folder and stores the individual filenames per session.\n\n    Caches timestamp and loss data on request and returns this data from the cache.\n\n    Parameters\n    ----------\n    logs_folder: str\n        The folder that contains the Tensorboard log files\n    is_training: bool\n        ``True`` if the events are being read whilst Faceswap is training otherwise ``False``\n    \"\"\"\n    def __init__(self, logs_folder: str, is_training: bool) -> None:\n        logger.debug(parse_class_init(locals()))\n        self._is_training = False\n        self._training_iterator = None\n\n        self._log_files = _LogFiles(logs_folder)\n        self.set_training(is_training)\n\n        self._cache = _Cache()\n\n        logger.debug(\"Initialized: %s\", self.__class__.__name__)\n\n    @property\n    def session_ids(self) -> list[int]:\n        \"\"\" list[int]: Sorted list of integers of available session ids. \"\"\"\n        return self._log_files.session_ids\n\n    def set_training(self, is_training: bool) -> None:\n        \"\"\" Set the internal training flag to the given `is_training` value.\n\n        If a new training session is being instigated, refresh the log filenames\n\n        Parameters\n        ----------\n        is_training: bool\n            ``True`` to indicate that the logs to be read are from the currently training\n            session otherwise ``False``\n        \"\"\"\n        if self._is_training == is_training:\n            logger.debug(\"Training flag already set to %s. Returning\", is_training)\n            return\n\n        logger.debug(\"Setting is_training to %s\", is_training)\n        self._is_training = is_training\n        if is_training:\n            self._log_files.refresh()\n            log_file = self._log_files.get(self.session_ids[-1])\n            logger.debug(\"Setting training iterator for log file: '%s'\", log_file)\n            self._training_iterator = tf.compat.v1.io.tf_record_iterator(log_file)\n        else:\n            logger.debug(\"Removing training iterator\")\n            del self._training_iterator\n            self._training_iterator = None\n\n    def _cache_data(self, session_id: int) -> None:\n        \"\"\" Cache TensorBoard logs for the given session ID on first access.\n\n        Populates :attr:`_cache` with timestamps and loss data.\n\n        If this is a training session and the data is being queried for the training session ID\n        then get the latest available data and append to the cache\n\n        Parameters\n        -------\n        session_id: int\n            The session ID to cache the data for\n        \"\"\"\n        live_data = self._is_training and session_id == max(self.session_ids)\n        iterator = self._training_iterator if live_data else tf.compat.v1.io.tf_record_iterator(\n            self._log_files.get(session_id))\n        assert iterator is not None\n        parser = _EventParser(iterator, self._cache, live_data)\n        parser.cache_events(session_id)\n\n    def _check_cache(self, session_id: int | None = None) -> None:\n        \"\"\" Check if the given session_id has been cached and if not, cache it.\n\n        Parameters\n        ----------\n        session_id: int, optional\n            The Session ID to return the data for. Set to ``None`` to return all session\n            data. Default ``None`\n        \"\"\"\n        if session_id is not None and not self._cache.is_cached(session_id):\n            self._cache_data(session_id)\n        elif self._is_training and session_id == self.session_ids[-1]:\n            self._cache_data(session_id)\n        elif session_id is None:\n            for idx in self.session_ids:\n                if not self._cache.is_cached(idx):\n                    self._cache_data(idx)\n\n    def get_loss(self, session_id: int | None = None) -> dict[int, dict[str, np.ndarray]]:\n        \"\"\" Read the loss from the TensorBoard event logs\n\n        Parameters\n        ----------\n        session_id: int, optional\n            The Session ID to return the loss for. Set to ``None`` to return all session\n            losses. Default ``None``\n\n        Returns\n        -------\n        dict\n            The session id(s) as key, with a further dictionary as value containing the loss name\n            and list of loss values for each step\n        \"\"\"\n        logger.debug(\"Getting loss: (session_id: %s)\", session_id)\n        retval: dict[int, dict[str, np.ndarray]] = {}\n        for idx in [session_id] if session_id else self.session_ids:\n            self._check_cache(idx)\n            full_data = self._cache.get_data(idx, \"loss\")\n            if not full_data:\n                continue\n            data = full_data[idx]\n            loss = data[\"loss\"]\n            assert isinstance(loss, np.ndarray)\n            retval[idx] = {title: loss[:, idx] for idx, title in enumerate(data[\"labels\"])}\n\n        logger.debug({key: {k: v.shape for k, v in val.items()}\n                      for key, val in retval.items()})\n        return retval\n\n    def get_timestamps(self, session_id: int | None = None) -> dict[int, np.ndarray]:\n        \"\"\" Read the timestamps from the TensorBoard logs.\n\n        As loss timestamps are slightly different for each loss, we collect the timestamp from the\n        `batch_loss` key.\n\n        Parameters\n        ----------\n        session_id: int, optional\n            The Session ID to return the timestamps for. Set to ``None`` to return all session\n            timestamps. Default ``None``\n\n        Returns\n        -------\n        dict\n            The session id(s) as key with list of timestamps per step as value\n        \"\"\"\n\n        logger.debug(\"Getting timestamps: (session_id: %s, is_training: %s)\",\n                     session_id, self._is_training)\n        retval: dict[int, np.ndarray] = {}\n        for idx in [session_id] if session_id else self.session_ids:\n            self._check_cache(idx)\n            data = self._cache.get_data(idx, \"timestamps\")\n            if not data:\n                continue\n            timestamps = data[idx][\"timestamps\"]\n            assert isinstance(timestamps, np.ndarray)\n            retval[idx] = timestamps\n        logger.debug({k: v.shape for k, v in retval.items()})\n        return retval\n\n\nclass _EventParser():\n    \"\"\" Parses Tensorflow event and populates data to :class:`_Cache`.\n\n    Parameters\n    ----------\n    iterator: :func:`tf.compat.v1.io.tf_record_iterator`\n        The iterator to use for reading Tensorflow event logs\n    cache: :class:`_Cache`\n        The cache object to store the collected parsed events to\n    live_data: bool\n        ``True`` if the iterator to be loaded is a training iterator for reading live data\n        otherwise ``False``\n    \"\"\"\n    def __init__(self, iterator: Iterator[bytes], cache: _Cache, live_data: bool) -> None:\n        logger.debug(parse_class_init(locals()))\n        self._live_data = live_data\n        self._cache = cache\n        self._iterator = self._get_latest_live(iterator) if live_data else iterator\n        self._loss_labels: list[str] = []\n        self._num_strip = re.compile(r\"_\\d+$\")\n        logger.debug(\"Initialized: %s\", self.__class__.__name__)\n\n    @classmethod\n    def _get_latest_live(cls, iterator: Iterator[bytes]) -> Generator[bytes, None, None]:\n        \"\"\" Obtain the latest event logs for live training data.\n\n        The live data iterator remains open so that it can be re-queried\n\n        Parameters\n        ----------\n        iterator: :func:`tf.compat.v1.io.tf_record_iterator`\n            The live training iterator to use for reading Tensorflow event logs\n\n        Yields\n        ------\n        dict\n            A Tensorflow event in dictionary form for a single step\n        \"\"\"\n        i = 0\n        while True:\n            try:\n                yield next(iterator)\n                i += 1\n            except StopIteration:\n                logger.debug(\"End of data reached\")\n                break\n            except tf.errors.DataLossError as err:\n                # Truncated records are ignored. The iterator holds the offset, so the record will\n                # be completed at the next call.\n                logger.debug(\"Truncated record. Original Error: %s\", err)\n                break\n        logger.debug(\"Collected %s records from live log file\", i)\n\n    def cache_events(self, session_id: int) -> None:\n        \"\"\" Parse the Tensorflow events logs and add to :attr:`_cache`.\n\n        Parameters\n        ----------\n        session_id: int\n            The session id that the data is being cached for\n        \"\"\"\n        assert self._iterator is not None\n        data: dict[int, EventData] = {}\n        try:\n            for record in self._iterator:\n                event = event_pb2.Event.FromString(record)  # pylint:disable=no-member\n                if not event.summary.value:\n                    continue\n                if event.summary.value[0].tag == \"keras\":\n                    self._parse_outputs(event)\n                if event.summary.value[0].tag.startswith(\"batch_\"):\n                    data[event.step] = self._process_event(event,\n                                                           data.get(event.step, EventData()))\n\n        except tf_errors.DataLossError as err:\n            logger.warning(\"The logs for Session %s are corrupted and cannot be displayed. \"\n                           \"The totals do not include this session. Original error message: \"\n                           \"'%s'\", session_id, str(err))\n\n        self._cache.cache_data(session_id, data, self._loss_labels, is_live=self._live_data)\n\n    def _parse_outputs(self, event: event_pb2.Event) -> None:\n        \"\"\" Parse the outputs from the stored model structure for mapping loss names to\n        model outputs.\n\n        Loss names are added to :attr:`_loss_labels`\n\n        Notes\n        -----\n        The master model does not actually contain the specified output name, so we dig into the\n        sub-model to obtain the name of the output layers\n\n        Parameters\n        ----------\n        event: :class:`tensorflow.core.util.event_pb2`\n            The event data containing the keras model structure to be parsed\n        \"\"\"\n        serializer = get_serializer(\"json\")\n        struct = event.summary.value[0].tensor.string_val[0]\n\n        config = serializer.unmarshal(struct)[\"config\"]\n        model_outputs = self._get_outputs(config)\n\n        for side_outputs, side in zip(model_outputs, (\"a\", \"b\")):\n            logger.debug(\"side: '%s', outputs: '%s'\", side, side_outputs)\n            layer_name = side_outputs[0][0]\n\n            output_config = next(layer for layer in config[\"layers\"]\n                                 if layer[\"name\"] == layer_name)[\"config\"]\n            layer_outputs = self._get_outputs(output_config)\n            for output in layer_outputs:  # Drill into sub-model to get the actual output names\n                loss_name = self._num_strip.sub(\"\", output[0][0])  # strip trailing numbers\n                if loss_name[-2:] not in (\"_a\", \"_b\"):  # Rename losses to reflect the side output\n                    new_name = f\"{loss_name.replace('_both', '')}_{side}\"\n                    logger.debug(\"Renaming loss output from '%s' to '%s'\", loss_name, new_name)\n                    loss_name = new_name\n                if loss_name not in self._loss_labels:\n                    logger.debug(\"Adding loss name: '%s'\", loss_name)\n                    self._loss_labels.append(loss_name)\n        logger.debug(\"Collated loss labels: %s\", self._loss_labels)\n\n    @classmethod\n    def _get_outputs(cls, model_config: dict[str, T.Any]) -> np.ndarray:\n        \"\"\" Obtain the output names, instance index and output index for the given model.\n\n        If there is only a single output, the shape of the array is expanded to remain consistent\n        with multi model outputs\n\n        Parameters\n        ----------\n        model_config: dict\n            The saved Keras model configuration dictionary\n\n        Returns\n        -------\n        :class:`numpy.ndarray`\n            The layer output names, their instance index and their output index\n        \"\"\"\n        outputs = np.array(model_config[\"output_layers\"])\n        logger.debug(\"Obtained model outputs: %s, shape: %s\", outputs, outputs.shape)\n        if outputs.ndim == 2:  # Insert extra dimension for non learn mask models\n            outputs = np.expand_dims(outputs, axis=1)\n            logger.debug(\"Expanded dimensions for single output model. outputs: %s, shape: %s\",\n                         outputs, outputs.shape)\n        return outputs\n\n    @classmethod\n    def _process_event(cls, event: event_pb2.Event, step: EventData) -> EventData:\n        \"\"\" Process a single Tensorflow event.\n\n        Adds timestamp to the step `dict` if a total loss value is received, process the labels for\n        any new loss entries and adds the side loss value to the step `dict`.\n\n        Parameters\n        ----------\n        event: :class:`tensorflow.core.util.event_pb2`\n            The event data to be processed\n        step: :class:`EventData`\n            The currently processing dictionary to be populated with the extracted data from the\n            tensorflow event for this step\n\n        Returns\n        -------\n         :class:`EventData`\n            The given step :class:`EventData` with the given event data added to it.\n        \"\"\"\n        summary = event.summary.value[0]\n\n        if summary.tag == \"batch_total\":\n            step.timestamp = event.wall_time\n            return step\n\n        loss = summary.simple_value\n        if not loss:\n            # Need to convert a tensor to a float for TF2.8 logged data. This maybe due to change\n            # in logging or may be due to work around put in place in FS training function for the\n            # following bug in TF 2.8/2.9 when writing records:\n            #  https://github.com/keras-team/keras/issues/16173\n            loss = float(tf.make_ndarray(summary.tensor))\n\n        step.loss.append(loss)\n\n        return step\n", "lib/gui/analysis/stats.py": "#!/usr/bin python3\n\"\"\" Stats functions for the GUI.\n\nHolds the globally loaded training session. This will either be a user selected session (loaded in\nthe analysis tab) or the currently training session.\n\n\"\"\"\nfrom __future__ import annotations\nimport logging\nimport os\nimport time\nimport typing as T\nimport warnings\n\nfrom math import ceil\nfrom threading import Event\n\nimport numpy as np\n\nfrom lib.logger import parse_class_init\nfrom lib.serializer import get_serializer\n\nfrom .event_reader import TensorBoardLogs\n\nlogger = logging.getLogger(__name__)\n\n\nclass GlobalSession():\n    \"\"\" Holds information about a loaded or current training session by accessing a model's state\n    file and Tensorboard logs. This class should not be accessed directly, rather through\n    :attr:`lib.gui.analysis.Session`\n    \"\"\"\n    def __init__(self) -> None:\n        logger.debug(parse_class_init(locals()))\n        self._state: dict[str, T.Any] = {}\n        self._model_dir = \"\"\n        self._model_name = \"\"\n\n        self._tb_logs: TensorBoardLogs | None = None\n        self._summary: SessionsSummary | None = None\n\n        self._is_training = False\n        self._is_querying = Event()\n\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    @property\n    def is_loaded(self) -> bool:\n        \"\"\" bool: ``True`` if session data is loaded otherwise ``False`` \"\"\"\n        return bool(self._model_dir)\n\n    @property\n    def is_training(self) -> bool:\n        \"\"\" bool: ``True`` if the loaded session is the currently training model, otherwise\n        ``False`` \"\"\"\n        return self._is_training\n\n    @property\n    def model_filename(self) -> str:\n        \"\"\" str: The full model filename \"\"\"\n        return os.path.join(self._model_dir, self._model_name)\n\n    @property\n    def batch_sizes(self) -> dict[int, int]:\n        \"\"\" dict: The batch sizes for each session_id for the model. \"\"\"\n        if not self._state:\n            return {}\n        return {int(sess_id): sess[\"batchsize\"]\n                for sess_id, sess in self._state.get(\"sessions\", {}).items()}\n\n    @property\n    def full_summary(self) -> list[dict]:\n        \"\"\" list: List of dictionaries containing summary statistics for each session id. \"\"\"\n        assert self._summary is not None\n        return self._summary.get_summary_stats()\n\n    @property\n    def logging_disabled(self) -> bool:\n        \"\"\" bool: ``True`` if logging is enabled for the currently training session otherwise\n        ``False``. \"\"\"\n        if not self._state:\n            return True\n        max_id = str(max(int(idx) for idx in self._state[\"sessions\"]))\n        return self._state[\"sessions\"][max_id][\"no_logs\"]\n\n    @property\n    def session_ids(self) -> list[int]:\n        \"\"\" list: The sorted list of all existing session ids in the state file \"\"\"\n        if self._tb_logs is None:\n            return []\n        return self._tb_logs.session_ids\n\n    def _load_state_file(self) -> None:\n        \"\"\" Load the current state file to :attr:`_state`. \"\"\"\n        state_file = os.path.join(self._model_dir, f\"{self._model_name}_state.json\")\n        logger.debug(\"Loading State: '%s'\", state_file)\n        serializer = get_serializer(\"json\")\n        self._state = serializer.load(state_file)\n        logger.debug(\"Loaded state: %s\", self._state)\n\n    def initialize_session(self,\n                           model_folder: str,\n                           model_name: str,\n                           is_training: bool = False) -> None:\n        \"\"\" Initialize a Session.\n\n        Load's the model's state file, and sets the paths to any underlying Tensorboard logs, ready\n        for access on request.\n\n        Parameters\n        ----------\n        model_folder: str,\n            If loading a session manually (e.g. for the analysis tab), then the path to the model\n            folder must be provided. For training sessions, this should be passed through from the\n            launcher\n        model_name: str, optional\n            If loading a session manually (e.g. for the analysis tab), then the model filename\n            must be provided. For training sessions, this should be passed through from the\n            launcher\n        is_training: bool, optional\n            ``True`` if the session is being initialized for a training session, otherwise\n            ``False``. Default: ``False``\n         \"\"\"\n        logger.debug(\"Initializing session: (is_training: %s)\", is_training)\n\n        if self._model_dir == model_folder and self._model_name == model_name:\n            if is_training:\n                assert self._tb_logs is not None\n                self._tb_logs.set_training(is_training)\n                self._load_state_file()\n                self._is_training = True\n            logger.debug(\"Requested session is already loaded. Not initializing: (model_folder: \"\n                         \"%s, model_name: %s)\", model_folder, model_name)\n            return\n\n        self._is_training = is_training\n        self._model_dir = model_folder\n        self._model_name = model_name\n        self._load_state_file()\n        if not self.logging_disabled:\n            self._tb_logs = TensorBoardLogs(os.path.join(self._model_dir,\n                                                         f\"{self._model_name}_logs\"),\n                                            is_training)\n\n        self._summary = SessionsSummary(self)\n        logger.debug(\"Initialized session. Session_IDS: %s\", self.session_ids)\n\n    def stop_training(self) -> None:\n        \"\"\" Clears the internal training flag. To be called when training completes. \"\"\"\n        self._is_training = False\n        if self._tb_logs is not None:\n            self._tb_logs.set_training(False)\n\n    def clear(self) -> None:\n        \"\"\" Clear the currently loaded session. \"\"\"\n        self._state = {}\n        self._model_dir = \"\"\n        self._model_name = \"\"\n\n        del self._tb_logs\n        self._tb_logs = None\n\n        del self._summary\n        self._summary = None\n\n        self._is_training = False\n\n    def get_loss(self, session_id: int | None) -> dict[str, np.ndarray]:\n        \"\"\" Obtain the loss values for the given session_id.\n\n        Parameters\n        ----------\n        session_id: int or ``None``\n            The session ID to return loss for. Pass ``None`` to return loss for all sessions.\n\n        Returns\n        -------\n        dict\n            Loss names as key, :class:`numpy.ndarray` as value. If No session ID was provided\n            all session's losses are collated\n        \"\"\"\n        self._wait_for_thread()\n\n        if self._is_training:\n            self._is_querying.set()\n\n        assert self._tb_logs is not None\n        loss_dict = self._tb_logs.get_loss(session_id=session_id)\n        if session_id is None:\n            all_loss: dict[str, list[float]] = {}\n            for key in sorted(loss_dict):\n                for loss_key, loss in loss_dict[key].items():\n                    all_loss.setdefault(loss_key, []).extend(loss)\n            retval: dict[str, np.ndarray] = {key: np.array(val, dtype=\"float32\")\n                                             for key, val in all_loss.items()}\n        else:\n            retval = loss_dict.get(session_id, {})\n\n        if self._is_training:\n            self._is_querying.clear()\n        return retval\n\n    @T.overload\n    def get_timestamps(self, session_id: None) -> dict[int, np.ndarray]:\n        ...\n\n    @T.overload\n    def get_timestamps(self, session_id: int) -> np.ndarray:\n        ...\n\n    def get_timestamps(self, session_id):\n        \"\"\" Obtain the time stamps keys for the given session_id.\n\n        Parameters\n        ----------\n        session_id: int or ``None``\n            The session ID to return the time stamps for. Pass ``None`` to return time stamps for\n            all sessions.\n\n        Returns\n        -------\n        dict[int] or :class:`numpy.ndarray`\n            If a session ID has been given then a single :class:`numpy.ndarray` will be returned\n            with the session's time stamps. Otherwise a 'dict' will be returned with the session\n            IDs as key with :class:`numpy.ndarray` of timestamps as values\n        \"\"\"\n        self._wait_for_thread()\n\n        if self._is_training:\n            self._is_querying.set()\n\n        assert self._tb_logs is not None\n        retval = self._tb_logs.get_timestamps(session_id=session_id)\n        if session_id is not None:\n            retval = retval[session_id]\n\n        if self._is_training:\n            self._is_querying.clear()\n\n        return retval\n\n    def _wait_for_thread(self) -> None:\n        \"\"\" If a thread is querying the log files for live data, then block until task clears. \"\"\"\n        while True:\n            if self._is_training and self._is_querying.is_set():\n                logger.debug(\"Waiting for available thread\")\n                time.sleep(1)\n                continue\n            break\n\n    def get_loss_keys(self, session_id: int | None) -> list[str]:\n        \"\"\" Obtain the loss keys for the given session_id.\n\n        Parameters\n        ----------\n        session_id: int or ``None``\n            The session ID to return the loss keys for. Pass ``None`` to return loss keys for\n            all sessions.\n\n        Returns\n        -------\n        list\n            The loss keys for the given session. If ``None`` is passed as session_id then a unique\n            list of all loss keys for all sessions is returned\n        \"\"\"\n        assert self._tb_logs is not None\n        loss_keys = {sess_id: list(logs.keys())\n                     for sess_id, logs\n                     in self._tb_logs.get_loss(session_id=session_id).items()}\n\n        if session_id is None:\n            retval: list[str] = list(set(loss_key\n                                         for session in loss_keys.values()\n                                         for loss_key in session))\n        else:\n            retval = loss_keys.get(session_id, [])\n        return retval\n\n\n_SESSION = GlobalSession()\n\n\nclass SessionsSummary():\n    \"\"\" Performs top level summary calculations for each session ID within the loaded or currently\n    training Session for display in the Analysis tree view.\n\n    Parameters\n    ----------\n    session: :class:`GlobalSession`\n        The loaded or currently training session\n    \"\"\"\n    def __init__(self, session: GlobalSession) -> None:\n        logger.debug(parse_class_init(locals()))\n        self._session = session\n        self._state = session._state\n\n        self._time_stats: dict[int, dict[str, float | int]] = {}\n        self._per_session_stats: list[dict[str, T.Any]] = []\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    def get_summary_stats(self) -> list[dict]:\n        \"\"\" Compile the individual session statistics and calculate the total.\n\n        Format the stats for display\n\n        Returns\n        -------\n        list\n            A list of summary statistics dictionaries containing the Session ID, start time, end\n            time, elapsed time, rate, batch size and number of iterations for each session id\n            within the loaded data as well as the totals.\n        \"\"\"\n        logger.debug(\"Compiling sessions summary data\")\n        self._get_time_stats()\n        self._get_per_session_stats()\n        if not self._per_session_stats:\n            return self._per_session_stats\n\n        total_stats = self._total_stats()\n        retval = self._per_session_stats + [total_stats]\n        retval = self._format_stats(retval)\n        logger.debug(\"Final stats: %s\", retval)\n        return retval\n\n    def _get_time_stats(self) -> None:\n        \"\"\" Populates the attribute :attr:`_time_stats` with the start start time, end time and\n        data points for each session id within the loaded session if it has not already been\n        calculated.\n\n        If the main Session is currently training, then the training session ID is updated with the\n        latest stats.\n        \"\"\"\n        if not self._time_stats:\n            logger.debug(\"Collating summary time stamps\")\n\n            self._time_stats = {\n                sess_id: {\"start_time\": np.min(timestamps) if np.any(timestamps) else 0,\n                          \"end_time\": np.max(timestamps) if np.any(timestamps) else 0,\n                          \"iterations\": timestamps.shape[0] if np.any(timestamps) else 0}\n                for sess_id, timestamps in T.cast(dict[int, np.ndarray],\n                                                  self._session.get_timestamps(None)).items()}\n\n        elif _SESSION.is_training:\n            logger.debug(\"Updating summary time stamps for training session\")\n\n            session_id = _SESSION.session_ids[-1]\n            latest = T.cast(np.ndarray, self._session.get_timestamps(session_id))\n\n            self._time_stats[session_id] = {\n                \"start_time\": np.min(latest) if np.any(latest) else 0,\n                \"end_time\": np.max(latest) if np.any(latest) else 0,\n                \"iterations\": latest.shape[0] if np.any(latest) else 0}\n\n        logger.debug(\"time_stats: %s\", self._time_stats)\n\n    def _get_per_session_stats(self) -> None:\n        \"\"\" Populate the attribute :attr:`_per_session_stats` with a sorted list by session ID\n        of each ID in the training/loaded session. Stats contain the session ID, start, end and\n        elapsed times, the training rate, batch size and number of iterations for each session.\n\n        If a training session is running, then updates the training sessions stats only.\n        \"\"\"\n        if not self._per_session_stats:\n            logger.debug(\"Collating per session stats\")\n            compiled = []\n            for session_id in self._time_stats:\n                logger.debug(\"Compiling session ID: %s\", session_id)\n                if not self._state:\n                    logger.debug(\"Session state dict doesn't exist. Most likely task has been \"\n                                 \"terminated during compilation\")\n                    return\n                compiled.append(self._collate_stats(session_id))\n\n            self._per_session_stats = list(sorted(compiled, key=lambda k: k[\"session\"]))\n\n        elif self._session.is_training:\n            logger.debug(\"Collating per session stats for latest training data\")\n            session_id = self._session.session_ids[-1]\n            ts_data = self._time_stats[session_id]\n\n            if session_id > len(self._per_session_stats):\n                self._per_session_stats.append(self._collate_stats(session_id))\n\n            stats = self._per_session_stats[-1]\n\n            start = np.nan_to_num(ts_data[\"start_time\"])\n            end = np.nan_to_num(ts_data[\"end_time\"])\n            stats[\"start\"] = start\n            stats[\"end\"] = end\n            stats[\"elapsed\"] = int(end - start)\n            stats[\"iterations\"] = ts_data[\"iterations\"]\n            stats[\"rate\"] = (((stats[\"batch\"] * 2) * stats[\"iterations\"])\n                             / stats[\"elapsed\"] if stats[\"elapsed\"] > 0 else 0)\n        logger.debug(\"per_session_stats: %s\", self._per_session_stats)\n\n    def _collate_stats(self, session_id: int) -> dict[str, int | float]:\n        \"\"\" Collate the session summary statistics for the given session ID.\n\n        Parameters\n        ----------\n        session_id: int\n            The session id to compile the stats for\n\n        Returns\n        -------\n        dict\n            The collated session summary statistics\n        \"\"\"\n        timestamps = self._time_stats[session_id]\n        start = np.nan_to_num(timestamps[\"start_time\"])\n        end = np.nan_to_num(timestamps[\"end_time\"])\n        elapsed = int(end - start)\n        batchsize = self._session.batch_sizes.get(session_id, 0)\n        retval = {\n            \"session\": session_id,\n            \"start\": start,\n            \"end\": end,\n            \"elapsed\": elapsed,\n            \"rate\": (((batchsize * 2) * timestamps[\"iterations\"]) / elapsed\n                     if elapsed != 0 else 0),\n            \"batch\": batchsize,\n            \"iterations\": timestamps[\"iterations\"]}\n        logger.debug(retval)\n        return retval\n\n    def _total_stats(self) -> dict[str, str | int | float]:\n        \"\"\" Compile the Totals stats.\n        Totals are fully calculated each time as they will change on the basis of the training\n        session.\n\n        Returns\n        -------\n        dict\n            The Session name, start time, end time, elapsed time, rate, batch size and number of\n            iterations for all session ids within the loaded data.\n        \"\"\"\n        logger.debug(\"Compiling Totals\")\n        elapsed = 0\n        examples = 0\n        iterations = 0\n        batchset = set()\n        total_summaries = len(self._per_session_stats)\n        for idx, summary in enumerate(self._per_session_stats):\n            if idx == 0:\n                starttime = summary[\"start\"]\n            if idx == total_summaries - 1:\n                endtime = summary[\"end\"]\n            elapsed += summary[\"elapsed\"]\n            examples += ((summary[\"batch\"] * 2) * summary[\"iterations\"])\n            batchset.add(summary[\"batch\"])\n            iterations += summary[\"iterations\"]\n        batch = \",\".join(str(bs) for bs in batchset)\n        totals = {\"session\": \"Total\",\n                  \"start\": starttime,\n                  \"end\": endtime,\n                  \"elapsed\": elapsed,\n                  \"rate\": examples / elapsed if elapsed != 0 else 0,\n                  \"batch\": batch,\n                  \"iterations\": iterations}\n        logger.debug(totals)\n        return totals\n\n    def _format_stats(self, compiled_stats: list[dict]) -> list[dict]:\n        \"\"\" Format for the incoming list of statistics for display.\n\n        Parameters\n        ----------\n        compiled_stats: list\n            List of summary statistics dictionaries to be formatted for display\n\n        Returns\n        -------\n        list\n            The original statistics formatted for display\n        \"\"\"\n        logger.debug(\"Formatting stats\")\n        retval = []\n        for summary in compiled_stats:\n            hrs, mins, secs = self._convert_time(summary[\"elapsed\"])\n            stats = {}\n            for key in summary:\n                if key not in (\"start\", \"end\", \"elapsed\", \"rate\"):\n                    stats[key] = summary[key]\n                    continue\n                stats[\"start\"] = time.strftime(\"%x %X\", time.localtime(summary[\"start\"]))\n                stats[\"end\"] = time.strftime(\"%x %X\", time.localtime(summary[\"end\"]))\n                stats[\"elapsed\"] = f\"{hrs}:{mins}:{secs}\"\n                stats[\"rate\"] = f\"{summary['rate']:.1f}\"\n            retval.append(stats)\n        return retval\n\n    @classmethod\n    def _convert_time(cls, timestamp: float) -> tuple[str, str, str]:\n        \"\"\" Convert time stamp to total hours, minutes and seconds.\n\n        Parameters\n        ----------\n        timestamp: float\n            The Unix timestamp to be converted\n\n        Returns\n        -------\n        tuple\n            (`hours`, `minutes`, `seconds`) as strings\n        \"\"\"\n        ihrs = int(timestamp // 3600)\n        hrs = f\"{ihrs:02d}\" if ihrs < 10 else str(ihrs)\n        mins = f\"{(int(timestamp % 3600) // 60):02d}\"\n        secs = f\"{(int(timestamp % 3600) % 60):02d}\"\n        return hrs, mins, secs\n\n\nclass Calculations():\n    \"\"\" Class that performs calculations on the :class:`GlobalSession` raw data for the given\n    session id.\n\n    Parameters\n    ----------\n    session_id: int or ``None``\n        The session id number for the selected session from the Analysis tab. Should be ``None``\n        if all sessions are being calculated\n    display: {\"loss\", \"rate\"}, optional\n        Whether to display a graph for loss or training rate. Default: `\"loss\"`\n    loss_keys: list, optional\n        The list of loss keys to display on the graph. Default: `[\"loss\"]`\n    selections: list, optional\n        The selected annotations to display. Default: `[\"raw\"]`\n    avg_samples: int, optional\n        The number of samples to use for performing moving average calculation. Default: `500`.\n    smooth_amount: float, optional\n        The amount of smoothing to apply for performing smoothing calculation. Default: `0.9`.\n    flatten_outliers: bool, optional\n        ``True`` if values significantly away from the average should be excluded, otherwise\n        ``False``. Default: ``False``\n    \"\"\"\n    def __init__(self, session_id,\n                 display: str = \"loss\",\n                 loss_keys: list[str] | str = \"loss\",\n                 selections: list[str] | str = \"raw\",\n                 avg_samples: int = 500,\n                 smooth_amount: float = 0.90,\n                 flatten_outliers: bool = False) -> None:\n        logger.debug(parse_class_init(locals()))\n        warnings.simplefilter(\"ignore\", np.RankWarning)\n\n        self._session_id = session_id\n\n        self._display = display\n        self._loss_keys = loss_keys if isinstance(loss_keys, list) else [loss_keys]\n        self._selections = selections if isinstance(selections, list) else [selections]\n        self._is_totals = session_id is None\n        self._args: dict[str, int | float] = {\"avg_samples\": avg_samples,\n                                              \"smooth_amount\": smooth_amount,\n                                              \"flatten_outliers\": flatten_outliers}\n        self._iterations = 0\n        self._limit = 0\n        self._start_iteration = 0\n        self._stats: dict[str, np.ndarray] = {}\n        self.refresh()\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    @property\n    def iterations(self) -> int:\n        \"\"\" int: The number of iterations in the data set. \"\"\"\n        return self._iterations\n\n    @property\n    def start_iteration(self) -> int:\n        \"\"\" int: The starting iteration number of a limit has been set on the amount of data. \"\"\"\n        return self._start_iteration\n\n    @property\n    def stats(self) -> dict[str, np.ndarray]:\n        \"\"\" dict: The final calculated statistics \"\"\"\n        return self._stats\n\n    def refresh(self) -> Calculations | None:\n        \"\"\" Refresh the stats \"\"\"\n        logger.debug(\"Refreshing\")\n        if not _SESSION.is_loaded:\n            logger.warning(\"Session data is not initialized. Not refreshing\")\n            return None\n        self._iterations = 0\n        self._get_raw()\n        self._get_calculations()\n        self._remove_raw()\n        logger.debug(\"Refreshed: %s\", {k: f\"Total: {len(v)}, Min: {np.nanmin(v)}, \"\n                                          f\"Max: {np.nanmax(v)}, \"\n                                          f\"nans: {np.count_nonzero(np.isnan(v))}\"\n                                       for k, v in self.stats.items()})\n        return self\n\n    def set_smooth_amount(self, amount: float) -> None:\n        \"\"\" Set the amount of smoothing to apply to smoothed graph.\n\n        Parameters\n        ----------\n        amount: float\n            The amount of smoothing to apply to smoothed graph\n        \"\"\"\n        update = max(min(amount, 0.999), 0.001)\n        logger.debug(\"Setting smooth amount to: %s (provided value: %s)\", update, amount)\n        self._args[\"smooth_amount\"] = update\n\n    def update_selections(self, selection: str, option: bool) -> None:\n        \"\"\" Update the type of selected data.\n\n        Parameters\n        ----------\n        selection: str\n            The selection to update (as can exist in :attr:`_selections`)\n        option: bool\n            ``True`` if the selection should be included, ``False`` if it should be removed\n        \"\"\"\n        # TODO Somewhat hacky, to ensure values are inserted in the correct order. Fine for\n        # now as this is only called from Live Graph and selections can only be \"raw\" and\n        # smoothed.\n        if option:\n            if selection not in self._selections:\n                if selection == \"raw\":\n                    self._selections.insert(0, selection)\n                else:\n                    self._selections.append(selection)\n        else:\n            if selection in self._selections:\n                self._selections.remove(selection)\n\n    def set_iterations_limit(self, limit: int) -> None:\n        \"\"\" Set the number of iterations to display in the calculations.\n\n        If a value greater than 0 is passed, then the latest iterations up to the given\n        limit will be calculated.\n\n        Parameters\n        ----------\n        limit: int\n            The number of iterations to calculate data for. `0` to calculate for all data\n        \"\"\"\n        limit = max(0, limit)\n        logger.debug(\"Setting iteration limit to: %s\", limit)\n        self._limit = limit\n\n    def _get_raw(self) -> None:\n        \"\"\" Obtain the raw loss values and add them to a new :attr:`stats` dictionary. \"\"\"\n        logger.debug(\"Getting Raw Data\")\n        self.stats.clear()\n        iterations = set()\n\n        if self._display.lower() == \"loss\":\n            loss_dict = _SESSION.get_loss(self._session_id)\n            for loss_name, loss in loss_dict.items():\n                if loss_name not in self._loss_keys:\n                    continue\n                iterations.add(loss.shape[0])\n\n                if self._limit > 0:\n                    loss = loss[-self._limit:]\n\n                if self._args[\"flatten_outliers\"]:\n                    loss = self._flatten_outliers(loss)\n\n                self.stats[f\"raw_{loss_name}\"] = loss\n\n            self._iterations = 0 if not iterations else min(iterations)\n            if self._limit > 1:\n                self._start_iteration = max(0, self._iterations - self._limit)\n                self._iterations = min(self._iterations, self._limit)\n            else:\n                self._start_iteration = 0\n\n            if len(iterations) > 1:\n                # Crop all losses to the same number of items\n                if self._iterations == 0:\n                    self._stats = {lossname: np.array([], dtype=loss.dtype)\n                                   for lossname, loss in self.stats.items()}\n                else:\n                    self._stats = {lossname: loss[:self._iterations]\n                                   for lossname, loss in self.stats.items()}\n\n        else:  # Rate calculation\n            data = self._calc_rate_total() if self._is_totals else self._calc_rate()\n            if self._args[\"flatten_outliers\"]:\n                data = self._flatten_outliers(data)\n            self._iterations = data.shape[0]\n            self.stats[\"raw_rate\"] = data\n\n        logger.debug(\"Got Raw Data: %s\", {k: f\"Total: {len(v)}, Min: {np.nanmin(v)}, \"\n                                             f\"Max: {np.nanmax(v)}, \"\n                                             f\"nans: {np.count_nonzero(np.isnan(v))}\"\n                                          for k, v in self.stats.items()})\n\n    @classmethod\n    def _flatten_outliers(cls, data: np.ndarray) -> np.ndarray:\n        \"\"\" Remove the outliers from a provided list.\n\n        Removes data more than 1 Standard Deviation from the mean.\n\n        Parameters\n        ----------\n        data: :class:`numpy.ndarray`\n            The data to remove the outliers from\n\n        Returns\n        -------\n        :class:`numpy.ndarray`\n            The data with outliers removed\n        \"\"\"\n        logger.debug(\"Flattening outliers: %s\", data.shape)\n        mean = np.mean(np.nan_to_num(data))\n        limit = np.std(np.nan_to_num(data))\n        logger.debug(\"mean: %s, limit: %s\", mean, limit)\n        retdata = np.where(abs(data - mean) < limit, data, mean)\n        logger.debug(\"Flattened outliers\")\n        return retdata\n\n    def _remove_raw(self) -> None:\n        \"\"\" Remove raw values from :attr:`stats` if they are not requested. \"\"\"\n        if \"raw\" in self._selections:\n            return\n        logger.debug(\"Removing Raw Data from output\")\n        for key in list(self._stats.keys()):\n            if key.startswith(\"raw\"):\n                del self._stats[key]\n        logger.debug(\"Removed Raw Data from output\")\n\n    def _calc_rate(self) -> np.ndarray:\n        \"\"\" Calculate rate per iteration.\n\n        Returns\n        -------\n        :class:`numpy.ndarray`\n            The training rate for each iteration of the selected session\n        \"\"\"\n        logger.debug(\"Calculating rate\")\n        batch_size = _SESSION.batch_sizes[self._session_id] * 2\n        retval = batch_size / np.diff(T.cast(np.ndarray,\n                                             _SESSION.get_timestamps(self._session_id)))\n        logger.debug(\"Calculated rate: Item_count: %s\", len(retval))\n        return retval\n\n    @classmethod\n    def _calc_rate_total(cls) -> np.ndarray:\n        \"\"\" Calculate rate per iteration for all sessions.\n\n        Returns\n        -------\n        :class:`numpy.ndarray`\n            The training rate for each iteration in all sessions\n\n        Notes\n        -----\n        For totals, gaps between sessions can be large so the time difference has to be reset for\n        each session's rate calculation.\n        \"\"\"\n        logger.debug(\"Calculating totals rate\")\n        batchsizes = _SESSION.batch_sizes\n        total_timestamps = _SESSION.get_timestamps(None)\n        rate: list[float] = []\n        for sess_id in sorted(total_timestamps.keys()):\n            batchsize = batchsizes[sess_id]\n            timestamps = total_timestamps[sess_id]\n            rate.extend((batchsize * 2) / np.diff(timestamps))\n        retval = np.array(rate)\n        logger.debug(\"Calculated totals rate: Item_count: %s\", len(retval))\n        return retval\n\n    def _get_calculations(self) -> None:\n        \"\"\" Perform the required calculations and populate :attr:`stats`. \"\"\"\n        for selection in self._selections:\n            if selection == \"raw\":\n                continue\n            logger.debug(\"Calculating: %s\", selection)\n            method = getattr(self, f\"_calc_{selection}\")\n            raw_keys = [key for key in self._stats if key.startswith(\"raw_\")]\n            for key in raw_keys:\n                selected_key = f\"{selection}_{key.replace('raw_', '')}\"\n                self._stats[selected_key] = method(self._stats[key])\n        logger.debug(\"Got calculations: %s\", {k: f\"Total: {len(v)}, Min: {np.nanmin(v)}, \"\n                                                 f\"Max: {np.nanmax(v)}, \"\n                                                 f\"nans: {np.count_nonzero(np.isnan(v))}\"\n                                              for k, v in self.stats.items()\n                                              if not k.startswith(\"raw\")})\n\n    def _calc_avg(self, data: np.ndarray) -> np.ndarray:\n        \"\"\" Calculate moving average.\n\n        Parameters\n        ----------\n        data: :class:`numpy.ndarray`\n            The data to calculate the moving average for\n\n        Returns\n        -------\n        :class:`numpy.ndarray`\n            The moving average for the given data\n        \"\"\"\n        logger.debug(\"Calculating Average. Data points: %s\", len(data))\n        window = T.cast(int, self._args[\"avg_samples\"])\n        pad = ceil(window / 2)\n        datapoints = data.shape[0]\n\n        if datapoints <= (self._args[\"avg_samples\"] * 2):\n            logger.info(\"Not enough data to compile rolling average\")\n            return np.array([], dtype=\"float64\")\n\n        avgs = np.cumsum(np.nan_to_num(data), dtype=\"float64\")\n        avgs[window:] = avgs[window:] - avgs[:-window]\n        avgs = avgs[window - 1:] / window\n        avgs = np.pad(avgs, (pad, datapoints - (avgs.shape[0] + pad)), constant_values=(np.nan,))\n        logger.debug(\"Calculated Average: shape: %s\", avgs.shape)\n        return avgs\n\n    def _calc_smoothed(self, data: np.ndarray) -> np.ndarray:\n        \"\"\" Smooth the data.\n\n        Parameters\n        ----------\n        data: :class:`numpy.ndarray`\n            The data to smooth\n\n        Returns\n        -------\n        :class:`numpy.ndarray`\n            The smoothed data\n        \"\"\"\n        retval = _ExponentialMovingAverage(data, self._args[\"smooth_amount\"])()\n        logger.debug(\"Calculated Smoothed data: shape: %s\", retval.shape)\n        return retval\n\n    @classmethod\n    def _calc_trend(cls, data: np.ndarray) -> np.ndarray:\n        \"\"\" Calculate polynomial trend of the given data.\n\n        Parameters\n        ----------\n        data: :class:`numpy.ndarray`\n            The data to calculate the trend for\n\n        Returns\n        -------\n        :class:`numpy.ndarray`\n            The trend for the given data\n        \"\"\"\n        logger.debug(\"Calculating Trend\")\n        points = data.shape[0]\n        if points < 10:\n            dummy = np.empty((points, ), dtype=data.dtype)\n            dummy[:] = np.nan\n            return dummy\n        x_range = range(points)\n        trend = np.poly1d(np.polyfit(x_range, np.nan_to_num(data), 3))(x_range)\n        logger.debug(\"Calculated Trend: shape: %s\", trend.shape)\n        return trend\n\n\nclass _ExponentialMovingAverage():\n    \"\"\" Reshapes data before calculating exponential moving average, then iterates once over the\n    rows to calculate the offset without precision issues.\n\n    Parameters\n    ----------\n    data: :class:`numpy.ndarray`\n        A 1 dimensional numpy array to obtain smoothed data for\n    amount: float\n        in the range (0.0, 1.0) The alpha parameter (smoothing amount) for the moving average.\n\n    Notes\n    -----\n    Adapted from: https://stackoverflow.com/questions/42869495\n    \"\"\"\n    def __init__(self, data: np.ndarray, amount: float) -> None:\n        logger.debug(parse_class_init(locals()))\n        assert data.ndim == 1\n        amount = min(max(amount, 0.001), 0.999)\n\n        self._data = np.nan_to_num(data)\n        self._alpha = 1. - amount\n        self._dtype = \"float32\" if data.dtype == np.float32 else \"float64\"\n        self._row_size = self._get_max_row_size()\n        self._out = np.empty_like(data, dtype=self._dtype)\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    def __call__(self) -> np.ndarray:\n        \"\"\" Perform the exponential moving average calculation.\n\n        Returns\n        -------\n        :class:`numpy.ndarray`\n            The smoothed data\n        \"\"\"\n        if self._data.size <= self._row_size:\n            self._ewma_vectorized(self._data, self._out)  # Normal function can handle this input\n        else:\n            self._ewma_vectorized_safe()  # Use the safe version\n        return self._out\n\n    def _get_max_row_size(self) -> int:\n        \"\"\" Calculate the maximum row size for the running platform for the given dtype.\n\n        Returns\n        -------\n        int\n            The maximum row size possible on the running platform for the given :attr:`_dtype`\n\n        Notes\n        -----\n        Might not be the optimal value for speed, which is hard to predict due to numpy\n        optimizations.\n        \"\"\"\n        # Use :func:`np.finfo(dtype).eps` if you are worried about accuracy and want to be safe.\n        epsilon = np.finfo(self._dtype).tiny  # pylint:disable=no-member\n        # If this produces an OverflowError, make epsilon larger:\n        retval = int(np.log(epsilon) / np.log(1 - self._alpha)) + 1\n        logger.debug(\"row_size: %s\", retval)\n        return retval\n\n    def _ewma_vectorized_safe(self) -> None:\n        \"\"\" Perform the vectorized exponential moving average in a safe way. \"\"\"\n        num_rows = int(self._data.size // self._row_size)  # the number of rows to use\n        leftover = int(self._data.size % self._row_size)  # the amount of data leftover\n        first_offset = self._data[0]\n\n        if leftover > 0:\n            # set temporary results to slice view of out parameter\n            out_main_view = np.reshape(self._out[:-leftover], (num_rows, self._row_size))\n            data_main_view = np.reshape(self._data[:-leftover], (num_rows, self._row_size))\n        else:\n            out_main_view = self._out.reshape(-1, self._row_size)\n            data_main_view = self._data.reshape(-1, self._row_size)\n\n        self._ewma_vectorized_2d(data_main_view, out_main_view)  # get the scaled cumulative sums\n\n        scaling_factors = (1 - self._alpha) ** np.arange(1, self._row_size + 1)\n        last_scaling_factor = scaling_factors[-1]\n\n        # create offset array\n        offsets = np.empty(out_main_view.shape[0], dtype=self._dtype)\n        offsets[0] = first_offset\n        # iteratively calculate offset for each row\n\n        for i in range(1, out_main_view.shape[0]):\n            offsets[i] = offsets[i - 1] * last_scaling_factor + out_main_view[i - 1, -1]\n\n        # add the offsets to the result\n        out_main_view += offsets[:, np.newaxis] * scaling_factors[np.newaxis, :]\n\n        if leftover > 0:\n            # process trailing data in the 2nd slice of the out parameter\n            self._ewma_vectorized(self._data[-leftover:],\n                                  self._out[-leftover:],\n                                  offset=out_main_view[-1, -1])\n\n    def _ewma_vectorized(self,\n                         data: np.ndarray,\n                         out: np.ndarray,\n                         offset: float | None = None) -> None:\n        \"\"\" Calculates the exponential moving average over a vector. Will fail for large inputs.\n\n        The result is processed in place into the array passed to the `out` parameter\n\n        Parameters\n        ----------\n        data: :class:`numpy.ndarray`\n            A 1 dimensional numpy array to obtain smoothed data for\n        out: :class:`numpy.ndarray`\n            A location into which the result is stored. It must have the same shape and dtype as\n            the input data\n        offset: float, optional\n            The offset for the moving average, scalar. Default: the value held in data[0].\n        \"\"\"\n        if data.size < 1:  # empty input, return empty array\n            return\n\n        offset = data[0] if offset is None else offset\n\n        # scaling_factors -> 0 as len(data) gets large. This leads to divide-by-zeros below\n        scaling_factors = np.power(1. - self._alpha, np.arange(data.size + 1, dtype=self._dtype),\n                                   dtype=self._dtype)\n        # create cumulative sum array\n        np.multiply(data, (self._alpha * scaling_factors[-2]) / scaling_factors[:-1],\n                    dtype=self._dtype, out=out)\n        np.cumsum(out, dtype=self._dtype, out=out)\n\n        out /= scaling_factors[-2::-1]  # cumulative sums / scaling\n\n        if offset != 0:\n            noffset = np.array(offset, copy=False).astype(self._dtype, copy=False)\n            out += noffset * scaling_factors[1:]\n\n    def _ewma_vectorized_2d(self, data: np.ndarray, out: np.ndarray) -> None:\n        \"\"\" Calculates the exponential moving average over the last axis.\n\n        The result is processed in place into the array passed to the `out` parameter\n\n        Parameters\n        ----------\n        data: :class:`numpy.ndarray`\n            A 1 or 2 dimensional numpy array to obtain smoothed data for.\n        out: :class:`numpy.ndarray`\n            A location into which the result is stored. It must have the same shape and dtype as\n            the input data\n        \"\"\"\n        if data.size < 1:  # empty input, return empty array\n            return\n\n        # calculate the moving average\n        scaling_factors = np.power(1. - self._alpha, np.arange(data.shape[1] + 1,\n                                                               dtype=self._dtype),\n                                   dtype=self._dtype)\n        # create a scaled cumulative sum array\n        np.multiply(data,\n                    np.multiply(self._alpha * scaling_factors[-2],\n                                np.ones((data.shape[0], 1), dtype=self._dtype),\n                                dtype=self._dtype) / scaling_factors[np.newaxis, :-1],\n                    dtype=self._dtype, out=out)\n        np.cumsum(out, axis=1, dtype=self._dtype, out=out)\n        out /= scaling_factors[np.newaxis, -2::-1]\n", "lib/gui/analysis/__init__.py": "#!/usr/bin/env python3\n\"\"\" Methods for querying and compiling statistical data for the Faceswap GUI Analysis tab. \"\"\"\n\nfrom .stats import Calculations, _SESSION as Session  # noqa\n", "lib/gui/utils/config.py": "#!/usr/bin python3\n\"\"\" Global configuration optiopns for the Faceswap GUI \"\"\"\nfrom __future__ import annotations\nimport logging\nimport os\nimport sys\nimport tkinter as tk\nimport typing as T\n\nfrom dataclasses import dataclass, field\n\nfrom lib.gui._config import Config as UserConfig\nfrom lib.gui.project import Project, Tasks\nfrom lib.gui.theme import Style\nfrom .file_handler import FileHandler\n\nif T.TYPE_CHECKING:\n    from lib.gui.options import CliOptions\n    from lib.gui.custom_widgets import StatusBar\n    from lib.gui.command import CommandNotebook\n    from lib.gui.command import ToolsNotebook\n\nlogger = logging.getLogger(__name__)\n\nPATHCACHE = os.path.join(os.path.realpath(os.path.dirname(sys.argv[0])), \"lib\", \"gui\", \".cache\")\n_CONFIG: Config | None = None\n\n\ndef initialize_config(root: tk.Tk,\n                      cli_opts: CliOptions | None,\n                      statusbar: StatusBar | None) -> Config | None:\n    \"\"\" Initialize the GUI Master :class:`Config` and add to global constant.\n\n    This should only be called once on first GUI startup. Future access to :class:`Config`\n    should only be executed through :func:`get_config`.\n\n    Parameters\n    ----------\n    root: :class:`tkinter.Tk`\n        The root Tkinter object\n    cli_opts: :class:`lib.gui.options.CliOptions` or ``None``\n        The command line options object. Must be provided for main GUI. Must be ``None`` for tools\n    statusbar: :class:`lib.gui.custom_widgets.StatusBar` or ``None``\n        The GUI Status bar. Must be provided for main GUI. Must be ``None`` for tools\n\n    Returns\n    -------\n    :class:`Config` or ``None``\n        ``None`` if the config has already been initialized otherwise the global configuration\n        options\n    \"\"\"\n    global _CONFIG  # pylint:disable=global-statement\n    if _CONFIG is not None:\n        return None\n    logger.debug(\"Initializing config: (root: %s, cli_opts: %s, \"\n                 \"statusbar: %s)\", root, cli_opts, statusbar)\n    _CONFIG = Config(root, cli_opts, statusbar)\n    return _CONFIG\n\n\ndef get_config() -> \"Config\":\n    \"\"\" Get the Master GUI configuration.\n\n    Returns\n    -------\n    :class:`Config`\n        The Master GUI Config\n    \"\"\"\n    assert _CONFIG is not None\n    return _CONFIG\n\n\nclass GlobalVariables():\n    \"\"\" Global tkinter variables accessible from all parts of the GUI. Should only be accessed from\n    :attr:`get_config().tk_vars` \"\"\"\n    def __init__(self) -> None:\n        logger.debug(\"Initializing %s\", self.__class__.__name__)\n        self._display = tk.StringVar()\n        self._running_task = tk.BooleanVar()\n        self._is_training = tk.BooleanVar()\n        self._action_command = tk.StringVar()\n        self._generate_command = tk.StringVar()\n        self._console_clear = tk.BooleanVar()\n        self._refresh_graph = tk.BooleanVar()\n        self._analysis_folder = tk.StringVar()\n\n        self._initialize_variables()\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    @property\n    def display(self) -> tk.StringVar:\n        \"\"\" :class:`tkinter.StringVar`: The current Faceswap command running \"\"\"\n        return self._display\n\n    @property\n    def running_task(self) -> tk.BooleanVar:\n        \"\"\" :class:`tkinter.BooleanVar`: ``True`` if a Faceswap task is running otherwise\n        ``False`` \"\"\"\n        return self._running_task\n\n    @property\n    def is_training(self) -> tk.BooleanVar:\n        \"\"\" :class:`tkinter.BooleanVar`: ``True`` if Faceswap is currently training otherwise\n        ``False`` \"\"\"\n        return self._is_training\n\n    @property\n    def action_command(self) -> tk.StringVar:\n        \"\"\" :class:`tkinter.StringVar`: The command line action to perform \"\"\"\n        return self._action_command\n\n    @property\n    def generate_command(self) -> tk.StringVar:\n        \"\"\" :class:`tkinter.StringVar`: The command line action to generate \"\"\"\n        return self._generate_command\n\n    @property\n    def console_clear(self) -> tk.BooleanVar:\n        \"\"\" :class:`tkinter.BooleanVar`: ``True`` if the console should be cleared otherwise\n        ``False`` \"\"\"\n        return self._console_clear\n\n    @property\n    def refresh_graph(self) -> tk.BooleanVar:\n        \"\"\" :class:`tkinter.BooleanVar`:  ``True`` if the training graph should be refreshed\n        otherwise ``False`` \"\"\"\n        return self._refresh_graph\n\n    @property\n    def analysis_folder(self) -> tk.StringVar:\n        \"\"\" :class:`tkinter.StringVar`: Full path the analysis folder\"\"\"\n        return self._analysis_folder\n\n    def _initialize_variables(self) -> None:\n        \"\"\" Initialize the default variable values\"\"\"\n        self._display.set(\"\")\n        self._running_task.set(False)\n        self._is_training.set(False)\n        self._action_command.set(\"\")\n        self._generate_command.set(\"\")\n        self._console_clear.set(False)\n        self._refresh_graph.set(False)\n        self._analysis_folder.set(\"\")\n\n\n@dataclass\nclass _GuiObjects:\n    \"\"\" Data class for commonly accessed GUI Objects \"\"\"\n    cli_opts: CliOptions | None\n    tk_vars: GlobalVariables\n    project: Project\n    tasks: Tasks\n    status_bar: StatusBar | None\n    default_options: dict[str, dict[str, T.Any]] = field(default_factory=dict)\n    command_notebook: CommandNotebook | None = None\n\n\nclass Config():\n    \"\"\" The centralized configuration class for holding items that should be made available to all\n    parts of the GUI.\n\n    This class should be initialized on GUI startup through :func:`initialize_config`. Any further\n    access to this class should be through :func:`get_config`.\n\n    Parameters\n    ----------\n    root: :class:`tkinter.Tk`\n        The root Tkinter object\n    cli_opts: :class:`lib.gui.options.CliOptions` or ``None``\n        The command line options object. Must be provided for main GUI. Must be ``None`` for tools\n    statusbar: :class:`lib.gui.custom_widgets.StatusBar` or ``None``\n        The GUI Status bar. Must be provided for main GUI. Must be ``None`` for tools\n    \"\"\"\n    def __init__(self,\n                 root: tk.Tk,\n                 cli_opts: CliOptions | None,\n                 statusbar: StatusBar | None) -> None:\n        logger.debug(\"Initializing %s: (root %s, cli_opts: %s, statusbar: %s)\",\n                     self.__class__.__name__, root, cli_opts, statusbar)\n        self._default_font = T.cast(dict,\n                                    tk.font.nametofont(\"TkDefaultFont\").configure())[\"family\"]\n        self._constants = {\"root\": root,\n                           \"scaling_factor\": self._get_scaling(root),\n                           \"default_font\": self._default_font}\n        self._gui_objects = _GuiObjects(\n            cli_opts=cli_opts,\n            tk_vars=GlobalVariables(),\n            project=Project(self, FileHandler),\n            tasks=Tasks(self, FileHandler),\n            status_bar=statusbar)\n\n        self._user_config = UserConfig(None)\n        self._style = Style(self.default_font, root, PATHCACHE)\n        self._user_theme = self._style.user_theme\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    # Constants\n    @property\n    def root(self) -> tk.Tk:\n        \"\"\" :class:`tkinter.Tk`: The root tkinter window. \"\"\"\n        return self._constants[\"root\"]\n\n    @property\n    def scaling_factor(self) -> float:\n        \"\"\" float: The scaling factor for current display. \"\"\"\n        return self._constants[\"scaling_factor\"]\n\n    @property\n    def pathcache(self) -> str:\n        \"\"\" str: The path to the GUI cache folder \"\"\"\n        return PATHCACHE\n\n    # GUI Objects\n    @property\n    def cli_opts(self) -> CliOptions:\n        \"\"\" :class:`lib.gui.options.CliOptions`: The command line options for this GUI Session. \"\"\"\n        # This should only be None when a separate tool (not main GUI) is used, at which point\n        # cli_opts do not exist\n        assert self._gui_objects.cli_opts is not None\n        return self._gui_objects.cli_opts\n\n    @property\n    def tk_vars(self) -> GlobalVariables:\n        \"\"\" dict: The global tkinter variables. \"\"\"\n        return self._gui_objects.tk_vars\n\n    @property\n    def project(self) -> Project:\n        \"\"\" :class:`lib.gui.project.Project`: The project session handler. \"\"\"\n        return self._gui_objects.project\n\n    @property\n    def tasks(self) -> Tasks:\n        \"\"\" :class:`lib.gui.project.Tasks`: The session tasks handler. \"\"\"\n        return self._gui_objects.tasks\n\n    @property\n    def default_options(self) -> dict[str, dict[str, T.Any]]:\n        \"\"\" dict: The default options for all tabs \"\"\"\n        return self._gui_objects.default_options\n\n    @property\n    def statusbar(self) -> StatusBar:\n        \"\"\" :class:`lib.gui.custom_widgets.StatusBar`: The GUI StatusBar\n        :class:`tkinter.ttk.Frame`. \"\"\"\n        # This should only be None when a separate tool (not main GUI) is used, at which point\n        # this statusbar does not exist\n        assert self._gui_objects.status_bar is not None\n        return self._gui_objects.status_bar\n\n    @property\n    def command_notebook(self) -> CommandNotebook | None:\n        \"\"\" :class:`lib.gui.command.CommandNotebook`: The main Faceswap Command Notebook. \"\"\"\n        return self._gui_objects.command_notebook\n\n    # Convenience GUI Objects\n    @property\n    def tools_notebook(self) -> ToolsNotebook:\n        \"\"\" :class:`lib.gui.command.ToolsNotebook`: The Faceswap Tools sub-Notebook. \"\"\"\n        assert self.command_notebook is not None\n        return self.command_notebook.tools_notebook\n\n    @property\n    def modified_vars(self) -> dict[str, tk.BooleanVar]:\n        \"\"\" dict: The command notebook modified tkinter variables. \"\"\"\n        assert self.command_notebook is not None\n        return self.command_notebook.modified_vars\n\n    @property\n    def _command_tabs(self) -> dict[str, int]:\n        \"\"\" dict: Command tab titles with their IDs. \"\"\"\n        assert self.command_notebook is not None\n        return self.command_notebook.tab_names\n\n    @property\n    def _tools_tabs(self) -> dict[str, int]:\n        \"\"\" dict: Tools command tab titles with their IDs. \"\"\"\n        assert self.command_notebook is not None\n        return self.command_notebook.tools_tab_names\n\n    # Config\n    @property\n    def user_config(self) -> UserConfig:\n        \"\"\" dict: The GUI config in dict form. \"\"\"\n        return self._user_config\n\n    @property\n    def user_config_dict(self) -> dict[str, T.Any]:  # TODO Dataclass\n        \"\"\" dict: The GUI config in dict form. \"\"\"\n        return self._user_config.config_dict\n\n    @property\n    def user_theme(self) -> dict[str, T.Any]:  # TODO Dataclass\n        \"\"\" dict: The GUI theme selection options. \"\"\"\n        return self._user_theme\n\n    @property\n    def default_font(self) -> tuple[str, int]:\n        \"\"\" tuple: The selected font as configured in user settings. First item is the font (`str`)\n        second item the font size (`int`). \"\"\"\n        font = self.user_config_dict[\"font\"]\n        font = self._default_font if font == \"default\" else font\n        return (font, self.user_config_dict[\"font_size\"])\n\n    @staticmethod\n    def _get_scaling(root) -> float:\n        \"\"\" Get the display DPI.\n\n        Returns\n        -------\n        float:\n            The scaling factor\n        \"\"\"\n        dpi = root.winfo_fpixels(\"1i\")\n        scaling = dpi / 72.0\n        logger.debug(\"dpi: %s, scaling: %s'\", dpi, scaling)\n        return scaling\n\n    def set_default_options(self) -> None:\n        \"\"\" Set the default options for :mod:`lib.gui.projects`\n\n        The Default GUI options are stored on Faceswap startup.\n\n        Exposed as the :attr:`_default_opts` for a project cannot be set until after the main\n        Command Tabs have been loaded.\n        \"\"\"\n        default = self.cli_opts.get_option_values()\n        logger.debug(default)\n        self._gui_objects.default_options = default\n        self.project.set_default_options()\n\n    def set_command_notebook(self, notebook: CommandNotebook) -> None:\n        \"\"\" Set the command notebook to the :attr:`command_notebook` attribute\n        and enable the modified callback for :attr:`project`.\n\n        Parameters\n        ----------\n        notebook: :class:`lib.gui.command.CommandNotebook`\n            The main command notebook for the Faceswap GUI\n        \"\"\"\n        logger.debug(\"Setting commane notebook: %s\", notebook)\n        self._gui_objects.command_notebook = notebook\n        self.project.set_modified_callback()\n\n    def set_active_tab_by_name(self, name: str) -> None:\n        \"\"\" Sets the :attr:`command_notebook` or :attr:`tools_notebook` to active based on given\n        name.\n\n        Parameters\n        ----------\n        name: str\n            The name of the tab to set active\n        \"\"\"\n        assert self.command_notebook is not None\n        name = name.lower()\n        if name in self._command_tabs:\n            tab_id = self._command_tabs[name]\n            logger.debug(\"Setting active tab to: (name: %s, id: %s)\", name, tab_id)\n            self.command_notebook.select(tab_id)\n        elif name in self._tools_tabs:\n            self.command_notebook.select(self._command_tabs[\"tools\"])\n            tab_id = self._tools_tabs[name]\n            logger.debug(\"Setting active Tools tab to: (name: %s, id: %s)\", name, tab_id)\n            self.tools_notebook.select()\n        else:\n            logger.debug(\"Name couldn't be found. Setting to id 0: %s\", name)\n            self.command_notebook.select(0)\n\n    def set_modified_true(self, command: str) -> None:\n        \"\"\" Set the modified variable to ``True`` for the given command in :attr:`modified_vars`.\n\n        Parameters\n        ----------\n        command: str\n            The command to set the modified state to ``True``\n\n        \"\"\"\n        tkvar = self.modified_vars.get(command, None)\n        if tkvar is None:\n            logger.debug(\"No tkvar for command: '%s'\", command)\n            return\n        tkvar.set(True)\n        logger.debug(\"Set modified var to True for: '%s'\", command)\n\n    def refresh_config(self) -> None:\n        \"\"\" Reload the user config from file. \"\"\"\n        self._user_config = UserConfig(None)\n\n    def set_cursor_busy(self, widget: tk.Widget | None = None) -> None:\n        \"\"\" Set the root or widget cursor to busy.\n\n        Parameters\n        ----------\n        widget: tkinter object, optional\n            The widget to set busy cursor for. If the provided value is ``None`` then sets the\n            cursor busy for the whole of the GUI. Default: ``None``.\n        \"\"\"\n        logger.debug(\"Setting cursor to busy. widget: %s\", widget)\n        component = self.root if widget is None else widget\n        component.config(cursor=\"watch\")  # type: ignore\n        component.update_idletasks()\n\n    def set_cursor_default(self, widget: tk.Widget | None = None) -> None:\n        \"\"\" Set the root or widget cursor to default.\n\n        Parameters\n        ----------\n        widget: tkinter object, optional\n            The widget to set default cursor for. If the provided value is ``None`` then sets the\n            cursor busy for the whole of the GUI. Default: ``None``\n        \"\"\"\n        logger.debug(\"Setting cursor to default. widget: %s\", widget)\n        component = self.root if widget is None else widget\n        component.config(cursor=\"\")  # type: ignore\n        component.update_idletasks()\n\n    def set_root_title(self, text: str | None = None) -> None:\n        \"\"\" Set the main title text for Faceswap.\n\n        The title will always begin with 'Faceswap.py'. Additional text can be appended.\n\n        Parameters\n        ----------\n        text: str, optional\n            Additional text to be appended to the GUI title bar. Default: ``None``\n        \"\"\"\n        title = \"Faceswap.py\"\n        title += f\" - {text}\" if text is not None and text else \"\"\n        self.root.title(title)\n\n    def set_geometry(self, width: int, height: int, fullscreen: bool = False) -> None:\n        \"\"\" Set the geometry for the root tkinter object.\n\n        Parameters\n        ----------\n        width: int\n            The width to set the window to (prior to scaling)\n        height: int\n            The height to set the window to (prior to scaling)\n        fullscreen: bool, optional\n            Whether to set the window to full-screen mode. If ``True`` then :attr:`width` and\n            :attr:`height` are ignored. Default: ``False``\n        \"\"\"\n        self.root.tk.call(\"tk\", \"scaling\", self.scaling_factor)\n        if fullscreen:\n            initial_dimensions = (self.root.winfo_screenwidth(), self.root.winfo_screenheight())\n        else:\n            initial_dimensions = (round(width * self.scaling_factor),\n                                  round(height * self.scaling_factor))\n\n        if fullscreen and sys.platform in (\"win32\", \"darwin\"):\n            self.root.state('zoomed')\n        elif fullscreen:\n            self.root.attributes('-zoomed', True)\n        else:\n            self.root.geometry(f\"{str(initial_dimensions[0])}x{str(initial_dimensions[1])}+80+80\")\n        logger.debug(\"Geometry: %sx%s\", *initial_dimensions)\n", "lib/gui/utils/file_handler.py": "#!/usr/bin/env python3\n\"\"\" File browser utility functions for the Faceswap GUI. \"\"\"\nimport logging\nimport platform\nimport tkinter as tk\nfrom tkinter import filedialog\nimport typing as T\n\nlogger = logging.getLogger(__name__)\n_FILETYPE = T.Literal[\"default\", \"alignments\", \"config_project\", \"config_task\",\n                      \"config_all\", \"csv\", \"image\", \"ini\", \"state\", \"log\", \"video\"]\n_HANDLETYPE = T.Literal[\"open\", \"save\", \"filename\", \"filename_multi\", \"save_filename\",\n                        \"context\", \"dir\"]\n\n\nclass FileHandler():  # pylint:disable=too-few-public-methods\n    \"\"\" Handles all GUI File Dialog actions and tasks.\n\n    Parameters\n    ----------\n    handle_type: ['open', 'save', 'filename', 'filename_multi', 'save_filename', 'context', 'dir']\n        The type of file dialog to return. `open` and `save` will perform the open and save actions\n        and return the file. `filename` returns the filename from an `open` dialog.\n        `filename_multi` allows for multi-selection of files and returns a list of files selected.\n        `save_filename` returns the filename from a `save as` dialog. `context` is a context\n        sensitive parameter that returns a certain dialog based on the current options. `dir` asks\n        for a folder location.\n    file_type: ['default', 'alignments', 'config_project', 'config_task', 'config_all', 'csv', \\\n               'image', 'ini', 'state', 'log', 'video'] or ``None``\n        The type of file that this dialog is for. `default` allows selection of any files. Other\n        options limit the file type selection\n    title: str, optional\n        The title to display on the file dialog. If `None` then the default title will be used.\n        Default: ``None``\n    initial_folder: str, optional\n        The folder to initially open with the file dialog. If `None` then tkinter will decide.\n        Default: ``None``\n    initial_file: str, optional\n        The filename to set with the file dialog. If `None` then tkinter no initial filename is.\n        specified. Default: ``None``\n    command: str, optional\n        Required for context handling file dialog, otherwise unused. Default: ``None``\n    action: str, optional\n        Required for context handling file dialog, otherwise unused. Default: ``None``\n    variable: str, optional\n        Required for context handling file dialog, otherwise unused. The variable to associate\n        with this file dialog. Default: ``None``\n    parent: :class:`tkinter.Frame`, optional\n        The parent that is launching the file dialog. ``None`` sets this to root. Default: ``None``\n\n    Attributes\n    ----------\n    return_file: str or object\n        The return value from the file dialog\n\n    Example\n    -------\n    >>> handler = FileHandler('filename', 'video', title='Select a video...')\n    >>> video_file = handler.return_file\n    >>> print(video_file)\n    '/path/to/selected/video.mp4'\n    \"\"\"\n\n    def __init__(self,\n                 handle_type: _HANDLETYPE,\n                 file_type: _FILETYPE | None,\n                 title: str | None = None,\n                 initial_folder: str | None = None,\n                 initial_file: str | None = None,\n                 command: str | None = None,\n                 action: str | None = None,\n                 variable: str | None = None,\n                 parent: tk.Frame | None = None) -> None:\n        logger.debug(\"Initializing %s: (handle_type: '%s', file_type: '%s', title: '%s', \"\n                     \"initial_folder: '%s', initial_file: '%s', command: '%s', action: '%s', \"\n                     \"variable: %s, parent: %s)\", self.__class__.__name__, handle_type, file_type,\n                     title, initial_folder, initial_file, command, action, variable, parent)\n        self._handletype = handle_type\n        self._dummy_master = self._set_dummy_master()\n        self._defaults = self._set_defaults()\n        self._kwargs = self._set_kwargs(title,\n                                        initial_folder,\n                                        initial_file,\n                                        file_type,\n                                        command,\n                                        action,\n                                        variable,\n                                        parent)\n        self.return_file = getattr(self, f\"_{self._handletype.lower()}\")()\n        self._remove_dummy_master()\n\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    @property\n    def _filetypes(self) -> dict[str, list[tuple[str, str]]]:\n        \"\"\" dict: The accepted extensions for each file type for opening/saving \"\"\"\n        all_files = (\"All files\", \"*.*\")\n        filetypes = {\n            \"default\": [all_files],\n            \"alignments\": [(\"Faceswap Alignments\", \"*.fsa\"), all_files],\n            \"config_project\": [(\"Faceswap Project files\", \"*.fsw\"), all_files],\n            \"config_task\": [(\"Faceswap Task files\", \"*.fst\"), all_files],\n            \"config_all\": [(\"Faceswap Project and Task files\", \"*.fst *.fsw\"), all_files],\n            \"csv\": [(\"Comma separated values\", \"*.csv\"), all_files],\n            \"image\": [(\"Bitmap\", \"*.bmp\"),\n                      (\"JPG\", \"*.jpeg *.jpg\"),\n                      (\"PNG\", \"*.png\"),\n                      (\"TIFF\", \"*.tif *.tiff\"),\n                      all_files],\n            \"ini\": [(\"Faceswap config files\", \"*.ini\"), all_files],\n            \"json\": [(\"JSON file\", \"*.json\"), all_files],\n            \"model\": [(\"Keras model files\", \"*.h5\"), all_files],\n            \"state\": [(\"State files\", \"*.json\"), all_files],\n            \"log\": [(\"Log files\", \"*.log\"), all_files],\n            \"video\": [(\"Audio Video Interleave\", \"*.avi\"),\n                      (\"Flash Video\", \"*.flv\"),\n                      (\"Matroska\", \"*.mkv\"),\n                      (\"MOV\", \"*.mov\"),\n                      (\"MP4\", \"*.mp4\"),\n                      (\"MPEG\", \"*.mpeg *.mpg *.ts *.vob\"),\n                      (\"WebM\", \"*.webm\"),\n                      (\"Windows Media Video\", \"*.wmv\"),\n                      all_files]}\n\n        # Add in multi-select options and upper case extensions for Linux\n        for key in filetypes:\n            if platform.system() == \"Linux\":\n                filetypes[key] = [item\n                                  if item[0] == \"All files\"\n                                  else (item[0], f\"{item[1]} {item[1].upper()}\")\n                                  for item in filetypes[key]]\n            if len(filetypes[key]) > 2:\n                multi = [f\"{key.title()} Files\"]\n                multi.append(\" \".join([ftype[1]\n                                       for ftype in filetypes[key] if ftype[0] != \"All files\"]))\n                filetypes[key].insert(0, T.cast(tuple[str, str], tuple(multi)))\n        return filetypes\n\n    @property\n    def _contexts(self) -> dict[str, dict[str, str | dict[str, str]]]:\n        \"\"\"dict: Mapping of commands, actions and their corresponding file dialog for context\n        handle types. \"\"\"\n        return {\"effmpeg\": {\"input\": {\"extract\": \"filename\",\n                                      \"gen-vid\": \"dir\",\n                                      \"get-fps\": \"filename\",\n                                      \"get-info\": \"filename\",\n                                      \"mux-audio\": \"filename\",\n                                      \"rescale\": \"filename\",\n                                      \"rotate\": \"filename\",\n                                      \"slice\": \"filename\"},\n                            \"output\": {\"extract\": \"dir\",\n                                       \"gen-vid\": \"save_filename\",\n                                       \"get-fps\": \"nothing\",\n                                       \"get-info\": \"nothing\",\n                                       \"mux-audio\": \"save_filename\",\n                                       \"rescale\": \"save_filename\",\n                                       \"rotate\": \"save_filename\",\n                                       \"slice\": \"save_filename\"}}}\n\n    @classmethod\n    def _set_dummy_master(cls) -> tk.Frame | None:\n        \"\"\" Add an option to force black font on Linux file dialogs KDE issue that displays light\n        font on white background).\n\n        This is a pretty hacky solution, but tkinter does not allow direct editing of file dialogs,\n        so we create a dummy frame and add the foreground option there, so that the file dialog can\n        inherit the foreground.\n\n        Returns\n        -------\n        tkinter.Frame or ``None``\n            The dummy master frame for Linux systems, otherwise ``None``\n        \"\"\"\n        if platform.system().lower() == \"linux\":\n            frame = tk.Frame()\n            frame.option_add(\"*foreground\", \"black\")\n            retval: tk.Frame | None = frame\n        else:\n            retval = None\n        return retval\n\n    def _remove_dummy_master(self) -> None:\n        \"\"\" Destroy the dummy master widget on Linux systems. \"\"\"\n        if platform.system().lower() != \"linux\" or self._dummy_master is None:\n            return\n        self._dummy_master.destroy()\n        del self._dummy_master\n        self._dummy_master = None\n\n    def _set_defaults(self) -> dict[str, str | None]:\n        \"\"\" Set the default file type for the file dialog. Generally the first found file type\n        will be used, but this is overridden if it is not appropriate.\n\n        Returns\n        -------\n        dict:\n            The default file extension for each file type\n        \"\"\"\n        defaults: dict[str, str | None] = {\n            key: next(ext for ext in val[0][1].split(\" \")).replace(\"*\", \"\")\n            for key, val in self._filetypes.items()}\n        defaults[\"default\"] = None\n        defaults[\"video\"] = \".mp4\"\n        defaults[\"image\"] = \".png\"\n        logger.debug(defaults)\n        return defaults\n\n    def _set_kwargs(self,\n                    title: str | None,\n                    initial_folder: str | None,\n                    initial_file: str | None,\n                    file_type: _FILETYPE | None,\n                    command: str | None,\n                    action: str | None,\n                    variable: str | None,\n                    parent: tk.Frame | None\n                    ) -> dict[str, None | tk.Frame | str | list[tuple[str, str]]]:\n        \"\"\" Generate the required kwargs for the requested file dialog browser.\n\n        Parameters\n        ----------\n        title: str\n            The title to display on the file dialog. If `None` then the default title will be used.\n        initial_folder: str\n            The folder to initially open with the file dialog. If `None` then tkinter will decide.\n        initial_file: str\n            The filename to set with the file dialog. If `None` then tkinter no initial filename\n            is.\n        file_type: ['default', 'alignments', 'config_project', 'config_task', 'config_all', \\\n                    'csv',  'image', 'ini', 'state', 'log', 'video'] or ``None``\n            The type of file that this dialog is for. `default` allows selection of any files.\n            Other options limit the file type selection\n        command: str\n            Required for context handling file dialog, otherwise unused.\n        action: str\n            Required for context handling file dialog, otherwise unused.\n        variable: str, optional\n            Required for context handling file dialog, otherwise unused. The variable to associate\n            with this file dialog. Default: ``None``\n        parent: :class:`tkinter.Frame`\n            The parent that is launching the file dialog. ``None`` sets this to root\n\n        Returns\n        -------\n        dict:\n            The key word arguments for the file dialog to be launched\n        \"\"\"\n        logger.debug(\"Setting Kwargs: (title: %s, initial_folder: %s, initial_file: '%s', \"\n                     \"file_type: '%s', command: '%s': action: '%s', variable: '%s', parent: %s)\",\n                     title, initial_folder, initial_file, file_type, command, action, variable,\n                     parent)\n\n        kwargs: dict[str, None | tk.Frame | str | list[tuple[str, str]]] = {\n            \"master\": self._dummy_master}\n\n        if self._handletype.lower() == \"context\":\n            assert command is not None and action is not None and variable is not None\n            self._set_context_handletype(command, action, variable)\n\n        if title is not None:\n            kwargs[\"title\"] = title\n\n        if initial_folder is not None:\n            kwargs[\"initialdir\"] = initial_folder\n\n        if initial_file is not None:\n            kwargs[\"initialfile\"] = initial_file\n\n        if parent is not None:\n            kwargs[\"parent\"] = parent\n\n        if self._handletype.lower() in (\n                \"open\", \"save\", \"filename\", \"filename_multi\", \"save_filename\"):\n            assert file_type is not None\n            kwargs[\"filetypes\"] = self._filetypes[file_type]\n            if self._defaults.get(file_type):\n                kwargs['defaultextension'] = self._defaults[file_type]\n        if self._handletype.lower() == \"save\":\n            kwargs[\"mode\"] = \"w\"\n        if self._handletype.lower() == \"open\":\n            kwargs[\"mode\"] = \"r\"\n        logger.debug(\"Set Kwargs: %s\", kwargs)\n        return kwargs\n\n    def _set_context_handletype(self, command: str, action: str, variable: str) -> None:\n        \"\"\" Sets the correct handle type  based on context.\n\n        Parameters\n        ----------\n        command: str\n            The command that is being executed. Used to look up the context actions\n        action: str\n            The action that is being performed. Used to look up the correct file dialog\n        variable: str\n            The variable associated with this file dialog\n        \"\"\"\n        if self._contexts[command].get(variable, None) is not None:\n            handletype = T.cast(dict[str, dict[str, dict[str, str]]],\n                                self._contexts)[command][variable][action]\n        else:\n            handletype = T.cast(dict[str, dict[str, str]],\n                                self._contexts)[command][action]\n        logger.debug(handletype)\n        self._handletype = T.cast(_HANDLETYPE, handletype)\n\n    def _open(self) -> T.IO | None:\n        \"\"\" Open a file. \"\"\"\n        logger.debug(\"Popping Open browser\")\n        return filedialog.askopenfile(**self._kwargs)  # type: ignore\n\n    def _save(self) -> T.IO | None:\n        \"\"\" Save a file. \"\"\"\n        logger.debug(\"Popping Save browser\")\n        return filedialog.asksaveasfile(**self._kwargs)  # type: ignore\n\n    def _dir(self) -> str:\n        \"\"\" Get a directory location. \"\"\"\n        logger.debug(\"Popping Dir browser\")\n        return filedialog.askdirectory(**self._kwargs)  # type: ignore\n\n    def _savedir(self) -> str:\n        \"\"\" Get a save directory location. \"\"\"\n        logger.debug(\"Popping SaveDir browser\")\n        return filedialog.askdirectory(**self._kwargs)  # type: ignore\n\n    def _filename(self) -> str:\n        \"\"\" Get an existing file location. \"\"\"\n        logger.debug(\"Popping Filename browser\")\n        return filedialog.askopenfilename(**self._kwargs)  # type: ignore\n\n    def _filename_multi(self) -> tuple[str, ...]:\n        \"\"\" Get multiple existing file locations. \"\"\"\n        logger.debug(\"Popping Filename browser\")\n        return filedialog.askopenfilenames(**self._kwargs)  # type: ignore\n\n    def _save_filename(self) -> str:\n        \"\"\" Get a save file location. \"\"\"\n        logger.debug(\"Popping Save Filename browser\")\n        return filedialog.asksaveasfilename(**self._kwargs)  # type: ignore\n\n    @staticmethod\n    def _nothing() -> None:  # pylint:disable=useless-return\n        \"\"\" Method that does nothing, used for disabling open/save pop up.  \"\"\"\n        logger.debug(\"Popping Nothing browser\")\n        return\n", "lib/gui/utils/misc.py": "#!/usr/bin/env python3\n\"\"\" Miscellaneous Utility functions for the GUI. Includes LongRunningTask object \"\"\"\nfrom __future__ import annotations\nimport logging\nimport sys\nimport typing as T\n\nfrom threading import Event, Thread\nfrom queue import Queue\n\nfrom .config import get_config\n\nif T.TYPE_CHECKING:\n    from collections.abc import Callable\n    from types import TracebackType\n    from lib.multithreading import _ErrorType\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass LongRunningTask(Thread):\n    \"\"\" Runs long running tasks in a background thread to prevent the GUI from becoming\n    unresponsive.\n\n    This is sub-classed from :class:`Threading.Thread` so check documentation there for base\n    parameters. Additional parameters listed below.\n\n    Parameters\n    ----------\n    widget: tkinter object, optional\n        The widget that this :class:`LongRunningTask` is associated with. Used for setting the busy\n        cursor in the correct location. Default: ``None``.\n    \"\"\"\n    _target: Callable\n    _args: tuple\n    _kwargs: dict[str, T.Any]\n    _name: str\n\n    def __init__(self,\n                 target: Callable | None = None,\n                 name: str | None = None,\n                 args: tuple = (),\n                 kwargs: dict[str, T.Any] | None = None,\n                 *,\n                 daemon: bool = True,\n                 widget=None):\n        logger.debug(\"Initializing %s: (target: %s, name: %s, args: %s, kwargs: %s, \"\n                     \"daemon: %s)\", self.__class__.__name__, target, name, args, kwargs,\n                     daemon)\n        super().__init__(target=target, name=name, args=args, kwargs=kwargs,\n                         daemon=daemon)\n        self.err: _ErrorType = None\n        self._widget = widget\n        self._config = get_config()\n        self._config.set_cursor_busy(widget=self._widget)\n        self._complete = Event()\n        self._queue: Queue = Queue()\n        logger.debug(\"Initialized %s\", self.__class__.__name__,)\n\n    @property\n    def complete(self) -> Event:\n        \"\"\" :class:`threading.Event`:  Event is set if the thread has completed its task,\n        otherwise it is unset.\n        \"\"\"\n        return self._complete\n\n    def run(self) -> None:\n        \"\"\" Commence the given task in a background thread. \"\"\"\n        try:\n            if self._target is not None:\n                retval = self._target(*self._args, **self._kwargs)\n                self._queue.put(retval)\n        except Exception:  # pylint:disable=broad-except\n            self.err = T.cast(tuple[type[BaseException], BaseException, \"TracebackType\"],\n                              sys.exc_info())\n            assert self.err is not None\n            logger.debug(\"Error in thread (%s): %s\", self._name,\n                         self.err[1].with_traceback(self.err[2]))\n        finally:\n            self._complete.set()\n            # Avoid a ref-cycle if the thread is running a function with\n            # an argument that has a member that points to the thread.\n            del self._target, self._args, self._kwargs\n\n    def get_result(self) -> T.Any:\n        \"\"\" Return the result from the given task.\n\n        Returns\n        -------\n        varies:\n            The result of the thread will depend on the given task. If a call is made to\n            :func:`get_result` prior to the thread completing its task then ``None`` will be\n            returned\n        \"\"\"\n        if not self._complete.is_set():\n            logger.warning(\"Aborting attempt to retrieve result from a LongRunningTask that is \"\n                           \"still running\")\n            return None\n        if self.err:\n            logger.debug(\"Error caught in thread\")\n            self._config.set_cursor_default(widget=self._widget)\n            raise self.err[1].with_traceback(self.err[2])\n\n        logger.debug(\"Getting result from thread\")\n        retval = self._queue.get()\n        logger.debug(\"Got result from thread\")\n        self._config.set_cursor_default(widget=self._widget)\n        return retval\n", "lib/gui/utils/image.py": "#!/usr/bin python3\n\"\"\" Utilities for handling images in the Faceswap GUI \"\"\"\nfrom __future__ import annotations\nimport logging\nimport os\nimport typing as T\n\nimport cv2\nimport numpy as np\nfrom PIL import Image, ImageDraw, ImageTk\n\nfrom lib.training.preview_cv import PreviewBuffer\n\nfrom .config import get_config, PATHCACHE\n\nif T.TYPE_CHECKING:\n    from collections.abc import Sequence\n\nlogger = logging.getLogger(__name__)\n_IMAGES: \"Images\" | None = None\n_PREVIEW_TRIGGER: \"PreviewTrigger\" | None = None\nTRAININGPREVIEW = \".gui_training_preview.png\"\n\n\ndef initialize_images() -> None:\n    \"\"\" Initialize the :class:`Images` handler  and add to global constant.\n\n    This should only be called once on first GUI startup. Future access to :class:`Images`\n    handler should only be executed through :func:`get_images`.\n    \"\"\"\n    global _IMAGES  # pylint:disable=global-statement\n    if _IMAGES is not None:\n        return\n    logger.debug(\"Initializing images\")\n    _IMAGES = Images()\n\n\ndef get_images() -> \"Images\":\n    \"\"\" Get the Master GUI Images handler.\n\n    Returns\n    -------\n    :class:`Images`\n        The Master GUI Images handler\n    \"\"\"\n    assert _IMAGES is not None\n    return _IMAGES\n\n\ndef _get_previews(image_path: str) -> list[str]:\n    \"\"\" Get the images stored within the given directory.\n\n    Parameters\n    ----------\n    image_path: str\n        The folder containing images to be scanned\n\n    Returns\n    -------\n    list:\n        The image filenames stored within the given folder\n\n    \"\"\"\n    logger.debug(\"Getting images: '%s'\", image_path)\n    if not os.path.isdir(image_path):\n        logger.debug(\"Folder does not exist\")\n        return []\n    files = [os.path.join(image_path, f)\n             for f in os.listdir(image_path) if f.lower().endswith((\".png\", \".jpg\"))]\n    logger.debug(\"Image files: %s\", files)\n    return files\n\n\nclass PreviewTrain():\n    \"\"\" Handles the loading of the training preview image(s) and adding to the display buffer\n\n    Parameters\n    ----------\n    cache_path: str\n        Full path to the cache folder that contains the preview images\n    \"\"\"\n    def __init__(self, cache_path: str) -> None:\n        logger.debug(\"Initializing %s: (cache_path: '%s')\", self.__class__.__name__, cache_path)\n        self._buffer = PreviewBuffer()\n        self._cache_path = cache_path\n        self._modified: float = 0.0\n        self._error_count: int = 0\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    @property\n    def buffer(self) -> PreviewBuffer:\n        \"\"\" :class:`~lib.training.PreviewBuffer` The preview buffer for the training preview\n        image. \"\"\"\n        return self._buffer\n\n    def load(self) -> bool:\n        \"\"\" Load the latest training preview image(s) from disk and add to :attr:`buffer` \"\"\"\n        logger.trace(\"Loading Training preview images\")  # type:ignore\n        image_files = _get_previews(self._cache_path)\n        filename = next((fname for fname in image_files\n                         if os.path.basename(fname) == TRAININGPREVIEW), \"\")\n        if not filename:\n            logger.trace(\"No preview to display\")  # type:ignore\n            return False\n        try:\n            modified = os.path.getmtime(filename)\n            if modified <= self._modified:\n                logger.trace(\"preview '%s' not updated. Current timestamp: %s, \"  # type:ignore\n                             \"existing timestamp: %s\", filename, modified, self._modified)\n                return False\n\n            logger.debug(\"Loading preview: '%s'\", filename)\n            img = cv2.imread(filename, cv2.IMREAD_UNCHANGED)\n            assert img is not None\n            self._modified = modified\n            self._buffer.add_image(os.path.basename(filename), img)\n            self._error_count = 0\n        except (ValueError, AssertionError):\n            # This is probably an error reading the file whilst it's being saved so ignore it\n            # for now and only pick up if there have been multiple consecutive fails\n            logger.debug(\"Unable to display preview: (image: '%s', attempt: %s)\",\n                         img, self._error_count)\n            if self._error_count < 10:\n                self._error_count += 1\n            else:\n                logger.error(\"Error reading the preview file for '%s'\", filename)\n            return False\n\n        logger.debug(\"Loaded preview: '%s' (%s)\", filename, img.shape)\n        return True\n\n    def reset(self) -> None:\n        \"\"\" Reset the preview buffer when the display page has been disabled.\n\n        Notes\n        -----\n        The buffer requires resetting, otherwise the re-enabled preview window hangs waiting for a\n        training image that has already been marked as processed\n        \"\"\"\n        logger.debug(\"Resetting training preview\")\n        del self._buffer\n        self._buffer = PreviewBuffer()\n        self._modified = 0.0\n        self._error_count = 0\n\n\nclass PreviewExtract():\n    \"\"\" Handles the loading of preview images for extract and convert\n\n    Parameters\n    ----------\n    cache_path: str\n        Full path to the cache folder that contains the preview images\n    \"\"\"\n    def __init__(self, cache_path: str) -> None:\n        logger.debug(\"Initializing %s: (cache_path: '%s')\", self.__class__.__name__, cache_path)\n        self._cache_path = cache_path\n\n        self._batch_mode = False\n        self._output_path = \"\"\n\n        self._modified: float = 0.0\n        self._filenames: list[str] = []\n        self._images: np.ndarray | None = None\n        self._placeholder: np.ndarray | None = None\n\n        self._preview_image: Image.Image | None = None\n        self._preview_image_tk: ImageTk.PhotoImage | None = None\n\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    @property\n    def image(self) -> ImageTk.PhotoImage:\n        \"\"\":class:`PIL.ImageTk.PhotoImage` The preview image for displaying in a tkinter canvas \"\"\"\n        assert self._preview_image_tk is not None\n        return self._preview_image_tk\n\n    def save(self, filename: str) -> None:\n        \"\"\" Save the currently displaying preview image to the given location\n\n        Parameters\n        ----------\n        filename: str\n            The full path to the filename to save the preview image to\n        \"\"\"\n        logger.debug(\"Saving preview to %s\", filename)\n        assert self._preview_image is not None\n        self._preview_image.save(filename)\n\n    def set_faceswap_output_path(self, location: str, batch_mode: bool = False) -> None:\n        \"\"\" Set the path that will contain the output from an Extract or Convert task.\n\n        Required so that the GUI can fetch output images to display for return in\n        :attr:`preview_image`.\n\n        Parameters\n        ----------\n        location: str\n            The output location that has been specified for an Extract or Convert task\n        batch_mode: bool\n            ``True`` if extracting in batch mode otherwise False\n        \"\"\"\n        self._output_path = location\n        self._batch_mode = batch_mode\n\n    def _get_newest_folder(self) -> str:\n        \"\"\" Obtain the most recent folder created in the extraction output folder when processing\n        in batch mode.\n\n        Returns\n        -------\n        str\n            The most recently modified folder within the parent output folder. If no folders have\n            been created, returns the parent output folder\n\n        \"\"\"\n        folders = [] if not os.path.exists(self._output_path) else [\n            os.path.join(self._output_path, folder)\n            for folder in os.listdir(self._output_path)\n            if os.path.isdir(os.path.join(self._output_path, folder))]\n\n        folders.sort(key=os.path.getmtime)\n        retval = folders[-1] if folders else self._output_path\n        logger.debug(\"sorted folders: %s, return value: %s\", folders, retval)\n        return retval\n\n    def _get_newest_filenames(self, image_files: list[str]) -> list[str]:\n        \"\"\" Return image filenames that have been modified since the last check.\n\n        Parameters\n        ----------\n        image_files: list\n            The list of image files to check the modification date for\n\n        Returns\n        -------\n        list:\n            A list of images that have been modified since the last check\n        \"\"\"\n        if not self._modified:\n            retval = image_files\n        else:\n            retval = [fname for fname in image_files\n                      if os.path.getmtime(fname) > self._modified]\n        if not retval:\n            logger.debug(\"No new images in output folder\")\n        else:\n            self._modified = max(os.path.getmtime(img) for img in retval)\n            logger.debug(\"Number new images: %s, Last Modified: %s\",\n                         len(retval), self._modified)\n        return retval\n\n    def _pad_and_border(self, image: Image.Image, size: int) -> np.ndarray:\n        \"\"\" Pad rectangle images to a square and draw borders\n\n        Parameters\n        ----------\n        image: :class:`PIL.Image`\n            The image to process\n        size: int\n            The size of the image as it should be displayed\n\n        Returns\n        -------\n        :class:`numpy.ndarray`:\n            The processed image\n        \"\"\"\n        if image.size[0] != image.size[1]:\n            # Pad to square\n            new_img = Image.new(\"RGB\", (size, size))\n            new_img.paste(image, ((size - image.size[0]) // 2, (size - image.size[1]) // 2))\n            image = new_img\n        draw = ImageDraw.Draw(image)\n        draw.rectangle(((0, 0), (size, size)), outline=\"#E5E5E5\", width=1)\n        retval = np.array(image)\n        logger.trace(\"image shape: %s\", retval.shape)  # type: ignore\n        return retval\n\n    def _process_samples(self,\n                         samples: list[np.ndarray],\n                         filenames: list[str],\n                         num_images: int) -> bool:\n        \"\"\" Process the latest sample images into a displayable image.\n\n        Parameters\n        ----------\n        samples: list\n            The list of extract/convert preview images to display\n        filenames: list\n            The full path to the filenames corresponding to the images\n        num_images: int\n            The number of images that should be displayed\n\n        Returns\n        -------\n        bool\n            ``True`` if samples succesfully compiled otherwise ``False``\n        \"\"\"\n        asamples = np.array(samples)\n        if not np.any(asamples):\n            logger.debug(\"No preview images collected.\")\n            return False\n\n        self._filenames = (self._filenames + filenames)[-num_images:]\n        cache = self._images\n\n        if cache is None:\n            logger.debug(\"Creating new cache\")\n            cache = asamples[-num_images:]\n        else:\n            logger.debug(\"Appending to existing cache\")\n            cache = np.concatenate((cache, asamples))[-num_images:]\n\n        self._images = cache\n        assert self._images is not None\n        logger.debug(\"Cache shape: %s\", self._images.shape)\n        return True\n\n    def _load_images_to_cache(self,\n                              image_files: list[str],\n                              frame_dims: tuple[int, int],\n                              thumbnail_size: int) -> bool:\n        \"\"\" Load preview images to the image cache.\n\n        Load new images and append to cache, filtering the cache to the number of thumbnails that\n        will fit inside the display panel.\n\n        Parameters\n        ----------\n        image_files: list\n            A list of new image files that have been modified since the last check\n        frame_dims: tuple\n            The (width (`int`), height (`int`)) of the display panel that will display the preview\n        thumbnail_size: int\n            The size of each thumbnail that should be created\n\n        Returns\n        -------\n        bool\n            ``True`` if images were successfully loaded to cache otherwise ``False``\n        \"\"\"\n        logger.debug(\"Number image_files: %s, frame_dims: %s, thumbnail_size: %s\",\n                     len(image_files), frame_dims, thumbnail_size)\n        num_images = (frame_dims[0] // thumbnail_size) * (frame_dims[1] // thumbnail_size)\n        logger.debug(\"num_images: %s\", num_images)\n        if num_images == 0:\n            return False\n        samples: list[np.ndarray] = []\n        start_idx = len(image_files) - num_images if len(image_files) > num_images else 0\n        show_files = sorted(image_files, key=os.path.getctime)[start_idx:]\n        dropped_files = []\n        for fname in show_files:\n            try:\n                img = Image.open(fname)\n            except PermissionError as err:\n                logger.debug(\"Permission error opening preview file: '%s'. Original error: %s\",\n                             fname, str(err))\n                dropped_files.append(fname)\n                continue\n            except Exception as err:  # pylint:disable=broad-except\n                # Swallow any issues with opening an image rather than spamming console\n                # Can happen when trying to read partially saved images\n                logger.debug(\"Error opening preview file: '%s'. Original error: %s\",\n                             fname, str(err))\n                dropped_files.append(fname)\n                continue\n\n            width, height = img.size\n            scaling = thumbnail_size / max(width, height)\n            logger.debug(\"image width: %s, height: %s, scaling: %s\", width, height, scaling)\n\n            try:\n                img = img.resize((int(width * scaling), int(height * scaling)))\n            except OSError as err:\n                # Image only gets loaded when we call a method, so may error on partial loads\n                logger.debug(\"OS Error resizing preview image: '%s'. Original error: %s\",\n                             fname, err)\n                dropped_files.append(fname)\n                continue\n\n            samples.append(self._pad_and_border(img, thumbnail_size))\n\n        return self._process_samples(samples,\n                                     [fname for fname in show_files if fname not in dropped_files],\n                                     num_images)\n\n    def _create_placeholder(self, thumbnail_size: int) -> None:\n        \"\"\" Create a placeholder image for when there are fewer thumbnails available\n        than columns to display them.\n\n        Parameters\n        ----------\n        thumbnail_size: int\n            The size of the thumbnail that the placeholder should replicate\n        \"\"\"\n        logger.debug(\"Creating placeholder. thumbnail_size: %s\", thumbnail_size)\n        placeholder = Image.new(\"RGB\", (thumbnail_size, thumbnail_size))\n        draw = ImageDraw.Draw(placeholder)\n        draw.rectangle(((0, 0), (thumbnail_size, thumbnail_size)), outline=\"#E5E5E5\", width=1)\n        placeholder = np.array(placeholder)\n        self._placeholder = placeholder\n        logger.debug(\"Created placeholder. shape: %s\", placeholder.shape)\n\n    def _place_previews(self, frame_dims: tuple[int, int]) -> Image.Image:\n        \"\"\" Format the preview thumbnails stored in the cache into a grid fitting the display\n        panel.\n\n        Parameters\n        ----------\n        frame_dims: tuple\n            The (width (`int`), height (`int`)) of the display panel that will display the preview\n\n        Returns\n        -------\n        :class:`PIL.Image`:\n            The final preview display image\n        \"\"\"\n        if self._images is None:\n            logger.debug(\"No images in cache. Returning None\")\n            return None\n        samples = self._images.copy()\n        num_images, thumbnail_size = samples.shape[:2]\n        if self._placeholder is None:\n            self._create_placeholder(thumbnail_size)\n\n        logger.debug(\"num_images: %s, thumbnail_size: %s\", num_images, thumbnail_size)\n        cols, rows = frame_dims[0] // thumbnail_size, frame_dims[1] // thumbnail_size\n        logger.debug(\"cols: %s, rows: %s\", cols, rows)\n        if cols == 0 or rows == 0:\n            logger.debug(\"Cols or Rows is zero. No items to display\")\n            return None\n\n        remainder = (cols * rows) - num_images\n        if remainder != 0:\n            logger.debug(\"Padding sample display. Remainder: %s\", remainder)\n            assert self._placeholder is not None\n            placeholder = np.concatenate([np.expand_dims(self._placeholder, 0)] * remainder)\n            samples = np.concatenate((samples, placeholder))\n\n        display = np.vstack([np.hstack(T.cast(\"Sequence\", samples[row * cols: (row + 1) * cols]))\n                             for row in range(rows)])\n        logger.debug(\"display shape: %s\", display.shape)\n        return Image.fromarray(display)\n\n    def load_latest_preview(self, thumbnail_size: int, frame_dims: tuple[int, int]) -> bool:\n        \"\"\" Load the latest preview image for extract and convert.\n\n        Retrieves the latest preview images from the faceswap output folder, resizes to thumbnails\n        and lays out for display. Places the images into :attr:`preview_image` for loading into\n        the display panel.\n\n        Parameters\n        ----------\n        thumbnail_size: int\n            The size of each thumbnail that should be created\n        frame_dims: tuple\n            The (width (`int`), height (`int`)) of the display panel that will display the preview\n\n        Returns\n        -------\n        bool\n            ``True`` if a preview was succesfully loaded otherwise ``False``\n        \"\"\"\n        logger.debug(\"Loading preview image: (thumbnail_size: %s, frame_dims: %s)\",\n                     thumbnail_size, frame_dims)\n        image_path = self._get_newest_folder() if self._batch_mode else self._output_path\n        image_files = _get_previews(image_path)\n        gui_preview = os.path.join(self._output_path, \".gui_preview.jpg\")\n        if not image_files or (len(image_files) == 1 and gui_preview not in image_files):\n            logger.debug(\"No preview to display\")\n            return False\n        # Filter to just the gui_preview if it exists in folder output\n        image_files = [gui_preview] if gui_preview in image_files else image_files\n        logger.debug(\"Image Files: %s\", len(image_files))\n\n        image_files = self._get_newest_filenames(image_files)\n        if not image_files:\n            return False\n\n        if not self._load_images_to_cache(image_files, frame_dims, thumbnail_size):\n            logger.debug(\"Failed to load any preview images\")\n            if gui_preview in image_files:\n                # Reset last modified for failed loading of a gui preview image so it is picked\n                # up next time\n                self._modified = 0.0\n            return False\n\n        if image_files == [gui_preview]:\n            # Delete the preview image so that the main scripts know to output another\n            logger.debug(\"Deleting preview image\")\n            os.remove(image_files[0])\n        show_image = self._place_previews(frame_dims)\n        if not show_image:\n            self._preview_image = None\n            self._preview_image_tk = None\n            return False\n\n        logger.debug(\"Displaying preview: %s\", self._filenames)\n        self._preview_image = show_image\n        self._preview_image_tk = ImageTk.PhotoImage(show_image)\n        return True\n\n    def delete_previews(self) -> None:\n        \"\"\" Remove any image preview files \"\"\"\n        for fname in self._filenames:\n            if os.path.basename(fname) == \".gui_preview.jpg\":\n                logger.debug(\"Deleting: '%s'\", fname)\n                try:\n                    os.remove(fname)\n                except FileNotFoundError:\n                    logger.debug(\"File does not exist: %s\", fname)\n\n\nclass Images():\n    \"\"\" The centralized image repository for holding all icons and images required by the GUI.\n\n    This class should be initialized on GUI startup through :func:`initialize_images`. Any further\n    access to this class should be through :func:`get_images`.\n    \"\"\"\n    def __init__(self) -> None:\n        logger.debug(\"Initializing %s\", self.__class__.__name__)\n        self._pathpreview = os.path.join(PATHCACHE, \"preview\")\n        self._pathoutput: str | None = None\n        self._batch_mode = False\n        self._preview_train = PreviewTrain(self._pathpreview)\n        self._preview_extract = PreviewExtract(self._pathpreview)\n        self._icons = self._load_icons()\n        logger.debug(\"Initialized %s\", self.__class__.__name__)\n\n    @property\n    def preview_train(self) -> PreviewTrain:\n        \"\"\" :class:`PreviewTrain` The object handling the training preview images \"\"\"\n        return self._preview_train\n\n    @property\n    def preview_extract(self) -> PreviewExtract:\n        \"\"\" :class:`PreviewTrain` The object handling the training preview images \"\"\"\n        return self._preview_extract\n\n    @property\n    def icons(self) -> dict[str, ImageTk.PhotoImage]:\n        \"\"\" dict: The faceswap icons for all parts of the GUI. The dictionary key is the icon\n        name (`str`) the value is the icon sized and formatted for display\n        (:class:`PIL.ImageTK.PhotoImage`).\n\n        Example\n        -------\n        >>> icons = get_images().icons\n        >>> save = icons[\"save\"]\n        >>> button = ttk.Button(parent, image=save)\n        >>> button.pack()\n        \"\"\"\n        return self._icons\n\n    @staticmethod\n    def _load_icons() -> dict[str, ImageTk.PhotoImage]:\n        \"\"\" Scan the icons cache folder and load the icons into :attr:`icons` for retrieval\n        throughout the GUI.\n\n        Returns\n        -------\n        dict:\n            The icons formatted as described in :attr:`icons`\n\n        \"\"\"\n        size = get_config().user_config_dict.get(\"icon_size\", 16)\n        size = int(round(size * get_config().scaling_factor))\n        icons: dict[str, ImageTk.PhotoImage] = {}\n        pathicons = os.path.join(PATHCACHE, \"icons\")\n        for fname in os.listdir(pathicons):\n            name, ext = os.path.splitext(fname)\n            if ext != \".png\":\n                continue\n            img = Image.open(os.path.join(pathicons, fname))\n            img = ImageTk.PhotoImage(img.resize((size, size), resample=Image.HAMMING))\n            icons[name] = img\n        logger.debug(icons)\n        return icons\n\n    def delete_preview(self) -> None:\n        \"\"\" Delete the preview files in the cache folder and reset the image cache.\n\n        Should be called when terminating tasks, or when Faceswap starts up or shuts down.\n        \"\"\"\n        logger.debug(\"Deleting previews\")\n        for item in os.listdir(self._pathpreview):\n            if item.startswith(os.path.splitext(TRAININGPREVIEW)[0]) and item.endswith((\".jpg\",\n                                                                                        \".png\")):\n                fullitem = os.path.join(self._pathpreview, item)\n                logger.debug(\"Deleting: '%s'\", fullitem)\n                os.remove(fullitem)\n\n        self._preview_extract.delete_previews()\n        del self._preview_train\n        del self._preview_extract\n        self._preview_train = PreviewTrain(self._pathpreview)\n        self._preview_extract = PreviewExtract(self._pathpreview)\n\n\nclass PreviewTrigger():\n    \"\"\" Triggers to indicate to underlying Faceswap process that the preview image should\n    be updated.\n\n    Writes a file to the cache folder that is picked up by the main process.\n    \"\"\"\n    def __init__(self) -> None:\n        logger.debug(\"Initializing: %s\", self.__class__.__name__)\n        self._trigger_files = {\"update\": os.path.join(PATHCACHE, \".preview_trigger\"),\n                               \"mask_toggle\": os.path.join(PATHCACHE, \".preview_mask_toggle\")}\n        logger.debug(\"Initialized: %s (trigger_files: %s)\",\n                     self.__class__.__name__, self._trigger_files)\n\n    def set(self, trigger_type: T.Literal[\"update\", \"mask_toggle\"]):\n        \"\"\" Place the trigger file into the cache folder\n\n        Parameters\n        ----------\n        trigger_type: [\"update\", \"mask_toggle\"]\n            The type of action to trigger. 'update': Full preview update. 'mask_toggle': toggle\n            mask on and off\n         \"\"\"\n        trigger = self._trigger_files[trigger_type]\n        if not os.path.isfile(trigger):\n            with open(trigger, \"w\", encoding=\"utf8\"):\n                pass\n            logger.debug(\"Set preview trigger: %s\", trigger)\n\n    def clear(self, trigger_type: T.Literal[\"update\", \"mask_toggle\"] | None = None) -> None:\n        \"\"\" Remove the trigger file from the cache folder.\n\n        Parameters\n        ----------\n        trigger_type: [\"update\", \"mask_toggle\", ``None``], optional\n            The trigger to clear. 'update': Full preview update. 'mask_toggle': toggle mask on\n            and off. ``None`` - clear all triggers. Default: ``None``\n        \"\"\"\n        if trigger_type is None:\n            triggers = list(self._trigger_files.values())\n        else:\n            triggers = [self._trigger_files[trigger_type]]\n        for trigger in triggers:\n            if os.path.isfile(trigger):\n                os.remove(trigger)\n                logger.debug(\"Removed preview trigger: %s\", trigger)\n\n\ndef preview_trigger() -> PreviewTrigger:\n    \"\"\" Set the global preview trigger if it has not already been set and return.\n\n    Returns\n    -------\n    :class:`PreviewTrigger`\n        The trigger to indicate to the main faceswap process that it should perform a training\n        preview update\n    \"\"\"\n    global _PREVIEW_TRIGGER  # pylint:disable=global-statement\n    if _PREVIEW_TRIGGER is None:\n        _PREVIEW_TRIGGER = PreviewTrigger()\n    return _PREVIEW_TRIGGER\n", "lib/gui/utils/__init__.py": "#!/usr/bin python3\n\"\"\" Utilities for the Faceswap GUI \"\"\"\n\nfrom .config import get_config, initialize_config, PATHCACHE\nfrom .file_handler import FileHandler\nfrom .image import get_images, initialize_images, preview_trigger\nfrom .misc import LongRunningTask\n"}