{"setup.py": "\nNAME = 'PyYAML'\nVERSION = '6.0.1'\nDESCRIPTION = \"YAML parser and emitter for Python\"\nLONG_DESCRIPTION = \"\"\"\\\nYAML is a data serialization format designed for human readability\nand interaction with scripting languages.  PyYAML is a YAML parser\nand emitter for Python.\n\nPyYAML features a complete YAML 1.1 parser, Unicode support, pickle\nsupport, capable extension API, and sensible error messages.  PyYAML\nsupports standard YAML tags and provides Python-specific tags that\nallow to represent an arbitrary Python object.\n\nPyYAML is applicable for a broad range of tasks from complex\nconfiguration files to object serialization and persistence.\"\"\"\nAUTHOR = \"Kirill Simonov\"\nAUTHOR_EMAIL = 'xi@resolvent.net'\nLICENSE = \"MIT\"\nPLATFORMS = \"Any\"\nURL = \"https://pyyaml.org/\"\nDOWNLOAD_URL = \"https://pypi.org/project/PyYAML/\"\nCLASSIFIERS = [\n    \"Development Status :: 5 - Production/Stable\",\n    \"Intended Audience :: Developers\",\n    \"License :: OSI Approved :: MIT License\",\n    \"Operating System :: OS Independent\",\n    \"Programming Language :: Cython\",\n    \"Programming Language :: Python\",\n    \"Programming Language :: Python :: 3\",\n    \"Programming Language :: Python :: 3.6\",\n    \"Programming Language :: Python :: 3.7\",\n    \"Programming Language :: Python :: 3.8\",\n    \"Programming Language :: Python :: 3.9\",\n    \"Programming Language :: Python :: 3.10\",\n    \"Programming Language :: Python :: 3.11\",\n    \"Programming Language :: Python :: 3.12\",\n    \"Programming Language :: Python :: 3.13\",\n    \"Programming Language :: Python :: Implementation :: CPython\",\n    \"Programming Language :: Python :: Implementation :: PyPy\",\n    \"Topic :: Software Development :: Libraries :: Python Modules\",\n    \"Topic :: Text Processing :: Markup\",\n]\nPROJECT_URLS = {\n   'Bug Tracker': 'https://github.com/yaml/pyyaml/issues',\n   'CI': 'https://github.com/yaml/pyyaml/actions',\n   'Documentation': 'https://pyyaml.org/wiki/PyYAMLDocumentation',\n   'Mailing lists': 'http://lists.sourceforge.net/lists/listinfo/yaml-core',\n   'Source Code': 'https://github.com/yaml/pyyaml',\n}\n\nLIBYAML_CHECK = \"\"\"\n#include <yaml.h>\n\nint main(void) {\n    yaml_parser_t parser;\n    yaml_emitter_t emitter;\n\n    yaml_parser_initialize(&parser);\n    yaml_parser_delete(&parser);\n\n    yaml_emitter_initialize(&emitter);\n    yaml_emitter_delete(&emitter);\n\n    return 0;\n}\n\"\"\"\n\n\nimport sys, os, os.path, pathlib, platform, shutil, tempfile, warnings\n\n# for newer setuptools, enable the embedded distutils before importing setuptools/distutils to avoid warnings\nos.environ['SETUPTOOLS_USE_DISTUTILS'] = 'local'\n\nfrom setuptools import setup, Command, Distribution as _Distribution, Extension as _Extension\nfrom setuptools.command.build_ext import build_ext as _build_ext\n# NB: distutils imports must remain below setuptools to ensure we use the embedded version\nfrom distutils import log\nfrom distutils.errors import DistutilsError, CompileError, LinkError, DistutilsPlatformError\n\nwith_cython = False\nif 'sdist' in sys.argv or os.environ.get('PYYAML_FORCE_CYTHON') == '1':\n    # we need cython here\n    with_cython = True\ntry:\n    from Cython.Distutils.extension import Extension as _Extension\n    try:\n        # try old_build_ext from Cython > 3 first, until we can dump it entirely\n        from Cython.Distutils.old_build_ext import old_build_ext as _build_ext\n    except ImportError:\n        # Cython < 3\n        from Cython.Distutils import build_ext as _build_ext\n    with_cython = True\nexcept ImportError:\n    if with_cython:\n        raise\n\ntry:\n    from wheel.bdist_wheel import bdist_wheel\nexcept ImportError:\n    bdist_wheel = None\n\n\ntry:\n    from _pyyaml_pep517 import ActiveConfigSettings\nexcept ImportError:\n    class ActiveConfigSettings:\n        @staticmethod\n        def current():\n            return {}\n\n# on Windows, disable wheel generation warning noise\nwindows_ignore_warnings = [\n\"Unknown distribution option: 'python_requires'\",\n\"Config variable 'Py_DEBUG' is unset\",\n\"Config variable 'WITH_PYMALLOC' is unset\",\n\"Config variable 'Py_UNICODE_SIZE' is unset\",\n\"Cython directive 'language_level' not set\"\n]\n\nif platform.system() == 'Windows':\n    for w in windows_ignore_warnings:\n        warnings.filterwarnings('ignore', w)\n\n\nclass Distribution(_Distribution):\n    def __init__(self, attrs=None):\n        _Distribution.__init__(self, attrs)\n        if not self.ext_modules:\n            return\n        for idx in range(len(self.ext_modules)-1, -1, -1):\n            ext = self.ext_modules[idx]\n            if not isinstance(ext, Extension):\n                continue\n            setattr(self, ext.attr_name, None)\n            self.global_options = [\n                    (ext.option_name, None,\n                        \"include %s (default if %s is available)\"\n                        % (ext.feature_description, ext.feature_name)),\n                    (ext.neg_option_name, None,\n                        \"exclude %s\" % ext.feature_description),\n            ] + self.global_options\n            self.negative_opt = self.negative_opt.copy()\n            self.negative_opt[ext.neg_option_name] = ext.option_name\n\n    def has_ext_modules(self):\n        if not self.ext_modules:\n            return False\n        for ext in self.ext_modules:\n            with_ext = self.ext_status(ext)\n            if with_ext is None or with_ext:\n                return True\n        return False\n\n    def ext_status(self, ext):\n        implementation = platform.python_implementation()\n        if implementation not in ['CPython', 'PyPy']:\n            return False\n        if isinstance(ext, Extension):\n            # the \"build by default\" behavior is implemented by this returning None\n            with_ext = getattr(self, ext.attr_name) or os.environ.get('PYYAML_FORCE_{0}'.format(ext.feature_name.upper()))\n            try:\n                with_ext = int(with_ext)  # attempt coerce envvar to int\n            except TypeError:\n                pass\n            return with_ext\n        else:\n            return True\n\n\nclass Extension(_Extension):\n\n    def __init__(self, name, sources, feature_name, feature_description,\n            feature_check, **kwds):\n        if not with_cython:\n            for filename in sources[:]:\n                base, ext = os.path.splitext(filename)\n                if ext == '.pyx':\n                    sources.remove(filename)\n                    sources.append('%s.c' % base)\n        _Extension.__init__(self, name, sources, **kwds)\n        self.feature_name = feature_name\n        self.feature_description = feature_description\n        self.feature_check = feature_check\n        self.attr_name = 'with_' + feature_name.replace('-', '_')\n        self.option_name = 'with-' + feature_name\n        self.neg_option_name = 'without-' + feature_name\n\n\nclass build_ext(_build_ext):\n    def finalize_options(self):\n        super().finalize_options()\n        pep517_config = ActiveConfigSettings.current()\n\n        build_config = pep517_config.get('pyyaml_build_config')\n\n        if build_config:\n            import json\n            build_config = json.loads(build_config)\n            print(f\"`pyyaml_build_config`: {build_config}\")\n        else:\n            build_config = {}\n            print(\"No `pyyaml_build_config` setting found.\")\n\n        for key, value in build_config.items():\n            existing_value = getattr(self, key, ...)\n            if existing_value is ...:\n                print(f\"ignoring unknown config key {key!r}\")\n                continue\n\n            if existing_value:\n                print(f\"combining {key!r} {existing_value!r} and {value!r}\")\n                value = existing_value + value  # FIXME: handle type diff\n\n            setattr(self, key, value)\n\n    def run(self):\n        optional = True\n        disabled = True\n        for ext in self.extensions:\n            with_ext = self.distribution.ext_status(ext)\n            if with_ext is None:\n                disabled = False\n            elif with_ext:\n                optional = False\n                disabled = False\n                break\n        if disabled:\n            return\n        try:\n            _build_ext.run(self)\n        except DistutilsPlatformError:\n            exc = sys.exc_info()[1]\n            if optional:\n                log.warn(str(exc))\n                log.warn(\"skipping build_ext\")\n            else:\n                raise\n\n    def get_source_files(self):\n        self.check_extensions_list(self.extensions)\n        filenames = []\n        for ext in self.extensions:\n            if with_cython:\n                self.cython_sources(ext.sources, ext)\n            for filename in ext.sources:\n                filenames.append(filename)\n                base = os.path.splitext(filename)[0]\n                for ext in ['c', 'h', 'pyx', 'pxd']:\n                    filename = '%s.%s' % (base, ext)\n                    if filename not in filenames and os.path.isfile(filename):\n                        filenames.append(filename)\n        return filenames\n\n    def get_outputs(self):\n        self.check_extensions_list(self.extensions)\n        outputs = []\n        for ext in self.extensions:\n            fullname = self.get_ext_fullname(ext.name)\n            filename = os.path.join(self.build_lib,\n                                    self.get_ext_filename(fullname))\n            if os.path.isfile(filename):\n                outputs.append(filename)\n        return outputs\n\n    def build_extensions(self):\n        self.check_extensions_list(self.extensions)\n        for ext in self.extensions:\n            with_ext = self.distribution.ext_status(ext)\n            if with_ext is not None and not with_ext:\n                continue\n            if with_cython:\n                print(f\"BUILDING CYTHON EXT; {self.include_dirs=} {self.library_dirs=} {self.define=}\")\n                ext.sources = self.cython_sources(ext.sources, ext)\n            try:\n                self.build_extension(ext)\n            except (CompileError, LinkError):\n                if with_ext is not None:\n                    raise\n                log.warn(\"Error compiling module, falling back to pure Python\")\n\n\nclass test(Command):\n\n    user_options = []\n\n    def initialize_options(self):\n        pass\n\n    def finalize_options(self):\n        pass\n\n    def run(self):\n        warnings.warn('Running tests via `setup.py test` is deprecated and will be removed in a future release. Use `pytest` instead to ensure that the complete test suite is run.', DeprecationWarning)\n        build_cmd = self.get_finalized_command('build')\n        build_cmd.run()\n\n        # running the tests this way can pollute the post-MANIFEST build sources\n        # (see https://github.com/yaml/pyyaml/issues/527#issuecomment-921058344)\n        # until we remove the test command, run tests from an ephemeral copy of the intermediate build sources\n        tempdir = tempfile.TemporaryDirectory(prefix='test_pyyaml')\n\n        try:\n            # have to create a subdir since we don't get dir_exists_ok on copytree until 3.8\n            temp_test_path = pathlib.Path(tempdir.name) / 'pyyaml'\n            shutil.copytree(build_cmd.build_lib, temp_test_path)\n            sys.path.insert(0, str(temp_test_path))\n            sys.path.insert(0, 'tests/legacy_tests')\n\n            import test_all\n            if not test_all.main([]):\n                raise DistutilsError(\"Tests failed\")\n        finally:\n            try:\n                # this can fail under Windows; best-effort cleanup\n                tempdir.cleanup()\n            except Exception:\n                pass\n\n\ncmdclass = {\n    'build_ext': build_ext,\n    'test': test,\n}\nif bdist_wheel:\n    cmdclass['bdist_wheel'] = bdist_wheel\n\n\nif __name__ == '__main__':\n\n    setup(\n        name=NAME,\n        version=VERSION,\n        description=DESCRIPTION,\n        long_description=LONG_DESCRIPTION,\n        author=AUTHOR,\n        author_email=AUTHOR_EMAIL,\n        license=LICENSE,\n        platforms=PLATFORMS,\n        url=URL,\n        download_url=DOWNLOAD_URL,\n        classifiers=CLASSIFIERS,\n        project_urls=PROJECT_URLS,\n\n        package_dir={'': 'lib'},\n        packages=['yaml', '_yaml'],\n        ext_modules=[\n            Extension('yaml._yaml', ['yaml/_yaml.pyx'],\n                'libyaml', \"LibYAML bindings\", LIBYAML_CHECK,\n                libraries=['yaml']),\n        ],\n\n        distclass=Distribution,\n        cmdclass=cmdclass,\n        python_requires='>=3.6',\n    )\n", ".github/actions/dynamatrix/matrix_yaml_to_json.py": "from __future__ import annotations\n\nimport argparse\nimport json\nimport os\nimport pathlib\nimport sys\nimport typing as t\nimport yaml\n\nfrom collections.abc import MutableMapping, Sequence\n\nskipped_entries = []\n\ndef _filter_omit_entries(value):\n    if isinstance(value, MutableMapping):\n        if (omit_value := value.pop('omit', ...)) is not ...:\n            if omit_value is True or str(omit_value).lower().strip() == 'true':\n                print(f'omitting {value} from matrix')\n                skipped_entries.append(value)\n                return ...\n\n        return {k: v for k, v in ((k, _filter_omit_entries(v)) for k, v in value.items()) if v is not ...}\n\n    if isinstance(value, str):\n        return value\n\n    if isinstance(value, Sequence):\n        return [v for v in (_filter_omit_entries(v) for v in value) if v is not ...]\n\n    return value\n\ndef main():\n    p = argparse.ArgumentParser(description='GHA YAML matrix filter')\n    required_grp = p.add_mutually_exclusive_group(required=True)\n    required_grp.add_argument('--from-stdin', action='store_true', help='read input YAML from stdin')\n    required_grp.add_argument('--from-file', type=pathlib.Path, help='read input YAML from file path')\n\n    args = p.parse_args()\n\n    path: pathlib.Path | None\n\n    matrix_yaml: str\n\n    if path := args.from_file:\n        matrix_yaml = path.read_text()\n    elif args.from_stdin:\n        matrix_yaml = sys.stdin.read()\n    else:\n        raise Exception('no source provided for matrix yaml')\n\n    raw_matrix = yaml.safe_load(matrix_yaml)\n    filtered_matrix = _filter_omit_entries(raw_matrix)\n\n    output_matrix_json = json.dumps(filtered_matrix)\n    output_skipped_matrix_json = json.dumps(skipped_entries)\n\n    print(f'filtered matrix: {output_matrix_json}')\n    print(f'skipped entries: {output_skipped_matrix_json}')\n\n    if (gh_output := os.environ.get('GITHUB_OUTPUT')):\n        print('setting step output var matrix_json; skipped_matrix_json...')\n        with pathlib.Path(gh_output).open('a') as env_fd:\n            env_fd.write(f'matrix_json<<__MATRIX_EOF\\n{output_matrix_json}\\n__MATRIX_EOF\\n')\n            env_fd.write(f'skipped_matrix_json<<__MATRIX_EOF\\n{output_skipped_matrix_json}\\n__MATRIX_EOF\\n')\n    else:\n        print(\"GITHUB_OUTPUT not set; skipping variable output\")\n\n\nif __name__ == '__main__':\n    main()\n", "packaging/_pyyaml_pep517.py": "import inspect\n\n\ndef _bridge_build_meta():\n    import functools\n    import sys\n\n    from setuptools import build_meta\n\n    self_module = sys.modules[__name__]\n\n    for attr_name in build_meta.__all__:\n        attr_value = getattr(build_meta, attr_name)\n        if callable(attr_value):\n            setattr(self_module, attr_name, functools.partial(_expose_config_settings, attr_value))\n\n\nclass ActiveConfigSettings:\n    _current = {}\n\n    def __init__(self, config_settings):\n        self._config = config_settings\n\n    def __enter__(self):\n        type(self)._current = self._config\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        type(self)._current = {}\n\n    @classmethod\n    def current(cls):\n        return cls._current\n\n\ndef _expose_config_settings(real_method, *args, **kwargs):\n    from contextlib import nullcontext\n    import inspect\n\n    sig = inspect.signature(real_method)\n    boundargs = sig.bind(*args, **kwargs)\n\n    config = boundargs.arguments.get('config_settings')\n\n    ctx = ActiveConfigSettings(config) if config else nullcontext()\n\n    with ctx:\n        return real_method(*args, **kwargs)\n\n\n_bridge_build_meta()\n\n", "examples/pygments-lexer/yaml.py": "\n\"\"\"\nyaml.py\n\nLexer for YAML, a human-friendly data serialization language\n(http://yaml.org/).\n\nWritten by Kirill Simonov <xi@resolvent.net>.\n\nLicense: Whatever suitable for inclusion into the Pygments package.\n\"\"\"\n\nfrom pygments.lexer import  \\\n        ExtendedRegexLexer, LexerContext, include, bygroups\nfrom pygments.token import  \\\n        Text, Comment, Punctuation, Name, Literal\n\n__all__ = ['YAMLLexer']\n\n\nclass YAMLLexerContext(LexerContext):\n    \"\"\"Indentation context for the YAML lexer.\"\"\"\n\n    def __init__(self, *args, **kwds):\n        super(YAMLLexerContext, self).__init__(*args, **kwds)\n        self.indent_stack = []\n        self.indent = -1\n        self.next_indent = 0\n        self.block_scalar_indent = None\n\n\ndef something(TokenClass):\n    \"\"\"Do not produce empty tokens.\"\"\"\n    def callback(lexer, match, context):\n        text = match.group()\n        if not text:\n            return\n        yield match.start(), TokenClass, text\n        context.pos = match.end()\n    return callback\n\ndef reset_indent(TokenClass):\n    \"\"\"Reset the indentation levels.\"\"\"\n    def callback(lexer, match, context):\n        text = match.group()\n        context.indent_stack = []\n        context.indent = -1\n        context.next_indent = 0\n        context.block_scalar_indent = None\n        yield match.start(), TokenClass, text\n        context.pos = match.end()\n    return callback\n\ndef save_indent(TokenClass, start=False):\n    \"\"\"Save a possible indentation level.\"\"\"\n    def callback(lexer, match, context):\n        text = match.group()\n        extra = ''\n        if start:\n            context.next_indent = len(text)\n            if context.next_indent < context.indent:\n                while context.next_indent < context.indent:\n                    context.indent = context.indent_stack.pop()\n                if context.next_indent > context.indent:\n                    extra = text[context.indent:]\n                    text = text[:context.indent]\n        else:\n            context.next_indent += len(text)\n        if text:\n            yield match.start(), TokenClass, text\n        if extra:\n            yield match.start()+len(text), TokenClass.Error, extra\n        context.pos = match.end()\n    return callback\n\ndef set_indent(TokenClass, implicit=False):\n    \"\"\"Set the previously saved indentation level.\"\"\"\n    def callback(lexer, match, context):\n        text = match.group()\n        if context.indent < context.next_indent:\n            context.indent_stack.append(context.indent)\n            context.indent = context.next_indent\n        if not implicit:\n            context.next_indent += len(text)\n        yield match.start(), TokenClass, text\n        context.pos = match.end()\n    return callback\n\ndef set_block_scalar_indent(TokenClass):\n    \"\"\"Set an explicit indentation level for a block scalar.\"\"\"\n    def callback(lexer, match, context):\n        text = match.group()\n        context.block_scalar_indent = None\n        if not text:\n            return\n        increment = match.group(1)\n        if increment:\n            current_indent = max(context.indent, 0)\n            increment = int(increment)\n            context.block_scalar_indent = current_indent + increment\n        if text:\n            yield match.start(), TokenClass, text\n            context.pos = match.end()\n    return callback\n\ndef parse_block_scalar_empty_line(IndentTokenClass, ContentTokenClass):\n    \"\"\"Process an empty line in a block scalar.\"\"\"\n    def callback(lexer, match, context):\n        text = match.group()\n        if (context.block_scalar_indent is None or\n                len(text) <= context.block_scalar_indent):\n            if text:\n                yield match.start(), IndentTokenClass, text\n        else:\n            indentation = text[:context.block_scalar_indent]\n            content = text[context.block_scalar_indent:]\n            yield match.start(), IndentTokenClass, indentation\n            yield (match.start()+context.block_scalar_indent,\n                    ContentTokenClass, content)\n        context.pos = match.end()\n    return callback\n\ndef parse_block_scalar_indent(TokenClass):\n    \"\"\"Process indentation spaces in a block scalar.\"\"\"\n    def callback(lexer, match, context):\n        text = match.group()\n        if context.block_scalar_indent is None:\n            if len(text) <= max(context.indent, 0):\n                context.stack.pop()\n                context.stack.pop()\n                return\n            context.block_scalar_indent = len(text)\n        else:\n            if len(text) < context.block_scalar_indent:\n                context.stack.pop()\n                context.stack.pop()\n                return\n        if text:\n            yield match.start(), TokenClass, text\n            context.pos = match.end()\n    return callback\n\ndef parse_plain_scalar_indent(TokenClass):\n    \"\"\"Process indentation spaces in a plain scalar.\"\"\"\n    def callback(lexer, match, context):\n        text = match.group()\n        if len(text) <= context.indent:\n            context.stack.pop()\n            context.stack.pop()\n            return\n        if text:\n            yield match.start(), TokenClass, text\n            context.pos = match.end()\n    return callback\n\n\nclass YAMLLexer(ExtendedRegexLexer):\n    \"\"\"Lexer for the YAML language.\"\"\"\n\n    name = 'YAML'\n    aliases = ['yaml']\n    filenames = ['*.yaml', '*.yml']\n    mimetypes = ['text/x-yaml']\n\n    tokens = {\n\n        # the root rules\n        'root': [\n            # ignored whitespaces\n            (r'[ ]+(?=#|$)', Text.Blank),\n            # line breaks\n            (r'\\n+', Text.Break),\n            # a comment\n            (r'#[^\\n]*', Comment.Single),\n            # the '%YAML' directive\n            (r'^%YAML(?=[ ]|$)', reset_indent(Name.Directive),\n                'yaml-directive'),\n            # the %TAG directive\n            (r'^%TAG(?=[ ]|$)', reset_indent(Name.Directive),\n                'tag-directive'),\n            # document start and document end indicators\n            (r'^(?:---|\\.\\.\\.)(?=[ ]|$)',\n                reset_indent(Punctuation.Document), 'block-line'),\n            # indentation spaces\n            (r'[ ]*(?![ \\t\\n\\r\\f\\v]|$)',\n                save_indent(Text.Indent, start=True),\n                ('block-line', 'indentation')),\n        ],\n\n        # trailing whitespaces after directives or a block scalar indicator\n        'ignored-line': [\n            # ignored whitespaces\n            (r'[ ]+(?=#|$)', Text.Blank),\n            # a comment\n            (r'#[^\\n]*', Comment.Single),\n            # line break\n            (r'\\n', Text.Break, '#pop:2'),\n        ],\n\n        # the %YAML directive\n        'yaml-directive': [\n            # the version number\n            (r'([ ]+)([0-9]+\\.[0-9]+)',\n                bygroups(Text.Blank, Literal.Version), 'ignored-line'),\n        ],\n\n        # the %YAG directive\n        'tag-directive': [\n            # a tag handle and the corresponding prefix\n            (r'([ ]+)(!|![0-9A-Za-z_-]*!)'\n                r'([ ]+)(!|!?[0-9A-Za-z;/?:@&=+$,_.!~*\\'()\\[\\]%-]+)',\n                bygroups(Text.Blank, Name.Type, Text.Blank, Name.Type),\n                'ignored-line'),\n        ],\n\n        # block scalar indicators and indentation spaces\n        'indentation': [\n            # trailing whitespaces are ignored\n            (r'[ ]*$', something(Text.Blank), '#pop:2'),\n            # whitespaces preceding block collection indicators\n            (r'[ ]+(?=[?:-](?:[ ]|$))', save_indent(Text.Indent)),\n            # block collection indicators\n            (r'[?:-](?=[ ]|$)', set_indent(Punctuation.Indicator)),\n            # the beginning a block line\n            (r'[ ]*', save_indent(Text.Indent), '#pop'),\n        ],\n\n        # an indented line in the block context\n        'block-line': [\n            # the line end\n            (r'[ ]*(?=#|$)', something(Text.Blank), '#pop'),\n            # whitespaces separating tokens\n            (r'[ ]+', Text.Blank),\n            # tags, anchors and aliases,\n            include('descriptors'),\n            # block collections and scalars\n            include('block-nodes'),\n            # flow collections and quoted scalars\n            include('flow-nodes'),\n            # a plain scalar\n            (r'(?=[^ \\t\\n\\r\\f\\v?:,\\[\\]{}#&*!|>\\'\"%@`-]|[?:-][^ \\t\\n\\r\\f\\v])',\n                something(Literal.Scalar.Plain),\n                'plain-scalar-in-block-context'),\n        ],\n\n        # tags, anchors, aliases\n        'descriptors' : [\n            # a full-form tag\n            (r'!<[0-9A-Za-z;/?:@&=+$,_.!~*\\'()\\[\\]%-]+>', Name.Type),\n            # a tag in the form '!', '!suffix' or '!handle!suffix'\n            (r'!(?:[0-9A-Za-z_-]+)?'\n                r'(?:![0-9A-Za-z;/?:@&=+$,_.!~*\\'()\\[\\]%-]+)?', Name.Type),\n            # an anchor\n            (r'&[0-9A-Za-z_-]+', Name.Anchor),\n            # an alias\n            (r'\\*[0-9A-Za-z_-]+', Name.Alias),\n        ],\n\n        # block collections and scalars\n        'block-nodes': [\n            # implicit key\n            (r':(?=[ ]|$)', set_indent(Punctuation.Indicator, implicit=True)),\n            # literal and folded scalars\n            (r'[|>]', Punctuation.Indicator,\n                ('block-scalar-content', 'block-scalar-header')),\n        ],\n\n        # flow collections and quoted scalars\n        'flow-nodes': [\n            # a flow sequence\n            (r'\\[', Punctuation.Indicator, 'flow-sequence'),\n            # a flow mapping\n            (r'\\{', Punctuation.Indicator, 'flow-mapping'),\n            # a single-quoted scalar\n            (r'\\'', Literal.Scalar.Flow.Quote, 'single-quoted-scalar'),\n            # a double-quoted scalar\n            (r'\\\"', Literal.Scalar.Flow.Quote, 'double-quoted-scalar'),\n        ],\n\n        # the content of a flow collection\n        'flow-collection': [\n            # whitespaces\n            (r'[ ]+', Text.Blank),\n            # line breaks\n            (r'\\n+', Text.Break),\n            # a comment\n            (r'#[^\\n]*', Comment.Single),\n            # simple indicators\n            (r'[?:,]', Punctuation.Indicator),\n            # tags, anchors and aliases\n            include('descriptors'),\n            # nested collections and quoted scalars\n            include('flow-nodes'),\n            # a plain scalar\n            (r'(?=[^ \\t\\n\\r\\f\\v?:,\\[\\]{}#&*!|>\\'\"%@`])',\n                something(Literal.Scalar.Plain),\n                'plain-scalar-in-flow-context'),\n        ],\n\n        # a flow sequence indicated by '[' and ']'\n        'flow-sequence': [\n            # include flow collection rules\n            include('flow-collection'),\n            # the closing indicator\n            (r'\\]', Punctuation.Indicator, '#pop'),\n        ],\n\n        # a flow mapping indicated by '{' and '}'\n        'flow-mapping': [\n            # include flow collection rules\n            include('flow-collection'),\n            # the closing indicator\n            (r'\\}', Punctuation.Indicator, '#pop'),\n        ],\n\n        # block scalar lines\n        'block-scalar-content': [\n            # line break\n            (r'\\n', Text.Break),\n            # empty line\n            (r'^[ ]+$',\n                parse_block_scalar_empty_line(Text.Indent,\n                    Literal.Scalar.Block)),\n            # indentation spaces (we may leave the state here)\n            (r'^[ ]*', parse_block_scalar_indent(Text.Indent)),\n            # line content\n            (r'[^\\n\\r\\f\\v]+', Literal.Scalar.Block),\n        ],\n\n        # the content of a literal or folded scalar\n        'block-scalar-header': [\n            # indentation indicator followed by chomping flag\n            (r'([1-9])?[+-]?(?=[ ]|$)',\n                set_block_scalar_indent(Punctuation.Indicator),\n                'ignored-line'),\n            # chomping flag followed by indentation indicator\n            (r'[+-]?([1-9])?(?=[ ]|$)',\n                set_block_scalar_indent(Punctuation.Indicator),\n                'ignored-line'),\n        ],\n\n        # ignored and regular whitespaces in quoted scalars\n        'quoted-scalar-whitespaces': [\n            # leading and trailing whitespaces are ignored\n            (r'^[ ]+|[ ]+$', Text.Blank),\n            # line breaks are ignored\n            (r'\\n+', Text.Break),\n            # other whitespaces are a part of the value\n            (r'[ ]+', Literal.Scalar.Flow),\n        ],\n\n        # single-quoted scalars\n        'single-quoted-scalar': [\n            # include whitespace and line break rules\n            include('quoted-scalar-whitespaces'),\n            # escaping of the quote character\n            (r'\\'\\'', Literal.Scalar.Flow.Escape),\n            # regular non-whitespace characters\n            (r'[^ \\t\\n\\r\\f\\v\\']+', Literal.Scalar.Flow),\n            # the closing quote\n            (r'\\'', Literal.Scalar.Flow.Quote, '#pop'),\n        ],\n\n        # double-quoted scalars\n        'double-quoted-scalar': [\n            # include whitespace and line break rules\n            include('quoted-scalar-whitespaces'),\n            # escaping of special characters\n            (r'\\\\[0abt\\tn\\nvfre \"\\\\N_LP]', Literal.Scalar.Flow.Escape),\n            # escape codes\n            (r'\\\\(?:x[0-9A-Fa-f]{2}|u[0-9A-Fa-f]{4}|U[0-9A-Fa-f]{8})',\n                Literal.Scalar.Flow.Escape),\n            # regular non-whitespace characters\n            (r'[^ \\t\\n\\r\\f\\v\\\"\\\\]+', Literal.Scalar.Flow),\n            # the closing quote\n            (r'\"', Literal.Scalar.Flow.Quote, '#pop'),\n        ],\n\n        # the beginning of a new line while scanning a plain scalar\n        'plain-scalar-in-block-context-new-line': [\n            # empty lines\n            (r'^[ ]+$', Text.Blank),\n            # line breaks\n            (r'\\n+', Text.Break),\n            # document start and document end indicators\n            (r'^(?=---|\\.\\.\\.)', something(Punctuation.Document), '#pop:3'),\n            # indentation spaces (we may leave the block line state here)\n            (r'^[ ]*', parse_plain_scalar_indent(Text.Indent), '#pop'),\n        ],\n\n        # a plain scalar in the block context\n        'plain-scalar-in-block-context': [\n            # the scalar ends with the ':' indicator\n            (r'[ ]*(?=:[ ]|:$)', something(Text.Blank), '#pop'),\n            # the scalar ends with whitespaces followed by a comment\n            (r'[ ]+(?=#)', Text.Blank, '#pop'),\n            # trailing whitespaces are ignored\n            (r'[ ]+$', Text.Blank),\n            # line breaks are ignored\n            (r'\\n+', Text.Break, 'plain-scalar-in-block-context-new-line'),\n            # other whitespaces are a part of the value\n            (r'[ ]+', Literal.Scalar.Plain),\n            # regular non-whitespace characters\n            (r'(?::(?![ \\t\\n\\r\\f\\v])|[^ \\t\\n\\r\\f\\v:])+',\n                Literal.Scalar.Plain),\n        ],\n\n        # a plain scalar is the flow context\n        'plain-scalar-in-flow-context': [\n            # the scalar ends with an indicator character\n            (r'[ ]*(?=[,:?\\[\\]{}])', something(Text.Blank), '#pop'),\n            # the scalar ends with a comment\n            (r'[ ]+(?=#)', Text.Blank, '#pop'),\n            # leading and trailing whitespaces are ignored\n            (r'^[ ]+|[ ]+$', Text.Blank),\n            # line breaks are ignored\n            (r'\\n+', Text.Break),\n            # other whitespaces are a part of the value\n            (r'[ ]+', Literal.Scalar.Plain),\n            # regular non-whitespace characters\n            (r'[^ \\t\\n\\r\\f\\v,:?\\[\\]{}]+', Literal.Scalar.Plain),\n        ],\n\n    }\n\n    def get_tokens_unprocessed(self, text=None, context=None):\n        if context is None:\n            context = YAMLLexerContext(text, 0)\n        return super(YAMLLexer, self).get_tokens_unprocessed(text, context)\n\n\n", "examples/yaml-highlight/yaml_hl.py": "#!/usr/bin/python\n\nimport yaml, codecs, sys, os.path, optparse\n\nclass Style:\n\n    def __init__(self, header=None, footer=None,\n            tokens=None, events=None, replaces=None):\n        self.header = header\n        self.footer = footer\n        self.replaces = replaces\n        self.substitutions = {}\n        for domain, Class in [(tokens, 'Token'), (events, 'Event')]:\n            if not domain:\n                continue\n            for key in domain:\n                name = ''.join([part.capitalize() for part in key.split('-')])\n                cls = getattr(yaml, '%s%s' % (name, Class))\n                value = domain[key]\n                if not value:\n                    continue\n                start = value.get('start')\n                end = value.get('end')\n                if start:\n                    self.substitutions[cls, -1] = start\n                if end:\n                    self.substitutions[cls, +1] = end\n\n    def __setstate__(self, state):\n        self.__init__(**state)\n\nyaml.add_path_resolver(u'tag:yaml.org,2002:python/object:__main__.Style',\n        [None], dict)\nyaml.add_path_resolver(u'tag:yaml.org,2002:pairs',\n        [None, u'replaces'], list)\n\nclass YAMLHighlight:\n\n    def __init__(self, options):\n        config = yaml.full_load(file(options.config, 'rb').read())\n        self.style = config[options.style]\n        if options.input:\n            self.input = file(options.input, 'rb')\n        else:\n            self.input = sys.stdin\n        if options.output:\n            self.output = file(options.output, 'wb')\n        else:\n            self.output = sys.stdout\n\n    def highlight(self):\n        input = self.input.read()\n        if input.startswith(codecs.BOM_UTF16_LE):\n            input = unicode(input, 'utf-16-le')\n        elif input.startswith(codecs.BOM_UTF16_BE):\n            input = unicode(input, 'utf-16-be')\n        else:\n            input = unicode(input, 'utf-8')\n        substitutions = self.style.substitutions\n        tokens = yaml.scan(input)\n        events = yaml.parse(input)\n        markers = []\n        number = 0\n        for token in tokens:\n            number += 1\n            if token.start_mark.index != token.end_mark.index:\n                cls = token.__class__\n                if (cls, -1) in substitutions:\n                    markers.append([token.start_mark.index, +2, number, substitutions[cls, -1]])\n                if (cls, +1) in substitutions:\n                    markers.append([token.end_mark.index, -2, number, substitutions[cls, +1]])\n        number = 0\n        for event in events:\n            number += 1\n            cls = event.__class__\n            if (cls, -1) in substitutions:\n                markers.append([event.start_mark.index, +1, number, substitutions[cls, -1]])\n            if (cls, +1) in substitutions:\n                markers.append([event.end_mark.index, -1, number, substitutions[cls, +1]])\n        markers.sort()\n        markers.reverse()\n        chunks = []\n        position = len(input)\n        for index, weight1, weight2, substitution in markers:\n            if index < position:\n                chunk = input[index:position]\n                for substring, replacement in self.style.replaces:\n                    chunk = chunk.replace(substring, replacement)\n                chunks.append(chunk)\n                position = index\n            chunks.append(substitution)\n        chunks.reverse()\n        result = u''.join(chunks)\n        if self.style.header:\n            self.output.write(self.style.header)\n        self.output.write(result.encode('utf-8'))\n        if self.style.footer:\n            self.output.write(self.style.footer)\n\nif __name__ == '__main__':\n    parser = optparse.OptionParser()\n    parser.add_option('-s', '--style', dest='style', default='ascii',\n            help=\"specify the highlighting style\", metavar='STYLE')\n    parser.add_option('-c', '--config', dest='config',\n            default=os.path.join(os.path.dirname(sys.argv[0]), 'yaml_hl.cfg'),\n            help=\"set an alternative configuration file\", metavar='CONFIG')\n    parser.add_option('-i', '--input', dest='input', default=None,\n            help=\"set the input file (default: stdin)\", metavar='FILE')\n    parser.add_option('-o', '--output', dest='output', default=None,\n            help=\"set the output file (default: stdout)\", metavar='FILE')\n    (options, args) = parser.parse_args()\n    hl = YAMLHighlight(options)\n    hl.highlight()\n\n", "lib/_yaml/__init__.py": "# This is a stub package designed to roughly emulate the _yaml\n# extension module, which previously existed as a standalone module\n# and has been moved into the `yaml` package namespace.\n# It does not perfectly mimic its old counterpart, but should get\n# close enough for anyone who's relying on it even when they shouldn't.\nimport yaml\n\n# in some circumstances, the yaml module we imoprted may be from a different version, so we need\n# to tread carefully when poking at it here (it may not have the attributes we expect)\nif not getattr(yaml, '__with_libyaml__', False):\n    from sys import version_info\n\n    exc = ModuleNotFoundError if version_info >= (3, 6) else ImportError\n    raise exc(\"No module named '_yaml'\")\nelse:\n    from yaml._yaml import *\n    import warnings\n    warnings.warn(\n        'The _yaml extension module is now located at yaml._yaml'\n        ' and its location is subject to change.  To use the'\n        ' LibYAML-based parser and emitter, import from `yaml`:'\n        ' `from yaml import CLoader as Loader, CDumper as Dumper`.',\n        DeprecationWarning\n    )\n    del warnings\n    # Don't `del yaml` here because yaml is actually an existing\n    # namespace member of _yaml.\n\n__name__ = '_yaml'\n# If the module is top-level (i.e. not a part of any specific package)\n# then the attribute should be set to ''.\n# https://docs.python.org/3.8/library/types.html\n__package__ = ''\n", "lib/yaml/tokens.py": "\nclass Token(object):\n    def __init__(self, start_mark, end_mark):\n        self.start_mark = start_mark\n        self.end_mark = end_mark\n    def __repr__(self):\n        attributes = [key for key in self.__dict__\n                if not key.endswith('_mark')]\n        attributes.sort()\n        arguments = ', '.join(['%s=%r' % (key, getattr(self, key))\n                for key in attributes])\n        return '%s(%s)' % (self.__class__.__name__, arguments)\n\n#class BOMToken(Token):\n#    id = '<byte order mark>'\n\nclass DirectiveToken(Token):\n    id = '<directive>'\n    def __init__(self, name, value, start_mark, end_mark):\n        self.name = name\n        self.value = value\n        self.start_mark = start_mark\n        self.end_mark = end_mark\n\nclass DocumentStartToken(Token):\n    id = '<document start>'\n\nclass DocumentEndToken(Token):\n    id = '<document end>'\n\nclass StreamStartToken(Token):\n    id = '<stream start>'\n    def __init__(self, start_mark=None, end_mark=None,\n            encoding=None):\n        self.start_mark = start_mark\n        self.end_mark = end_mark\n        self.encoding = encoding\n\nclass StreamEndToken(Token):\n    id = '<stream end>'\n\nclass BlockSequenceStartToken(Token):\n    id = '<block sequence start>'\n\nclass BlockMappingStartToken(Token):\n    id = '<block mapping start>'\n\nclass BlockEndToken(Token):\n    id = '<block end>'\n\nclass FlowSequenceStartToken(Token):\n    id = '['\n\nclass FlowMappingStartToken(Token):\n    id = '{'\n\nclass FlowSequenceEndToken(Token):\n    id = ']'\n\nclass FlowMappingEndToken(Token):\n    id = '}'\n\nclass KeyToken(Token):\n    id = '?'\n\nclass ValueToken(Token):\n    id = ':'\n\nclass BlockEntryToken(Token):\n    id = '-'\n\nclass FlowEntryToken(Token):\n    id = ','\n\nclass AliasToken(Token):\n    id = '<alias>'\n    def __init__(self, value, start_mark, end_mark):\n        self.value = value\n        self.start_mark = start_mark\n        self.end_mark = end_mark\n\nclass AnchorToken(Token):\n    id = '<anchor>'\n    def __init__(self, value, start_mark, end_mark):\n        self.value = value\n        self.start_mark = start_mark\n        self.end_mark = end_mark\n\nclass TagToken(Token):\n    id = '<tag>'\n    def __init__(self, value, start_mark, end_mark):\n        self.value = value\n        self.start_mark = start_mark\n        self.end_mark = end_mark\n\nclass ScalarToken(Token):\n    id = '<scalar>'\n    def __init__(self, value, plain, start_mark, end_mark, style=None):\n        self.value = value\n        self.plain = plain\n        self.start_mark = start_mark\n        self.end_mark = end_mark\n        self.style = style\n\n", "lib/yaml/constructor.py": "\n__all__ = [\n    'BaseConstructor',\n    'SafeConstructor',\n    'FullConstructor',\n    'UnsafeConstructor',\n    'Constructor',\n    'ConstructorError'\n]\n\nfrom .error import *\nfrom .nodes import *\n\nimport collections.abc, datetime, base64, binascii, re, sys, types\n\nclass ConstructorError(MarkedYAMLError):\n    pass\n\nclass BaseConstructor:\n\n    yaml_constructors = {}\n    yaml_multi_constructors = {}\n\n    def __init__(self):\n        self.constructed_objects = {}\n        self.recursive_objects = {}\n        self.state_generators = []\n        self.deep_construct = False\n\n    def check_data(self):\n        # If there are more documents available?\n        return self.check_node()\n\n    def check_state_key(self, key):\n        \"\"\"Block special attributes/methods from being set in a newly created\n        object, to prevent user-controlled methods from being called during\n        deserialization\"\"\"\n        if self.get_state_keys_blacklist_regexp().match(key):\n            raise ConstructorError(None, None,\n                \"blacklisted key '%s' in instance state found\" % (key,), None)\n\n    def get_data(self):\n        # Construct and return the next document.\n        if self.check_node():\n            return self.construct_document(self.get_node())\n\n    def get_single_data(self):\n        # Ensure that the stream contains a single document and construct it.\n        node = self.get_single_node()\n        if node is not None:\n            return self.construct_document(node)\n        return None\n\n    def construct_document(self, node):\n        data = self.construct_object(node)\n        while self.state_generators:\n            state_generators = self.state_generators\n            self.state_generators = []\n            for generator in state_generators:\n                for dummy in generator:\n                    pass\n        self.constructed_objects = {}\n        self.recursive_objects = {}\n        self.deep_construct = False\n        return data\n\n    def construct_object(self, node, deep=False):\n        if node in self.constructed_objects:\n            return self.constructed_objects[node]\n        if deep:\n            old_deep = self.deep_construct\n            self.deep_construct = True\n        if node in self.recursive_objects:\n            raise ConstructorError(None, None,\n                    \"found unconstructable recursive node\", node.start_mark)\n        self.recursive_objects[node] = None\n        constructor = None\n        tag_suffix = None\n        if node.tag in self.yaml_constructors:\n            constructor = self.yaml_constructors[node.tag]\n        else:\n            for tag_prefix in self.yaml_multi_constructors:\n                if tag_prefix is not None and node.tag.startswith(tag_prefix):\n                    tag_suffix = node.tag[len(tag_prefix):]\n                    constructor = self.yaml_multi_constructors[tag_prefix]\n                    break\n            else:\n                if None in self.yaml_multi_constructors:\n                    tag_suffix = node.tag\n                    constructor = self.yaml_multi_constructors[None]\n                elif None in self.yaml_constructors:\n                    constructor = self.yaml_constructors[None]\n                elif isinstance(node, ScalarNode):\n                    constructor = self.__class__.construct_scalar\n                elif isinstance(node, SequenceNode):\n                    constructor = self.__class__.construct_sequence\n                elif isinstance(node, MappingNode):\n                    constructor = self.__class__.construct_mapping\n        if tag_suffix is None:\n            data = constructor(self, node)\n        else:\n            data = constructor(self, tag_suffix, node)\n        if isinstance(data, types.GeneratorType):\n            generator = data\n            data = next(generator)\n            if self.deep_construct:\n                for dummy in generator:\n                    pass\n            else:\n                self.state_generators.append(generator)\n        self.constructed_objects[node] = data\n        del self.recursive_objects[node]\n        if deep:\n            self.deep_construct = old_deep\n        return data\n\n    def construct_scalar(self, node):\n        if not isinstance(node, ScalarNode):\n            raise ConstructorError(None, None,\n                    \"expected a scalar node, but found %s\" % node.id,\n                    node.start_mark)\n        return node.value\n\n    def construct_sequence(self, node, deep=False):\n        if not isinstance(node, SequenceNode):\n            raise ConstructorError(None, None,\n                    \"expected a sequence node, but found %s\" % node.id,\n                    node.start_mark)\n        return [self.construct_object(child, deep=deep)\n                for child in node.value]\n\n    def construct_mapping(self, node, deep=False):\n        if not isinstance(node, MappingNode):\n            raise ConstructorError(None, None,\n                    \"expected a mapping node, but found %s\" % node.id,\n                    node.start_mark)\n        mapping = {}\n        for key_node, value_node in node.value:\n            key = self.construct_object(key_node, deep=deep)\n            if not isinstance(key, collections.abc.Hashable):\n                raise ConstructorError(\"while constructing a mapping\", node.start_mark,\n                        \"found unhashable key\", key_node.start_mark)\n            value = self.construct_object(value_node, deep=deep)\n            mapping[key] = value\n        return mapping\n\n    def construct_pairs(self, node, deep=False):\n        if not isinstance(node, MappingNode):\n            raise ConstructorError(None, None,\n                    \"expected a mapping node, but found %s\" % node.id,\n                    node.start_mark)\n        pairs = []\n        for key_node, value_node in node.value:\n            key = self.construct_object(key_node, deep=deep)\n            value = self.construct_object(value_node, deep=deep)\n            pairs.append((key, value))\n        return pairs\n\n    @classmethod\n    def add_constructor(cls, tag, constructor):\n        if not 'yaml_constructors' in cls.__dict__:\n            cls.yaml_constructors = cls.yaml_constructors.copy()\n        cls.yaml_constructors[tag] = constructor\n\n    @classmethod\n    def add_multi_constructor(cls, tag_prefix, multi_constructor):\n        if not 'yaml_multi_constructors' in cls.__dict__:\n            cls.yaml_multi_constructors = cls.yaml_multi_constructors.copy()\n        cls.yaml_multi_constructors[tag_prefix] = multi_constructor\n\nclass SafeConstructor(BaseConstructor):\n\n    def construct_scalar(self, node):\n        if isinstance(node, MappingNode):\n            for key_node, value_node in node.value:\n                if key_node.tag == 'tag:yaml.org,2002:value':\n                    return self.construct_scalar(value_node)\n        return super().construct_scalar(node)\n\n    def flatten_mapping(self, node):\n        merge = []\n        index = 0\n        while index < len(node.value):\n            key_node, value_node = node.value[index]\n            if key_node.tag == 'tag:yaml.org,2002:merge':\n                del node.value[index]\n                if isinstance(value_node, MappingNode):\n                    self.flatten_mapping(value_node)\n                    merge.extend(value_node.value)\n                elif isinstance(value_node, SequenceNode):\n                    submerge = []\n                    for subnode in value_node.value:\n                        if not isinstance(subnode, MappingNode):\n                            raise ConstructorError(\"while constructing a mapping\",\n                                    node.start_mark,\n                                    \"expected a mapping for merging, but found %s\"\n                                    % subnode.id, subnode.start_mark)\n                        self.flatten_mapping(subnode)\n                        submerge.append(subnode.value)\n                    submerge.reverse()\n                    for value in submerge:\n                        merge.extend(value)\n                else:\n                    raise ConstructorError(\"while constructing a mapping\", node.start_mark,\n                            \"expected a mapping or list of mappings for merging, but found %s\"\n                            % value_node.id, value_node.start_mark)\n            elif key_node.tag == 'tag:yaml.org,2002:value':\n                key_node.tag = 'tag:yaml.org,2002:str'\n                index += 1\n            else:\n                index += 1\n        if merge:\n            node.value = merge + node.value\n\n    def construct_mapping(self, node, deep=False):\n        if isinstance(node, MappingNode):\n            self.flatten_mapping(node)\n        return super().construct_mapping(node, deep=deep)\n\n    def construct_yaml_null(self, node):\n        self.construct_scalar(node)\n        return None\n\n    bool_values = {\n        'yes':      True,\n        'no':       False,\n        'true':     True,\n        'false':    False,\n        'on':       True,\n        'off':      False,\n    }\n\n    def construct_yaml_bool(self, node):\n        value = self.construct_scalar(node)\n        return self.bool_values[value.lower()]\n\n    def construct_yaml_int(self, node):\n        value = self.construct_scalar(node)\n        value = value.replace('_', '')\n        sign = +1\n        if value[0] == '-':\n            sign = -1\n        if value[0] in '+-':\n            value = value[1:]\n        if value == '0':\n            return 0\n        elif value.startswith('0b'):\n            return sign*int(value[2:], 2)\n        elif value.startswith('0x'):\n            return sign*int(value[2:], 16)\n        elif value[0] == '0':\n            return sign*int(value, 8)\n        elif ':' in value:\n            digits = [int(part) for part in value.split(':')]\n            digits.reverse()\n            base = 1\n            value = 0\n            for digit in digits:\n                value += digit*base\n                base *= 60\n            return sign*value\n        else:\n            return sign*int(value)\n\n    inf_value = 1e300\n    while inf_value != inf_value*inf_value:\n        inf_value *= inf_value\n    nan_value = -inf_value/inf_value   # Trying to make a quiet NaN (like C99).\n\n    def construct_yaml_float(self, node):\n        value = self.construct_scalar(node)\n        value = value.replace('_', '').lower()\n        sign = +1\n        if value[0] == '-':\n            sign = -1\n        if value[0] in '+-':\n            value = value[1:]\n        if value == '.inf':\n            return sign*self.inf_value\n        elif value == '.nan':\n            return self.nan_value\n        elif ':' in value:\n            digits = [float(part) for part in value.split(':')]\n            digits.reverse()\n            base = 1\n            value = 0.0\n            for digit in digits:\n                value += digit*base\n                base *= 60\n            return sign*value\n        else:\n            return sign*float(value)\n\n    def construct_yaml_binary(self, node):\n        try:\n            value = self.construct_scalar(node).encode('ascii')\n        except UnicodeEncodeError as exc:\n            raise ConstructorError(None, None,\n                    \"failed to convert base64 data into ascii: %s\" % exc,\n                    node.start_mark)\n        try:\n            if hasattr(base64, 'decodebytes'):\n                return base64.decodebytes(value)\n            else:\n                return base64.decodestring(value)\n        except binascii.Error as exc:\n            raise ConstructorError(None, None,\n                    \"failed to decode base64 data: %s\" % exc, node.start_mark)\n\n    timestamp_regexp = re.compile(\n            r'''^(?P<year>[0-9][0-9][0-9][0-9])\n                -(?P<month>[0-9][0-9]?)\n                -(?P<day>[0-9][0-9]?)\n                (?:(?:[Tt]|[ \\t]+)\n                (?P<hour>[0-9][0-9]?)\n                :(?P<minute>[0-9][0-9])\n                :(?P<second>[0-9][0-9])\n                (?:\\.(?P<fraction>[0-9]*))?\n                (?:[ \\t]*(?P<tz>Z|(?P<tz_sign>[-+])(?P<tz_hour>[0-9][0-9]?)\n                (?::(?P<tz_minute>[0-9][0-9]))?))?)?$''', re.X)\n\n    def construct_yaml_timestamp(self, node):\n        value = self.construct_scalar(node)\n        match = self.timestamp_regexp.match(node.value)\n        values = match.groupdict()\n        year = int(values['year'])\n        month = int(values['month'])\n        day = int(values['day'])\n        if not values['hour']:\n            return datetime.date(year, month, day)\n        hour = int(values['hour'])\n        minute = int(values['minute'])\n        second = int(values['second'])\n        fraction = 0\n        tzinfo = None\n        if values['fraction']:\n            fraction = values['fraction'][:6]\n            while len(fraction) < 6:\n                fraction += '0'\n            fraction = int(fraction)\n        if values['tz_sign']:\n            tz_hour = int(values['tz_hour'])\n            tz_minute = int(values['tz_minute'] or 0)\n            delta = datetime.timedelta(hours=tz_hour, minutes=tz_minute)\n            if values['tz_sign'] == '-':\n                delta = -delta\n            tzinfo = datetime.timezone(delta)\n        elif values['tz']:\n            tzinfo = datetime.timezone.utc\n        return datetime.datetime(year, month, day, hour, minute, second, fraction,\n                                 tzinfo=tzinfo)\n\n    def construct_yaml_omap(self, node):\n        # Note: we do not check for duplicate keys, because it's too\n        # CPU-expensive.\n        omap = []\n        yield omap\n        if not isinstance(node, SequenceNode):\n            raise ConstructorError(\"while constructing an ordered map\", node.start_mark,\n                    \"expected a sequence, but found %s\" % node.id, node.start_mark)\n        for subnode in node.value:\n            if not isinstance(subnode, MappingNode):\n                raise ConstructorError(\"while constructing an ordered map\", node.start_mark,\n                        \"expected a mapping of length 1, but found %s\" % subnode.id,\n                        subnode.start_mark)\n            if len(subnode.value) != 1:\n                raise ConstructorError(\"while constructing an ordered map\", node.start_mark,\n                        \"expected a single mapping item, but found %d items\" % len(subnode.value),\n                        subnode.start_mark)\n            key_node, value_node = subnode.value[0]\n            key = self.construct_object(key_node)\n            value = self.construct_object(value_node)\n            omap.append((key, value))\n\n    def construct_yaml_pairs(self, node):\n        # Note: the same code as `construct_yaml_omap`.\n        pairs = []\n        yield pairs\n        if not isinstance(node, SequenceNode):\n            raise ConstructorError(\"while constructing pairs\", node.start_mark,\n                    \"expected a sequence, but found %s\" % node.id, node.start_mark)\n        for subnode in node.value:\n            if not isinstance(subnode, MappingNode):\n                raise ConstructorError(\"while constructing pairs\", node.start_mark,\n                        \"expected a mapping of length 1, but found %s\" % subnode.id,\n                        subnode.start_mark)\n            if len(subnode.value) != 1:\n                raise ConstructorError(\"while constructing pairs\", node.start_mark,\n                        \"expected a single mapping item, but found %d items\" % len(subnode.value),\n                        subnode.start_mark)\n            key_node, value_node = subnode.value[0]\n            key = self.construct_object(key_node)\n            value = self.construct_object(value_node)\n            pairs.append((key, value))\n\n    def construct_yaml_set(self, node):\n        data = set()\n        yield data\n        value = self.construct_mapping(node)\n        data.update(value)\n\n    def construct_yaml_str(self, node):\n        return self.construct_scalar(node)\n\n    def construct_yaml_seq(self, node):\n        data = []\n        yield data\n        data.extend(self.construct_sequence(node))\n\n    def construct_yaml_map(self, node):\n        data = {}\n        yield data\n        value = self.construct_mapping(node)\n        data.update(value)\n\n    def construct_yaml_object(self, node, cls):\n        data = cls.__new__(cls)\n        yield data\n        if hasattr(data, '__setstate__'):\n            state = self.construct_mapping(node, deep=True)\n            data.__setstate__(state)\n        else:\n            state = self.construct_mapping(node)\n            data.__dict__.update(state)\n\n    def construct_undefined(self, node):\n        raise ConstructorError(None, None,\n                \"could not determine a constructor for the tag %r\" % node.tag,\n                node.start_mark)\n\nSafeConstructor.add_constructor(\n        'tag:yaml.org,2002:null',\n        SafeConstructor.construct_yaml_null)\n\nSafeConstructor.add_constructor(\n        'tag:yaml.org,2002:bool',\n        SafeConstructor.construct_yaml_bool)\n\nSafeConstructor.add_constructor(\n        'tag:yaml.org,2002:int',\n        SafeConstructor.construct_yaml_int)\n\nSafeConstructor.add_constructor(\n        'tag:yaml.org,2002:float',\n        SafeConstructor.construct_yaml_float)\n\nSafeConstructor.add_constructor(\n        'tag:yaml.org,2002:binary',\n        SafeConstructor.construct_yaml_binary)\n\nSafeConstructor.add_constructor(\n        'tag:yaml.org,2002:timestamp',\n        SafeConstructor.construct_yaml_timestamp)\n\nSafeConstructor.add_constructor(\n        'tag:yaml.org,2002:omap',\n        SafeConstructor.construct_yaml_omap)\n\nSafeConstructor.add_constructor(\n        'tag:yaml.org,2002:pairs',\n        SafeConstructor.construct_yaml_pairs)\n\nSafeConstructor.add_constructor(\n        'tag:yaml.org,2002:set',\n        SafeConstructor.construct_yaml_set)\n\nSafeConstructor.add_constructor(\n        'tag:yaml.org,2002:str',\n        SafeConstructor.construct_yaml_str)\n\nSafeConstructor.add_constructor(\n        'tag:yaml.org,2002:seq',\n        SafeConstructor.construct_yaml_seq)\n\nSafeConstructor.add_constructor(\n        'tag:yaml.org,2002:map',\n        SafeConstructor.construct_yaml_map)\n\nSafeConstructor.add_constructor(None,\n        SafeConstructor.construct_undefined)\n\nclass FullConstructor(SafeConstructor):\n    # 'extend' is blacklisted because it is used by\n    # construct_python_object_apply to add `listitems` to a newly generate\n    # python instance\n    def get_state_keys_blacklist(self):\n        return ['^extend$', '^__.*__$']\n\n    def get_state_keys_blacklist_regexp(self):\n        if not hasattr(self, 'state_keys_blacklist_regexp'):\n            self.state_keys_blacklist_regexp = re.compile('(' + '|'.join(self.get_state_keys_blacklist()) + ')')\n        return self.state_keys_blacklist_regexp\n\n    def construct_python_str(self, node):\n        return self.construct_scalar(node)\n\n    def construct_python_unicode(self, node):\n        return self.construct_scalar(node)\n\n    def construct_python_bytes(self, node):\n        try:\n            value = self.construct_scalar(node).encode('ascii')\n        except UnicodeEncodeError as exc:\n            raise ConstructorError(None, None,\n                    \"failed to convert base64 data into ascii: %s\" % exc,\n                    node.start_mark)\n        try:\n            if hasattr(base64, 'decodebytes'):\n                return base64.decodebytes(value)\n            else:\n                return base64.decodestring(value)\n        except binascii.Error as exc:\n            raise ConstructorError(None, None,\n                    \"failed to decode base64 data: %s\" % exc, node.start_mark)\n\n    def construct_python_long(self, node):\n        return self.construct_yaml_int(node)\n\n    def construct_python_complex(self, node):\n       return complex(self.construct_scalar(node))\n\n    def construct_python_tuple(self, node):\n        return tuple(self.construct_sequence(node))\n\n    def find_python_module(self, name, mark, unsafe=False):\n        if not name:\n            raise ConstructorError(\"while constructing a Python module\", mark,\n                    \"expected non-empty name appended to the tag\", mark)\n        if unsafe:\n            try:\n                __import__(name)\n            except ImportError as exc:\n                raise ConstructorError(\"while constructing a Python module\", mark,\n                        \"cannot find module %r (%s)\" % (name, exc), mark)\n        if name not in sys.modules:\n            raise ConstructorError(\"while constructing a Python module\", mark,\n                    \"module %r is not imported\" % name, mark)\n        return sys.modules[name]\n\n    def find_python_name(self, name, mark, unsafe=False):\n        if not name:\n            raise ConstructorError(\"while constructing a Python object\", mark,\n                    \"expected non-empty name appended to the tag\", mark)\n        if '.' in name:\n            module_name, object_name = name.rsplit('.', 1)\n        else:\n            module_name = 'builtins'\n            object_name = name\n        if unsafe:\n            try:\n                __import__(module_name)\n            except ImportError as exc:\n                raise ConstructorError(\"while constructing a Python object\", mark,\n                        \"cannot find module %r (%s)\" % (module_name, exc), mark)\n        if module_name not in sys.modules:\n            raise ConstructorError(\"while constructing a Python object\", mark,\n                    \"module %r is not imported\" % module_name, mark)\n        module = sys.modules[module_name]\n        if not hasattr(module, object_name):\n            raise ConstructorError(\"while constructing a Python object\", mark,\n                    \"cannot find %r in the module %r\"\n                    % (object_name, module.__name__), mark)\n        return getattr(module, object_name)\n\n    def construct_python_name(self, suffix, node):\n        value = self.construct_scalar(node)\n        if value:\n            raise ConstructorError(\"while constructing a Python name\", node.start_mark,\n                    \"expected the empty value, but found %r\" % value, node.start_mark)\n        return self.find_python_name(suffix, node.start_mark)\n\n    def construct_python_module(self, suffix, node):\n        value = self.construct_scalar(node)\n        if value:\n            raise ConstructorError(\"while constructing a Python module\", node.start_mark,\n                    \"expected the empty value, but found %r\" % value, node.start_mark)\n        return self.find_python_module(suffix, node.start_mark)\n\n    def make_python_instance(self, suffix, node,\n            args=None, kwds=None, newobj=False, unsafe=False):\n        if not args:\n            args = []\n        if not kwds:\n            kwds = {}\n        cls = self.find_python_name(suffix, node.start_mark)\n        if not (unsafe or isinstance(cls, type)):\n            raise ConstructorError(\"while constructing a Python instance\", node.start_mark,\n                    \"expected a class, but found %r\" % type(cls),\n                    node.start_mark)\n        if newobj and isinstance(cls, type):\n            return cls.__new__(cls, *args, **kwds)\n        else:\n            return cls(*args, **kwds)\n\n    def set_python_instance_state(self, instance, state, unsafe=False):\n        if hasattr(instance, '__setstate__'):\n            instance.__setstate__(state)\n        else:\n            slotstate = {}\n            if isinstance(state, tuple) and len(state) == 2:\n                state, slotstate = state\n            if hasattr(instance, '__dict__'):\n                if not unsafe and state:\n                    for key in state.keys():\n                        self.check_state_key(key)\n                instance.__dict__.update(state)\n            elif state:\n                slotstate.update(state)\n            for key, value in slotstate.items():\n                if not unsafe:\n                    self.check_state_key(key)\n                setattr(instance, key, value)\n\n    def construct_python_object(self, suffix, node):\n        # Format:\n        #   !!python/object:module.name { ... state ... }\n        instance = self.make_python_instance(suffix, node, newobj=True)\n        yield instance\n        deep = hasattr(instance, '__setstate__')\n        state = self.construct_mapping(node, deep=deep)\n        self.set_python_instance_state(instance, state)\n\n    def construct_python_object_apply(self, suffix, node, newobj=False):\n        # Format:\n        #   !!python/object/apply       # (or !!python/object/new)\n        #   args: [ ... arguments ... ]\n        #   kwds: { ... keywords ... }\n        #   state: ... state ...\n        #   listitems: [ ... listitems ... ]\n        #   dictitems: { ... dictitems ... }\n        # or short format:\n        #   !!python/object/apply [ ... arguments ... ]\n        # The difference between !!python/object/apply and !!python/object/new\n        # is how an object is created, check make_python_instance for details.\n        if isinstance(node, SequenceNode):\n            args = self.construct_sequence(node, deep=True)\n            kwds = {}\n            state = {}\n            listitems = []\n            dictitems = {}\n        else:\n            value = self.construct_mapping(node, deep=True)\n            args = value.get('args', [])\n            kwds = value.get('kwds', {})\n            state = value.get('state', {})\n            listitems = value.get('listitems', [])\n            dictitems = value.get('dictitems', {})\n        instance = self.make_python_instance(suffix, node, args, kwds, newobj)\n        if state:\n            self.set_python_instance_state(instance, state)\n        if listitems:\n            instance.extend(listitems)\n        if dictitems:\n            for key in dictitems:\n                instance[key] = dictitems[key]\n        return instance\n\n    def construct_python_object_new(self, suffix, node):\n        return self.construct_python_object_apply(suffix, node, newobj=True)\n\nFullConstructor.add_constructor(\n    'tag:yaml.org,2002:python/none',\n    FullConstructor.construct_yaml_null)\n\nFullConstructor.add_constructor(\n    'tag:yaml.org,2002:python/bool',\n    FullConstructor.construct_yaml_bool)\n\nFullConstructor.add_constructor(\n    'tag:yaml.org,2002:python/str',\n    FullConstructor.construct_python_str)\n\nFullConstructor.add_constructor(\n    'tag:yaml.org,2002:python/unicode',\n    FullConstructor.construct_python_unicode)\n\nFullConstructor.add_constructor(\n    'tag:yaml.org,2002:python/bytes',\n    FullConstructor.construct_python_bytes)\n\nFullConstructor.add_constructor(\n    'tag:yaml.org,2002:python/int',\n    FullConstructor.construct_yaml_int)\n\nFullConstructor.add_constructor(\n    'tag:yaml.org,2002:python/long',\n    FullConstructor.construct_python_long)\n\nFullConstructor.add_constructor(\n    'tag:yaml.org,2002:python/float',\n    FullConstructor.construct_yaml_float)\n\nFullConstructor.add_constructor(\n    'tag:yaml.org,2002:python/complex',\n    FullConstructor.construct_python_complex)\n\nFullConstructor.add_constructor(\n    'tag:yaml.org,2002:python/list',\n    FullConstructor.construct_yaml_seq)\n\nFullConstructor.add_constructor(\n    'tag:yaml.org,2002:python/tuple',\n    FullConstructor.construct_python_tuple)\n\nFullConstructor.add_constructor(\n    'tag:yaml.org,2002:python/dict',\n    FullConstructor.construct_yaml_map)\n\nFullConstructor.add_multi_constructor(\n    'tag:yaml.org,2002:python/name:',\n    FullConstructor.construct_python_name)\n\nclass UnsafeConstructor(FullConstructor):\n\n    def find_python_module(self, name, mark):\n        return super(UnsafeConstructor, self).find_python_module(name, mark, unsafe=True)\n\n    def find_python_name(self, name, mark):\n        return super(UnsafeConstructor, self).find_python_name(name, mark, unsafe=True)\n\n    def make_python_instance(self, suffix, node, args=None, kwds=None, newobj=False):\n        return super(UnsafeConstructor, self).make_python_instance(\n            suffix, node, args, kwds, newobj, unsafe=True)\n\n    def set_python_instance_state(self, instance, state):\n        return super(UnsafeConstructor, self).set_python_instance_state(\n            instance, state, unsafe=True)\n\nUnsafeConstructor.add_multi_constructor(\n    'tag:yaml.org,2002:python/module:',\n    UnsafeConstructor.construct_python_module)\n\nUnsafeConstructor.add_multi_constructor(\n    'tag:yaml.org,2002:python/object:',\n    UnsafeConstructor.construct_python_object)\n\nUnsafeConstructor.add_multi_constructor(\n    'tag:yaml.org,2002:python/object/new:',\n    UnsafeConstructor.construct_python_object_new)\n\nUnsafeConstructor.add_multi_constructor(\n    'tag:yaml.org,2002:python/object/apply:',\n    UnsafeConstructor.construct_python_object_apply)\n\n# Constructor is same as UnsafeConstructor. Need to leave this in place in case\n# people have extended it directly.\nclass Constructor(UnsafeConstructor):\n    pass\n", "lib/yaml/emitter.py": "\n# Emitter expects events obeying the following grammar:\n# stream ::= STREAM-START document* STREAM-END\n# document ::= DOCUMENT-START node DOCUMENT-END\n# node ::= SCALAR | sequence | mapping\n# sequence ::= SEQUENCE-START node* SEQUENCE-END\n# mapping ::= MAPPING-START (node node)* MAPPING-END\n\n__all__ = ['Emitter', 'EmitterError']\n\nfrom .error import YAMLError\nfrom .events import *\n\nclass EmitterError(YAMLError):\n    pass\n\nclass ScalarAnalysis:\n    def __init__(self, scalar, empty, multiline,\n            allow_flow_plain, allow_block_plain,\n            allow_single_quoted, allow_double_quoted,\n            allow_block):\n        self.scalar = scalar\n        self.empty = empty\n        self.multiline = multiline\n        self.allow_flow_plain = allow_flow_plain\n        self.allow_block_plain = allow_block_plain\n        self.allow_single_quoted = allow_single_quoted\n        self.allow_double_quoted = allow_double_quoted\n        self.allow_block = allow_block\n\nclass Emitter:\n\n    DEFAULT_TAG_PREFIXES = {\n        '!' : '!',\n        'tag:yaml.org,2002:' : '!!',\n    }\n\n    def __init__(self, stream, canonical=None, indent=None, width=None,\n            allow_unicode=None, line_break=None):\n\n        # The stream should have the methods `write` and possibly `flush`.\n        self.stream = stream\n\n        # Encoding can be overridden by STREAM-START.\n        self.encoding = None\n\n        # Emitter is a state machine with a stack of states to handle nested\n        # structures.\n        self.states = []\n        self.state = self.expect_stream_start\n\n        # Current event and the event queue.\n        self.events = []\n        self.event = None\n\n        # The current indentation level and the stack of previous indents.\n        self.indents = []\n        self.indent = None\n\n        # Flow level.\n        self.flow_level = 0\n\n        # Contexts.\n        self.root_context = False\n        self.sequence_context = False\n        self.mapping_context = False\n        self.simple_key_context = False\n\n        # Characteristics of the last emitted character:\n        #  - current position.\n        #  - is it a whitespace?\n        #  - is it an indention character\n        #    (indentation space, '-', '?', or ':')?\n        self.line = 0\n        self.column = 0\n        self.whitespace = True\n        self.indention = True\n\n        # Whether the document requires an explicit document indicator\n        self.open_ended = False\n\n        # Formatting details.\n        self.canonical = canonical\n        self.allow_unicode = allow_unicode\n        self.best_indent = 2\n        if indent and 1 < indent < 10:\n            self.best_indent = indent\n        self.best_width = 80\n        if width and width > self.best_indent*2:\n            self.best_width = width\n        self.best_line_break = '\\n'\n        if line_break in ['\\r', '\\n', '\\r\\n']:\n            self.best_line_break = line_break\n\n        # Tag prefixes.\n        self.tag_prefixes = None\n\n        # Prepared anchor and tag.\n        self.prepared_anchor = None\n        self.prepared_tag = None\n\n        # Scalar analysis and style.\n        self.analysis = None\n        self.style = None\n\n    def dispose(self):\n        # Reset the state attributes (to clear self-references)\n        self.states = []\n        self.state = None\n\n    def emit(self, event):\n        self.events.append(event)\n        while not self.need_more_events():\n            self.event = self.events.pop(0)\n            self.state()\n            self.event = None\n\n    # In some cases, we wait for a few next events before emitting.\n\n    def need_more_events(self):\n        if not self.events:\n            return True\n        event = self.events[0]\n        if isinstance(event, DocumentStartEvent):\n            return self.need_events(1)\n        elif isinstance(event, SequenceStartEvent):\n            return self.need_events(2)\n        elif isinstance(event, MappingStartEvent):\n            return self.need_events(3)\n        else:\n            return False\n\n    def need_events(self, count):\n        level = 0\n        for event in self.events[1:]:\n            if isinstance(event, (DocumentStartEvent, CollectionStartEvent)):\n                level += 1\n            elif isinstance(event, (DocumentEndEvent, CollectionEndEvent)):\n                level -= 1\n            elif isinstance(event, StreamEndEvent):\n                level = -1\n            if level < 0:\n                return False\n        return (len(self.events) < count+1)\n\n    def increase_indent(self, flow=False, indentless=False):\n        self.indents.append(self.indent)\n        if self.indent is None:\n            if flow:\n                self.indent = self.best_indent\n            else:\n                self.indent = 0\n        elif not indentless:\n            self.indent += self.best_indent\n\n    # States.\n\n    # Stream handlers.\n\n    def expect_stream_start(self):\n        if isinstance(self.event, StreamStartEvent):\n            if self.event.encoding and not hasattr(self.stream, 'encoding'):\n                self.encoding = self.event.encoding\n            self.write_stream_start()\n            self.state = self.expect_first_document_start\n        else:\n            raise EmitterError(\"expected StreamStartEvent, but got %s\"\n                    % self.event)\n\n    def expect_nothing(self):\n        raise EmitterError(\"expected nothing, but got %s\" % self.event)\n\n    # Document handlers.\n\n    def expect_first_document_start(self):\n        return self.expect_document_start(first=True)\n\n    def expect_document_start(self, first=False):\n        if isinstance(self.event, DocumentStartEvent):\n            if (self.event.version or self.event.tags) and self.open_ended:\n                self.write_indicator('...', True)\n                self.write_indent()\n            if self.event.version:\n                version_text = self.prepare_version(self.event.version)\n                self.write_version_directive(version_text)\n            self.tag_prefixes = self.DEFAULT_TAG_PREFIXES.copy()\n            if self.event.tags:\n                handles = sorted(self.event.tags.keys())\n                for handle in handles:\n                    prefix = self.event.tags[handle]\n                    self.tag_prefixes[prefix] = handle\n                    handle_text = self.prepare_tag_handle(handle)\n                    prefix_text = self.prepare_tag_prefix(prefix)\n                    self.write_tag_directive(handle_text, prefix_text)\n            implicit = (first and not self.event.explicit and not self.canonical\n                    and not self.event.version and not self.event.tags\n                    and not self.check_empty_document())\n            if not implicit:\n                self.write_indent()\n                self.write_indicator('---', True)\n                if self.canonical:\n                    self.write_indent()\n            self.state = self.expect_document_root\n        elif isinstance(self.event, StreamEndEvent):\n            if self.open_ended:\n                self.write_indicator('...', True)\n                self.write_indent()\n            self.write_stream_end()\n            self.state = self.expect_nothing\n        else:\n            raise EmitterError(\"expected DocumentStartEvent, but got %s\"\n                    % self.event)\n\n    def expect_document_end(self):\n        if isinstance(self.event, DocumentEndEvent):\n            self.write_indent()\n            if self.event.explicit:\n                self.write_indicator('...', True)\n                self.write_indent()\n            self.flush_stream()\n            self.state = self.expect_document_start\n        else:\n            raise EmitterError(\"expected DocumentEndEvent, but got %s\"\n                    % self.event)\n\n    def expect_document_root(self):\n        self.states.append(self.expect_document_end)\n        self.expect_node(root=True)\n\n    # Node handlers.\n\n    def expect_node(self, root=False, sequence=False, mapping=False,\n            simple_key=False):\n        self.root_context = root\n        self.sequence_context = sequence\n        self.mapping_context = mapping\n        self.simple_key_context = simple_key\n        if isinstance(self.event, AliasEvent):\n            self.expect_alias()\n        elif isinstance(self.event, (ScalarEvent, CollectionStartEvent)):\n            self.process_anchor('&')\n            self.process_tag()\n            if isinstance(self.event, ScalarEvent):\n                self.expect_scalar()\n            elif isinstance(self.event, SequenceStartEvent):\n                if self.flow_level or self.canonical or self.event.flow_style   \\\n                        or self.check_empty_sequence():\n                    self.expect_flow_sequence()\n                else:\n                    self.expect_block_sequence()\n            elif isinstance(self.event, MappingStartEvent):\n                if self.flow_level or self.canonical or self.event.flow_style   \\\n                        or self.check_empty_mapping():\n                    self.expect_flow_mapping()\n                else:\n                    self.expect_block_mapping()\n        else:\n            raise EmitterError(\"expected NodeEvent, but got %s\" % self.event)\n\n    def expect_alias(self):\n        if self.event.anchor is None:\n            raise EmitterError(\"anchor is not specified for alias\")\n        self.process_anchor('*')\n        self.state = self.states.pop()\n\n    def expect_scalar(self):\n        self.increase_indent(flow=True)\n        self.process_scalar()\n        self.indent = self.indents.pop()\n        self.state = self.states.pop()\n\n    # Flow sequence handlers.\n\n    def expect_flow_sequence(self):\n        self.write_indicator('[', True, whitespace=True)\n        self.flow_level += 1\n        self.increase_indent(flow=True)\n        self.state = self.expect_first_flow_sequence_item\n\n    def expect_first_flow_sequence_item(self):\n        if isinstance(self.event, SequenceEndEvent):\n            self.indent = self.indents.pop()\n            self.flow_level -= 1\n            self.write_indicator(']', False)\n            self.state = self.states.pop()\n        else:\n            if self.canonical or self.column > self.best_width:\n                self.write_indent()\n            self.states.append(self.expect_flow_sequence_item)\n            self.expect_node(sequence=True)\n\n    def expect_flow_sequence_item(self):\n        if isinstance(self.event, SequenceEndEvent):\n            self.indent = self.indents.pop()\n            self.flow_level -= 1\n            if self.canonical:\n                self.write_indicator(',', False)\n                self.write_indent()\n            self.write_indicator(']', False)\n            self.state = self.states.pop()\n        else:\n            self.write_indicator(',', False)\n            if self.canonical or self.column > self.best_width:\n                self.write_indent()\n            self.states.append(self.expect_flow_sequence_item)\n            self.expect_node(sequence=True)\n\n    # Flow mapping handlers.\n\n    def expect_flow_mapping(self):\n        self.write_indicator('{', True, whitespace=True)\n        self.flow_level += 1\n        self.increase_indent(flow=True)\n        self.state = self.expect_first_flow_mapping_key\n\n    def expect_first_flow_mapping_key(self):\n        if isinstance(self.event, MappingEndEvent):\n            self.indent = self.indents.pop()\n            self.flow_level -= 1\n            self.write_indicator('}', False)\n            self.state = self.states.pop()\n        else:\n            if self.canonical or self.column > self.best_width:\n                self.write_indent()\n            if not self.canonical and self.check_simple_key():\n                self.states.append(self.expect_flow_mapping_simple_value)\n                self.expect_node(mapping=True, simple_key=True)\n            else:\n                self.write_indicator('?', True)\n                self.states.append(self.expect_flow_mapping_value)\n                self.expect_node(mapping=True)\n\n    def expect_flow_mapping_key(self):\n        if isinstance(self.event, MappingEndEvent):\n            self.indent = self.indents.pop()\n            self.flow_level -= 1\n            if self.canonical:\n                self.write_indicator(',', False)\n                self.write_indent()\n            self.write_indicator('}', False)\n            self.state = self.states.pop()\n        else:\n            self.write_indicator(',', False)\n            if self.canonical or self.column > self.best_width:\n                self.write_indent()\n            if not self.canonical and self.check_simple_key():\n                self.states.append(self.expect_flow_mapping_simple_value)\n                self.expect_node(mapping=True, simple_key=True)\n            else:\n                self.write_indicator('?', True)\n                self.states.append(self.expect_flow_mapping_value)\n                self.expect_node(mapping=True)\n\n    def expect_flow_mapping_simple_value(self):\n        self.write_indicator(':', False)\n        self.states.append(self.expect_flow_mapping_key)\n        self.expect_node(mapping=True)\n\n    def expect_flow_mapping_value(self):\n        if self.canonical or self.column > self.best_width:\n            self.write_indent()\n        self.write_indicator(':', True)\n        self.states.append(self.expect_flow_mapping_key)\n        self.expect_node(mapping=True)\n\n    # Block sequence handlers.\n\n    def expect_block_sequence(self):\n        indentless = (self.mapping_context and not self.indention)\n        self.increase_indent(flow=False, indentless=indentless)\n        self.state = self.expect_first_block_sequence_item\n\n    def expect_first_block_sequence_item(self):\n        return self.expect_block_sequence_item(first=True)\n\n    def expect_block_sequence_item(self, first=False):\n        if not first and isinstance(self.event, SequenceEndEvent):\n            self.indent = self.indents.pop()\n            self.state = self.states.pop()\n        else:\n            self.write_indent()\n            self.write_indicator('-', True, indention=True)\n            self.states.append(self.expect_block_sequence_item)\n            self.expect_node(sequence=True)\n\n    # Block mapping handlers.\n\n    def expect_block_mapping(self):\n        self.increase_indent(flow=False)\n        self.state = self.expect_first_block_mapping_key\n\n    def expect_first_block_mapping_key(self):\n        return self.expect_block_mapping_key(first=True)\n\n    def expect_block_mapping_key(self, first=False):\n        if not first and isinstance(self.event, MappingEndEvent):\n            self.indent = self.indents.pop()\n            self.state = self.states.pop()\n        else:\n            self.write_indent()\n            if self.check_simple_key():\n                self.states.append(self.expect_block_mapping_simple_value)\n                self.expect_node(mapping=True, simple_key=True)\n            else:\n                self.write_indicator('?', True, indention=True)\n                self.states.append(self.expect_block_mapping_value)\n                self.expect_node(mapping=True)\n\n    def expect_block_mapping_simple_value(self):\n        self.write_indicator(':', False)\n        self.states.append(self.expect_block_mapping_key)\n        self.expect_node(mapping=True)\n\n    def expect_block_mapping_value(self):\n        self.write_indent()\n        self.write_indicator(':', True, indention=True)\n        self.states.append(self.expect_block_mapping_key)\n        self.expect_node(mapping=True)\n\n    # Checkers.\n\n    def check_empty_sequence(self):\n        return (isinstance(self.event, SequenceStartEvent) and self.events\n                and isinstance(self.events[0], SequenceEndEvent))\n\n    def check_empty_mapping(self):\n        return (isinstance(self.event, MappingStartEvent) and self.events\n                and isinstance(self.events[0], MappingEndEvent))\n\n    def check_empty_document(self):\n        if not isinstance(self.event, DocumentStartEvent) or not self.events:\n            return False\n        event = self.events[0]\n        return (isinstance(event, ScalarEvent) and event.anchor is None\n                and event.tag is None and event.implicit and event.value == '')\n\n    def check_simple_key(self):\n        length = 0\n        if isinstance(self.event, NodeEvent) and self.event.anchor is not None:\n            if self.prepared_anchor is None:\n                self.prepared_anchor = self.prepare_anchor(self.event.anchor)\n            length += len(self.prepared_anchor)\n        if isinstance(self.event, (ScalarEvent, CollectionStartEvent))  \\\n                and self.event.tag is not None:\n            if self.prepared_tag is None:\n                self.prepared_tag = self.prepare_tag(self.event.tag)\n            length += len(self.prepared_tag)\n        if isinstance(self.event, ScalarEvent):\n            if self.analysis is None:\n                self.analysis = self.analyze_scalar(self.event.value)\n            length += len(self.analysis.scalar)\n        return (length < 128 and (isinstance(self.event, AliasEvent)\n            or (isinstance(self.event, ScalarEvent)\n                    and not self.analysis.empty and not self.analysis.multiline)\n            or self.check_empty_sequence() or self.check_empty_mapping()))\n\n    # Anchor, Tag, and Scalar processors.\n\n    def process_anchor(self, indicator):\n        if self.event.anchor is None:\n            self.prepared_anchor = None\n            return\n        if self.prepared_anchor is None:\n            self.prepared_anchor = self.prepare_anchor(self.event.anchor)\n        if self.prepared_anchor:\n            self.write_indicator(indicator+self.prepared_anchor, True)\n        self.prepared_anchor = None\n\n    def process_tag(self):\n        tag = self.event.tag\n        if isinstance(self.event, ScalarEvent):\n            if self.style is None:\n                self.style = self.choose_scalar_style()\n            if ((not self.canonical or tag is None) and\n                ((self.style == '' and self.event.implicit[0])\n                        or (self.style != '' and self.event.implicit[1]))):\n                self.prepared_tag = None\n                return\n            if self.event.implicit[0] and tag is None:\n                tag = '!'\n                self.prepared_tag = None\n        else:\n            if (not self.canonical or tag is None) and self.event.implicit:\n                self.prepared_tag = None\n                return\n        if tag is None:\n            raise EmitterError(\"tag is not specified\")\n        if self.prepared_tag is None:\n            self.prepared_tag = self.prepare_tag(tag)\n        if self.prepared_tag:\n            self.write_indicator(self.prepared_tag, True)\n        self.prepared_tag = None\n\n    def choose_scalar_style(self):\n        if self.analysis is None:\n            self.analysis = self.analyze_scalar(self.event.value)\n        if self.event.style == '\"' or self.canonical:\n            return '\"'\n        if not self.event.style and self.event.implicit[0]:\n            if (not (self.simple_key_context and\n                    (self.analysis.empty or self.analysis.multiline))\n                and (self.flow_level and self.analysis.allow_flow_plain\n                    or (not self.flow_level and self.analysis.allow_block_plain))):\n                return ''\n        if self.event.style and self.event.style in '|>':\n            if (not self.flow_level and not self.simple_key_context\n                    and self.analysis.allow_block):\n                return self.event.style\n        if not self.event.style or self.event.style == '\\'':\n            if (self.analysis.allow_single_quoted and\n                    not (self.simple_key_context and self.analysis.multiline)):\n                return '\\''\n        return '\"'\n\n    def process_scalar(self):\n        if self.analysis is None:\n            self.analysis = self.analyze_scalar(self.event.value)\n        if self.style is None:\n            self.style = self.choose_scalar_style()\n        split = (not self.simple_key_context)\n        #if self.analysis.multiline and split    \\\n        #        and (not self.style or self.style in '\\'\\\"'):\n        #    self.write_indent()\n        if self.style == '\"':\n            self.write_double_quoted(self.analysis.scalar, split)\n        elif self.style == '\\'':\n            self.write_single_quoted(self.analysis.scalar, split)\n        elif self.style == '>':\n            self.write_folded(self.analysis.scalar)\n        elif self.style == '|':\n            self.write_literal(self.analysis.scalar)\n        else:\n            self.write_plain(self.analysis.scalar, split)\n        self.analysis = None\n        self.style = None\n\n    # Analyzers.\n\n    def prepare_version(self, version):\n        major, minor = version\n        if major != 1:\n            raise EmitterError(\"unsupported YAML version: %d.%d\" % (major, minor))\n        return '%d.%d' % (major, minor)\n\n    def prepare_tag_handle(self, handle):\n        if not handle:\n            raise EmitterError(\"tag handle must not be empty\")\n        if handle[0] != '!' or handle[-1] != '!':\n            raise EmitterError(\"tag handle must start and end with '!': %r\" % handle)\n        for ch in handle[1:-1]:\n            if not ('0' <= ch <= '9' or 'A' <= ch <= 'Z' or 'a' <= ch <= 'z'    \\\n                    or ch in '-_'):\n                raise EmitterError(\"invalid character %r in the tag handle: %r\"\n                        % (ch, handle))\n        return handle\n\n    def prepare_tag_prefix(self, prefix):\n        if not prefix:\n            raise EmitterError(\"tag prefix must not be empty\")\n        chunks = []\n        start = end = 0\n        if prefix[0] == '!':\n            end = 1\n        while end < len(prefix):\n            ch = prefix[end]\n            if '0' <= ch <= '9' or 'A' <= ch <= 'Z' or 'a' <= ch <= 'z' \\\n                    or ch in '-;/?!:@&=+$,_.~*\\'()[]':\n                end += 1\n            else:\n                if start < end:\n                    chunks.append(prefix[start:end])\n                start = end = end+1\n                data = ch.encode('utf-8')\n                for ch in data:\n                    chunks.append('%%%02X' % ord(ch))\n        if start < end:\n            chunks.append(prefix[start:end])\n        return ''.join(chunks)\n\n    def prepare_tag(self, tag):\n        if not tag:\n            raise EmitterError(\"tag must not be empty\")\n        if tag == '!':\n            return tag\n        handle = None\n        suffix = tag\n        prefixes = sorted(self.tag_prefixes.keys())\n        for prefix in prefixes:\n            if tag.startswith(prefix)   \\\n                    and (prefix == '!' or len(prefix) < len(tag)):\n                handle = self.tag_prefixes[prefix]\n                suffix = tag[len(prefix):]\n        chunks = []\n        start = end = 0\n        while end < len(suffix):\n            ch = suffix[end]\n            if '0' <= ch <= '9' or 'A' <= ch <= 'Z' or 'a' <= ch <= 'z' \\\n                    or ch in '-;/?:@&=+$,_.~*\\'()[]'   \\\n                    or (ch == '!' and handle != '!'):\n                end += 1\n            else:\n                if start < end:\n                    chunks.append(suffix[start:end])\n                start = end = end+1\n                data = ch.encode('utf-8')\n                for ch in data:\n                    chunks.append('%%%02X' % ch)\n        if start < end:\n            chunks.append(suffix[start:end])\n        suffix_text = ''.join(chunks)\n        if handle:\n            return '%s%s' % (handle, suffix_text)\n        else:\n            return '!<%s>' % suffix_text\n\n    def prepare_anchor(self, anchor):\n        if not anchor:\n            raise EmitterError(\"anchor must not be empty\")\n        for ch in anchor:\n            if not ('0' <= ch <= '9' or 'A' <= ch <= 'Z' or 'a' <= ch <= 'z'    \\\n                    or ch in '-_'):\n                raise EmitterError(\"invalid character %r in the anchor: %r\"\n                        % (ch, anchor))\n        return anchor\n\n    def analyze_scalar(self, scalar):\n\n        # Empty scalar is a special case.\n        if not scalar:\n            return ScalarAnalysis(scalar=scalar, empty=True, multiline=False,\n                    allow_flow_plain=False, allow_block_plain=True,\n                    allow_single_quoted=True, allow_double_quoted=True,\n                    allow_block=False)\n\n        # Indicators and special characters.\n        block_indicators = False\n        flow_indicators = False\n        line_breaks = False\n        special_characters = False\n\n        # Important whitespace combinations.\n        leading_space = False\n        leading_break = False\n        trailing_space = False\n        trailing_break = False\n        break_space = False\n        space_break = False\n\n        # Check document indicators.\n        if scalar.startswith('---') or scalar.startswith('...'):\n            block_indicators = True\n            flow_indicators = True\n\n        # First character or preceded by a whitespace.\n        preceded_by_whitespace = True\n\n        # Last character or followed by a whitespace.\n        followed_by_whitespace = (len(scalar) == 1 or\n                scalar[1] in '\\0 \\t\\r\\n\\x85\\u2028\\u2029')\n\n        # The previous character is a space.\n        previous_space = False\n\n        # The previous character is a break.\n        previous_break = False\n\n        index = 0\n        while index < len(scalar):\n            ch = scalar[index]\n\n            # Check for indicators.\n            if index == 0:\n                # Leading indicators are special characters.\n                if ch in '#,[]{}&*!|>\\'\\\"%@`':\n                    flow_indicators = True\n                    block_indicators = True\n                if ch in '?:':\n                    flow_indicators = True\n                    if followed_by_whitespace:\n                        block_indicators = True\n                if ch == '-' and followed_by_whitespace:\n                    flow_indicators = True\n                    block_indicators = True\n            else:\n                # Some indicators cannot appear within a scalar as well.\n                if ch in ',?[]{}':\n                    flow_indicators = True\n                if ch == ':':\n                    flow_indicators = True\n                    if followed_by_whitespace:\n                        block_indicators = True\n                if ch == '#' and preceded_by_whitespace:\n                    flow_indicators = True\n                    block_indicators = True\n\n            # Check for line breaks, special, and unicode characters.\n            if ch in '\\n\\x85\\u2028\\u2029':\n                line_breaks = True\n            if not (ch == '\\n' or '\\x20' <= ch <= '\\x7E'):\n                if (ch == '\\x85' or '\\xA0' <= ch <= '\\uD7FF'\n                        or '\\uE000' <= ch <= '\\uFFFD'\n                        or '\\U00010000' <= ch < '\\U0010ffff') and ch != '\\uFEFF':\n                    unicode_characters = True\n                    if not self.allow_unicode:\n                        special_characters = True\n                else:\n                    special_characters = True\n\n            # Detect important whitespace combinations.\n            if ch == ' ':\n                if index == 0:\n                    leading_space = True\n                if index == len(scalar)-1:\n                    trailing_space = True\n                if previous_break:\n                    break_space = True\n                previous_space = True\n                previous_break = False\n            elif ch in '\\n\\x85\\u2028\\u2029':\n                if index == 0:\n                    leading_break = True\n                if index == len(scalar)-1:\n                    trailing_break = True\n                if previous_space:\n                    space_break = True\n                previous_space = False\n                previous_break = True\n            else:\n                previous_space = False\n                previous_break = False\n\n            # Prepare for the next character.\n            index += 1\n            preceded_by_whitespace = (ch in '\\0 \\t\\r\\n\\x85\\u2028\\u2029')\n            followed_by_whitespace = (index+1 >= len(scalar) or\n                    scalar[index+1] in '\\0 \\t\\r\\n\\x85\\u2028\\u2029')\n\n        # Let's decide what styles are allowed.\n        allow_flow_plain = True\n        allow_block_plain = True\n        allow_single_quoted = True\n        allow_double_quoted = True\n        allow_block = True\n\n        # Leading and trailing whitespaces are bad for plain scalars.\n        if (leading_space or leading_break\n                or trailing_space or trailing_break):\n            allow_flow_plain = allow_block_plain = False\n\n        # We do not permit trailing spaces for block scalars.\n        if trailing_space:\n            allow_block = False\n\n        # Spaces at the beginning of a new line are only acceptable for block\n        # scalars.\n        if break_space:\n            allow_flow_plain = allow_block_plain = allow_single_quoted = False\n\n        # Spaces followed by breaks, as well as special character are only\n        # allowed for double quoted scalars.\n        if space_break or special_characters:\n            allow_flow_plain = allow_block_plain =  \\\n            allow_single_quoted = allow_block = False\n\n        # Although the plain scalar writer supports breaks, we never emit\n        # multiline plain scalars.\n        if line_breaks:\n            allow_flow_plain = allow_block_plain = False\n\n        # Flow indicators are forbidden for flow plain scalars.\n        if flow_indicators:\n            allow_flow_plain = False\n\n        # Block indicators are forbidden for block plain scalars.\n        if block_indicators:\n            allow_block_plain = False\n\n        return ScalarAnalysis(scalar=scalar,\n                empty=False, multiline=line_breaks,\n                allow_flow_plain=allow_flow_plain,\n                allow_block_plain=allow_block_plain,\n                allow_single_quoted=allow_single_quoted,\n                allow_double_quoted=allow_double_quoted,\n                allow_block=allow_block)\n\n    # Writers.\n\n    def flush_stream(self):\n        if hasattr(self.stream, 'flush'):\n            self.stream.flush()\n\n    def write_stream_start(self):\n        # Write BOM if needed.\n        if self.encoding and self.encoding.startswith('utf-16'):\n            self.stream.write('\\uFEFF'.encode(self.encoding))\n\n    def write_stream_end(self):\n        self.flush_stream()\n\n    def write_indicator(self, indicator, need_whitespace,\n            whitespace=False, indention=False):\n        if self.whitespace or not need_whitespace:\n            data = indicator\n        else:\n            data = ' '+indicator\n        self.whitespace = whitespace\n        self.indention = self.indention and indention\n        self.column += len(data)\n        self.open_ended = False\n        if self.encoding:\n            data = data.encode(self.encoding)\n        self.stream.write(data)\n\n    def write_indent(self):\n        indent = self.indent or 0\n        if not self.indention or self.column > indent   \\\n                or (self.column == indent and not self.whitespace):\n            self.write_line_break()\n        if self.column < indent:\n            self.whitespace = True\n            data = ' '*(indent-self.column)\n            self.column = indent\n            if self.encoding:\n                data = data.encode(self.encoding)\n            self.stream.write(data)\n\n    def write_line_break(self, data=None):\n        if data is None:\n            data = self.best_line_break\n        self.whitespace = True\n        self.indention = True\n        self.line += 1\n        self.column = 0\n        if self.encoding:\n            data = data.encode(self.encoding)\n        self.stream.write(data)\n\n    def write_version_directive(self, version_text):\n        data = '%%YAML %s' % version_text\n        if self.encoding:\n            data = data.encode(self.encoding)\n        self.stream.write(data)\n        self.write_line_break()\n\n    def write_tag_directive(self, handle_text, prefix_text):\n        data = '%%TAG %s %s' % (handle_text, prefix_text)\n        if self.encoding:\n            data = data.encode(self.encoding)\n        self.stream.write(data)\n        self.write_line_break()\n\n    # Scalar streams.\n\n    def write_single_quoted(self, text, split=True):\n        self.write_indicator('\\'', True)\n        spaces = False\n        breaks = False\n        start = end = 0\n        while end <= len(text):\n            ch = None\n            if end < len(text):\n                ch = text[end]\n            if spaces:\n                if ch is None or ch != ' ':\n                    if start+1 == end and self.column > self.best_width and split   \\\n                            and start != 0 and end != len(text):\n                        self.write_indent()\n                    else:\n                        data = text[start:end]\n                        self.column += len(data)\n                        if self.encoding:\n                            data = data.encode(self.encoding)\n                        self.stream.write(data)\n                    start = end\n            elif breaks:\n                if ch is None or ch not in '\\n\\x85\\u2028\\u2029':\n                    if text[start] == '\\n':\n                        self.write_line_break()\n                    for br in text[start:end]:\n                        if br == '\\n':\n                            self.write_line_break()\n                        else:\n                            self.write_line_break(br)\n                    self.write_indent()\n                    start = end\n            else:\n                if ch is None or ch in ' \\n\\x85\\u2028\\u2029' or ch == '\\'':\n                    if start < end:\n                        data = text[start:end]\n                        self.column += len(data)\n                        if self.encoding:\n                            data = data.encode(self.encoding)\n                        self.stream.write(data)\n                        start = end\n            if ch == '\\'':\n                data = '\\'\\''\n                self.column += 2\n                if self.encoding:\n                    data = data.encode(self.encoding)\n                self.stream.write(data)\n                start = end + 1\n            if ch is not None:\n                spaces = (ch == ' ')\n                breaks = (ch in '\\n\\x85\\u2028\\u2029')\n            end += 1\n        self.write_indicator('\\'', False)\n\n    ESCAPE_REPLACEMENTS = {\n        '\\0':       '0',\n        '\\x07':     'a',\n        '\\x08':     'b',\n        '\\x09':     't',\n        '\\x0A':     'n',\n        '\\x0B':     'v',\n        '\\x0C':     'f',\n        '\\x0D':     'r',\n        '\\x1B':     'e',\n        '\\\"':       '\\\"',\n        '\\\\':       '\\\\',\n        '\\x85':     'N',\n        '\\xA0':     '_',\n        '\\u2028':   'L',\n        '\\u2029':   'P',\n    }\n\n    def write_double_quoted(self, text, split=True):\n        self.write_indicator('\"', True)\n        start = end = 0\n        while end <= len(text):\n            ch = None\n            if end < len(text):\n                ch = text[end]\n            if ch is None or ch in '\"\\\\\\x85\\u2028\\u2029\\uFEFF' \\\n                    or not ('\\x20' <= ch <= '\\x7E'\n                        or (self.allow_unicode\n                            and ('\\xA0' <= ch <= '\\uD7FF'\n                                or '\\uE000' <= ch <= '\\uFFFD'))):\n                if start < end:\n                    data = text[start:end]\n                    self.column += len(data)\n                    if self.encoding:\n                        data = data.encode(self.encoding)\n                    self.stream.write(data)\n                    start = end\n                if ch is not None:\n                    if ch in self.ESCAPE_REPLACEMENTS:\n                        data = '\\\\'+self.ESCAPE_REPLACEMENTS[ch]\n                    elif ch <= '\\xFF':\n                        data = '\\\\x%02X' % ord(ch)\n                    elif ch <= '\\uFFFF':\n                        data = '\\\\u%04X' % ord(ch)\n                    else:\n                        data = '\\\\U%08X' % ord(ch)\n                    self.column += len(data)\n                    if self.encoding:\n                        data = data.encode(self.encoding)\n                    self.stream.write(data)\n                    start = end+1\n            if 0 < end < len(text)-1 and (ch == ' ' or start >= end)    \\\n                    and self.column+(end-start) > self.best_width and split:\n                data = text[start:end]+'\\\\'\n                if start < end:\n                    start = end\n                self.column += len(data)\n                if self.encoding:\n                    data = data.encode(self.encoding)\n                self.stream.write(data)\n                self.write_indent()\n                self.whitespace = False\n                self.indention = False\n                if text[start] == ' ':\n                    data = '\\\\'\n                    self.column += len(data)\n                    if self.encoding:\n                        data = data.encode(self.encoding)\n                    self.stream.write(data)\n            end += 1\n        self.write_indicator('\"', False)\n\n    def determine_block_hints(self, text):\n        hints = ''\n        if text:\n            if text[0] in ' \\n\\x85\\u2028\\u2029':\n                hints += str(self.best_indent)\n            if text[-1] not in '\\n\\x85\\u2028\\u2029':\n                hints += '-'\n            elif len(text) == 1 or text[-2] in '\\n\\x85\\u2028\\u2029':\n                hints += '+'\n        return hints\n\n    def write_folded(self, text):\n        hints = self.determine_block_hints(text)\n        self.write_indicator('>'+hints, True)\n        if hints[-1:] == '+':\n            self.open_ended = True\n        self.write_line_break()\n        leading_space = True\n        spaces = False\n        breaks = True\n        start = end = 0\n        while end <= len(text):\n            ch = None\n            if end < len(text):\n                ch = text[end]\n            if breaks:\n                if ch is None or ch not in '\\n\\x85\\u2028\\u2029':\n                    if not leading_space and ch is not None and ch != ' '   \\\n                            and text[start] == '\\n':\n                        self.write_line_break()\n                    leading_space = (ch == ' ')\n                    for br in text[start:end]:\n                        if br == '\\n':\n                            self.write_line_break()\n                        else:\n                            self.write_line_break(br)\n                    if ch is not None:\n                        self.write_indent()\n                    start = end\n            elif spaces:\n                if ch != ' ':\n                    if start+1 == end and self.column > self.best_width:\n                        self.write_indent()\n                    else:\n                        data = text[start:end]\n                        self.column += len(data)\n                        if self.encoding:\n                            data = data.encode(self.encoding)\n                        self.stream.write(data)\n                    start = end\n            else:\n                if ch is None or ch in ' \\n\\x85\\u2028\\u2029':\n                    data = text[start:end]\n                    self.column += len(data)\n                    if self.encoding:\n                        data = data.encode(self.encoding)\n                    self.stream.write(data)\n                    if ch is None:\n                        self.write_line_break()\n                    start = end\n            if ch is not None:\n                breaks = (ch in '\\n\\x85\\u2028\\u2029')\n                spaces = (ch == ' ')\n            end += 1\n\n    def write_literal(self, text):\n        hints = self.determine_block_hints(text)\n        self.write_indicator('|'+hints, True)\n        if hints[-1:] == '+':\n            self.open_ended = True\n        self.write_line_break()\n        breaks = True\n        start = end = 0\n        while end <= len(text):\n            ch = None\n            if end < len(text):\n                ch = text[end]\n            if breaks:\n                if ch is None or ch not in '\\n\\x85\\u2028\\u2029':\n                    for br in text[start:end]:\n                        if br == '\\n':\n                            self.write_line_break()\n                        else:\n                            self.write_line_break(br)\n                    if ch is not None:\n                        self.write_indent()\n                    start = end\n            else:\n                if ch is None or ch in '\\n\\x85\\u2028\\u2029':\n                    data = text[start:end]\n                    if self.encoding:\n                        data = data.encode(self.encoding)\n                    self.stream.write(data)\n                    if ch is None:\n                        self.write_line_break()\n                    start = end\n            if ch is not None:\n                breaks = (ch in '\\n\\x85\\u2028\\u2029')\n            end += 1\n\n    def write_plain(self, text, split=True):\n        if self.root_context:\n            self.open_ended = True\n        if not text:\n            return\n        if not self.whitespace:\n            data = ' '\n            self.column += len(data)\n            if self.encoding:\n                data = data.encode(self.encoding)\n            self.stream.write(data)\n        self.whitespace = False\n        self.indention = False\n        spaces = False\n        breaks = False\n        start = end = 0\n        while end <= len(text):\n            ch = None\n            if end < len(text):\n                ch = text[end]\n            if spaces:\n                if ch != ' ':\n                    if start+1 == end and self.column > self.best_width and split:\n                        self.write_indent()\n                        self.whitespace = False\n                        self.indention = False\n                    else:\n                        data = text[start:end]\n                        self.column += len(data)\n                        if self.encoding:\n                            data = data.encode(self.encoding)\n                        self.stream.write(data)\n                    start = end\n            elif breaks:\n                if ch not in '\\n\\x85\\u2028\\u2029':\n                    if text[start] == '\\n':\n                        self.write_line_break()\n                    for br in text[start:end]:\n                        if br == '\\n':\n                            self.write_line_break()\n                        else:\n                            self.write_line_break(br)\n                    self.write_indent()\n                    self.whitespace = False\n                    self.indention = False\n                    start = end\n            else:\n                if ch is None or ch in ' \\n\\x85\\u2028\\u2029':\n                    data = text[start:end]\n                    self.column += len(data)\n                    if self.encoding:\n                        data = data.encode(self.encoding)\n                    self.stream.write(data)\n                    start = end\n            if ch is not None:\n                spaces = (ch == ' ')\n                breaks = (ch in '\\n\\x85\\u2028\\u2029')\n            end += 1\n", "lib/yaml/events.py": "\n# Abstract classes.\n\nclass Event(object):\n    def __init__(self, start_mark=None, end_mark=None):\n        self.start_mark = start_mark\n        self.end_mark = end_mark\n    def __repr__(self):\n        attributes = [key for key in ['anchor', 'tag', 'implicit', 'value']\n                if hasattr(self, key)]\n        arguments = ', '.join(['%s=%r' % (key, getattr(self, key))\n                for key in attributes])\n        return '%s(%s)' % (self.__class__.__name__, arguments)\n\nclass NodeEvent(Event):\n    def __init__(self, anchor, start_mark=None, end_mark=None):\n        self.anchor = anchor\n        self.start_mark = start_mark\n        self.end_mark = end_mark\n\nclass CollectionStartEvent(NodeEvent):\n    def __init__(self, anchor, tag, implicit, start_mark=None, end_mark=None,\n            flow_style=None):\n        self.anchor = anchor\n        self.tag = tag\n        self.implicit = implicit\n        self.start_mark = start_mark\n        self.end_mark = end_mark\n        self.flow_style = flow_style\n\nclass CollectionEndEvent(Event):\n    pass\n\n# Implementations.\n\nclass StreamStartEvent(Event):\n    def __init__(self, start_mark=None, end_mark=None, encoding=None):\n        self.start_mark = start_mark\n        self.end_mark = end_mark\n        self.encoding = encoding\n\nclass StreamEndEvent(Event):\n    pass\n\nclass DocumentStartEvent(Event):\n    def __init__(self, start_mark=None, end_mark=None,\n            explicit=None, version=None, tags=None):\n        self.start_mark = start_mark\n        self.end_mark = end_mark\n        self.explicit = explicit\n        self.version = version\n        self.tags = tags\n\nclass DocumentEndEvent(Event):\n    def __init__(self, start_mark=None, end_mark=None,\n            explicit=None):\n        self.start_mark = start_mark\n        self.end_mark = end_mark\n        self.explicit = explicit\n\nclass AliasEvent(NodeEvent):\n    pass\n\nclass ScalarEvent(NodeEvent):\n    def __init__(self, anchor, tag, implicit, value,\n            start_mark=None, end_mark=None, style=None):\n        self.anchor = anchor\n        self.tag = tag\n        self.implicit = implicit\n        self.value = value\n        self.start_mark = start_mark\n        self.end_mark = end_mark\n        self.style = style\n\nclass SequenceStartEvent(CollectionStartEvent):\n    pass\n\nclass SequenceEndEvent(CollectionEndEvent):\n    pass\n\nclass MappingStartEvent(CollectionStartEvent):\n    pass\n\nclass MappingEndEvent(CollectionEndEvent):\n    pass\n\n", "lib/yaml/scanner.py": "\n# Scanner produces tokens of the following types:\n# STREAM-START\n# STREAM-END\n# DIRECTIVE(name, value)\n# DOCUMENT-START\n# DOCUMENT-END\n# BLOCK-SEQUENCE-START\n# BLOCK-MAPPING-START\n# BLOCK-END\n# FLOW-SEQUENCE-START\n# FLOW-MAPPING-START\n# FLOW-SEQUENCE-END\n# FLOW-MAPPING-END\n# BLOCK-ENTRY\n# FLOW-ENTRY\n# KEY\n# VALUE\n# ALIAS(value)\n# ANCHOR(value)\n# TAG(value)\n# SCALAR(value, plain, style)\n#\n# Read comments in the Scanner code for more details.\n#\n\n__all__ = ['Scanner', 'ScannerError']\n\nfrom .error import MarkedYAMLError\nfrom .tokens import *\n\nclass ScannerError(MarkedYAMLError):\n    pass\n\nclass SimpleKey:\n    # See below simple keys treatment.\n\n    def __init__(self, token_number, required, index, line, column, mark):\n        self.token_number = token_number\n        self.required = required\n        self.index = index\n        self.line = line\n        self.column = column\n        self.mark = mark\n\nclass Scanner:\n\n    def __init__(self):\n        \"\"\"Initialize the scanner.\"\"\"\n        # It is assumed that Scanner and Reader will have a common descendant.\n        # Reader do the dirty work of checking for BOM and converting the\n        # input data to Unicode. It also adds NUL to the end.\n        #\n        # Reader supports the following methods\n        #   self.peek(i=0)       # peek the next i-th character\n        #   self.prefix(l=1)     # peek the next l characters\n        #   self.forward(l=1)    # read the next l characters and move the pointer.\n\n        # Had we reached the end of the stream?\n        self.done = False\n\n        # The number of unclosed '{' and '['. `flow_level == 0` means block\n        # context.\n        self.flow_level = 0\n\n        # List of processed tokens that are not yet emitted.\n        self.tokens = []\n\n        # Add the STREAM-START token.\n        self.fetch_stream_start()\n\n        # Number of tokens that were emitted through the `get_token` method.\n        self.tokens_taken = 0\n\n        # The current indentation level.\n        self.indent = -1\n\n        # Past indentation levels.\n        self.indents = []\n\n        # Variables related to simple keys treatment.\n\n        # A simple key is a key that is not denoted by the '?' indicator.\n        # Example of simple keys:\n        #   ---\n        #   block simple key: value\n        #   ? not a simple key:\n        #   : { flow simple key: value }\n        # We emit the KEY token before all keys, so when we find a potential\n        # simple key, we try to locate the corresponding ':' indicator.\n        # Simple keys should be limited to a single line and 1024 characters.\n\n        # Can a simple key start at the current position? A simple key may\n        # start:\n        # - at the beginning of the line, not counting indentation spaces\n        #       (in block context),\n        # - after '{', '[', ',' (in the flow context),\n        # - after '?', ':', '-' (in the block context).\n        # In the block context, this flag also signifies if a block collection\n        # may start at the current position.\n        self.allow_simple_key = True\n\n        # Keep track of possible simple keys. This is a dictionary. The key\n        # is `flow_level`; there can be no more that one possible simple key\n        # for each level. The value is a SimpleKey record:\n        #   (token_number, required, index, line, column, mark)\n        # A simple key may start with ALIAS, ANCHOR, TAG, SCALAR(flow),\n        # '[', or '{' tokens.\n        self.possible_simple_keys = {}\n\n    # Public methods.\n\n    def check_token(self, *choices):\n        # Check if the next token is one of the given types.\n        while self.need_more_tokens():\n            self.fetch_more_tokens()\n        if self.tokens:\n            if not choices:\n                return True\n            for choice in choices:\n                if isinstance(self.tokens[0], choice):\n                    return True\n        return False\n\n    def peek_token(self):\n        # Return the next token, but do not delete if from the queue.\n        # Return None if no more tokens.\n        while self.need_more_tokens():\n            self.fetch_more_tokens()\n        if self.tokens:\n            return self.tokens[0]\n        else:\n            return None\n\n    def get_token(self):\n        # Return the next token.\n        while self.need_more_tokens():\n            self.fetch_more_tokens()\n        if self.tokens:\n            self.tokens_taken += 1\n            return self.tokens.pop(0)\n\n    # Private methods.\n\n    def need_more_tokens(self):\n        if self.done:\n            return False\n        if not self.tokens:\n            return True\n        # The current token may be a potential simple key, so we\n        # need to look further.\n        self.stale_possible_simple_keys()\n        if self.next_possible_simple_key() == self.tokens_taken:\n            return True\n\n    def fetch_more_tokens(self):\n\n        # Eat whitespaces and comments until we reach the next token.\n        self.scan_to_next_token()\n\n        # Remove obsolete possible simple keys.\n        self.stale_possible_simple_keys()\n\n        # Compare the current indentation and column. It may add some tokens\n        # and decrease the current indentation level.\n        self.unwind_indent(self.column)\n\n        # Peek the next character.\n        ch = self.peek()\n\n        # Is it the end of stream?\n        if ch == '\\0':\n            return self.fetch_stream_end()\n\n        # Is it a directive?\n        if ch == '%' and self.check_directive():\n            return self.fetch_directive()\n\n        # Is it the document start?\n        if ch == '-' and self.check_document_start():\n            return self.fetch_document_start()\n\n        # Is it the document end?\n        if ch == '.' and self.check_document_end():\n            return self.fetch_document_end()\n\n        # TODO: support for BOM within a stream.\n        #if ch == '\\uFEFF':\n        #    return self.fetch_bom()    <-- issue BOMToken\n\n        # Note: the order of the following checks is NOT significant.\n\n        # Is it the flow sequence start indicator?\n        if ch == '[':\n            return self.fetch_flow_sequence_start()\n\n        # Is it the flow mapping start indicator?\n        if ch == '{':\n            return self.fetch_flow_mapping_start()\n\n        # Is it the flow sequence end indicator?\n        if ch == ']':\n            return self.fetch_flow_sequence_end()\n\n        # Is it the flow mapping end indicator?\n        if ch == '}':\n            return self.fetch_flow_mapping_end()\n\n        # Is it the flow entry indicator?\n        if ch == ',':\n            return self.fetch_flow_entry()\n\n        # Is it the block entry indicator?\n        if ch == '-' and self.check_block_entry():\n            return self.fetch_block_entry()\n\n        # Is it the key indicator?\n        if ch == '?' and self.check_key():\n            return self.fetch_key()\n\n        # Is it the value indicator?\n        if ch == ':' and self.check_value():\n            return self.fetch_value()\n\n        # Is it an alias?\n        if ch == '*':\n            return self.fetch_alias()\n\n        # Is it an anchor?\n        if ch == '&':\n            return self.fetch_anchor()\n\n        # Is it a tag?\n        if ch == '!':\n            return self.fetch_tag()\n\n        # Is it a literal scalar?\n        if ch == '|' and not self.flow_level:\n            return self.fetch_literal()\n\n        # Is it a folded scalar?\n        if ch == '>' and not self.flow_level:\n            return self.fetch_folded()\n\n        # Is it a single quoted scalar?\n        if ch == '\\'':\n            return self.fetch_single()\n\n        # Is it a double quoted scalar?\n        if ch == '\\\"':\n            return self.fetch_double()\n\n        # It must be a plain scalar then.\n        if self.check_plain():\n            return self.fetch_plain()\n\n        # No? It's an error. Let's produce a nice error message.\n        raise ScannerError(\"while scanning for the next token\", None,\n                \"found character %r that cannot start any token\" % ch,\n                self.get_mark())\n\n    # Simple keys treatment.\n\n    def next_possible_simple_key(self):\n        # Return the number of the nearest possible simple key. Actually we\n        # don't need to loop through the whole dictionary. We may replace it\n        # with the following code:\n        #   if not self.possible_simple_keys:\n        #       return None\n        #   return self.possible_simple_keys[\n        #           min(self.possible_simple_keys.keys())].token_number\n        min_token_number = None\n        for level in self.possible_simple_keys:\n            key = self.possible_simple_keys[level]\n            if min_token_number is None or key.token_number < min_token_number:\n                min_token_number = key.token_number\n        return min_token_number\n\n    def stale_possible_simple_keys(self):\n        # Remove entries that are no longer possible simple keys. According to\n        # the YAML specification, simple keys\n        # - should be limited to a single line,\n        # - should be no longer than 1024 characters.\n        # Disabling this procedure will allow simple keys of any length and\n        # height (may cause problems if indentation is broken though).\n        for level in list(self.possible_simple_keys):\n            key = self.possible_simple_keys[level]\n            if key.line != self.line  \\\n                    or self.index-key.index > 1024:\n                if key.required:\n                    raise ScannerError(\"while scanning a simple key\", key.mark,\n                            \"could not find expected ':'\", self.get_mark())\n                del self.possible_simple_keys[level]\n\n    def save_possible_simple_key(self):\n        # The next token may start a simple key. We check if it's possible\n        # and save its position. This function is called for\n        #   ALIAS, ANCHOR, TAG, SCALAR(flow), '[', and '{'.\n\n        # Check if a simple key is required at the current position.\n        required = not self.flow_level and self.indent == self.column\n\n        # The next token might be a simple key. Let's save it's number and\n        # position.\n        if self.allow_simple_key:\n            self.remove_possible_simple_key()\n            token_number = self.tokens_taken+len(self.tokens)\n            key = SimpleKey(token_number, required,\n                    self.index, self.line, self.column, self.get_mark())\n            self.possible_simple_keys[self.flow_level] = key\n\n    def remove_possible_simple_key(self):\n        # Remove the saved possible key position at the current flow level.\n        if self.flow_level in self.possible_simple_keys:\n            key = self.possible_simple_keys[self.flow_level]\n            \n            if key.required:\n                raise ScannerError(\"while scanning a simple key\", key.mark,\n                        \"could not find expected ':'\", self.get_mark())\n\n            del self.possible_simple_keys[self.flow_level]\n\n    # Indentation functions.\n\n    def unwind_indent(self, column):\n\n        ## In flow context, tokens should respect indentation.\n        ## Actually the condition should be `self.indent >= column` according to\n        ## the spec. But this condition will prohibit intuitively correct\n        ## constructions such as\n        ## key : {\n        ## }\n        #if self.flow_level and self.indent > column:\n        #    raise ScannerError(None, None,\n        #            \"invalid indentation or unclosed '[' or '{'\",\n        #            self.get_mark())\n\n        # In the flow context, indentation is ignored. We make the scanner less\n        # restrictive then specification requires.\n        if self.flow_level:\n            return\n\n        # In block context, we may need to issue the BLOCK-END tokens.\n        while self.indent > column:\n            mark = self.get_mark()\n            self.indent = self.indents.pop()\n            self.tokens.append(BlockEndToken(mark, mark))\n\n    def add_indent(self, column):\n        # Check if we need to increase indentation.\n        if self.indent < column:\n            self.indents.append(self.indent)\n            self.indent = column\n            return True\n        return False\n\n    # Fetchers.\n\n    def fetch_stream_start(self):\n        # We always add STREAM-START as the first token and STREAM-END as the\n        # last token.\n\n        # Read the token.\n        mark = self.get_mark()\n        \n        # Add STREAM-START.\n        self.tokens.append(StreamStartToken(mark, mark,\n            encoding=self.encoding))\n        \n\n    def fetch_stream_end(self):\n\n        # Set the current indentation to -1.\n        self.unwind_indent(-1)\n\n        # Reset simple keys.\n        self.remove_possible_simple_key()\n        self.allow_simple_key = False\n        self.possible_simple_keys = {}\n\n        # Read the token.\n        mark = self.get_mark()\n        \n        # Add STREAM-END.\n        self.tokens.append(StreamEndToken(mark, mark))\n\n        # The steam is finished.\n        self.done = True\n\n    def fetch_directive(self):\n        \n        # Set the current indentation to -1.\n        self.unwind_indent(-1)\n\n        # Reset simple keys.\n        self.remove_possible_simple_key()\n        self.allow_simple_key = False\n\n        # Scan and add DIRECTIVE.\n        self.tokens.append(self.scan_directive())\n\n    def fetch_document_start(self):\n        self.fetch_document_indicator(DocumentStartToken)\n\n    def fetch_document_end(self):\n        self.fetch_document_indicator(DocumentEndToken)\n\n    def fetch_document_indicator(self, TokenClass):\n\n        # Set the current indentation to -1.\n        self.unwind_indent(-1)\n\n        # Reset simple keys. Note that there could not be a block collection\n        # after '---'.\n        self.remove_possible_simple_key()\n        self.allow_simple_key = False\n\n        # Add DOCUMENT-START or DOCUMENT-END.\n        start_mark = self.get_mark()\n        self.forward(3)\n        end_mark = self.get_mark()\n        self.tokens.append(TokenClass(start_mark, end_mark))\n\n    def fetch_flow_sequence_start(self):\n        self.fetch_flow_collection_start(FlowSequenceStartToken)\n\n    def fetch_flow_mapping_start(self):\n        self.fetch_flow_collection_start(FlowMappingStartToken)\n\n    def fetch_flow_collection_start(self, TokenClass):\n\n        # '[' and '{' may start a simple key.\n        self.save_possible_simple_key()\n\n        # Increase the flow level.\n        self.flow_level += 1\n\n        # Simple keys are allowed after '[' and '{'.\n        self.allow_simple_key = True\n\n        # Add FLOW-SEQUENCE-START or FLOW-MAPPING-START.\n        start_mark = self.get_mark()\n        self.forward()\n        end_mark = self.get_mark()\n        self.tokens.append(TokenClass(start_mark, end_mark))\n\n    def fetch_flow_sequence_end(self):\n        self.fetch_flow_collection_end(FlowSequenceEndToken)\n\n    def fetch_flow_mapping_end(self):\n        self.fetch_flow_collection_end(FlowMappingEndToken)\n\n    def fetch_flow_collection_end(self, TokenClass):\n\n        # Reset possible simple key on the current level.\n        self.remove_possible_simple_key()\n\n        # Decrease the flow level.\n        self.flow_level -= 1\n\n        # No simple keys after ']' or '}'.\n        self.allow_simple_key = False\n\n        # Add FLOW-SEQUENCE-END or FLOW-MAPPING-END.\n        start_mark = self.get_mark()\n        self.forward()\n        end_mark = self.get_mark()\n        self.tokens.append(TokenClass(start_mark, end_mark))\n\n    def fetch_flow_entry(self):\n\n        # Simple keys are allowed after ','.\n        self.allow_simple_key = True\n\n        # Reset possible simple key on the current level.\n        self.remove_possible_simple_key()\n\n        # Add FLOW-ENTRY.\n        start_mark = self.get_mark()\n        self.forward()\n        end_mark = self.get_mark()\n        self.tokens.append(FlowEntryToken(start_mark, end_mark))\n\n    def fetch_block_entry(self):\n\n        # Block context needs additional checks.\n        if not self.flow_level:\n\n            # Are we allowed to start a new entry?\n            if not self.allow_simple_key:\n                raise ScannerError(None, None,\n                        \"sequence entries are not allowed here\",\n                        self.get_mark())\n\n            # We may need to add BLOCK-SEQUENCE-START.\n            if self.add_indent(self.column):\n                mark = self.get_mark()\n                self.tokens.append(BlockSequenceStartToken(mark, mark))\n\n        # It's an error for the block entry to occur in the flow context,\n        # but we let the parser detect this.\n        else:\n            pass\n\n        # Simple keys are allowed after '-'.\n        self.allow_simple_key = True\n\n        # Reset possible simple key on the current level.\n        self.remove_possible_simple_key()\n\n        # Add BLOCK-ENTRY.\n        start_mark = self.get_mark()\n        self.forward()\n        end_mark = self.get_mark()\n        self.tokens.append(BlockEntryToken(start_mark, end_mark))\n\n    def fetch_key(self):\n        \n        # Block context needs additional checks.\n        if not self.flow_level:\n\n            # Are we allowed to start a key (not necessary a simple)?\n            if not self.allow_simple_key:\n                raise ScannerError(None, None,\n                        \"mapping keys are not allowed here\",\n                        self.get_mark())\n\n            # We may need to add BLOCK-MAPPING-START.\n            if self.add_indent(self.column):\n                mark = self.get_mark()\n                self.tokens.append(BlockMappingStartToken(mark, mark))\n\n        # Simple keys are allowed after '?' in the block context.\n        self.allow_simple_key = not self.flow_level\n\n        # Reset possible simple key on the current level.\n        self.remove_possible_simple_key()\n\n        # Add KEY.\n        start_mark = self.get_mark()\n        self.forward()\n        end_mark = self.get_mark()\n        self.tokens.append(KeyToken(start_mark, end_mark))\n\n    def fetch_value(self):\n\n        # Do we determine a simple key?\n        if self.flow_level in self.possible_simple_keys:\n\n            # Add KEY.\n            key = self.possible_simple_keys[self.flow_level]\n            del self.possible_simple_keys[self.flow_level]\n            self.tokens.insert(key.token_number-self.tokens_taken,\n                    KeyToken(key.mark, key.mark))\n\n            # If this key starts a new block mapping, we need to add\n            # BLOCK-MAPPING-START.\n            if not self.flow_level:\n                if self.add_indent(key.column):\n                    self.tokens.insert(key.token_number-self.tokens_taken,\n                            BlockMappingStartToken(key.mark, key.mark))\n\n            # There cannot be two simple keys one after another.\n            self.allow_simple_key = False\n\n        # It must be a part of a complex key.\n        else:\n            \n            # Block context needs additional checks.\n            # (Do we really need them? They will be caught by the parser\n            # anyway.)\n            if not self.flow_level:\n\n                # We are allowed to start a complex value if and only if\n                # we can start a simple key.\n                if not self.allow_simple_key:\n                    raise ScannerError(None, None,\n                            \"mapping values are not allowed here\",\n                            self.get_mark())\n\n            # If this value starts a new block mapping, we need to add\n            # BLOCK-MAPPING-START.  It will be detected as an error later by\n            # the parser.\n            if not self.flow_level:\n                if self.add_indent(self.column):\n                    mark = self.get_mark()\n                    self.tokens.append(BlockMappingStartToken(mark, mark))\n\n            # Simple keys are allowed after ':' in the block context.\n            self.allow_simple_key = not self.flow_level\n\n            # Reset possible simple key on the current level.\n            self.remove_possible_simple_key()\n\n        # Add VALUE.\n        start_mark = self.get_mark()\n        self.forward()\n        end_mark = self.get_mark()\n        self.tokens.append(ValueToken(start_mark, end_mark))\n\n    def fetch_alias(self):\n\n        # ALIAS could be a simple key.\n        self.save_possible_simple_key()\n\n        # No simple keys after ALIAS.\n        self.allow_simple_key = False\n\n        # Scan and add ALIAS.\n        self.tokens.append(self.scan_anchor(AliasToken))\n\n    def fetch_anchor(self):\n\n        # ANCHOR could start a simple key.\n        self.save_possible_simple_key()\n\n        # No simple keys after ANCHOR.\n        self.allow_simple_key = False\n\n        # Scan and add ANCHOR.\n        self.tokens.append(self.scan_anchor(AnchorToken))\n\n    def fetch_tag(self):\n\n        # TAG could start a simple key.\n        self.save_possible_simple_key()\n\n        # No simple keys after TAG.\n        self.allow_simple_key = False\n\n        # Scan and add TAG.\n        self.tokens.append(self.scan_tag())\n\n    def fetch_literal(self):\n        self.fetch_block_scalar(style='|')\n\n    def fetch_folded(self):\n        self.fetch_block_scalar(style='>')\n\n    def fetch_block_scalar(self, style):\n\n        # A simple key may follow a block scalar.\n        self.allow_simple_key = True\n\n        # Reset possible simple key on the current level.\n        self.remove_possible_simple_key()\n\n        # Scan and add SCALAR.\n        self.tokens.append(self.scan_block_scalar(style))\n\n    def fetch_single(self):\n        self.fetch_flow_scalar(style='\\'')\n\n    def fetch_double(self):\n        self.fetch_flow_scalar(style='\"')\n\n    def fetch_flow_scalar(self, style):\n\n        # A flow scalar could be a simple key.\n        self.save_possible_simple_key()\n\n        # No simple keys after flow scalars.\n        self.allow_simple_key = False\n\n        # Scan and add SCALAR.\n        self.tokens.append(self.scan_flow_scalar(style))\n\n    def fetch_plain(self):\n\n        # A plain scalar could be a simple key.\n        self.save_possible_simple_key()\n\n        # No simple keys after plain scalars. But note that `scan_plain` will\n        # change this flag if the scan is finished at the beginning of the\n        # line.\n        self.allow_simple_key = False\n\n        # Scan and add SCALAR. May change `allow_simple_key`.\n        self.tokens.append(self.scan_plain())\n\n    # Checkers.\n\n    def check_directive(self):\n\n        # DIRECTIVE:        ^ '%' ...\n        # The '%' indicator is already checked.\n        if self.column == 0:\n            return True\n\n    def check_document_start(self):\n\n        # DOCUMENT-START:   ^ '---' (' '|'\\n')\n        if self.column == 0:\n            if self.prefix(3) == '---'  \\\n                    and self.peek(3) in '\\0 \\t\\r\\n\\x85\\u2028\\u2029':\n                return True\n\n    def check_document_end(self):\n\n        # DOCUMENT-END:     ^ '...' (' '|'\\n')\n        if self.column == 0:\n            if self.prefix(3) == '...'  \\\n                    and self.peek(3) in '\\0 \\t\\r\\n\\x85\\u2028\\u2029':\n                return True\n\n    def check_block_entry(self):\n\n        # BLOCK-ENTRY:      '-' (' '|'\\n')\n        return self.peek(1) in '\\0 \\t\\r\\n\\x85\\u2028\\u2029'\n\n    def check_key(self):\n\n        # KEY(flow context):    '?'\n        if self.flow_level:\n            return True\n\n        # KEY(block context):   '?' (' '|'\\n')\n        else:\n            return self.peek(1) in '\\0 \\t\\r\\n\\x85\\u2028\\u2029'\n\n    def check_value(self):\n\n        # VALUE(flow context):  ':'\n        if self.flow_level:\n            return True\n\n        # VALUE(block context): ':' (' '|'\\n')\n        else:\n            return self.peek(1) in '\\0 \\t\\r\\n\\x85\\u2028\\u2029'\n\n    def check_plain(self):\n\n        # A plain scalar may start with any non-space character except:\n        #   '-', '?', ':', ',', '[', ']', '{', '}',\n        #   '#', '&', '*', '!', '|', '>', '\\'', '\\\"',\n        #   '%', '@', '`'.\n        #\n        # It may also start with\n        #   '-', '?', ':'\n        # if it is followed by a non-space character.\n        #\n        # Note that we limit the last rule to the block context (except the\n        # '-' character) because we want the flow context to be space\n        # independent.\n        ch = self.peek()\n        return ch not in '\\0 \\t\\r\\n\\x85\\u2028\\u2029-?:,[]{}#&*!|>\\'\\\"%@`'  \\\n                or (self.peek(1) not in '\\0 \\t\\r\\n\\x85\\u2028\\u2029'\n                        and (ch == '-' or (not self.flow_level and ch in '?:')))\n\n    # Scanners.\n\n    def scan_to_next_token(self):\n        # We ignore spaces, line breaks and comments.\n        # If we find a line break in the block context, we set the flag\n        # `allow_simple_key` on.\n        # The byte order mark is stripped if it's the first character in the\n        # stream. We do not yet support BOM inside the stream as the\n        # specification requires. Any such mark will be considered as a part\n        # of the document.\n        #\n        # TODO: We need to make tab handling rules more sane. A good rule is\n        #   Tabs cannot precede tokens\n        #   BLOCK-SEQUENCE-START, BLOCK-MAPPING-START, BLOCK-END,\n        #   KEY(block), VALUE(block), BLOCK-ENTRY\n        # So the checking code is\n        #   if <TAB>:\n        #       self.allow_simple_keys = False\n        # We also need to add the check for `allow_simple_keys == True` to\n        # `unwind_indent` before issuing BLOCK-END.\n        # Scanners for block, flow, and plain scalars need to be modified.\n\n        if self.index == 0 and self.peek() == '\\uFEFF':\n            self.forward()\n        found = False\n        while not found:\n            while self.peek() == ' ':\n                self.forward()\n            if self.peek() == '#':\n                while self.peek() not in '\\0\\r\\n\\x85\\u2028\\u2029':\n                    self.forward()\n            if self.scan_line_break():\n                if not self.flow_level:\n                    self.allow_simple_key = True\n            else:\n                found = True\n\n    def scan_directive(self):\n        # See the specification for details.\n        start_mark = self.get_mark()\n        self.forward()\n        name = self.scan_directive_name(start_mark)\n        value = None\n        if name == 'YAML':\n            value = self.scan_yaml_directive_value(start_mark)\n            end_mark = self.get_mark()\n        elif name == 'TAG':\n            value = self.scan_tag_directive_value(start_mark)\n            end_mark = self.get_mark()\n        else:\n            end_mark = self.get_mark()\n            while self.peek() not in '\\0\\r\\n\\x85\\u2028\\u2029':\n                self.forward()\n        self.scan_directive_ignored_line(start_mark)\n        return DirectiveToken(name, value, start_mark, end_mark)\n\n    def scan_directive_name(self, start_mark):\n        # See the specification for details.\n        length = 0\n        ch = self.peek(length)\n        while '0' <= ch <= '9' or 'A' <= ch <= 'Z' or 'a' <= ch <= 'z'  \\\n                or ch in '-_':\n            length += 1\n            ch = self.peek(length)\n        if not length:\n            raise ScannerError(\"while scanning a directive\", start_mark,\n                    \"expected alphabetic or numeric character, but found %r\"\n                    % ch, self.get_mark())\n        value = self.prefix(length)\n        self.forward(length)\n        ch = self.peek()\n        if ch not in '\\0 \\r\\n\\x85\\u2028\\u2029':\n            raise ScannerError(\"while scanning a directive\", start_mark,\n                    \"expected alphabetic or numeric character, but found %r\"\n                    % ch, self.get_mark())\n        return value\n\n    def scan_yaml_directive_value(self, start_mark):\n        # See the specification for details.\n        while self.peek() == ' ':\n            self.forward()\n        major = self.scan_yaml_directive_number(start_mark)\n        if self.peek() != '.':\n            raise ScannerError(\"while scanning a directive\", start_mark,\n                    \"expected a digit or '.', but found %r\" % self.peek(),\n                    self.get_mark())\n        self.forward()\n        minor = self.scan_yaml_directive_number(start_mark)\n        if self.peek() not in '\\0 \\r\\n\\x85\\u2028\\u2029':\n            raise ScannerError(\"while scanning a directive\", start_mark,\n                    \"expected a digit or ' ', but found %r\" % self.peek(),\n                    self.get_mark())\n        return (major, minor)\n\n    def scan_yaml_directive_number(self, start_mark):\n        # See the specification for details.\n        ch = self.peek()\n        if not ('0' <= ch <= '9'):\n            raise ScannerError(\"while scanning a directive\", start_mark,\n                    \"expected a digit, but found %r\" % ch, self.get_mark())\n        length = 0\n        while '0' <= self.peek(length) <= '9':\n            length += 1\n        value = int(self.prefix(length))\n        self.forward(length)\n        return value\n\n    def scan_tag_directive_value(self, start_mark):\n        # See the specification for details.\n        while self.peek() == ' ':\n            self.forward()\n        handle = self.scan_tag_directive_handle(start_mark)\n        while self.peek() == ' ':\n            self.forward()\n        prefix = self.scan_tag_directive_prefix(start_mark)\n        return (handle, prefix)\n\n    def scan_tag_directive_handle(self, start_mark):\n        # See the specification for details.\n        value = self.scan_tag_handle('directive', start_mark)\n        ch = self.peek()\n        if ch != ' ':\n            raise ScannerError(\"while scanning a directive\", start_mark,\n                    \"expected ' ', but found %r\" % ch, self.get_mark())\n        return value\n\n    def scan_tag_directive_prefix(self, start_mark):\n        # See the specification for details.\n        value = self.scan_tag_uri('directive', start_mark)\n        ch = self.peek()\n        if ch not in '\\0 \\r\\n\\x85\\u2028\\u2029':\n            raise ScannerError(\"while scanning a directive\", start_mark,\n                    \"expected ' ', but found %r\" % ch, self.get_mark())\n        return value\n\n    def scan_directive_ignored_line(self, start_mark):\n        # See the specification for details.\n        while self.peek() == ' ':\n            self.forward()\n        if self.peek() == '#':\n            while self.peek() not in '\\0\\r\\n\\x85\\u2028\\u2029':\n                self.forward()\n        ch = self.peek()\n        if ch not in '\\0\\r\\n\\x85\\u2028\\u2029':\n            raise ScannerError(\"while scanning a directive\", start_mark,\n                    \"expected a comment or a line break, but found %r\"\n                        % ch, self.get_mark())\n        self.scan_line_break()\n\n    def scan_anchor(self, TokenClass):\n        # The specification does not restrict characters for anchors and\n        # aliases. This may lead to problems, for instance, the document:\n        #   [ *alias, value ]\n        # can be interpreted in two ways, as\n        #   [ \"value\" ]\n        # and\n        #   [ *alias , \"value\" ]\n        # Therefore we restrict aliases to numbers and ASCII letters.\n        start_mark = self.get_mark()\n        indicator = self.peek()\n        if indicator == '*':\n            name = 'alias'\n        else:\n            name = 'anchor'\n        self.forward()\n        length = 0\n        ch = self.peek(length)\n        while '0' <= ch <= '9' or 'A' <= ch <= 'Z' or 'a' <= ch <= 'z'  \\\n                or ch in '-_':\n            length += 1\n            ch = self.peek(length)\n        if not length:\n            raise ScannerError(\"while scanning an %s\" % name, start_mark,\n                    \"expected alphabetic or numeric character, but found %r\"\n                    % ch, self.get_mark())\n        value = self.prefix(length)\n        self.forward(length)\n        ch = self.peek()\n        if ch not in '\\0 \\t\\r\\n\\x85\\u2028\\u2029?:,]}%@`':\n            raise ScannerError(\"while scanning an %s\" % name, start_mark,\n                    \"expected alphabetic or numeric character, but found %r\"\n                    % ch, self.get_mark())\n        end_mark = self.get_mark()\n        return TokenClass(value, start_mark, end_mark)\n\n    def scan_tag(self):\n        # See the specification for details.\n        start_mark = self.get_mark()\n        ch = self.peek(1)\n        if ch == '<':\n            handle = None\n            self.forward(2)\n            suffix = self.scan_tag_uri('tag', start_mark)\n            if self.peek() != '>':\n                raise ScannerError(\"while parsing a tag\", start_mark,\n                        \"expected '>', but found %r\" % self.peek(),\n                        self.get_mark())\n            self.forward()\n        elif ch in '\\0 \\t\\r\\n\\x85\\u2028\\u2029':\n            handle = None\n            suffix = '!'\n            self.forward()\n        else:\n            length = 1\n            use_handle = False\n            while ch not in '\\0 \\r\\n\\x85\\u2028\\u2029':\n                if ch == '!':\n                    use_handle = True\n                    break\n                length += 1\n                ch = self.peek(length)\n            handle = '!'\n            if use_handle:\n                handle = self.scan_tag_handle('tag', start_mark)\n            else:\n                handle = '!'\n                self.forward()\n            suffix = self.scan_tag_uri('tag', start_mark)\n        ch = self.peek()\n        if ch not in '\\0 \\r\\n\\x85\\u2028\\u2029':\n            raise ScannerError(\"while scanning a tag\", start_mark,\n                    \"expected ' ', but found %r\" % ch, self.get_mark())\n        value = (handle, suffix)\n        end_mark = self.get_mark()\n        return TagToken(value, start_mark, end_mark)\n\n    def scan_block_scalar(self, style):\n        # See the specification for details.\n\n        if style == '>':\n            folded = True\n        else:\n            folded = False\n\n        chunks = []\n        start_mark = self.get_mark()\n\n        # Scan the header.\n        self.forward()\n        chomping, increment = self.scan_block_scalar_indicators(start_mark)\n        self.scan_block_scalar_ignored_line(start_mark)\n\n        # Determine the indentation level and go to the first non-empty line.\n        min_indent = self.indent+1\n        if min_indent < 1:\n            min_indent = 1\n        if increment is None:\n            breaks, max_indent, end_mark = self.scan_block_scalar_indentation()\n            indent = max(min_indent, max_indent)\n        else:\n            indent = min_indent+increment-1\n            breaks, end_mark = self.scan_block_scalar_breaks(indent)\n        line_break = ''\n\n        # Scan the inner part of the block scalar.\n        while self.column == indent and self.peek() != '\\0':\n            chunks.extend(breaks)\n            leading_non_space = self.peek() not in ' \\t'\n            length = 0\n            while self.peek(length) not in '\\0\\r\\n\\x85\\u2028\\u2029':\n                length += 1\n            chunks.append(self.prefix(length))\n            self.forward(length)\n            line_break = self.scan_line_break()\n            breaks, end_mark = self.scan_block_scalar_breaks(indent)\n            if self.column == indent and self.peek() != '\\0':\n\n                # Unfortunately, folding rules are ambiguous.\n                #\n                # This is the folding according to the specification:\n                \n                if folded and line_break == '\\n'    \\\n                        and leading_non_space and self.peek() not in ' \\t':\n                    if not breaks:\n                        chunks.append(' ')\n                else:\n                    chunks.append(line_break)\n                \n                # This is Clark Evans's interpretation (also in the spec\n                # examples):\n                #\n                #if folded and line_break == '\\n':\n                #    if not breaks:\n                #        if self.peek() not in ' \\t':\n                #            chunks.append(' ')\n                #        else:\n                #            chunks.append(line_break)\n                #else:\n                #    chunks.append(line_break)\n            else:\n                break\n\n        # Chomp the tail.\n        if chomping is not False:\n            chunks.append(line_break)\n        if chomping is True:\n            chunks.extend(breaks)\n\n        # We are done.\n        return ScalarToken(''.join(chunks), False, start_mark, end_mark,\n                style)\n\n    def scan_block_scalar_indicators(self, start_mark):\n        # See the specification for details.\n        chomping = None\n        increment = None\n        ch = self.peek()\n        if ch in '+-':\n            if ch == '+':\n                chomping = True\n            else:\n                chomping = False\n            self.forward()\n            ch = self.peek()\n            if ch in '0123456789':\n                increment = int(ch)\n                if increment == 0:\n                    raise ScannerError(\"while scanning a block scalar\", start_mark,\n                            \"expected indentation indicator in the range 1-9, but found 0\",\n                            self.get_mark())\n                self.forward()\n        elif ch in '0123456789':\n            increment = int(ch)\n            if increment == 0:\n                raise ScannerError(\"while scanning a block scalar\", start_mark,\n                        \"expected indentation indicator in the range 1-9, but found 0\",\n                        self.get_mark())\n            self.forward()\n            ch = self.peek()\n            if ch in '+-':\n                if ch == '+':\n                    chomping = True\n                else:\n                    chomping = False\n                self.forward()\n        ch = self.peek()\n        if ch not in '\\0 \\r\\n\\x85\\u2028\\u2029':\n            raise ScannerError(\"while scanning a block scalar\", start_mark,\n                    \"expected chomping or indentation indicators, but found %r\"\n                    % ch, self.get_mark())\n        return chomping, increment\n\n    def scan_block_scalar_ignored_line(self, start_mark):\n        # See the specification for details.\n        while self.peek() == ' ':\n            self.forward()\n        if self.peek() == '#':\n            while self.peek() not in '\\0\\r\\n\\x85\\u2028\\u2029':\n                self.forward()\n        ch = self.peek()\n        if ch not in '\\0\\r\\n\\x85\\u2028\\u2029':\n            raise ScannerError(\"while scanning a block scalar\", start_mark,\n                    \"expected a comment or a line break, but found %r\" % ch,\n                    self.get_mark())\n        self.scan_line_break()\n\n    def scan_block_scalar_indentation(self):\n        # See the specification for details.\n        chunks = []\n        max_indent = 0\n        end_mark = self.get_mark()\n        while self.peek() in ' \\r\\n\\x85\\u2028\\u2029':\n            if self.peek() != ' ':\n                chunks.append(self.scan_line_break())\n                end_mark = self.get_mark()\n            else:\n                self.forward()\n                if self.column > max_indent:\n                    max_indent = self.column\n        return chunks, max_indent, end_mark\n\n    def scan_block_scalar_breaks(self, indent):\n        # See the specification for details.\n        chunks = []\n        end_mark = self.get_mark()\n        while self.column < indent and self.peek() == ' ':\n            self.forward()\n        while self.peek() in '\\r\\n\\x85\\u2028\\u2029':\n            chunks.append(self.scan_line_break())\n            end_mark = self.get_mark()\n            while self.column < indent and self.peek() == ' ':\n                self.forward()\n        return chunks, end_mark\n\n    def scan_flow_scalar(self, style):\n        # See the specification for details.\n        # Note that we loose indentation rules for quoted scalars. Quoted\n        # scalars don't need to adhere indentation because \" and ' clearly\n        # mark the beginning and the end of them. Therefore we are less\n        # restrictive then the specification requires. We only need to check\n        # that document separators are not included in scalars.\n        if style == '\"':\n            double = True\n        else:\n            double = False\n        chunks = []\n        start_mark = self.get_mark()\n        quote = self.peek()\n        self.forward()\n        chunks.extend(self.scan_flow_scalar_non_spaces(double, start_mark))\n        while self.peek() != quote:\n            chunks.extend(self.scan_flow_scalar_spaces(double, start_mark))\n            chunks.extend(self.scan_flow_scalar_non_spaces(double, start_mark))\n        self.forward()\n        end_mark = self.get_mark()\n        return ScalarToken(''.join(chunks), False, start_mark, end_mark,\n                style)\n\n    ESCAPE_REPLACEMENTS = {\n        '0':    '\\0',\n        'a':    '\\x07',\n        'b':    '\\x08',\n        't':    '\\x09',\n        '\\t':   '\\x09',\n        'n':    '\\x0A',\n        'v':    '\\x0B',\n        'f':    '\\x0C',\n        'r':    '\\x0D',\n        'e':    '\\x1B',\n        ' ':    '\\x20',\n        '\\\"':   '\\\"',\n        '\\\\':   '\\\\',\n        '/':    '/',\n        'N':    '\\x85',\n        '_':    '\\xA0',\n        'L':    '\\u2028',\n        'P':    '\\u2029',\n    }\n\n    ESCAPE_CODES = {\n        'x':    2,\n        'u':    4,\n        'U':    8,\n    }\n\n    def scan_flow_scalar_non_spaces(self, double, start_mark):\n        # See the specification for details.\n        chunks = []\n        while True:\n            length = 0\n            while self.peek(length) not in '\\'\\\"\\\\\\0 \\t\\r\\n\\x85\\u2028\\u2029':\n                length += 1\n            if length:\n                chunks.append(self.prefix(length))\n                self.forward(length)\n            ch = self.peek()\n            if not double and ch == '\\'' and self.peek(1) == '\\'':\n                chunks.append('\\'')\n                self.forward(2)\n            elif (double and ch == '\\'') or (not double and ch in '\\\"\\\\'):\n                chunks.append(ch)\n                self.forward()\n            elif double and ch == '\\\\':\n                self.forward()\n                ch = self.peek()\n                if ch in self.ESCAPE_REPLACEMENTS:\n                    chunks.append(self.ESCAPE_REPLACEMENTS[ch])\n                    self.forward()\n                elif ch in self.ESCAPE_CODES:\n                    length = self.ESCAPE_CODES[ch]\n                    self.forward()\n                    for k in range(length):\n                        if self.peek(k) not in '0123456789ABCDEFabcdef':\n                            raise ScannerError(\"while scanning a double-quoted scalar\", start_mark,\n                                    \"expected escape sequence of %d hexadecimal numbers, but found %r\" %\n                                        (length, self.peek(k)), self.get_mark())\n                    code = int(self.prefix(length), 16)\n                    chunks.append(chr(code))\n                    self.forward(length)\n                elif ch in '\\r\\n\\x85\\u2028\\u2029':\n                    self.scan_line_break()\n                    chunks.extend(self.scan_flow_scalar_breaks(double, start_mark))\n                else:\n                    raise ScannerError(\"while scanning a double-quoted scalar\", start_mark,\n                            \"found unknown escape character %r\" % ch, self.get_mark())\n            else:\n                return chunks\n\n    def scan_flow_scalar_spaces(self, double, start_mark):\n        # See the specification for details.\n        chunks = []\n        length = 0\n        while self.peek(length) in ' \\t':\n            length += 1\n        whitespaces = self.prefix(length)\n        self.forward(length)\n        ch = self.peek()\n        if ch == '\\0':\n            raise ScannerError(\"while scanning a quoted scalar\", start_mark,\n                    \"found unexpected end of stream\", self.get_mark())\n        elif ch in '\\r\\n\\x85\\u2028\\u2029':\n            line_break = self.scan_line_break()\n            breaks = self.scan_flow_scalar_breaks(double, start_mark)\n            if line_break != '\\n':\n                chunks.append(line_break)\n            elif not breaks:\n                chunks.append(' ')\n            chunks.extend(breaks)\n        else:\n            chunks.append(whitespaces)\n        return chunks\n\n    def scan_flow_scalar_breaks(self, double, start_mark):\n        # See the specification for details.\n        chunks = []\n        while True:\n            # Instead of checking indentation, we check for document\n            # separators.\n            prefix = self.prefix(3)\n            if (prefix == '---' or prefix == '...')   \\\n                    and self.peek(3) in '\\0 \\t\\r\\n\\x85\\u2028\\u2029':\n                raise ScannerError(\"while scanning a quoted scalar\", start_mark,\n                        \"found unexpected document separator\", self.get_mark())\n            while self.peek() in ' \\t':\n                self.forward()\n            if self.peek() in '\\r\\n\\x85\\u2028\\u2029':\n                chunks.append(self.scan_line_break())\n            else:\n                return chunks\n\n    def scan_plain(self):\n        # See the specification for details.\n        # We add an additional restriction for the flow context:\n        #   plain scalars in the flow context cannot contain ',' or '?'.\n        # We also keep track of the `allow_simple_key` flag here.\n        # Indentation rules are loosed for the flow context.\n        chunks = []\n        start_mark = self.get_mark()\n        end_mark = start_mark\n        indent = self.indent+1\n        # We allow zero indentation for scalars, but then we need to check for\n        # document separators at the beginning of the line.\n        #if indent == 0:\n        #    indent = 1\n        spaces = []\n        while True:\n            length = 0\n            if self.peek() == '#':\n                break\n            while True:\n                ch = self.peek(length)\n                if ch in '\\0 \\t\\r\\n\\x85\\u2028\\u2029'    \\\n                        or (ch == ':' and\n                                self.peek(length+1) in '\\0 \\t\\r\\n\\x85\\u2028\\u2029'\n                                      + (u',[]{}' if self.flow_level else u''))\\\n                        or (self.flow_level and ch in ',?[]{}'):\n                    break\n                length += 1\n            if length == 0:\n                break\n            self.allow_simple_key = False\n            chunks.extend(spaces)\n            chunks.append(self.prefix(length))\n            self.forward(length)\n            end_mark = self.get_mark()\n            spaces = self.scan_plain_spaces(indent, start_mark)\n            if not spaces or self.peek() == '#' \\\n                    or (not self.flow_level and self.column < indent):\n                break\n        return ScalarToken(''.join(chunks), True, start_mark, end_mark)\n\n    def scan_plain_spaces(self, indent, start_mark):\n        # See the specification for details.\n        # The specification is really confusing about tabs in plain scalars.\n        # We just forbid them completely. Do not use tabs in YAML!\n        chunks = []\n        length = 0\n        while self.peek(length) in ' ':\n            length += 1\n        whitespaces = self.prefix(length)\n        self.forward(length)\n        ch = self.peek()\n        if ch in '\\r\\n\\x85\\u2028\\u2029':\n            line_break = self.scan_line_break()\n            self.allow_simple_key = True\n            prefix = self.prefix(3)\n            if (prefix == '---' or prefix == '...')   \\\n                    and self.peek(3) in '\\0 \\t\\r\\n\\x85\\u2028\\u2029':\n                return\n            breaks = []\n            while self.peek() in ' \\r\\n\\x85\\u2028\\u2029':\n                if self.peek() == ' ':\n                    self.forward()\n                else:\n                    breaks.append(self.scan_line_break())\n                    prefix = self.prefix(3)\n                    if (prefix == '---' or prefix == '...')   \\\n                            and self.peek(3) in '\\0 \\t\\r\\n\\x85\\u2028\\u2029':\n                        return\n            if line_break != '\\n':\n                chunks.append(line_break)\n            elif not breaks:\n                chunks.append(' ')\n            chunks.extend(breaks)\n        elif whitespaces:\n            chunks.append(whitespaces)\n        return chunks\n\n    def scan_tag_handle(self, name, start_mark):\n        # See the specification for details.\n        # For some strange reasons, the specification does not allow '_' in\n        # tag handles. I have allowed it anyway.\n        ch = self.peek()\n        if ch != '!':\n            raise ScannerError(\"while scanning a %s\" % name, start_mark,\n                    \"expected '!', but found %r\" % ch, self.get_mark())\n        length = 1\n        ch = self.peek(length)\n        if ch != ' ':\n            while '0' <= ch <= '9' or 'A' <= ch <= 'Z' or 'a' <= ch <= 'z'  \\\n                    or ch in '-_':\n                length += 1\n                ch = self.peek(length)\n            if ch != '!':\n                self.forward(length)\n                raise ScannerError(\"while scanning a %s\" % name, start_mark,\n                        \"expected '!', but found %r\" % ch, self.get_mark())\n            length += 1\n        value = self.prefix(length)\n        self.forward(length)\n        return value\n\n    def scan_tag_uri(self, name, start_mark):\n        # See the specification for details.\n        # Note: we do not check if URI is well-formed.\n        chunks = []\n        length = 0\n        ch = self.peek(length)\n        while '0' <= ch <= '9' or 'A' <= ch <= 'Z' or 'a' <= ch <= 'z'  \\\n                or ch in '-;/?:@&=+$,_.!~*\\'()[]%':\n            if ch == '%':\n                chunks.append(self.prefix(length))\n                self.forward(length)\n                length = 0\n                chunks.append(self.scan_uri_escapes(name, start_mark))\n            else:\n                length += 1\n            ch = self.peek(length)\n        if length:\n            chunks.append(self.prefix(length))\n            self.forward(length)\n            length = 0\n        if not chunks:\n            raise ScannerError(\"while parsing a %s\" % name, start_mark,\n                    \"expected URI, but found %r\" % ch, self.get_mark())\n        return ''.join(chunks)\n\n    def scan_uri_escapes(self, name, start_mark):\n        # See the specification for details.\n        codes = []\n        mark = self.get_mark()\n        while self.peek() == '%':\n            self.forward()\n            for k in range(2):\n                if self.peek(k) not in '0123456789ABCDEFabcdef':\n                    raise ScannerError(\"while scanning a %s\" % name, start_mark,\n                            \"expected URI escape sequence of 2 hexadecimal numbers, but found %r\"\n                            % self.peek(k), self.get_mark())\n            codes.append(int(self.prefix(2), 16))\n            self.forward(2)\n        try:\n            value = bytes(codes).decode('utf-8')\n        except UnicodeDecodeError as exc:\n            raise ScannerError(\"while scanning a %s\" % name, start_mark, str(exc), mark)\n        return value\n\n    def scan_line_break(self):\n        # Transforms:\n        #   '\\r\\n'      :   '\\n'\n        #   '\\r'        :   '\\n'\n        #   '\\n'        :   '\\n'\n        #   '\\x85'      :   '\\n'\n        #   '\\u2028'    :   '\\u2028'\n        #   '\\u2029     :   '\\u2029'\n        #   default     :   ''\n        ch = self.peek()\n        if ch in '\\r\\n\\x85':\n            if self.prefix(2) == '\\r\\n':\n                self.forward(2)\n            else:\n                self.forward()\n            return '\\n'\n        elif ch in '\\u2028\\u2029':\n            self.forward()\n            return ch\n        return ''\n", "lib/yaml/representer.py": "\n__all__ = ['BaseRepresenter', 'SafeRepresenter', 'Representer',\n    'RepresenterError']\n\nfrom .error import *\nfrom .nodes import *\n\nimport datetime, copyreg, types, base64, collections\n\nclass RepresenterError(YAMLError):\n    pass\n\nclass BaseRepresenter:\n\n    yaml_representers = {}\n    yaml_multi_representers = {}\n\n    def __init__(self, default_style=None, default_flow_style=False, sort_keys=True):\n        self.default_style = default_style\n        self.sort_keys = sort_keys\n        self.default_flow_style = default_flow_style\n        self.represented_objects = {}\n        self.object_keeper = []\n        self.alias_key = None\n\n    def represent(self, data):\n        node = self.represent_data(data)\n        self.serialize(node)\n        self.represented_objects = {}\n        self.object_keeper = []\n        self.alias_key = None\n\n    def represent_data(self, data):\n        if self.ignore_aliases(data):\n            self.alias_key = None\n        else:\n            self.alias_key = id(data)\n        if self.alias_key is not None:\n            if self.alias_key in self.represented_objects:\n                node = self.represented_objects[self.alias_key]\n                #if node is None:\n                #    raise RepresenterError(\"recursive objects are not allowed: %r\" % data)\n                return node\n            #self.represented_objects[alias_key] = None\n            self.object_keeper.append(data)\n        data_types = type(data).__mro__\n        if data_types[0] in self.yaml_representers:\n            node = self.yaml_representers[data_types[0]](self, data)\n        else:\n            for data_type in data_types:\n                if data_type in self.yaml_multi_representers:\n                    node = self.yaml_multi_representers[data_type](self, data)\n                    break\n            else:\n                if None in self.yaml_multi_representers:\n                    node = self.yaml_multi_representers[None](self, data)\n                elif None in self.yaml_representers:\n                    node = self.yaml_representers[None](self, data)\n                else:\n                    node = ScalarNode(None, str(data))\n        #if alias_key is not None:\n        #    self.represented_objects[alias_key] = node\n        return node\n\n    @classmethod\n    def add_representer(cls, data_type, representer):\n        if not 'yaml_representers' in cls.__dict__:\n            cls.yaml_representers = cls.yaml_representers.copy()\n        cls.yaml_representers[data_type] = representer\n\n    @classmethod\n    def add_multi_representer(cls, data_type, representer):\n        if not 'yaml_multi_representers' in cls.__dict__:\n            cls.yaml_multi_representers = cls.yaml_multi_representers.copy()\n        cls.yaml_multi_representers[data_type] = representer\n\n    def represent_scalar(self, tag, value, style=None):\n        if style is None:\n            style = self.default_style\n        node = ScalarNode(tag, value, style=style)\n        if self.alias_key is not None:\n            self.represented_objects[self.alias_key] = node\n        return node\n\n    def represent_sequence(self, tag, sequence, flow_style=None):\n        value = []\n        node = SequenceNode(tag, value, flow_style=flow_style)\n        if self.alias_key is not None:\n            self.represented_objects[self.alias_key] = node\n        best_style = True\n        for item in sequence:\n            node_item = self.represent_data(item)\n            if not (isinstance(node_item, ScalarNode) and not node_item.style):\n                best_style = False\n            value.append(node_item)\n        if flow_style is None:\n            if self.default_flow_style is not None:\n                node.flow_style = self.default_flow_style\n            else:\n                node.flow_style = best_style\n        return node\n\n    def represent_mapping(self, tag, mapping, flow_style=None):\n        value = []\n        node = MappingNode(tag, value, flow_style=flow_style)\n        if self.alias_key is not None:\n            self.represented_objects[self.alias_key] = node\n        best_style = True\n        if hasattr(mapping, 'items'):\n            mapping = list(mapping.items())\n            if self.sort_keys:\n                try:\n                    mapping = sorted(mapping)\n                except TypeError:\n                    pass\n        for item_key, item_value in mapping:\n            node_key = self.represent_data(item_key)\n            node_value = self.represent_data(item_value)\n            if not (isinstance(node_key, ScalarNode) and not node_key.style):\n                best_style = False\n            if not (isinstance(node_value, ScalarNode) and not node_value.style):\n                best_style = False\n            value.append((node_key, node_value))\n        if flow_style is None:\n            if self.default_flow_style is not None:\n                node.flow_style = self.default_flow_style\n            else:\n                node.flow_style = best_style\n        return node\n\n    def ignore_aliases(self, data):\n        return False\n\nclass SafeRepresenter(BaseRepresenter):\n\n    def ignore_aliases(self, data):\n        if data is None:\n            return True\n        if isinstance(data, tuple) and data == ():\n            return True\n        if isinstance(data, (str, bytes, bool, int, float)):\n            return True\n\n    def represent_none(self, data):\n        return self.represent_scalar('tag:yaml.org,2002:null', 'null')\n\n    def represent_str(self, data):\n        return self.represent_scalar('tag:yaml.org,2002:str', data)\n\n    def represent_binary(self, data):\n        if hasattr(base64, 'encodebytes'):\n            data = base64.encodebytes(data).decode('ascii')\n        else:\n            data = base64.encodestring(data).decode('ascii')\n        return self.represent_scalar('tag:yaml.org,2002:binary', data, style='|')\n\n    def represent_bool(self, data):\n        if data:\n            value = 'true'\n        else:\n            value = 'false'\n        return self.represent_scalar('tag:yaml.org,2002:bool', value)\n\n    def represent_int(self, data):\n        return self.represent_scalar('tag:yaml.org,2002:int', str(data))\n\n    inf_value = 1e300\n    while repr(inf_value) != repr(inf_value*inf_value):\n        inf_value *= inf_value\n\n    def represent_float(self, data):\n        if data != data or (data == 0.0 and data == 1.0):\n            value = '.nan'\n        elif data == self.inf_value:\n            value = '.inf'\n        elif data == -self.inf_value:\n            value = '-.inf'\n        else:\n            value = repr(data).lower()\n            # Note that in some cases `repr(data)` represents a float number\n            # without the decimal parts.  For instance:\n            #   >>> repr(1e17)\n            #   '1e17'\n            # Unfortunately, this is not a valid float representation according\n            # to the definition of the `!!float` tag.  We fix this by adding\n            # '.0' before the 'e' symbol.\n            if '.' not in value and 'e' in value:\n                value = value.replace('e', '.0e', 1)\n        return self.represent_scalar('tag:yaml.org,2002:float', value)\n\n    def represent_list(self, data):\n        #pairs = (len(data) > 0 and isinstance(data, list))\n        #if pairs:\n        #    for item in data:\n        #        if not isinstance(item, tuple) or len(item) != 2:\n        #            pairs = False\n        #            break\n        #if not pairs:\n            return self.represent_sequence('tag:yaml.org,2002:seq', data)\n        #value = []\n        #for item_key, item_value in data:\n        #    value.append(self.represent_mapping(u'tag:yaml.org,2002:map',\n        #        [(item_key, item_value)]))\n        #return SequenceNode(u'tag:yaml.org,2002:pairs', value)\n\n    def represent_dict(self, data):\n        return self.represent_mapping('tag:yaml.org,2002:map', data)\n\n    def represent_set(self, data):\n        value = {}\n        for key in data:\n            value[key] = None\n        return self.represent_mapping('tag:yaml.org,2002:set', value)\n\n    def represent_date(self, data):\n        value = data.isoformat()\n        return self.represent_scalar('tag:yaml.org,2002:timestamp', value)\n\n    def represent_datetime(self, data):\n        value = data.isoformat(' ')\n        return self.represent_scalar('tag:yaml.org,2002:timestamp', value)\n\n    def represent_yaml_object(self, tag, data, cls, flow_style=None):\n        if hasattr(data, '__getstate__'):\n            state = data.__getstate__()\n        else:\n            state = data.__dict__.copy()\n        return self.represent_mapping(tag, state, flow_style=flow_style)\n\n    def represent_undefined(self, data):\n        raise RepresenterError(\"cannot represent an object\", data)\n\nSafeRepresenter.add_representer(type(None),\n        SafeRepresenter.represent_none)\n\nSafeRepresenter.add_representer(str,\n        SafeRepresenter.represent_str)\n\nSafeRepresenter.add_representer(bytes,\n        SafeRepresenter.represent_binary)\n\nSafeRepresenter.add_representer(bool,\n        SafeRepresenter.represent_bool)\n\nSafeRepresenter.add_representer(int,\n        SafeRepresenter.represent_int)\n\nSafeRepresenter.add_representer(float,\n        SafeRepresenter.represent_float)\n\nSafeRepresenter.add_representer(list,\n        SafeRepresenter.represent_list)\n\nSafeRepresenter.add_representer(tuple,\n        SafeRepresenter.represent_list)\n\nSafeRepresenter.add_representer(dict,\n        SafeRepresenter.represent_dict)\n\nSafeRepresenter.add_representer(set,\n        SafeRepresenter.represent_set)\n\nSafeRepresenter.add_representer(datetime.date,\n        SafeRepresenter.represent_date)\n\nSafeRepresenter.add_representer(datetime.datetime,\n        SafeRepresenter.represent_datetime)\n\nSafeRepresenter.add_representer(None,\n        SafeRepresenter.represent_undefined)\n\nclass Representer(SafeRepresenter):\n\n    def represent_complex(self, data):\n        if data.imag == 0.0:\n            data = '%r' % data.real\n        elif data.real == 0.0:\n            data = '%rj' % data.imag\n        elif data.imag > 0:\n            data = '%r+%rj' % (data.real, data.imag)\n        else:\n            data = '%r%rj' % (data.real, data.imag)\n        return self.represent_scalar('tag:yaml.org,2002:python/complex', data)\n\n    def represent_tuple(self, data):\n        return self.represent_sequence('tag:yaml.org,2002:python/tuple', data)\n\n    def represent_name(self, data):\n        name = '%s.%s' % (data.__module__, data.__name__)\n        return self.represent_scalar('tag:yaml.org,2002:python/name:'+name, '')\n\n    def represent_module(self, data):\n        return self.represent_scalar(\n                'tag:yaml.org,2002:python/module:'+data.__name__, '')\n\n    def represent_object(self, data):\n        # We use __reduce__ API to save the data. data.__reduce__ returns\n        # a tuple of length 2-5:\n        #   (function, args, state, listitems, dictitems)\n\n        # For reconstructing, we calls function(*args), then set its state,\n        # listitems, and dictitems if they are not None.\n\n        # A special case is when function.__name__ == '__newobj__'. In this\n        # case we create the object with args[0].__new__(*args).\n\n        # Another special case is when __reduce__ returns a string - we don't\n        # support it.\n\n        # We produce a !!python/object, !!python/object/new or\n        # !!python/object/apply node.\n\n        cls = type(data)\n        if cls in copyreg.dispatch_table:\n            reduce = copyreg.dispatch_table[cls](data)\n        elif hasattr(data, '__reduce_ex__'):\n            reduce = data.__reduce_ex__(2)\n        elif hasattr(data, '__reduce__'):\n            reduce = data.__reduce__()\n        else:\n            raise RepresenterError(\"cannot represent an object\", data)\n        reduce = (list(reduce)+[None]*5)[:5]\n        function, args, state, listitems, dictitems = reduce\n        args = list(args)\n        if state is None:\n            state = {}\n        if listitems is not None:\n            listitems = list(listitems)\n        if dictitems is not None:\n            dictitems = dict(dictitems)\n        if function.__name__ == '__newobj__':\n            function = args[0]\n            args = args[1:]\n            tag = 'tag:yaml.org,2002:python/object/new:'\n            newobj = True\n        else:\n            tag = 'tag:yaml.org,2002:python/object/apply:'\n            newobj = False\n        function_name = '%s.%s' % (function.__module__, function.__name__)\n        if not args and not listitems and not dictitems \\\n                and isinstance(state, dict) and newobj:\n            return self.represent_mapping(\n                    'tag:yaml.org,2002:python/object:'+function_name, state)\n        if not listitems and not dictitems  \\\n                and isinstance(state, dict) and not state:\n            return self.represent_sequence(tag+function_name, args)\n        value = {}\n        if args:\n            value['args'] = args\n        if state or not isinstance(state, dict):\n            value['state'] = state\n        if listitems:\n            value['listitems'] = listitems\n        if dictitems:\n            value['dictitems'] = dictitems\n        return self.represent_mapping(tag+function_name, value)\n\n    def represent_ordered_dict(self, data):\n        # Provide uniform representation across different Python versions.\n        data_type = type(data)\n        tag = 'tag:yaml.org,2002:python/object/apply:%s.%s' \\\n                % (data_type.__module__, data_type.__name__)\n        items = [[key, value] for key, value in data.items()]\n        return self.represent_sequence(tag, [items])\n\nRepresenter.add_representer(complex,\n        Representer.represent_complex)\n\nRepresenter.add_representer(tuple,\n        Representer.represent_tuple)\n\nRepresenter.add_multi_representer(type,\n        Representer.represent_name)\n\nRepresenter.add_representer(collections.OrderedDict,\n        Representer.represent_ordered_dict)\n\nRepresenter.add_representer(types.FunctionType,\n        Representer.represent_name)\n\nRepresenter.add_representer(types.BuiltinFunctionType,\n        Representer.represent_name)\n\nRepresenter.add_representer(types.ModuleType,\n        Representer.represent_module)\n\nRepresenter.add_multi_representer(object,\n        Representer.represent_object)\n\n", "lib/yaml/resolver.py": "\n__all__ = ['BaseResolver', 'Resolver']\n\nfrom .error import *\nfrom .nodes import *\n\nimport re\n\nclass ResolverError(YAMLError):\n    pass\n\nclass BaseResolver:\n\n    DEFAULT_SCALAR_TAG = 'tag:yaml.org,2002:str'\n    DEFAULT_SEQUENCE_TAG = 'tag:yaml.org,2002:seq'\n    DEFAULT_MAPPING_TAG = 'tag:yaml.org,2002:map'\n\n    yaml_implicit_resolvers = {}\n    yaml_path_resolvers = {}\n\n    def __init__(self):\n        self.resolver_exact_paths = []\n        self.resolver_prefix_paths = []\n\n    @classmethod\n    def add_implicit_resolver(cls, tag, regexp, first):\n        if not 'yaml_implicit_resolvers' in cls.__dict__:\n            implicit_resolvers = {}\n            for key in cls.yaml_implicit_resolvers:\n                implicit_resolvers[key] = cls.yaml_implicit_resolvers[key][:]\n            cls.yaml_implicit_resolvers = implicit_resolvers\n        if first is None:\n            first = [None]\n        for ch in first:\n            cls.yaml_implicit_resolvers.setdefault(ch, []).append((tag, regexp))\n\n    @classmethod\n    def add_path_resolver(cls, tag, path, kind=None):\n        # Note: `add_path_resolver` is experimental.  The API could be changed.\n        # `new_path` is a pattern that is matched against the path from the\n        # root to the node that is being considered.  `node_path` elements are\n        # tuples `(node_check, index_check)`.  `node_check` is a node class:\n        # `ScalarNode`, `SequenceNode`, `MappingNode` or `None`.  `None`\n        # matches any kind of a node.  `index_check` could be `None`, a boolean\n        # value, a string value, or a number.  `None` and `False` match against\n        # any _value_ of sequence and mapping nodes.  `True` matches against\n        # any _key_ of a mapping node.  A string `index_check` matches against\n        # a mapping value that corresponds to a scalar key which content is\n        # equal to the `index_check` value.  An integer `index_check` matches\n        # against a sequence value with the index equal to `index_check`.\n        if not 'yaml_path_resolvers' in cls.__dict__:\n            cls.yaml_path_resolvers = cls.yaml_path_resolvers.copy()\n        new_path = []\n        for element in path:\n            if isinstance(element, (list, tuple)):\n                if len(element) == 2:\n                    node_check, index_check = element\n                elif len(element) == 1:\n                    node_check = element[0]\n                    index_check = True\n                else:\n                    raise ResolverError(\"Invalid path element: %s\" % element)\n            else:\n                node_check = None\n                index_check = element\n            if node_check is str:\n                node_check = ScalarNode\n            elif node_check is list:\n                node_check = SequenceNode\n            elif node_check is dict:\n                node_check = MappingNode\n            elif node_check not in [ScalarNode, SequenceNode, MappingNode]  \\\n                    and not isinstance(node_check, str) \\\n                    and node_check is not None:\n                raise ResolverError(\"Invalid node checker: %s\" % node_check)\n            if not isinstance(index_check, (str, int))  \\\n                    and index_check is not None:\n                raise ResolverError(\"Invalid index checker: %s\" % index_check)\n            new_path.append((node_check, index_check))\n        if kind is str:\n            kind = ScalarNode\n        elif kind is list:\n            kind = SequenceNode\n        elif kind is dict:\n            kind = MappingNode\n        elif kind not in [ScalarNode, SequenceNode, MappingNode]    \\\n                and kind is not None:\n            raise ResolverError(\"Invalid node kind: %s\" % kind)\n        cls.yaml_path_resolvers[tuple(new_path), kind] = tag\n\n    def descend_resolver(self, current_node, current_index):\n        if not self.yaml_path_resolvers:\n            return\n        exact_paths = {}\n        prefix_paths = []\n        if current_node:\n            depth = len(self.resolver_prefix_paths)\n            for path, kind in self.resolver_prefix_paths[-1]:\n                if self.check_resolver_prefix(depth, path, kind,\n                        current_node, current_index):\n                    if len(path) > depth:\n                        prefix_paths.append((path, kind))\n                    else:\n                        exact_paths[kind] = self.yaml_path_resolvers[path, kind]\n        else:\n            for path, kind in self.yaml_path_resolvers:\n                if not path:\n                    exact_paths[kind] = self.yaml_path_resolvers[path, kind]\n                else:\n                    prefix_paths.append((path, kind))\n        self.resolver_exact_paths.append(exact_paths)\n        self.resolver_prefix_paths.append(prefix_paths)\n\n    def ascend_resolver(self):\n        if not self.yaml_path_resolvers:\n            return\n        self.resolver_exact_paths.pop()\n        self.resolver_prefix_paths.pop()\n\n    def check_resolver_prefix(self, depth, path, kind,\n            current_node, current_index):\n        node_check, index_check = path[depth-1]\n        if isinstance(node_check, str):\n            if current_node.tag != node_check:\n                return\n        elif node_check is not None:\n            if not isinstance(current_node, node_check):\n                return\n        if index_check is True and current_index is not None:\n            return\n        if (index_check is False or index_check is None)    \\\n                and current_index is None:\n            return\n        if isinstance(index_check, str):\n            if not (isinstance(current_index, ScalarNode)\n                    and index_check == current_index.value):\n                return\n        elif isinstance(index_check, int) and not isinstance(index_check, bool):\n            if index_check != current_index:\n                return\n        return True\n\n    def resolve(self, kind, value, implicit):\n        if kind is ScalarNode and implicit[0]:\n            if value == '':\n                resolvers = self.yaml_implicit_resolvers.get('', [])\n            else:\n                resolvers = self.yaml_implicit_resolvers.get(value[0], [])\n            wildcard_resolvers = self.yaml_implicit_resolvers.get(None, [])\n            for tag, regexp in resolvers + wildcard_resolvers:\n                if regexp.match(value):\n                    return tag\n            implicit = implicit[1]\n        if self.yaml_path_resolvers:\n            exact_paths = self.resolver_exact_paths[-1]\n            if kind in exact_paths:\n                return exact_paths[kind]\n            if None in exact_paths:\n                return exact_paths[None]\n        if kind is ScalarNode:\n            return self.DEFAULT_SCALAR_TAG\n        elif kind is SequenceNode:\n            return self.DEFAULT_SEQUENCE_TAG\n        elif kind is MappingNode:\n            return self.DEFAULT_MAPPING_TAG\n\nclass Resolver(BaseResolver):\n    pass\n\nResolver.add_implicit_resolver(\n        'tag:yaml.org,2002:bool',\n        re.compile(r'''^(?:yes|Yes|YES|no|No|NO\n                    |true|True|TRUE|false|False|FALSE\n                    |on|On|ON|off|Off|OFF)$''', re.X),\n        list('yYnNtTfFoO'))\n\nResolver.add_implicit_resolver(\n        'tag:yaml.org,2002:float',\n        re.compile(r'''^(?:[-+]?(?:[0-9][0-9_]*)\\.[0-9_]*(?:[eE][-+][0-9]+)?\n                    |\\.[0-9][0-9_]*(?:[eE][-+][0-9]+)?\n                    |[-+]?[0-9][0-9_]*(?::[0-5]?[0-9])+\\.[0-9_]*\n                    |[-+]?\\.(?:inf|Inf|INF)\n                    |\\.(?:nan|NaN|NAN))$''', re.X),\n        list('-+0123456789.'))\n\nResolver.add_implicit_resolver(\n        'tag:yaml.org,2002:int',\n        re.compile(r'''^(?:[-+]?0b[0-1_]+\n                    |[-+]?0[0-7_]+\n                    |[-+]?(?:0|[1-9][0-9_]*)\n                    |[-+]?0x[0-9a-fA-F_]+\n                    |[-+]?[1-9][0-9_]*(?::[0-5]?[0-9])+)$''', re.X),\n        list('-+0123456789'))\n\nResolver.add_implicit_resolver(\n        'tag:yaml.org,2002:merge',\n        re.compile(r'^(?:<<)$'),\n        ['<'])\n\nResolver.add_implicit_resolver(\n        'tag:yaml.org,2002:null',\n        re.compile(r'''^(?: ~\n                    |null|Null|NULL\n                    | )$''', re.X),\n        ['~', 'n', 'N', ''])\n\nResolver.add_implicit_resolver(\n        'tag:yaml.org,2002:timestamp',\n        re.compile(r'''^(?:[0-9][0-9][0-9][0-9]-[0-9][0-9]-[0-9][0-9]\n                    |[0-9][0-9][0-9][0-9] -[0-9][0-9]? -[0-9][0-9]?\n                     (?:[Tt]|[ \\t]+)[0-9][0-9]?\n                     :[0-9][0-9] :[0-9][0-9] (?:\\.[0-9]*)?\n                     (?:[ \\t]*(?:Z|[-+][0-9][0-9]?(?::[0-9][0-9])?))?)$''', re.X),\n        list('0123456789'))\n\nResolver.add_implicit_resolver(\n        'tag:yaml.org,2002:value',\n        re.compile(r'^(?:=)$'),\n        ['='])\n\n# The following resolver is only for documentation purposes. It cannot work\n# because plain scalars cannot start with '!', '&', or '*'.\nResolver.add_implicit_resolver(\n        'tag:yaml.org,2002:yaml',\n        re.compile(r'^(?:!|&|\\*)$'),\n        list('!&*'))\n\n", "lib/yaml/nodes.py": "\nclass Node(object):\n    def __init__(self, tag, value, start_mark, end_mark):\n        self.tag = tag\n        self.value = value\n        self.start_mark = start_mark\n        self.end_mark = end_mark\n    def __repr__(self):\n        value = self.value\n        #if isinstance(value, list):\n        #    if len(value) == 0:\n        #        value = '<empty>'\n        #    elif len(value) == 1:\n        #        value = '<1 item>'\n        #    else:\n        #        value = '<%d items>' % len(value)\n        #else:\n        #    if len(value) > 75:\n        #        value = repr(value[:70]+u' ... ')\n        #    else:\n        #        value = repr(value)\n        value = repr(value)\n        return '%s(tag=%r, value=%s)' % (self.__class__.__name__, self.tag, value)\n\nclass ScalarNode(Node):\n    id = 'scalar'\n    def __init__(self, tag, value,\n            start_mark=None, end_mark=None, style=None):\n        self.tag = tag\n        self.value = value\n        self.start_mark = start_mark\n        self.end_mark = end_mark\n        self.style = style\n\nclass CollectionNode(Node):\n    def __init__(self, tag, value,\n            start_mark=None, end_mark=None, flow_style=None):\n        self.tag = tag\n        self.value = value\n        self.start_mark = start_mark\n        self.end_mark = end_mark\n        self.flow_style = flow_style\n\nclass SequenceNode(CollectionNode):\n    id = 'sequence'\n\nclass MappingNode(CollectionNode):\n    id = 'mapping'\n\n", "lib/yaml/serializer.py": "\n__all__ = ['Serializer', 'SerializerError']\n\nfrom .error import YAMLError\nfrom .events import *\nfrom .nodes import *\n\nclass SerializerError(YAMLError):\n    pass\n\nclass Serializer:\n\n    ANCHOR_TEMPLATE = 'id%03d'\n\n    def __init__(self, encoding=None,\n            explicit_start=None, explicit_end=None, version=None, tags=None):\n        self.use_encoding = encoding\n        self.use_explicit_start = explicit_start\n        self.use_explicit_end = explicit_end\n        self.use_version = version\n        self.use_tags = tags\n        self.serialized_nodes = {}\n        self.anchors = {}\n        self.last_anchor_id = 0\n        self.closed = None\n\n    def open(self):\n        if self.closed is None:\n            self.emit(StreamStartEvent(encoding=self.use_encoding))\n            self.closed = False\n        elif self.closed:\n            raise SerializerError(\"serializer is closed\")\n        else:\n            raise SerializerError(\"serializer is already opened\")\n\n    def close(self):\n        if self.closed is None:\n            raise SerializerError(\"serializer is not opened\")\n        elif not self.closed:\n            self.emit(StreamEndEvent())\n            self.closed = True\n\n    #def __del__(self):\n    #    self.close()\n\n    def serialize(self, node):\n        if self.closed is None:\n            raise SerializerError(\"serializer is not opened\")\n        elif self.closed:\n            raise SerializerError(\"serializer is closed\")\n        self.emit(DocumentStartEvent(explicit=self.use_explicit_start,\n            version=self.use_version, tags=self.use_tags))\n        self.anchor_node(node)\n        self.serialize_node(node, None, None)\n        self.emit(DocumentEndEvent(explicit=self.use_explicit_end))\n        self.serialized_nodes = {}\n        self.anchors = {}\n        self.last_anchor_id = 0\n\n    def anchor_node(self, node):\n        if node in self.anchors:\n            if self.anchors[node] is None:\n                self.anchors[node] = self.generate_anchor(node)\n        else:\n            self.anchors[node] = None\n            if isinstance(node, SequenceNode):\n                for item in node.value:\n                    self.anchor_node(item)\n            elif isinstance(node, MappingNode):\n                for key, value in node.value:\n                    self.anchor_node(key)\n                    self.anchor_node(value)\n\n    def generate_anchor(self, node):\n        self.last_anchor_id += 1\n        return self.ANCHOR_TEMPLATE % self.last_anchor_id\n\n    def serialize_node(self, node, parent, index):\n        alias = self.anchors[node]\n        if node in self.serialized_nodes:\n            self.emit(AliasEvent(alias))\n        else:\n            self.serialized_nodes[node] = True\n            self.descend_resolver(parent, index)\n            if isinstance(node, ScalarNode):\n                detected_tag = self.resolve(ScalarNode, node.value, (True, False))\n                default_tag = self.resolve(ScalarNode, node.value, (False, True))\n                implicit = (node.tag == detected_tag), (node.tag == default_tag)\n                self.emit(ScalarEvent(alias, node.tag, implicit, node.value,\n                    style=node.style))\n            elif isinstance(node, SequenceNode):\n                implicit = (node.tag\n                            == self.resolve(SequenceNode, node.value, True))\n                self.emit(SequenceStartEvent(alias, node.tag, implicit,\n                    flow_style=node.flow_style))\n                index = 0\n                for item in node.value:\n                    self.serialize_node(item, node, index)\n                    index += 1\n                self.emit(SequenceEndEvent())\n            elif isinstance(node, MappingNode):\n                implicit = (node.tag\n                            == self.resolve(MappingNode, node.value, True))\n                self.emit(MappingStartEvent(alias, node.tag, implicit,\n                    flow_style=node.flow_style))\n                for key, value in node.value:\n                    self.serialize_node(key, node, None)\n                    self.serialize_node(value, node, key)\n                self.emit(MappingEndEvent())\n            self.ascend_resolver()\n\n", "lib/yaml/loader.py": "\n__all__ = ['BaseLoader', 'FullLoader', 'SafeLoader', 'Loader', 'UnsafeLoader']\n\nfrom .reader import *\nfrom .scanner import *\nfrom .parser import *\nfrom .composer import *\nfrom .constructor import *\nfrom .resolver import *\n\nclass BaseLoader(Reader, Scanner, Parser, Composer, BaseConstructor, BaseResolver):\n\n    def __init__(self, stream):\n        Reader.__init__(self, stream)\n        Scanner.__init__(self)\n        Parser.__init__(self)\n        Composer.__init__(self)\n        BaseConstructor.__init__(self)\n        BaseResolver.__init__(self)\n\nclass FullLoader(Reader, Scanner, Parser, Composer, FullConstructor, Resolver):\n\n    def __init__(self, stream):\n        Reader.__init__(self, stream)\n        Scanner.__init__(self)\n        Parser.__init__(self)\n        Composer.__init__(self)\n        FullConstructor.__init__(self)\n        Resolver.__init__(self)\n\nclass SafeLoader(Reader, Scanner, Parser, Composer, SafeConstructor, Resolver):\n\n    def __init__(self, stream):\n        Reader.__init__(self, stream)\n        Scanner.__init__(self)\n        Parser.__init__(self)\n        Composer.__init__(self)\n        SafeConstructor.__init__(self)\n        Resolver.__init__(self)\n\nclass Loader(Reader, Scanner, Parser, Composer, Constructor, Resolver):\n\n    def __init__(self, stream):\n        Reader.__init__(self, stream)\n        Scanner.__init__(self)\n        Parser.__init__(self)\n        Composer.__init__(self)\n        Constructor.__init__(self)\n        Resolver.__init__(self)\n\n# UnsafeLoader is the same as Loader (which is and was always unsafe on\n# untrusted input). Use of either Loader or UnsafeLoader should be rare, since\n# FullLoad should be able to load almost all YAML safely. Loader is left intact\n# to ensure backwards compatibility.\nclass UnsafeLoader(Reader, Scanner, Parser, Composer, Constructor, Resolver):\n\n    def __init__(self, stream):\n        Reader.__init__(self, stream)\n        Scanner.__init__(self)\n        Parser.__init__(self)\n        Composer.__init__(self)\n        Constructor.__init__(self)\n        Resolver.__init__(self)\n", "lib/yaml/reader.py": "# This module contains abstractions for the input stream. You don't have to\n# looks further, there are no pretty code.\n#\n# We define two classes here.\n#\n#   Mark(source, line, column)\n# It's just a record and its only use is producing nice error messages.\n# Parser does not use it for any other purposes.\n#\n#   Reader(source, data)\n# Reader determines the encoding of `data` and converts it to unicode.\n# Reader provides the following methods and attributes:\n#   reader.peek(length=1) - return the next `length` characters\n#   reader.forward(length=1) - move the current position to `length` characters.\n#   reader.index - the number of the current character.\n#   reader.line, stream.column - the line and the column of the current character.\n\n__all__ = ['Reader', 'ReaderError']\n\nfrom .error import YAMLError, Mark\n\nimport codecs, re\n\nclass ReaderError(YAMLError):\n\n    def __init__(self, name, position, character, encoding, reason):\n        self.name = name\n        self.character = character\n        self.position = position\n        self.encoding = encoding\n        self.reason = reason\n\n    def __str__(self):\n        if isinstance(self.character, bytes):\n            return \"'%s' codec can't decode byte #x%02x: %s\\n\"  \\\n                    \"  in \\\"%s\\\", position %d\"    \\\n                    % (self.encoding, ord(self.character), self.reason,\n                            self.name, self.position)\n        else:\n            return \"unacceptable character #x%04x: %s\\n\"    \\\n                    \"  in \\\"%s\\\", position %d\"    \\\n                    % (self.character, self.reason,\n                            self.name, self.position)\n\nclass Reader(object):\n    # Reader:\n    # - determines the data encoding and converts it to a unicode string,\n    # - checks if characters are in allowed range,\n    # - adds '\\0' to the end.\n\n    # Reader accepts\n    #  - a `bytes` object,\n    #  - a `str` object,\n    #  - a file-like object with its `read` method returning `str`,\n    #  - a file-like object with its `read` method returning `unicode`.\n\n    # Yeah, it's ugly and slow.\n\n    def __init__(self, stream):\n        self.name = None\n        self.stream = None\n        self.stream_pointer = 0\n        self.eof = True\n        self.buffer = ''\n        self.pointer = 0\n        self.raw_buffer = None\n        self.raw_decode = None\n        self.encoding = None\n        self.index = 0\n        self.line = 0\n        self.column = 0\n        if isinstance(stream, str):\n            self.name = \"<unicode string>\"\n            self.check_printable(stream)\n            self.buffer = stream+'\\0'\n        elif isinstance(stream, bytes):\n            self.name = \"<byte string>\"\n            self.raw_buffer = stream\n            self.determine_encoding()\n        else:\n            self.stream = stream\n            self.name = getattr(stream, 'name', \"<file>\")\n            self.eof = False\n            self.raw_buffer = None\n            self.determine_encoding()\n\n    def peek(self, index=0):\n        try:\n            return self.buffer[self.pointer+index]\n        except IndexError:\n            self.update(index+1)\n            return self.buffer[self.pointer+index]\n\n    def prefix(self, length=1):\n        if self.pointer+length >= len(self.buffer):\n            self.update(length)\n        return self.buffer[self.pointer:self.pointer+length]\n\n    def forward(self, length=1):\n        if self.pointer+length+1 >= len(self.buffer):\n            self.update(length+1)\n        while length:\n            ch = self.buffer[self.pointer]\n            self.pointer += 1\n            self.index += 1\n            if ch in '\\n\\x85\\u2028\\u2029'  \\\n                    or (ch == '\\r' and self.buffer[self.pointer] != '\\n'):\n                self.line += 1\n                self.column = 0\n            elif ch != '\\uFEFF':\n                self.column += 1\n            length -= 1\n\n    def get_mark(self):\n        if self.stream is None:\n            return Mark(self.name, self.index, self.line, self.column,\n                    self.buffer, self.pointer)\n        else:\n            return Mark(self.name, self.index, self.line, self.column,\n                    None, None)\n\n    def determine_encoding(self):\n        while not self.eof and (self.raw_buffer is None or len(self.raw_buffer) < 2):\n            self.update_raw()\n        if isinstance(self.raw_buffer, bytes):\n            if self.raw_buffer.startswith(codecs.BOM_UTF16_LE):\n                self.raw_decode = codecs.utf_16_le_decode\n                self.encoding = 'utf-16-le'\n            elif self.raw_buffer.startswith(codecs.BOM_UTF16_BE):\n                self.raw_decode = codecs.utf_16_be_decode\n                self.encoding = 'utf-16-be'\n            else:\n                self.raw_decode = codecs.utf_8_decode\n                self.encoding = 'utf-8'\n        self.update(1)\n\n    NON_PRINTABLE = re.compile('[^\\x09\\x0A\\x0D\\x20-\\x7E\\x85\\xA0-\\uD7FF\\uE000-\\uFFFD\\U00010000-\\U0010ffff]')\n    def check_printable(self, data):\n        match = self.NON_PRINTABLE.search(data)\n        if match:\n            character = match.group()\n            position = self.index+(len(self.buffer)-self.pointer)+match.start()\n            raise ReaderError(self.name, position, ord(character),\n                    'unicode', \"special characters are not allowed\")\n\n    def update(self, length):\n        if self.raw_buffer is None:\n            return\n        self.buffer = self.buffer[self.pointer:]\n        self.pointer = 0\n        while len(self.buffer) < length:\n            if not self.eof:\n                self.update_raw()\n            if self.raw_decode is not None:\n                try:\n                    data, converted = self.raw_decode(self.raw_buffer,\n                            'strict', self.eof)\n                except UnicodeDecodeError as exc:\n                    character = self.raw_buffer[exc.start]\n                    if self.stream is not None:\n                        position = self.stream_pointer-len(self.raw_buffer)+exc.start\n                    else:\n                        position = exc.start\n                    raise ReaderError(self.name, position, character,\n                            exc.encoding, exc.reason)\n            else:\n                data = self.raw_buffer\n                converted = len(data)\n            self.check_printable(data)\n            self.buffer += data\n            self.raw_buffer = self.raw_buffer[converted:]\n            if self.eof:\n                self.buffer += '\\0'\n                self.raw_buffer = None\n                break\n\n    def update_raw(self, size=4096):\n        data = self.stream.read(size)\n        if self.raw_buffer is None:\n            self.raw_buffer = data\n        else:\n            self.raw_buffer += data\n        self.stream_pointer += len(data)\n        if not data:\n            self.eof = True\n", "lib/yaml/error.py": "\n__all__ = ['Mark', 'YAMLError', 'MarkedYAMLError']\n\nclass Mark:\n\n    def __init__(self, name, index, line, column, buffer, pointer):\n        self.name = name\n        self.index = index\n        self.line = line\n        self.column = column\n        self.buffer = buffer\n        self.pointer = pointer\n\n    def get_snippet(self, indent=4, max_length=75):\n        if self.buffer is None:\n            return None\n        head = ''\n        start = self.pointer\n        while start > 0 and self.buffer[start-1] not in '\\0\\r\\n\\x85\\u2028\\u2029':\n            start -= 1\n            if self.pointer-start > max_length/2-1:\n                head = ' ... '\n                start += 5\n                break\n        tail = ''\n        end = self.pointer\n        while end < len(self.buffer) and self.buffer[end] not in '\\0\\r\\n\\x85\\u2028\\u2029':\n            end += 1\n            if end-self.pointer > max_length/2-1:\n                tail = ' ... '\n                end -= 5\n                break\n        snippet = self.buffer[start:end]\n        return ' '*indent + head + snippet + tail + '\\n'  \\\n                + ' '*(indent+self.pointer-start+len(head)) + '^'\n\n    def __str__(self):\n        snippet = self.get_snippet()\n        where = \"  in \\\"%s\\\", line %d, column %d\"   \\\n                % (self.name, self.line+1, self.column+1)\n        if snippet is not None:\n            where += \":\\n\"+snippet\n        return where\n\nclass YAMLError(Exception):\n    pass\n\nclass MarkedYAMLError(YAMLError):\n\n    def __init__(self, context=None, context_mark=None,\n            problem=None, problem_mark=None, note=None):\n        self.context = context\n        self.context_mark = context_mark\n        self.problem = problem\n        self.problem_mark = problem_mark\n        self.note = note\n\n    def __str__(self):\n        lines = []\n        if self.context is not None:\n            lines.append(self.context)\n        if self.context_mark is not None  \\\n            and (self.problem is None or self.problem_mark is None\n                    or self.context_mark.name != self.problem_mark.name\n                    or self.context_mark.line != self.problem_mark.line\n                    or self.context_mark.column != self.problem_mark.column):\n            lines.append(str(self.context_mark))\n        if self.problem is not None:\n            lines.append(self.problem)\n        if self.problem_mark is not None:\n            lines.append(str(self.problem_mark))\n        if self.note is not None:\n            lines.append(self.note)\n        return '\\n'.join(lines)\n\n", "lib/yaml/__init__.py": "\nfrom .error import *\n\nfrom .tokens import *\nfrom .events import *\nfrom .nodes import *\n\nfrom .loader import *\nfrom .dumper import *\n\n__version__ = '6.0.1'\ntry:\n    from .cyaml import *\n    __with_libyaml__ = True\nexcept ImportError:\n    __with_libyaml__ = False\n\nimport io\n\n#------------------------------------------------------------------------------\n# XXX \"Warnings control\" is now deprecated. Leaving in the API function to not\n# break code that uses it.\n#------------------------------------------------------------------------------\ndef warnings(settings=None):\n    if settings is None:\n        return {}\n\n#------------------------------------------------------------------------------\ndef scan(stream, Loader=Loader):\n    \"\"\"\n    Scan a YAML stream and produce scanning tokens.\n    \"\"\"\n    loader = Loader(stream)\n    try:\n        while loader.check_token():\n            yield loader.get_token()\n    finally:\n        loader.dispose()\n\ndef parse(stream, Loader=Loader):\n    \"\"\"\n    Parse a YAML stream and produce parsing events.\n    \"\"\"\n    loader = Loader(stream)\n    try:\n        while loader.check_event():\n            yield loader.get_event()\n    finally:\n        loader.dispose()\n\ndef compose(stream, Loader=Loader):\n    \"\"\"\n    Parse the first YAML document in a stream\n    and produce the corresponding representation tree.\n    \"\"\"\n    loader = Loader(stream)\n    try:\n        return loader.get_single_node()\n    finally:\n        loader.dispose()\n\ndef compose_all(stream, Loader=Loader):\n    \"\"\"\n    Parse all YAML documents in a stream\n    and produce corresponding representation trees.\n    \"\"\"\n    loader = Loader(stream)\n    try:\n        while loader.check_node():\n            yield loader.get_node()\n    finally:\n        loader.dispose()\n\ndef load(stream, Loader):\n    \"\"\"\n    Parse the first YAML document in a stream\n    and produce the corresponding Python object.\n    \"\"\"\n    loader = Loader(stream)\n    try:\n        return loader.get_single_data()\n    finally:\n        loader.dispose()\n\ndef load_all(stream, Loader):\n    \"\"\"\n    Parse all YAML documents in a stream\n    and produce corresponding Python objects.\n    \"\"\"\n    loader = Loader(stream)\n    try:\n        while loader.check_data():\n            yield loader.get_data()\n    finally:\n        loader.dispose()\n\ndef full_load(stream):\n    \"\"\"\n    Parse the first YAML document in a stream\n    and produce the corresponding Python object.\n\n    Resolve all tags except those known to be\n    unsafe on untrusted input.\n    \"\"\"\n    return load(stream, FullLoader)\n\ndef full_load_all(stream):\n    \"\"\"\n    Parse all YAML documents in a stream\n    and produce corresponding Python objects.\n\n    Resolve all tags except those known to be\n    unsafe on untrusted input.\n    \"\"\"\n    return load_all(stream, FullLoader)\n\ndef safe_load(stream):\n    \"\"\"\n    Parse the first YAML document in a stream\n    and produce the corresponding Python object.\n\n    Resolve only basic YAML tags. This is known\n    to be safe for untrusted input.\n    \"\"\"\n    return load(stream, SafeLoader)\n\ndef safe_load_all(stream):\n    \"\"\"\n    Parse all YAML documents in a stream\n    and produce corresponding Python objects.\n\n    Resolve only basic YAML tags. This is known\n    to be safe for untrusted input.\n    \"\"\"\n    return load_all(stream, SafeLoader)\n\ndef unsafe_load(stream):\n    \"\"\"\n    Parse the first YAML document in a stream\n    and produce the corresponding Python object.\n\n    Resolve all tags, even those known to be\n    unsafe on untrusted input.\n    \"\"\"\n    return load(stream, UnsafeLoader)\n\ndef unsafe_load_all(stream):\n    \"\"\"\n    Parse all YAML documents in a stream\n    and produce corresponding Python objects.\n\n    Resolve all tags, even those known to be\n    unsafe on untrusted input.\n    \"\"\"\n    return load_all(stream, UnsafeLoader)\n\ndef emit(events, stream=None, Dumper=Dumper,\n        canonical=None, indent=None, width=None,\n        allow_unicode=None, line_break=None):\n    \"\"\"\n    Emit YAML parsing events into a stream.\n    If stream is None, return the produced string instead.\n    \"\"\"\n    getvalue = None\n    if stream is None:\n        stream = io.StringIO()\n        getvalue = stream.getvalue\n    dumper = Dumper(stream, canonical=canonical, indent=indent, width=width,\n            allow_unicode=allow_unicode, line_break=line_break)\n    try:\n        for event in events:\n            dumper.emit(event)\n    finally:\n        dumper.dispose()\n    if getvalue:\n        return getvalue()\n\ndef serialize_all(nodes, stream=None, Dumper=Dumper,\n        canonical=None, indent=None, width=None,\n        allow_unicode=None, line_break=None,\n        encoding=None, explicit_start=None, explicit_end=None,\n        version=None, tags=None):\n    \"\"\"\n    Serialize a sequence of representation trees into a YAML stream.\n    If stream is None, return the produced string instead.\n    \"\"\"\n    getvalue = None\n    if stream is None:\n        if encoding is None:\n            stream = io.StringIO()\n        else:\n            stream = io.BytesIO()\n        getvalue = stream.getvalue\n    dumper = Dumper(stream, canonical=canonical, indent=indent, width=width,\n            allow_unicode=allow_unicode, line_break=line_break,\n            encoding=encoding, version=version, tags=tags,\n            explicit_start=explicit_start, explicit_end=explicit_end)\n    try:\n        dumper.open()\n        for node in nodes:\n            dumper.serialize(node)\n        dumper.close()\n    finally:\n        dumper.dispose()\n    if getvalue:\n        return getvalue()\n\ndef serialize(node, stream=None, Dumper=Dumper, **kwds):\n    \"\"\"\n    Serialize a representation tree into a YAML stream.\n    If stream is None, return the produced string instead.\n    \"\"\"\n    return serialize_all([node], stream, Dumper=Dumper, **kwds)\n\ndef dump_all(documents, stream=None, Dumper=Dumper,\n        default_style=None, default_flow_style=False,\n        canonical=None, indent=None, width=None,\n        allow_unicode=None, line_break=None,\n        encoding=None, explicit_start=None, explicit_end=None,\n        version=None, tags=None, sort_keys=True):\n    \"\"\"\n    Serialize a sequence of Python objects into a YAML stream.\n    If stream is None, return the produced string instead.\n    \"\"\"\n    getvalue = None\n    if stream is None:\n        if encoding is None:\n            stream = io.StringIO()\n        else:\n            stream = io.BytesIO()\n        getvalue = stream.getvalue\n    dumper = Dumper(stream, default_style=default_style,\n            default_flow_style=default_flow_style,\n            canonical=canonical, indent=indent, width=width,\n            allow_unicode=allow_unicode, line_break=line_break,\n            encoding=encoding, version=version, tags=tags,\n            explicit_start=explicit_start, explicit_end=explicit_end, sort_keys=sort_keys)\n    try:\n        dumper.open()\n        for data in documents:\n            dumper.represent(data)\n        dumper.close()\n    finally:\n        dumper.dispose()\n    if getvalue:\n        return getvalue()\n\ndef dump(data, stream=None, Dumper=Dumper, **kwds):\n    \"\"\"\n    Serialize a Python object into a YAML stream.\n    If stream is None, return the produced string instead.\n    \"\"\"\n    return dump_all([data], stream, Dumper=Dumper, **kwds)\n\ndef safe_dump_all(documents, stream=None, **kwds):\n    \"\"\"\n    Serialize a sequence of Python objects into a YAML stream.\n    Produce only basic YAML tags.\n    If stream is None, return the produced string instead.\n    \"\"\"\n    return dump_all(documents, stream, Dumper=SafeDumper, **kwds)\n\ndef safe_dump(data, stream=None, **kwds):\n    \"\"\"\n    Serialize a Python object into a YAML stream.\n    Produce only basic YAML tags.\n    If stream is None, return the produced string instead.\n    \"\"\"\n    return dump_all([data], stream, Dumper=SafeDumper, **kwds)\n\ndef add_implicit_resolver(tag, regexp, first=None,\n        Loader=None, Dumper=Dumper):\n    \"\"\"\n    Add an implicit scalar detector.\n    If an implicit scalar value matches the given regexp,\n    the corresponding tag is assigned to the scalar.\n    first is a sequence of possible initial characters or None.\n    \"\"\"\n    if Loader is None:\n        loader.Loader.add_implicit_resolver(tag, regexp, first)\n        loader.FullLoader.add_implicit_resolver(tag, regexp, first)\n        loader.UnsafeLoader.add_implicit_resolver(tag, regexp, first)\n    else:\n        Loader.add_implicit_resolver(tag, regexp, first)\n    Dumper.add_implicit_resolver(tag, regexp, first)\n\ndef add_path_resolver(tag, path, kind=None, Loader=None, Dumper=Dumper):\n    \"\"\"\n    Add a path based resolver for the given tag.\n    A path is a list of keys that forms a path\n    to a node in the representation tree.\n    Keys can be string values, integers, or None.\n    \"\"\"\n    if Loader is None:\n        loader.Loader.add_path_resolver(tag, path, kind)\n        loader.FullLoader.add_path_resolver(tag, path, kind)\n        loader.UnsafeLoader.add_path_resolver(tag, path, kind)\n    else:\n        Loader.add_path_resolver(tag, path, kind)\n    Dumper.add_path_resolver(tag, path, kind)\n\ndef add_constructor(tag, constructor, Loader=None):\n    \"\"\"\n    Add a constructor for the given tag.\n    Constructor is a function that accepts a Loader instance\n    and a node object and produces the corresponding Python object.\n    \"\"\"\n    if Loader is None:\n        loader.Loader.add_constructor(tag, constructor)\n        loader.FullLoader.add_constructor(tag, constructor)\n        loader.UnsafeLoader.add_constructor(tag, constructor)\n    else:\n        Loader.add_constructor(tag, constructor)\n\ndef add_multi_constructor(tag_prefix, multi_constructor, Loader=None):\n    \"\"\"\n    Add a multi-constructor for the given tag prefix.\n    Multi-constructor is called for a node if its tag starts with tag_prefix.\n    Multi-constructor accepts a Loader instance, a tag suffix,\n    and a node object and produces the corresponding Python object.\n    \"\"\"\n    if Loader is None:\n        loader.Loader.add_multi_constructor(tag_prefix, multi_constructor)\n        loader.FullLoader.add_multi_constructor(tag_prefix, multi_constructor)\n        loader.UnsafeLoader.add_multi_constructor(tag_prefix, multi_constructor)\n    else:\n        Loader.add_multi_constructor(tag_prefix, multi_constructor)\n\ndef add_representer(data_type, representer, Dumper=Dumper):\n    \"\"\"\n    Add a representer for the given type.\n    Representer is a function accepting a Dumper instance\n    and an instance of the given data type\n    and producing the corresponding representation node.\n    \"\"\"\n    Dumper.add_representer(data_type, representer)\n\ndef add_multi_representer(data_type, multi_representer, Dumper=Dumper):\n    \"\"\"\n    Add a representer for the given type.\n    Multi-representer is a function accepting a Dumper instance\n    and an instance of the given data type or subtype\n    and producing the corresponding representation node.\n    \"\"\"\n    Dumper.add_multi_representer(data_type, multi_representer)\n\nclass YAMLObjectMetaclass(type):\n    \"\"\"\n    The metaclass for YAMLObject.\n    \"\"\"\n    def __init__(cls, name, bases, kwds):\n        super(YAMLObjectMetaclass, cls).__init__(name, bases, kwds)\n        if 'yaml_tag' in kwds and kwds['yaml_tag'] is not None:\n            if isinstance(cls.yaml_loader, list):\n                for loader in cls.yaml_loader:\n                    loader.add_constructor(cls.yaml_tag, cls.from_yaml)\n            else:\n                cls.yaml_loader.add_constructor(cls.yaml_tag, cls.from_yaml)\n\n            cls.yaml_dumper.add_representer(cls, cls.to_yaml)\n\nclass YAMLObject(metaclass=YAMLObjectMetaclass):\n    \"\"\"\n    An object that can dump itself to a YAML stream\n    and load itself from a YAML stream.\n    \"\"\"\n\n    __slots__ = ()  # no direct instantiation, so allow immutable subclasses\n\n    yaml_loader = [Loader, FullLoader, UnsafeLoader]\n    yaml_dumper = Dumper\n\n    yaml_tag = None\n    yaml_flow_style = None\n\n    @classmethod\n    def from_yaml(cls, loader, node):\n        \"\"\"\n        Convert a representation node to a Python object.\n        \"\"\"\n        return loader.construct_yaml_object(node, cls)\n\n    @classmethod\n    def to_yaml(cls, dumper, data):\n        \"\"\"\n        Convert a Python object to a representation node.\n        \"\"\"\n        return dumper.represent_yaml_object(cls.yaml_tag, data, cls,\n                flow_style=cls.yaml_flow_style)\n\n", "lib/yaml/cyaml.py": "\n__all__ = [\n    'CBaseLoader', 'CSafeLoader', 'CFullLoader', 'CUnsafeLoader', 'CLoader',\n    'CBaseDumper', 'CSafeDumper', 'CDumper'\n]\n\nfrom yaml._yaml import CParser, CEmitter\n\nfrom .constructor import *\n\nfrom .serializer import *\nfrom .representer import *\n\nfrom .resolver import *\n\nclass CBaseLoader(CParser, BaseConstructor, BaseResolver):\n\n    def __init__(self, stream):\n        CParser.__init__(self, stream)\n        BaseConstructor.__init__(self)\n        BaseResolver.__init__(self)\n\nclass CSafeLoader(CParser, SafeConstructor, Resolver):\n\n    def __init__(self, stream):\n        CParser.__init__(self, stream)\n        SafeConstructor.__init__(self)\n        Resolver.__init__(self)\n\nclass CFullLoader(CParser, FullConstructor, Resolver):\n\n    def __init__(self, stream):\n        CParser.__init__(self, stream)\n        FullConstructor.__init__(self)\n        Resolver.__init__(self)\n\nclass CUnsafeLoader(CParser, UnsafeConstructor, Resolver):\n\n    def __init__(self, stream):\n        CParser.__init__(self, stream)\n        UnsafeConstructor.__init__(self)\n        Resolver.__init__(self)\n\nclass CLoader(CParser, Constructor, Resolver):\n\n    def __init__(self, stream):\n        CParser.__init__(self, stream)\n        Constructor.__init__(self)\n        Resolver.__init__(self)\n\nclass CBaseDumper(CEmitter, BaseRepresenter, BaseResolver):\n\n    def __init__(self, stream,\n            default_style=None, default_flow_style=False,\n            canonical=None, indent=None, width=None,\n            allow_unicode=None, line_break=None,\n            encoding=None, explicit_start=None, explicit_end=None,\n            version=None, tags=None, sort_keys=True):\n        CEmitter.__init__(self, stream, canonical=canonical,\n                indent=indent, width=width, encoding=encoding,\n                allow_unicode=allow_unicode, line_break=line_break,\n                explicit_start=explicit_start, explicit_end=explicit_end,\n                version=version, tags=tags)\n        Representer.__init__(self, default_style=default_style,\n                default_flow_style=default_flow_style, sort_keys=sort_keys)\n        Resolver.__init__(self)\n\nclass CSafeDumper(CEmitter, SafeRepresenter, Resolver):\n\n    def __init__(self, stream,\n            default_style=None, default_flow_style=False,\n            canonical=None, indent=None, width=None,\n            allow_unicode=None, line_break=None,\n            encoding=None, explicit_start=None, explicit_end=None,\n            version=None, tags=None, sort_keys=True):\n        CEmitter.__init__(self, stream, canonical=canonical,\n                indent=indent, width=width, encoding=encoding,\n                allow_unicode=allow_unicode, line_break=line_break,\n                explicit_start=explicit_start, explicit_end=explicit_end,\n                version=version, tags=tags)\n        SafeRepresenter.__init__(self, default_style=default_style,\n                default_flow_style=default_flow_style, sort_keys=sort_keys)\n        Resolver.__init__(self)\n\nclass CDumper(CEmitter, Serializer, Representer, Resolver):\n\n    def __init__(self, stream,\n            default_style=None, default_flow_style=False,\n            canonical=None, indent=None, width=None,\n            allow_unicode=None, line_break=None,\n            encoding=None, explicit_start=None, explicit_end=None,\n            version=None, tags=None, sort_keys=True):\n        CEmitter.__init__(self, stream, canonical=canonical,\n                indent=indent, width=width, encoding=encoding,\n                allow_unicode=allow_unicode, line_break=line_break,\n                explicit_start=explicit_start, explicit_end=explicit_end,\n                version=version, tags=tags)\n        Representer.__init__(self, default_style=default_style,\n                default_flow_style=default_flow_style, sort_keys=sort_keys)\n        Resolver.__init__(self)\n\n", "lib/yaml/parser.py": "\n# The following YAML grammar is LL(1) and is parsed by a recursive descent\n# parser.\n#\n# stream            ::= STREAM-START implicit_document? explicit_document* STREAM-END\n# implicit_document ::= block_node DOCUMENT-END*\n# explicit_document ::= DIRECTIVE* DOCUMENT-START block_node? DOCUMENT-END*\n# block_node_or_indentless_sequence ::=\n#                       ALIAS\n#                       | properties (block_content | indentless_block_sequence)?\n#                       | block_content\n#                       | indentless_block_sequence\n# block_node        ::= ALIAS\n#                       | properties block_content?\n#                       | block_content\n# flow_node         ::= ALIAS\n#                       | properties flow_content?\n#                       | flow_content\n# properties        ::= TAG ANCHOR? | ANCHOR TAG?\n# block_content     ::= block_collection | flow_collection | SCALAR\n# flow_content      ::= flow_collection | SCALAR\n# block_collection  ::= block_sequence | block_mapping\n# flow_collection   ::= flow_sequence | flow_mapping\n# block_sequence    ::= BLOCK-SEQUENCE-START (BLOCK-ENTRY block_node?)* BLOCK-END\n# indentless_sequence   ::= (BLOCK-ENTRY block_node?)+\n# block_mapping     ::= BLOCK-MAPPING_START\n#                       ((KEY block_node_or_indentless_sequence?)?\n#                       (VALUE block_node_or_indentless_sequence?)?)*\n#                       BLOCK-END\n# flow_sequence     ::= FLOW-SEQUENCE-START\n#                       (flow_sequence_entry FLOW-ENTRY)*\n#                       flow_sequence_entry?\n#                       FLOW-SEQUENCE-END\n# flow_sequence_entry   ::= flow_node | KEY flow_node? (VALUE flow_node?)?\n# flow_mapping      ::= FLOW-MAPPING-START\n#                       (flow_mapping_entry FLOW-ENTRY)*\n#                       flow_mapping_entry?\n#                       FLOW-MAPPING-END\n# flow_mapping_entry    ::= flow_node | KEY flow_node? (VALUE flow_node?)?\n#\n# FIRST sets:\n#\n# stream: { STREAM-START }\n# explicit_document: { DIRECTIVE DOCUMENT-START }\n# implicit_document: FIRST(block_node)\n# block_node: { ALIAS TAG ANCHOR SCALAR BLOCK-SEQUENCE-START BLOCK-MAPPING-START FLOW-SEQUENCE-START FLOW-MAPPING-START }\n# flow_node: { ALIAS ANCHOR TAG SCALAR FLOW-SEQUENCE-START FLOW-MAPPING-START }\n# block_content: { BLOCK-SEQUENCE-START BLOCK-MAPPING-START FLOW-SEQUENCE-START FLOW-MAPPING-START SCALAR }\n# flow_content: { FLOW-SEQUENCE-START FLOW-MAPPING-START SCALAR }\n# block_collection: { BLOCK-SEQUENCE-START BLOCK-MAPPING-START }\n# flow_collection: { FLOW-SEQUENCE-START FLOW-MAPPING-START }\n# block_sequence: { BLOCK-SEQUENCE-START }\n# block_mapping: { BLOCK-MAPPING-START }\n# block_node_or_indentless_sequence: { ALIAS ANCHOR TAG SCALAR BLOCK-SEQUENCE-START BLOCK-MAPPING-START FLOW-SEQUENCE-START FLOW-MAPPING-START BLOCK-ENTRY }\n# indentless_sequence: { ENTRY }\n# flow_collection: { FLOW-SEQUENCE-START FLOW-MAPPING-START }\n# flow_sequence: { FLOW-SEQUENCE-START }\n# flow_mapping: { FLOW-MAPPING-START }\n# flow_sequence_entry: { ALIAS ANCHOR TAG SCALAR FLOW-SEQUENCE-START FLOW-MAPPING-START KEY }\n# flow_mapping_entry: { ALIAS ANCHOR TAG SCALAR FLOW-SEQUENCE-START FLOW-MAPPING-START KEY }\n\n__all__ = ['Parser', 'ParserError']\n\nfrom .error import MarkedYAMLError\nfrom .tokens import *\nfrom .events import *\nfrom .scanner import *\n\nclass ParserError(MarkedYAMLError):\n    pass\n\nclass Parser:\n    # Since writing a recursive-descendant parser is a straightforward task, we\n    # do not give many comments here.\n\n    DEFAULT_TAGS = {\n        '!':   '!',\n        '!!':  'tag:yaml.org,2002:',\n    }\n\n    def __init__(self):\n        self.current_event = None\n        self.yaml_version = None\n        self.tag_handles = {}\n        self.states = []\n        self.marks = []\n        self.state = self.parse_stream_start\n\n    def dispose(self):\n        # Reset the state attributes (to clear self-references)\n        self.states = []\n        self.state = None\n\n    def check_event(self, *choices):\n        # Check the type of the next event.\n        if self.current_event is None:\n            if self.state:\n                self.current_event = self.state()\n        if self.current_event is not None:\n            if not choices:\n                return True\n            for choice in choices:\n                if isinstance(self.current_event, choice):\n                    return True\n        return False\n\n    def peek_event(self):\n        # Get the next event.\n        if self.current_event is None:\n            if self.state:\n                self.current_event = self.state()\n        return self.current_event\n\n    def get_event(self):\n        # Get the next event and proceed further.\n        if self.current_event is None:\n            if self.state:\n                self.current_event = self.state()\n        value = self.current_event\n        self.current_event = None\n        return value\n\n    # stream    ::= STREAM-START implicit_document? explicit_document* STREAM-END\n    # implicit_document ::= block_node DOCUMENT-END*\n    # explicit_document ::= DIRECTIVE* DOCUMENT-START block_node? DOCUMENT-END*\n\n    def parse_stream_start(self):\n\n        # Parse the stream start.\n        token = self.get_token()\n        event = StreamStartEvent(token.start_mark, token.end_mark,\n                encoding=token.encoding)\n\n        # Prepare the next state.\n        self.state = self.parse_implicit_document_start\n\n        return event\n\n    def parse_implicit_document_start(self):\n\n        # Parse an implicit document.\n        if not self.check_token(DirectiveToken, DocumentStartToken,\n                StreamEndToken):\n            self.tag_handles = self.DEFAULT_TAGS\n            token = self.peek_token()\n            start_mark = end_mark = token.start_mark\n            event = DocumentStartEvent(start_mark, end_mark,\n                    explicit=False)\n\n            # Prepare the next state.\n            self.states.append(self.parse_document_end)\n            self.state = self.parse_block_node\n\n            return event\n\n        else:\n            return self.parse_document_start()\n\n    def parse_document_start(self):\n\n        # Parse any extra document end indicators.\n        while self.check_token(DocumentEndToken):\n            self.get_token()\n\n        # Parse an explicit document.\n        if not self.check_token(StreamEndToken):\n            token = self.peek_token()\n            start_mark = token.start_mark\n            version, tags = self.process_directives()\n            if not self.check_token(DocumentStartToken):\n                raise ParserError(None, None,\n                        \"expected '<document start>', but found %r\"\n                        % self.peek_token().id,\n                        self.peek_token().start_mark)\n            token = self.get_token()\n            end_mark = token.end_mark\n            event = DocumentStartEvent(start_mark, end_mark,\n                    explicit=True, version=version, tags=tags)\n            self.states.append(self.parse_document_end)\n            self.state = self.parse_document_content\n        else:\n            # Parse the end of the stream.\n            token = self.get_token()\n            event = StreamEndEvent(token.start_mark, token.end_mark)\n            assert not self.states\n            assert not self.marks\n            self.state = None\n        return event\n\n    def parse_document_end(self):\n\n        # Parse the document end.\n        token = self.peek_token()\n        start_mark = end_mark = token.start_mark\n        explicit = False\n        if self.check_token(DocumentEndToken):\n            token = self.get_token()\n            end_mark = token.end_mark\n            explicit = True\n        event = DocumentEndEvent(start_mark, end_mark,\n                explicit=explicit)\n\n        # Prepare the next state.\n        self.state = self.parse_document_start\n\n        return event\n\n    def parse_document_content(self):\n        if self.check_token(DirectiveToken,\n                DocumentStartToken, DocumentEndToken, StreamEndToken):\n            event = self.process_empty_scalar(self.peek_token().start_mark)\n            self.state = self.states.pop()\n            return event\n        else:\n            return self.parse_block_node()\n\n    def process_directives(self):\n        self.yaml_version = None\n        self.tag_handles = {}\n        while self.check_token(DirectiveToken):\n            token = self.get_token()\n            if token.name == 'YAML':\n                if self.yaml_version is not None:\n                    raise ParserError(None, None,\n                            \"found duplicate YAML directive\", token.start_mark)\n                major, minor = token.value\n                if major != 1:\n                    raise ParserError(None, None,\n                            \"found incompatible YAML document (version 1.* is required)\",\n                            token.start_mark)\n                self.yaml_version = token.value\n            elif token.name == 'TAG':\n                handle, prefix = token.value\n                if handle in self.tag_handles:\n                    raise ParserError(None, None,\n                            \"duplicate tag handle %r\" % handle,\n                            token.start_mark)\n                self.tag_handles[handle] = prefix\n        if self.tag_handles:\n            value = self.yaml_version, self.tag_handles.copy()\n        else:\n            value = self.yaml_version, None\n        for key in self.DEFAULT_TAGS:\n            if key not in self.tag_handles:\n                self.tag_handles[key] = self.DEFAULT_TAGS[key]\n        return value\n\n    # block_node_or_indentless_sequence ::= ALIAS\n    #               | properties (block_content | indentless_block_sequence)?\n    #               | block_content\n    #               | indentless_block_sequence\n    # block_node    ::= ALIAS\n    #                   | properties block_content?\n    #                   | block_content\n    # flow_node     ::= ALIAS\n    #                   | properties flow_content?\n    #                   | flow_content\n    # properties    ::= TAG ANCHOR? | ANCHOR TAG?\n    # block_content     ::= block_collection | flow_collection | SCALAR\n    # flow_content      ::= flow_collection | SCALAR\n    # block_collection  ::= block_sequence | block_mapping\n    # flow_collection   ::= flow_sequence | flow_mapping\n\n    def parse_block_node(self):\n        return self.parse_node(block=True)\n\n    def parse_flow_node(self):\n        return self.parse_node()\n\n    def parse_block_node_or_indentless_sequence(self):\n        return self.parse_node(block=True, indentless_sequence=True)\n\n    def parse_node(self, block=False, indentless_sequence=False):\n        if self.check_token(AliasToken):\n            token = self.get_token()\n            event = AliasEvent(token.value, token.start_mark, token.end_mark)\n            self.state = self.states.pop()\n        else:\n            anchor = None\n            tag = None\n            start_mark = end_mark = tag_mark = None\n            if self.check_token(AnchorToken):\n                token = self.get_token()\n                start_mark = token.start_mark\n                end_mark = token.end_mark\n                anchor = token.value\n                if self.check_token(TagToken):\n                    token = self.get_token()\n                    tag_mark = token.start_mark\n                    end_mark = token.end_mark\n                    tag = token.value\n            elif self.check_token(TagToken):\n                token = self.get_token()\n                start_mark = tag_mark = token.start_mark\n                end_mark = token.end_mark\n                tag = token.value\n                if self.check_token(AnchorToken):\n                    token = self.get_token()\n                    end_mark = token.end_mark\n                    anchor = token.value\n            if tag is not None:\n                handle, suffix = tag\n                if handle is not None:\n                    if handle not in self.tag_handles:\n                        raise ParserError(\"while parsing a node\", start_mark,\n                                \"found undefined tag handle %r\" % handle,\n                                tag_mark)\n                    tag = self.tag_handles[handle]+suffix\n                else:\n                    tag = suffix\n            #if tag == '!':\n            #    raise ParserError(\"while parsing a node\", start_mark,\n            #            \"found non-specific tag '!'\", tag_mark,\n            #            \"Please check 'http://pyyaml.org/wiki/YAMLNonSpecificTag' and share your opinion.\")\n            if start_mark is None:\n                start_mark = end_mark = self.peek_token().start_mark\n            event = None\n            implicit = (tag is None or tag == '!')\n            if indentless_sequence and self.check_token(BlockEntryToken):\n                end_mark = self.peek_token().end_mark\n                event = SequenceStartEvent(anchor, tag, implicit,\n                        start_mark, end_mark)\n                self.state = self.parse_indentless_sequence_entry\n            else:\n                if self.check_token(ScalarToken):\n                    token = self.get_token()\n                    end_mark = token.end_mark\n                    if (token.plain and tag is None) or tag == '!':\n                        implicit = (True, False)\n                    elif tag is None:\n                        implicit = (False, True)\n                    else:\n                        implicit = (False, False)\n                    event = ScalarEvent(anchor, tag, implicit, token.value,\n                            start_mark, end_mark, style=token.style)\n                    self.state = self.states.pop()\n                elif self.check_token(FlowSequenceStartToken):\n                    end_mark = self.peek_token().end_mark\n                    event = SequenceStartEvent(anchor, tag, implicit,\n                            start_mark, end_mark, flow_style=True)\n                    self.state = self.parse_flow_sequence_first_entry\n                elif self.check_token(FlowMappingStartToken):\n                    end_mark = self.peek_token().end_mark\n                    event = MappingStartEvent(anchor, tag, implicit,\n                            start_mark, end_mark, flow_style=True)\n                    self.state = self.parse_flow_mapping_first_key\n                elif block and self.check_token(BlockSequenceStartToken):\n                    end_mark = self.peek_token().start_mark\n                    event = SequenceStartEvent(anchor, tag, implicit,\n                            start_mark, end_mark, flow_style=False)\n                    self.state = self.parse_block_sequence_first_entry\n                elif block and self.check_token(BlockMappingStartToken):\n                    end_mark = self.peek_token().start_mark\n                    event = MappingStartEvent(anchor, tag, implicit,\n                            start_mark, end_mark, flow_style=False)\n                    self.state = self.parse_block_mapping_first_key\n                elif anchor is not None or tag is not None:\n                    # Empty scalars are allowed even if a tag or an anchor is\n                    # specified.\n                    event = ScalarEvent(anchor, tag, (implicit, False), '',\n                            start_mark, end_mark)\n                    self.state = self.states.pop()\n                else:\n                    if block:\n                        node = 'block'\n                    else:\n                        node = 'flow'\n                    token = self.peek_token()\n                    raise ParserError(\"while parsing a %s node\" % node, start_mark,\n                            \"expected the node content, but found %r\" % token.id,\n                            token.start_mark)\n        return event\n\n    # block_sequence ::= BLOCK-SEQUENCE-START (BLOCK-ENTRY block_node?)* BLOCK-END\n\n    def parse_block_sequence_first_entry(self):\n        token = self.get_token()\n        self.marks.append(token.start_mark)\n        return self.parse_block_sequence_entry()\n\n    def parse_block_sequence_entry(self):\n        if self.check_token(BlockEntryToken):\n            token = self.get_token()\n            if not self.check_token(BlockEntryToken, BlockEndToken):\n                self.states.append(self.parse_block_sequence_entry)\n                return self.parse_block_node()\n            else:\n                self.state = self.parse_block_sequence_entry\n                return self.process_empty_scalar(token.end_mark)\n        if not self.check_token(BlockEndToken):\n            token = self.peek_token()\n            raise ParserError(\"while parsing a block collection\", self.marks[-1],\n                    \"expected <block end>, but found %r\" % token.id, token.start_mark)\n        token = self.get_token()\n        event = SequenceEndEvent(token.start_mark, token.end_mark)\n        self.state = self.states.pop()\n        self.marks.pop()\n        return event\n\n    # indentless_sequence ::= (BLOCK-ENTRY block_node?)+\n\n    def parse_indentless_sequence_entry(self):\n        if self.check_token(BlockEntryToken):\n            token = self.get_token()\n            if not self.check_token(BlockEntryToken,\n                    KeyToken, ValueToken, BlockEndToken):\n                self.states.append(self.parse_indentless_sequence_entry)\n                return self.parse_block_node()\n            else:\n                self.state = self.parse_indentless_sequence_entry\n                return self.process_empty_scalar(token.end_mark)\n        token = self.peek_token()\n        event = SequenceEndEvent(token.start_mark, token.start_mark)\n        self.state = self.states.pop()\n        return event\n\n    # block_mapping     ::= BLOCK-MAPPING_START\n    #                       ((KEY block_node_or_indentless_sequence?)?\n    #                       (VALUE block_node_or_indentless_sequence?)?)*\n    #                       BLOCK-END\n\n    def parse_block_mapping_first_key(self):\n        token = self.get_token()\n        self.marks.append(token.start_mark)\n        return self.parse_block_mapping_key()\n\n    def parse_block_mapping_key(self):\n        if self.check_token(KeyToken):\n            token = self.get_token()\n            if not self.check_token(KeyToken, ValueToken, BlockEndToken):\n                self.states.append(self.parse_block_mapping_value)\n                return self.parse_block_node_or_indentless_sequence()\n            else:\n                self.state = self.parse_block_mapping_value\n                return self.process_empty_scalar(token.end_mark)\n        if not self.check_token(BlockEndToken):\n            token = self.peek_token()\n            raise ParserError(\"while parsing a block mapping\", self.marks[-1],\n                    \"expected <block end>, but found %r\" % token.id, token.start_mark)\n        token = self.get_token()\n        event = MappingEndEvent(token.start_mark, token.end_mark)\n        self.state = self.states.pop()\n        self.marks.pop()\n        return event\n\n    def parse_block_mapping_value(self):\n        if self.check_token(ValueToken):\n            token = self.get_token()\n            if not self.check_token(KeyToken, ValueToken, BlockEndToken):\n                self.states.append(self.parse_block_mapping_key)\n                return self.parse_block_node_or_indentless_sequence()\n            else:\n                self.state = self.parse_block_mapping_key\n                return self.process_empty_scalar(token.end_mark)\n        else:\n            self.state = self.parse_block_mapping_key\n            token = self.peek_token()\n            return self.process_empty_scalar(token.start_mark)\n\n    # flow_sequence     ::= FLOW-SEQUENCE-START\n    #                       (flow_sequence_entry FLOW-ENTRY)*\n    #                       flow_sequence_entry?\n    #                       FLOW-SEQUENCE-END\n    # flow_sequence_entry   ::= flow_node | KEY flow_node? (VALUE flow_node?)?\n    #\n    # Note that while production rules for both flow_sequence_entry and\n    # flow_mapping_entry are equal, their interpretations are different.\n    # For `flow_sequence_entry`, the part `KEY flow_node? (VALUE flow_node?)?`\n    # generate an inline mapping (set syntax).\n\n    def parse_flow_sequence_first_entry(self):\n        token = self.get_token()\n        self.marks.append(token.start_mark)\n        return self.parse_flow_sequence_entry(first=True)\n\n    def parse_flow_sequence_entry(self, first=False):\n        if not self.check_token(FlowSequenceEndToken):\n            if not first:\n                if self.check_token(FlowEntryToken):\n                    self.get_token()\n                else:\n                    token = self.peek_token()\n                    raise ParserError(\"while parsing a flow sequence\", self.marks[-1],\n                            \"expected ',' or ']', but got %r\" % token.id, token.start_mark)\n            \n            if self.check_token(KeyToken):\n                token = self.peek_token()\n                event = MappingStartEvent(None, None, True,\n                        token.start_mark, token.end_mark,\n                        flow_style=True)\n                self.state = self.parse_flow_sequence_entry_mapping_key\n                return event\n            elif not self.check_token(FlowSequenceEndToken):\n                self.states.append(self.parse_flow_sequence_entry)\n                return self.parse_flow_node()\n        token = self.get_token()\n        event = SequenceEndEvent(token.start_mark, token.end_mark)\n        self.state = self.states.pop()\n        self.marks.pop()\n        return event\n\n    def parse_flow_sequence_entry_mapping_key(self):\n        token = self.get_token()\n        if not self.check_token(ValueToken,\n                FlowEntryToken, FlowSequenceEndToken):\n            self.states.append(self.parse_flow_sequence_entry_mapping_value)\n            return self.parse_flow_node()\n        else:\n            self.state = self.parse_flow_sequence_entry_mapping_value\n            return self.process_empty_scalar(token.end_mark)\n\n    def parse_flow_sequence_entry_mapping_value(self):\n        if self.check_token(ValueToken):\n            token = self.get_token()\n            if not self.check_token(FlowEntryToken, FlowSequenceEndToken):\n                self.states.append(self.parse_flow_sequence_entry_mapping_end)\n                return self.parse_flow_node()\n            else:\n                self.state = self.parse_flow_sequence_entry_mapping_end\n                return self.process_empty_scalar(token.end_mark)\n        else:\n            self.state = self.parse_flow_sequence_entry_mapping_end\n            token = self.peek_token()\n            return self.process_empty_scalar(token.start_mark)\n\n    def parse_flow_sequence_entry_mapping_end(self):\n        self.state = self.parse_flow_sequence_entry\n        token = self.peek_token()\n        return MappingEndEvent(token.start_mark, token.start_mark)\n\n    # flow_mapping  ::= FLOW-MAPPING-START\n    #                   (flow_mapping_entry FLOW-ENTRY)*\n    #                   flow_mapping_entry?\n    #                   FLOW-MAPPING-END\n    # flow_mapping_entry    ::= flow_node | KEY flow_node? (VALUE flow_node?)?\n\n    def parse_flow_mapping_first_key(self):\n        token = self.get_token()\n        self.marks.append(token.start_mark)\n        return self.parse_flow_mapping_key(first=True)\n\n    def parse_flow_mapping_key(self, first=False):\n        if not self.check_token(FlowMappingEndToken):\n            if not first:\n                if self.check_token(FlowEntryToken):\n                    self.get_token()\n                else:\n                    token = self.peek_token()\n                    raise ParserError(\"while parsing a flow mapping\", self.marks[-1],\n                            \"expected ',' or '}', but got %r\" % token.id, token.start_mark)\n            if self.check_token(KeyToken):\n                token = self.get_token()\n                if not self.check_token(ValueToken,\n                        FlowEntryToken, FlowMappingEndToken):\n                    self.states.append(self.parse_flow_mapping_value)\n                    return self.parse_flow_node()\n                else:\n                    self.state = self.parse_flow_mapping_value\n                    return self.process_empty_scalar(token.end_mark)\n            elif not self.check_token(FlowMappingEndToken):\n                self.states.append(self.parse_flow_mapping_empty_value)\n                return self.parse_flow_node()\n        token = self.get_token()\n        event = MappingEndEvent(token.start_mark, token.end_mark)\n        self.state = self.states.pop()\n        self.marks.pop()\n        return event\n\n    def parse_flow_mapping_value(self):\n        if self.check_token(ValueToken):\n            token = self.get_token()\n            if not self.check_token(FlowEntryToken, FlowMappingEndToken):\n                self.states.append(self.parse_flow_mapping_key)\n                return self.parse_flow_node()\n            else:\n                self.state = self.parse_flow_mapping_key\n                return self.process_empty_scalar(token.end_mark)\n        else:\n            self.state = self.parse_flow_mapping_key\n            token = self.peek_token()\n            return self.process_empty_scalar(token.start_mark)\n\n    def parse_flow_mapping_empty_value(self):\n        self.state = self.parse_flow_mapping_key\n        return self.process_empty_scalar(self.peek_token().start_mark)\n\n    def process_empty_scalar(self, mark):\n        return ScalarEvent(None, None, (True, False), '', mark, mark)\n\n", "lib/yaml/composer.py": "\n__all__ = ['Composer', 'ComposerError']\n\nfrom .error import MarkedYAMLError\nfrom .events import *\nfrom .nodes import *\n\nclass ComposerError(MarkedYAMLError):\n    pass\n\nclass Composer:\n\n    def __init__(self):\n        self.anchors = {}\n\n    def check_node(self):\n        # Drop the STREAM-START event.\n        if self.check_event(StreamStartEvent):\n            self.get_event()\n\n        # If there are more documents available?\n        return not self.check_event(StreamEndEvent)\n\n    def get_node(self):\n        # Get the root node of the next document.\n        if not self.check_event(StreamEndEvent):\n            return self.compose_document()\n\n    def get_single_node(self):\n        # Drop the STREAM-START event.\n        self.get_event()\n\n        # Compose a document if the stream is not empty.\n        document = None\n        if not self.check_event(StreamEndEvent):\n            document = self.compose_document()\n\n        # Ensure that the stream contains no more documents.\n        if not self.check_event(StreamEndEvent):\n            event = self.get_event()\n            raise ComposerError(\"expected a single document in the stream\",\n                    document.start_mark, \"but found another document\",\n                    event.start_mark)\n\n        # Drop the STREAM-END event.\n        self.get_event()\n\n        return document\n\n    def compose_document(self):\n        # Drop the DOCUMENT-START event.\n        self.get_event()\n\n        # Compose the root node.\n        node = self.compose_node(None, None)\n\n        # Drop the DOCUMENT-END event.\n        self.get_event()\n\n        self.anchors = {}\n        return node\n\n    def compose_node(self, parent, index):\n        if self.check_event(AliasEvent):\n            event = self.get_event()\n            anchor = event.anchor\n            if anchor not in self.anchors:\n                raise ComposerError(None, None, \"found undefined alias %r\"\n                        % anchor, event.start_mark)\n            return self.anchors[anchor]\n        event = self.peek_event()\n        anchor = event.anchor\n        if anchor is not None:\n            if anchor in self.anchors:\n                raise ComposerError(\"found duplicate anchor %r; first occurrence\"\n                        % anchor, self.anchors[anchor].start_mark,\n                        \"second occurrence\", event.start_mark)\n        self.descend_resolver(parent, index)\n        if self.check_event(ScalarEvent):\n            node = self.compose_scalar_node(anchor)\n        elif self.check_event(SequenceStartEvent):\n            node = self.compose_sequence_node(anchor)\n        elif self.check_event(MappingStartEvent):\n            node = self.compose_mapping_node(anchor)\n        self.ascend_resolver()\n        return node\n\n    def compose_scalar_node(self, anchor):\n        event = self.get_event()\n        tag = event.tag\n        if tag is None or tag == '!':\n            tag = self.resolve(ScalarNode, event.value, event.implicit)\n        node = ScalarNode(tag, event.value,\n                event.start_mark, event.end_mark, style=event.style)\n        if anchor is not None:\n            self.anchors[anchor] = node\n        return node\n\n    def compose_sequence_node(self, anchor):\n        start_event = self.get_event()\n        tag = start_event.tag\n        if tag is None or tag == '!':\n            tag = self.resolve(SequenceNode, None, start_event.implicit)\n        node = SequenceNode(tag, [],\n                start_event.start_mark, None,\n                flow_style=start_event.flow_style)\n        if anchor is not None:\n            self.anchors[anchor] = node\n        index = 0\n        while not self.check_event(SequenceEndEvent):\n            node.value.append(self.compose_node(node, index))\n            index += 1\n        end_event = self.get_event()\n        node.end_mark = end_event.end_mark\n        return node\n\n    def compose_mapping_node(self, anchor):\n        start_event = self.get_event()\n        tag = start_event.tag\n        if tag is None or tag == '!':\n            tag = self.resolve(MappingNode, None, start_event.implicit)\n        node = MappingNode(tag, [],\n                start_event.start_mark, None,\n                flow_style=start_event.flow_style)\n        if anchor is not None:\n            self.anchors[anchor] = node\n        while not self.check_event(MappingEndEvent):\n            #key_event = self.peek_event()\n            item_key = self.compose_node(node, None)\n            #if item_key in node.value:\n            #    raise ComposerError(\"while composing a mapping\", start_event.start_mark,\n            #            \"found duplicate key\", key_event.start_mark)\n            item_value = self.compose_node(node, item_key)\n            #node.value[item_key] = item_value\n            node.value.append((item_key, item_value))\n        end_event = self.get_event()\n        node.end_mark = end_event.end_mark\n        return node\n\n", "lib/yaml/dumper.py": "\n__all__ = ['BaseDumper', 'SafeDumper', 'Dumper']\n\nfrom .emitter import *\nfrom .serializer import *\nfrom .representer import *\nfrom .resolver import *\n\nclass BaseDumper(Emitter, Serializer, BaseRepresenter, BaseResolver):\n\n    def __init__(self, stream,\n            default_style=None, default_flow_style=False,\n            canonical=None, indent=None, width=None,\n            allow_unicode=None, line_break=None,\n            encoding=None, explicit_start=None, explicit_end=None,\n            version=None, tags=None, sort_keys=True):\n        Emitter.__init__(self, stream, canonical=canonical,\n                indent=indent, width=width,\n                allow_unicode=allow_unicode, line_break=line_break)\n        Serializer.__init__(self, encoding=encoding,\n                explicit_start=explicit_start, explicit_end=explicit_end,\n                version=version, tags=tags)\n        Representer.__init__(self, default_style=default_style,\n                default_flow_style=default_flow_style, sort_keys=sort_keys)\n        Resolver.__init__(self)\n\nclass SafeDumper(Emitter, Serializer, SafeRepresenter, Resolver):\n\n    def __init__(self, stream,\n            default_style=None, default_flow_style=False,\n            canonical=None, indent=None, width=None,\n            allow_unicode=None, line_break=None,\n            encoding=None, explicit_start=None, explicit_end=None,\n            version=None, tags=None, sort_keys=True):\n        Emitter.__init__(self, stream, canonical=canonical,\n                indent=indent, width=width,\n                allow_unicode=allow_unicode, line_break=line_break)\n        Serializer.__init__(self, encoding=encoding,\n                explicit_start=explicit_start, explicit_end=explicit_end,\n                version=version, tags=tags)\n        SafeRepresenter.__init__(self, default_style=default_style,\n                default_flow_style=default_flow_style, sort_keys=sort_keys)\n        Resolver.__init__(self)\n\nclass Dumper(Emitter, Serializer, Representer, Resolver):\n\n    def __init__(self, stream,\n            default_style=None, default_flow_style=False,\n            canonical=None, indent=None, width=None,\n            allow_unicode=None, line_break=None,\n            encoding=None, explicit_start=None, explicit_end=None,\n            version=None, tags=None, sort_keys=True):\n        Emitter.__init__(self, stream, canonical=canonical,\n                indent=indent, width=width,\n                allow_unicode=allow_unicode, line_break=line_break)\n        Serializer.__init__(self, encoding=encoding,\n                explicit_start=explicit_start, explicit_end=explicit_end,\n                version=version, tags=tags)\n        Representer.__init__(self, default_style=default_style,\n                default_flow_style=default_flow_style, sort_keys=sort_keys)\n        Resolver.__init__(self)\n\n"}