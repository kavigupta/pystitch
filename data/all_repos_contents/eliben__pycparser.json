{"setup.py": "import os, sys\ntry:\n    from setuptools import setup\n    from setuptools.command.install import install as _install\n    from setuptools.command.sdist import sdist as _sdist\nexcept ImportError:\n    from distutils.core import setup\n    from distutils.command.install import install as _install\n    from distutils.command.sdist import sdist as _sdist\n\n\ndef _run_build_tables(dir):\n    from subprocess import check_call\n    # This is run inside the install staging directory (that had no .pyc files)\n    # We don't want to generate any.\n    # https://github.com/eliben/pycparser/pull/135\n    check_call([sys.executable, '-B', '_build_tables.py'],\n               cwd=os.path.join(dir, 'pycparser'))\n\n\nclass install(_install):\n    def run(self):\n        _install.run(self)\n        self.execute(_run_build_tables, (self.install_lib,),\n                     msg=\"Build the lexing/parsing tables\")\n\n\nclass sdist(_sdist):\n    def make_release_tree(self, basedir, files):\n        _sdist.make_release_tree(self, basedir, files)\n        self.execute(_run_build_tables, (basedir,),\n                     msg=\"Build the lexing/parsing tables\")\n\n\nsetup(\n    # metadata\n    name='pycparser',\n    description='C parser in Python',\n    long_description=\"\"\"\n        pycparser is a complete parser of the C language, written in\n        pure Python using the PLY parsing library.\n        It parses C code into an AST and can serve as a front-end for\n        C compilers or analysis tools.\n    \"\"\",\n    license='BSD-3-Clause',\n    version='2.22',\n    author='Eli Bendersky',\n    maintainer='Eli Bendersky',\n    author_email='eliben@gmail.com',\n    url='https://github.com/eliben/pycparser',\n    platforms='Cross Platform',\n    classifiers = [\n        'Development Status :: 5 - Production/Stable',\n        'License :: OSI Approved :: BSD License',\n        'Programming Language :: Python :: 3',\n        'Programming Language :: Python :: 3.8',\n        'Programming Language :: Python :: 3.9',\n        'Programming Language :: Python :: 3.10',\n        'Programming Language :: Python :: 3.11',\n        'Programming Language :: Python :: 3.12',\n    ],\n    python_requires=\">=3.8\",\n    packages=['pycparser', 'pycparser.ply'],\n    package_data={'pycparser': ['*.cfg']},\n    cmdclass={'install': install, 'sdist': sdist},\n)\n", "_clean_tables.py": "# Cleanup all tables and PYC files to ensure no PLY stuff is cached\nimport itertools\nimport fnmatch\nimport os, shutil\n\nfile_patterns = ('yacctab.*', 'lextab.*', '*.pyc', '__pycache__')\n\n\ndef do_cleanup(root):\n    for path, dirs, files in os.walk(root):\n        for file in itertools.chain(dirs, files):\n            try:\n                for pattern in file_patterns:\n                    if fnmatch.fnmatch(file, pattern):\n                        fullpath = os.path.join(path, file)\n                        if os.path.isdir(fullpath):\n                            shutil.rmtree(fullpath, ignore_errors=False)\n                        else:\n                            os.unlink(fullpath)\n                        print('Deleted', fullpath)\n            except OSError:\n                pass\n\n\nif __name__ == \"__main__\":\n    do_cleanup('.')\n", "utils/internal/make_fake_typedefs.py": "import sys\nsys.path.insert(0, '../..')\n\nfrom pycparser import c_ast, parse_file\n\n\nclass MyVisitor(c_ast.NodeVisitor):\n    def visit_Typedef(self, node):\n        print('typedef int %s;' % node.name)\n\n\n\ndef generate_fake_typedefs(filename):\n    ast = parse_file(filename, use_cpp=True, cpp_path=\"../cpp.exe\")\n    v = MyVisitor()\n    v.visit(ast)\n\n\nif __name__ == \"__main__\":\n    generate_fake_typedefs('example_c_file_pp.c')\n\n", "utils/internal/zz_parse.py": "from pycparser import c_parser, c_generator\n\n\nif __name__ == \"__main__\":\n    parser = c_parser.CParser()\n    code = r'''\n    const int ci;\n    const int* pci;\n    int* const pci;\n    _Atomic(int) ai;\n    _Atomic(int*) pai;\n    _Atomic(_Atomic(int)*) ppai;\n    '''\n\n    print(code)\n    ast = parser.parse(code, debug=False)\n    ast.show(attrnames=True, nodenames=True)\n    #print(ast.ext[0].__slots__)\n    #print(dir(ast.ext[0]))\n\n    #print(\"==== From C generator:\")\n    #generator = c_generator.CGenerator()\n    #print(generator.visit(ast))\n", "utils/internal/fake_includes.py": "import os.path\n\nfor cur_path, dirs, files in os.walk('.'):\n    if cur_path == '.':\n        for f in files:\n            if f.endswith('.h'):\n                print(f)\n                fo = open(f, 'w')\n                fo.write('#include \"_fake_defines.h\"\\n')\n                fo.write('#include \"_fake_typedefs.h\"\\n')\n                fo.close()\n\n\n", "utils/internal/zz-ctoc.py": "from pycparser import c_parser, c_generator\n\nif __name__ == '__main__':\n    src = r'''\n\n    void f(char * restrict joe){}\n\nint main(void)\n{\n    unsigned int long k = 4;\n    int p = - - k;\n    return 0;\n}\n'''\n    parser = c_parser.CParser()\n    ast = parser.parse(src)\n    ast.show()\n    generator = c_generator.CGenerator()\n\n    print(generator.visit(ast))\n\n    # tracing the generator for debugging\n    #~ import trace\n    #~ tr = trace.Trace(countcallers=1)\n    #~ tr.runfunc(generator.visit, ast)\n    #~ tr.results().write_results()\n", "utils/internal/memprofiling.py": "from pycparser import parse_file\nfrom pycparser.c_ast import *\nfrom pycparser.c_parser import CParser\n\n\ndef expand_decl(decl):\n    \"\"\" Converts the declaration into a nested list.\n    \"\"\"\n    typ = type(decl)\n\n    if typ == TypeDecl:\n        return ['TypeDecl', expand_decl(decl.type)]\n    elif typ == IdentifierType:\n        return ['IdentifierType', decl.names]\n    elif typ == ID:\n        return ['ID', decl.name]\n    elif typ in [Struct, Union]:\n        decls = [expand_decl(d) for d in decl.decls or []]\n        return [typ.__name__, decl.name, decls]\n    else:\n        nested = expand_decl(decl.type)\n\n        if typ == Decl:\n            if decl.quals:\n                return ['Decl', decl.quals, decl.name, nested]\n            else:\n                return ['Decl', decl.name, nested]\n        elif typ == Typename: # for function parameters\n            if decl.quals:\n                return ['Typename', decl.quals, nested]\n            else:\n                return ['Typename', nested]\n        elif typ == ArrayDecl:\n            dimval = decl.dim.value if decl.dim else ''\n            return ['ArrayDecl', dimval, nested]\n        elif typ == PtrDecl:\n            return ['PtrDecl', nested]\n        elif typ == Typedef:\n            return ['Typedef', decl.name, nested]\n        elif typ == FuncDecl:\n            if decl.args:\n                params = [expand_decl(param) for param in decl.args.params]\n            else:\n                params = []\n            return ['FuncDecl', params, nested]\n\n#-----------------------------------------------------------------\nclass NodeVisitor(object):\n    def __init__(self):\n        self.current_parent = None\n\n    def visit(self, node):\n        \"\"\" Visit a node.\n        \"\"\"\n        method = 'visit_' + node.__class__.__name__\n        visitor = getattr(self, method, self.generic_visit)\n        return visitor(node)\n\n    def visit_FuncCall(self, node):\n        print(\"Visiting FuncCall\")\n        print(node.show())\n        print('---- parent ----')\n        print(self.current_parent.show())\n\n    def generic_visit(self, node):\n        \"\"\" Called if no explicit visitor function exists for a\n            node. Implements preorder visiting of the node.\n        \"\"\"\n        oldparent = self.current_parent\n        self.current_parent = node\n        for c in node.children():\n            self.visit(c)\n        self.current_parent = oldparent\n\n\ndef heapyprofile():\n    # pip install guppy\n    # [works on python 2.7, AFAIK]\n    from guppy import hpy\n    import gc\n\n    hp = hpy()\n    ast = parse_file('/tmp/197.c')\n    gc.collect()\n    h = hp.heap()\n    print(h)\n\n\ndef memprofile():\n    import resource\n    import tracemalloc\n\n    tracemalloc.start()\n\n    ast = parse_file('/tmp/197.c')\n\n    print('Memory usage: %s (kb)' %\n            resource.getrusage(resource.RUSAGE_SELF).ru_maxrss)\n\n    snapshot = tracemalloc.take_snapshot()\n    print(\"[ tracemalloc stats ]\")\n    for stat in snapshot.statistics('lineno')[:20]:\n        print(stat)\n\n\nif __name__ == \"__main__\":\n    source_code = r'''void foo() {\n    L\"hi\" L\"there\";\n}\n    '''\n\n    memprofile()\n    #heapyprofile()\n\n    #parser = CParser()\n    #ast = parser.parse(source_code, filename='zz')\n    #ast.show(showcoord=True, attrnames=True, nodenames=True)\n\n\n", "utils/benchmark/benchmark-parse.py": "#-----------------------------------------------------------------\n# Benchmarking utility for internal use.\n#\n# Use with Python 3.6+\n#\n# Eli Bendersky [https://eli.thegreenplace.net/]\n# License: BSD\n#-----------------------------------------------------------------\nimport os\nimport statistics\nimport sys\nimport time\n\nsys.path.extend(['.', '..'])\n\nfrom pycparser import c_parser, c_ast\n\n\ndef measure_parse(text, n, progress_cb):\n    \"\"\"Measure the parsing of text with pycparser.\n\n    text should represent a full file. n is the number of iterations to measure.\n    progress_cb will be called with the iteration number each time one is done.\n\n    Returns a list of elapsed times, one per iteration.\n    \"\"\"\n    times = []\n    for i in range(n):\n        parser = c_parser.CParser()\n        t1 = time.time()\n        ast = parser.parse(text, '')\n        elapsed = time.time() - t1\n        assert isinstance(ast, c_ast.FileAST)\n        times.append(elapsed)\n        progress_cb(i)\n    return times\n\n\ndef measure_file(filename, n):\n    progress_cb = lambda i: print('.', sep='', end='', flush=True)\n    with open(filename) as f:\n        print('%-25s' % os.path.basename(filename), end='', flush=True)\n        text = f.read()\n        times = measure_parse(text, n, progress_cb)\n    print('    Mean: %.3f  Stddev: %.3f' % (statistics.mean(times),\n                                            statistics.stdev(times)))\n\n\nNUM_RUNS = 5\n\n\nif __name__ == '__main__':\n    if len(sys.argv) < 2:\n        print(\"Usage: %s <dir with input files>\")\n        sys.exit(1)\n    for filename in os.listdir(sys.argv[1]):\n        filename = os.path.join(sys.argv[1], filename)\n        measure_file(filename, NUM_RUNS)\n", "pycparser/_build_tables.py": "#-----------------------------------------------------------------\n# pycparser: _build_tables.py\n#\n# A dummy for generating the lexing/parsing tables and and\n# compiling them into .pyc for faster execution in optimized mode.\n# Also generates AST code from the configuration file.\n# Should be called from the pycparser directory.\n#\n# Eli Bendersky [https://eli.thegreenplace.net/]\n# License: BSD\n#-----------------------------------------------------------------\n\n# Insert '.' and '..' as first entries to the search path for modules.\n# Restricted environments like embeddable python do not include the\n# current working directory on startup.\nimport importlib\nimport sys\nsys.path[0:0] = ['.', '..']\n\n# Generate c_ast.py\nfrom _ast_gen import ASTCodeGenerator\nast_gen = ASTCodeGenerator('_c_ast.cfg')\nast_gen.generate(open('c_ast.py', 'w'))\n\nfrom pycparser import c_parser\n\n# Generates the tables\n#\nc_parser.CParser(\n    lex_optimize=True,\n    yacc_debug=False,\n    yacc_optimize=True)\n\n# Load to compile into .pyc\n#\nimportlib.invalidate_caches()\n\nimport lextab\nimport yacctab\nimport c_ast\n", "pycparser/c_generator.py": "#------------------------------------------------------------------------------\n# pycparser: c_generator.py\n#\n# C code generator from pycparser AST nodes.\n#\n# Eli Bendersky [https://eli.thegreenplace.net/]\n# License: BSD\n#------------------------------------------------------------------------------\nfrom . import c_ast\n\n\nclass CGenerator(object):\n    \"\"\" Uses the same visitor pattern as c_ast.NodeVisitor, but modified to\n        return a value from each visit method, using string accumulation in\n        generic_visit.\n    \"\"\"\n    def __init__(self, reduce_parentheses=False):\n        \"\"\" Constructs C-code generator\n\n            reduce_parentheses:\n                if True, eliminates needless parentheses on binary operators\n        \"\"\"\n        # Statements start with indentation of self.indent_level spaces, using\n        # the _make_indent method.\n        self.indent_level = 0\n        self.reduce_parentheses = reduce_parentheses\n\n    def _make_indent(self):\n        return ' ' * self.indent_level\n\n    def visit(self, node):\n        method = 'visit_' + node.__class__.__name__\n        return getattr(self, method, self.generic_visit)(node)\n\n    def generic_visit(self, node):\n        if node is None:\n            return ''\n        else:\n            return ''.join(self.visit(c) for c_name, c in node.children())\n\n    def visit_Constant(self, n):\n        return n.value\n\n    def visit_ID(self, n):\n        return n.name\n\n    def visit_Pragma(self, n):\n        ret = '#pragma'\n        if n.string:\n            ret += ' ' + n.string\n        return ret\n\n    def visit_ArrayRef(self, n):\n        arrref = self._parenthesize_unless_simple(n.name)\n        return arrref + '[' + self.visit(n.subscript) + ']'\n\n    def visit_StructRef(self, n):\n        sref = self._parenthesize_unless_simple(n.name)\n        return sref + n.type + self.visit(n.field)\n\n    def visit_FuncCall(self, n):\n        fref = self._parenthesize_unless_simple(n.name)\n        return fref + '(' + self.visit(n.args) + ')'\n\n    def visit_UnaryOp(self, n):\n        if n.op == 'sizeof':\n            # Always parenthesize the argument of sizeof since it can be\n            # a name.\n            return 'sizeof(%s)' % self.visit(n.expr)\n        else:\n            operand = self._parenthesize_unless_simple(n.expr)\n            if n.op == 'p++':\n                return '%s++' % operand\n            elif n.op == 'p--':\n                return '%s--' % operand\n            else:\n                return '%s%s' % (n.op, operand)\n\n    # Precedence map of binary operators:\n    precedence_map = {\n        # Should be in sync with c_parser.CParser.precedence\n        # Higher numbers are stronger binding\n        '||': 0,  # weakest binding\n        '&&': 1,\n        '|': 2,\n        '^': 3,\n        '&': 4,\n        '==': 5, '!=': 5,\n        '>': 6, '>=': 6, '<': 6, '<=': 6,\n        '>>': 7, '<<': 7,\n        '+': 8, '-': 8,\n        '*': 9, '/': 9, '%': 9  # strongest binding\n    }\n\n    def visit_BinaryOp(self, n):\n        # Note: all binary operators are left-to-right associative\n        #\n        # If `n.left.op` has a stronger or equally binding precedence in\n        # comparison to `n.op`, no parenthesis are needed for the left:\n        # e.g., `(a*b) + c` is equivalent to `a*b + c`, as well as\n        #       `(a+b) - c` is equivalent to `a+b - c` (same precedence).\n        # If the left operator is weaker binding than the current, then\n        # parentheses are necessary:\n        # e.g., `(a+b) * c` is NOT equivalent to `a+b * c`.\n        lval_str = self._parenthesize_if(\n            n.left,\n            lambda d: not (self._is_simple_node(d) or\n                      self.reduce_parentheses and isinstance(d, c_ast.BinaryOp) and\n                      self.precedence_map[d.op] >= self.precedence_map[n.op]))\n        # If `n.right.op` has a stronger -but not equal- binding precedence,\n        # parenthesis can be omitted on the right:\n        # e.g., `a + (b*c)` is equivalent to `a + b*c`.\n        # If the right operator is weaker or equally binding, then parentheses\n        # are necessary:\n        # e.g., `a * (b+c)` is NOT equivalent to `a * b+c` and\n        #       `a - (b+c)` is NOT equivalent to `a - b+c` (same precedence).\n        rval_str = self._parenthesize_if(\n            n.right,\n            lambda d: not (self._is_simple_node(d) or\n                      self.reduce_parentheses and isinstance(d, c_ast.BinaryOp) and\n                      self.precedence_map[d.op] > self.precedence_map[n.op]))\n        return '%s %s %s' % (lval_str, n.op, rval_str)\n\n    def visit_Assignment(self, n):\n        rval_str = self._parenthesize_if(\n                            n.rvalue,\n                            lambda n: isinstance(n, c_ast.Assignment))\n        return '%s %s %s' % (self.visit(n.lvalue), n.op, rval_str)\n\n    def visit_IdentifierType(self, n):\n        return ' '.join(n.names)\n\n    def _visit_expr(self, n):\n        if isinstance(n, c_ast.InitList):\n            return '{' + self.visit(n) + '}'\n        elif isinstance(n, c_ast.ExprList):\n            return '(' + self.visit(n) + ')'\n        else:\n            return self.visit(n)\n\n    def visit_Decl(self, n, no_type=False):\n        # no_type is used when a Decl is part of a DeclList, where the type is\n        # explicitly only for the first declaration in a list.\n        #\n        s = n.name if no_type else self._generate_decl(n)\n        if n.bitsize: s += ' : ' + self.visit(n.bitsize)\n        if n.init:\n            s += ' = ' + self._visit_expr(n.init)\n        return s\n\n    def visit_DeclList(self, n):\n        s = self.visit(n.decls[0])\n        if len(n.decls) > 1:\n            s += ', ' + ', '.join(self.visit_Decl(decl, no_type=True)\n                                    for decl in n.decls[1:])\n        return s\n\n    def visit_Typedef(self, n):\n        s = ''\n        if n.storage: s += ' '.join(n.storage) + ' '\n        s += self._generate_type(n.type)\n        return s\n\n    def visit_Cast(self, n):\n        s = '(' + self._generate_type(n.to_type, emit_declname=False) + ')'\n        return s + ' ' + self._parenthesize_unless_simple(n.expr)\n\n    def visit_ExprList(self, n):\n        visited_subexprs = []\n        for expr in n.exprs:\n            visited_subexprs.append(self._visit_expr(expr))\n        return ', '.join(visited_subexprs)\n\n    def visit_InitList(self, n):\n        visited_subexprs = []\n        for expr in n.exprs:\n            visited_subexprs.append(self._visit_expr(expr))\n        return ', '.join(visited_subexprs)\n\n    def visit_Enum(self, n):\n        return self._generate_struct_union_enum(n, name='enum')\n\n    def visit_Alignas(self, n):\n        return '_Alignas({})'.format(self.visit(n.alignment))\n\n    def visit_Enumerator(self, n):\n        if not n.value:\n            return '{indent}{name},\\n'.format(\n                indent=self._make_indent(),\n                name=n.name,\n            )\n        else:\n            return '{indent}{name} = {value},\\n'.format(\n                indent=self._make_indent(),\n                name=n.name,\n                value=self.visit(n.value),\n            )\n\n    def visit_FuncDef(self, n):\n        decl = self.visit(n.decl)\n        self.indent_level = 0\n        body = self.visit(n.body)\n        if n.param_decls:\n            knrdecls = ';\\n'.join(self.visit(p) for p in n.param_decls)\n            return decl + '\\n' + knrdecls + ';\\n' + body + '\\n'\n        else:\n            return decl + '\\n' + body + '\\n'\n\n    def visit_FileAST(self, n):\n        s = ''\n        for ext in n.ext:\n            if isinstance(ext, c_ast.FuncDef):\n                s += self.visit(ext)\n            elif isinstance(ext, c_ast.Pragma):\n                s += self.visit(ext) + '\\n'\n            else:\n                s += self.visit(ext) + ';\\n'\n        return s\n\n    def visit_Compound(self, n):\n        s = self._make_indent() + '{\\n'\n        self.indent_level += 2\n        if n.block_items:\n            s += ''.join(self._generate_stmt(stmt) for stmt in n.block_items)\n        self.indent_level -= 2\n        s += self._make_indent() + '}\\n'\n        return s\n\n    def visit_CompoundLiteral(self, n):\n        return '(' + self.visit(n.type) + '){' + self.visit(n.init) + '}'\n\n\n    def visit_EmptyStatement(self, n):\n        return ';'\n\n    def visit_ParamList(self, n):\n        return ', '.join(self.visit(param) for param in n.params)\n\n    def visit_Return(self, n):\n        s = 'return'\n        if n.expr: s += ' ' + self.visit(n.expr)\n        return s + ';'\n\n    def visit_Break(self, n):\n        return 'break;'\n\n    def visit_Continue(self, n):\n        return 'continue;'\n\n    def visit_TernaryOp(self, n):\n        s  = '(' + self._visit_expr(n.cond) + ') ? '\n        s += '(' + self._visit_expr(n.iftrue) + ') : '\n        s += '(' + self._visit_expr(n.iffalse) + ')'\n        return s\n\n    def visit_If(self, n):\n        s = 'if ('\n        if n.cond: s += self.visit(n.cond)\n        s += ')\\n'\n        s += self._generate_stmt(n.iftrue, add_indent=True)\n        if n.iffalse:\n            s += self._make_indent() + 'else\\n'\n            s += self._generate_stmt(n.iffalse, add_indent=True)\n        return s\n\n    def visit_For(self, n):\n        s = 'for ('\n        if n.init: s += self.visit(n.init)\n        s += ';'\n        if n.cond: s += ' ' + self.visit(n.cond)\n        s += ';'\n        if n.next: s += ' ' + self.visit(n.next)\n        s += ')\\n'\n        s += self._generate_stmt(n.stmt, add_indent=True)\n        return s\n\n    def visit_While(self, n):\n        s = 'while ('\n        if n.cond: s += self.visit(n.cond)\n        s += ')\\n'\n        s += self._generate_stmt(n.stmt, add_indent=True)\n        return s\n\n    def visit_DoWhile(self, n):\n        s = 'do\\n'\n        s += self._generate_stmt(n.stmt, add_indent=True)\n        s += self._make_indent() + 'while ('\n        if n.cond: s += self.visit(n.cond)\n        s += ');'\n        return s\n\n    def visit_StaticAssert(self, n):\n        s = '_Static_assert('\n        s += self.visit(n.cond)\n        if n.message:\n            s += ','\n            s += self.visit(n.message)\n        s += ')'\n        return s\n\n    def visit_Switch(self, n):\n        s = 'switch (' + self.visit(n.cond) + ')\\n'\n        s += self._generate_stmt(n.stmt, add_indent=True)\n        return s\n\n    def visit_Case(self, n):\n        s = 'case ' + self.visit(n.expr) + ':\\n'\n        for stmt in n.stmts:\n            s += self._generate_stmt(stmt, add_indent=True)\n        return s\n\n    def visit_Default(self, n):\n        s = 'default:\\n'\n        for stmt in n.stmts:\n            s += self._generate_stmt(stmt, add_indent=True)\n        return s\n\n    def visit_Label(self, n):\n        return n.name + ':\\n' + self._generate_stmt(n.stmt)\n\n    def visit_Goto(self, n):\n        return 'goto ' + n.name + ';'\n\n    def visit_EllipsisParam(self, n):\n        return '...'\n\n    def visit_Struct(self, n):\n        return self._generate_struct_union_enum(n, 'struct')\n\n    def visit_Typename(self, n):\n        return self._generate_type(n.type)\n\n    def visit_Union(self, n):\n        return self._generate_struct_union_enum(n, 'union')\n\n    def visit_NamedInitializer(self, n):\n        s = ''\n        for name in n.name:\n            if isinstance(name, c_ast.ID):\n                s += '.' + name.name\n            else:\n                s += '[' + self.visit(name) + ']'\n        s += ' = ' + self._visit_expr(n.expr)\n        return s\n\n    def visit_FuncDecl(self, n):\n        return self._generate_type(n)\n\n    def visit_ArrayDecl(self, n):\n        return self._generate_type(n, emit_declname=False)\n\n    def visit_TypeDecl(self, n):\n        return self._generate_type(n, emit_declname=False)\n\n    def visit_PtrDecl(self, n):\n        return self._generate_type(n, emit_declname=False)\n\n    def _generate_struct_union_enum(self, n, name):\n        \"\"\" Generates code for structs, unions, and enums. name should be\n            'struct', 'union', or 'enum'.\n        \"\"\"\n        if name in ('struct', 'union'):\n            members = n.decls\n            body_function = self._generate_struct_union_body\n        else:\n            assert name == 'enum'\n            members = None if n.values is None else n.values.enumerators\n            body_function = self._generate_enum_body\n        s = name + ' ' + (n.name or '')\n        if members is not None:\n            # None means no members\n            # Empty sequence means an empty list of members\n            s += '\\n'\n            s += self._make_indent()\n            self.indent_level += 2\n            s += '{\\n'\n            s += body_function(members)\n            self.indent_level -= 2\n            s += self._make_indent() + '}'\n        return s\n\n    def _generate_struct_union_body(self, members):\n        return ''.join(self._generate_stmt(decl) for decl in members)\n\n    def _generate_enum_body(self, members):\n        # `[:-2] + '\\n'` removes the final `,` from the enumerator list\n        return ''.join(self.visit(value) for value in members)[:-2] + '\\n'\n\n    def _generate_stmt(self, n, add_indent=False):\n        \"\"\" Generation from a statement node. This method exists as a wrapper\n            for individual visit_* methods to handle different treatment of\n            some statements in this context.\n        \"\"\"\n        typ = type(n)\n        if add_indent: self.indent_level += 2\n        indent = self._make_indent()\n        if add_indent: self.indent_level -= 2\n\n        if typ in (\n                c_ast.Decl, c_ast.Assignment, c_ast.Cast, c_ast.UnaryOp,\n                c_ast.BinaryOp, c_ast.TernaryOp, c_ast.FuncCall, c_ast.ArrayRef,\n                c_ast.StructRef, c_ast.Constant, c_ast.ID, c_ast.Typedef,\n                c_ast.ExprList):\n            # These can also appear in an expression context so no semicolon\n            # is added to them automatically\n            #\n            return indent + self.visit(n) + ';\\n'\n        elif typ in (c_ast.Compound,):\n            # No extra indentation required before the opening brace of a\n            # compound - because it consists of multiple lines it has to\n            # compute its own indentation.\n            #\n            return self.visit(n)\n        elif typ in (c_ast.If,):\n            return indent + self.visit(n)\n        else:\n            return indent + self.visit(n) + '\\n'\n\n    def _generate_decl(self, n):\n        \"\"\" Generation from a Decl node.\n        \"\"\"\n        s = ''\n        if n.funcspec: s = ' '.join(n.funcspec) + ' '\n        if n.storage: s += ' '.join(n.storage) + ' '\n        if n.align: s += self.visit(n.align[0]) + ' '\n        s += self._generate_type(n.type)\n        return s\n\n    def _generate_type(self, n, modifiers=[], emit_declname = True):\n        \"\"\" Recursive generation from a type node. n is the type node.\n            modifiers collects the PtrDecl, ArrayDecl and FuncDecl modifiers\n            encountered on the way down to a TypeDecl, to allow proper\n            generation from it.\n        \"\"\"\n        typ = type(n)\n        #~ print(n, modifiers)\n\n        if typ == c_ast.TypeDecl:\n            s = ''\n            if n.quals: s += ' '.join(n.quals) + ' '\n            s += self.visit(n.type)\n\n            nstr = n.declname if n.declname and emit_declname else ''\n            # Resolve modifiers.\n            # Wrap in parens to distinguish pointer to array and pointer to\n            # function syntax.\n            #\n            for i, modifier in enumerate(modifiers):\n                if isinstance(modifier, c_ast.ArrayDecl):\n                    if (i != 0 and\n                        isinstance(modifiers[i - 1], c_ast.PtrDecl)):\n                            nstr = '(' + nstr + ')'\n                    nstr += '['\n                    if modifier.dim_quals:\n                        nstr += ' '.join(modifier.dim_quals) + ' '\n                    nstr += self.visit(modifier.dim) + ']'\n                elif isinstance(modifier, c_ast.FuncDecl):\n                    if (i != 0 and\n                        isinstance(modifiers[i - 1], c_ast.PtrDecl)):\n                            nstr = '(' + nstr + ')'\n                    nstr += '(' + self.visit(modifier.args) + ')'\n                elif isinstance(modifier, c_ast.PtrDecl):\n                    if modifier.quals:\n                        nstr = '* %s%s' % (' '.join(modifier.quals),\n                                           ' ' + nstr if nstr else '')\n                    else:\n                        nstr = '*' + nstr\n            if nstr: s += ' ' + nstr\n            return s\n        elif typ == c_ast.Decl:\n            return self._generate_decl(n.type)\n        elif typ == c_ast.Typename:\n            return self._generate_type(n.type, emit_declname = emit_declname)\n        elif typ == c_ast.IdentifierType:\n            return ' '.join(n.names) + ' '\n        elif typ in (c_ast.ArrayDecl, c_ast.PtrDecl, c_ast.FuncDecl):\n            return self._generate_type(n.type, modifiers + [n],\n                                       emit_declname = emit_declname)\n        else:\n            return self.visit(n)\n\n    def _parenthesize_if(self, n, condition):\n        \"\"\" Visits 'n' and returns its string representation, parenthesized\n            if the condition function applied to the node returns True.\n        \"\"\"\n        s = self._visit_expr(n)\n        if condition(n):\n            return '(' + s + ')'\n        else:\n            return s\n\n    def _parenthesize_unless_simple(self, n):\n        \"\"\" Common use case for _parenthesize_if\n        \"\"\"\n        return self._parenthesize_if(n, lambda d: not self._is_simple_node(d))\n\n    def _is_simple_node(self, n):\n        \"\"\" Returns True for nodes that are \"simple\" - i.e. nodes that always\n            have higher precedence than operators.\n        \"\"\"\n        return isinstance(n, (c_ast.Constant, c_ast.ID, c_ast.ArrayRef,\n                              c_ast.StructRef, c_ast.FuncCall))\n", "pycparser/_ast_gen.py": "#-----------------------------------------------------------------\n# _ast_gen.py\n#\n# Generates the AST Node classes from a specification given in\n# a configuration file\n#\n# The design of this module was inspired by astgen.py from the\n# Python 2.5 code-base.\n#\n# Eli Bendersky [https://eli.thegreenplace.net/]\n# License: BSD\n#-----------------------------------------------------------------\nfrom string import Template\n\n\nclass ASTCodeGenerator(object):\n    def __init__(self, cfg_filename='_c_ast.cfg'):\n        \"\"\" Initialize the code generator from a configuration\n            file.\n        \"\"\"\n        self.cfg_filename = cfg_filename\n        self.node_cfg = [NodeCfg(name, contents)\n            for (name, contents) in self.parse_cfgfile(cfg_filename)]\n\n    def generate(self, file=None):\n        \"\"\" Generates the code into file, an open file buffer.\n        \"\"\"\n        src = Template(_PROLOGUE_COMMENT).substitute(\n            cfg_filename=self.cfg_filename)\n\n        src += _PROLOGUE_CODE\n        for node_cfg in self.node_cfg:\n            src += node_cfg.generate_source() + '\\n\\n'\n\n        file.write(src)\n\n    def parse_cfgfile(self, filename):\n        \"\"\" Parse the configuration file and yield pairs of\n            (name, contents) for each node.\n        \"\"\"\n        with open(filename, \"r\") as f:\n            for line in f:\n                line = line.strip()\n                if not line or line.startswith('#'):\n                    continue\n                colon_i = line.find(':')\n                lbracket_i = line.find('[')\n                rbracket_i = line.find(']')\n                if colon_i < 1 or lbracket_i <= colon_i or rbracket_i <= lbracket_i:\n                    raise RuntimeError(\"Invalid line in %s:\\n%s\\n\" % (filename, line))\n\n                name = line[:colon_i]\n                val = line[lbracket_i + 1:rbracket_i]\n                vallist = [v.strip() for v in val.split(',')] if val else []\n                yield name, vallist\n\n\nclass NodeCfg(object):\n    \"\"\" Node configuration.\n\n        name: node name\n        contents: a list of contents - attributes and child nodes\n        See comment at the top of the configuration file for details.\n    \"\"\"\n\n    def __init__(self, name, contents):\n        self.name = name\n        self.all_entries = []\n        self.attr = []\n        self.child = []\n        self.seq_child = []\n\n        for entry in contents:\n            clean_entry = entry.rstrip('*')\n            self.all_entries.append(clean_entry)\n\n            if entry.endswith('**'):\n                self.seq_child.append(clean_entry)\n            elif entry.endswith('*'):\n                self.child.append(clean_entry)\n            else:\n                self.attr.append(entry)\n\n    def generate_source(self):\n        src = self._gen_init()\n        src += '\\n' + self._gen_children()\n        src += '\\n' + self._gen_iter()\n        src += '\\n' + self._gen_attr_names()\n        return src\n\n    def _gen_init(self):\n        src = \"class %s(Node):\\n\" % self.name\n\n        if self.all_entries:\n            args = ', '.join(self.all_entries)\n            slots = ', '.join(\"'{0}'\".format(e) for e in self.all_entries)\n            slots += \", 'coord', '__weakref__'\"\n            arglist = '(self, %s, coord=None)' % args\n        else:\n            slots = \"'coord', '__weakref__'\"\n            arglist = '(self, coord=None)'\n\n        src += \"    __slots__ = (%s)\\n\" % slots\n        src += \"    def __init__%s:\\n\" % arglist\n\n        for name in self.all_entries + ['coord']:\n            src += \"        self.%s = %s\\n\" % (name, name)\n\n        return src\n\n    def _gen_children(self):\n        src = '    def children(self):\\n'\n\n        if self.all_entries:\n            src += '        nodelist = []\\n'\n\n            for child in self.child:\n                src += (\n                    '        if self.%(child)s is not None:' +\n                    ' nodelist.append((\"%(child)s\", self.%(child)s))\\n') % (\n                        dict(child=child))\n\n            for seq_child in self.seq_child:\n                src += (\n                    '        for i, child in enumerate(self.%(child)s or []):\\n'\n                    '            nodelist.append((\"%(child)s[%%d]\" %% i, child))\\n') % (\n                        dict(child=seq_child))\n\n            src += '        return tuple(nodelist)\\n'\n        else:\n            src += '        return ()\\n'\n\n        return src\n\n    def _gen_iter(self):\n        src = '    def __iter__(self):\\n'\n\n        if self.all_entries:\n            for child in self.child:\n                src += (\n                    '        if self.%(child)s is not None:\\n' +\n                    '            yield self.%(child)s\\n') % (dict(child=child))\n\n            for seq_child in self.seq_child:\n                src += (\n                    '        for child in (self.%(child)s or []):\\n'\n                    '            yield child\\n') % (dict(child=seq_child))\n\n            if not (self.child or self.seq_child):\n                # Empty generator\n                src += (\n                    '        return\\n' +\n                    '        yield\\n')\n        else:\n            # Empty generator\n            src += (\n                '        return\\n' +\n                '        yield\\n')\n\n        return src\n\n    def _gen_attr_names(self):\n        src = \"    attr_names = (\" + ''.join(\"%r, \" % nm for nm in self.attr) + ')'\n        return src\n\n\n_PROLOGUE_COMMENT = \\\nr'''#-----------------------------------------------------------------\n# ** ATTENTION **\n# This code was automatically generated from the file:\n# $cfg_filename\n#\n# Do not modify it directly. Modify the configuration file and\n# run the generator again.\n# ** ** *** ** **\n#\n# pycparser: c_ast.py\n#\n# AST Node classes.\n#\n# Eli Bendersky [https://eli.thegreenplace.net/]\n# License: BSD\n#-----------------------------------------------------------------\n\n'''\n\n_PROLOGUE_CODE = r'''\nimport sys\n\ndef _repr(obj):\n    \"\"\"\n    Get the representation of an object, with dedicated pprint-like format for lists.\n    \"\"\"\n    if isinstance(obj, list):\n        return '[' + (',\\n '.join((_repr(e).replace('\\n', '\\n ') for e in obj))) + '\\n]'\n    else:\n        return repr(obj)\n\nclass Node(object):\n    __slots__ = ()\n    \"\"\" Abstract base class for AST nodes.\n    \"\"\"\n    def __repr__(self):\n        \"\"\" Generates a python representation of the current node\n        \"\"\"\n        result = self.__class__.__name__ + '('\n\n        indent = ''\n        separator = ''\n        for name in self.__slots__[:-2]:\n            result += separator\n            result += indent\n            result += name + '=' + (_repr(getattr(self, name)).replace('\\n', '\\n  ' + (' ' * (len(name) + len(self.__class__.__name__)))))\n\n            separator = ','\n            indent = '\\n ' + (' ' * len(self.__class__.__name__))\n\n        result += indent + ')'\n\n        return result\n\n    def children(self):\n        \"\"\" A sequence of all children that are Nodes\n        \"\"\"\n        pass\n\n    def show(self, buf=sys.stdout, offset=0, attrnames=False, nodenames=False, showcoord=False, _my_node_name=None):\n        \"\"\" Pretty print the Node and all its attributes and\n            children (recursively) to a buffer.\n\n            buf:\n                Open IO buffer into which the Node is printed.\n\n            offset:\n                Initial offset (amount of leading spaces)\n\n            attrnames:\n                True if you want to see the attribute names in\n                name=value pairs. False to only see the values.\n\n            nodenames:\n                True if you want to see the actual node names\n                within their parents.\n\n            showcoord:\n                Do you want the coordinates of each Node to be\n                displayed.\n        \"\"\"\n        lead = ' ' * offset\n        if nodenames and _my_node_name is not None:\n            buf.write(lead + self.__class__.__name__+ ' <' + _my_node_name + '>: ')\n        else:\n            buf.write(lead + self.__class__.__name__+ ': ')\n\n        if self.attr_names:\n            if attrnames:\n                nvlist = [(n, getattr(self,n)) for n in self.attr_names]\n                attrstr = ', '.join('%s=%s' % nv for nv in nvlist)\n            else:\n                vlist = [getattr(self, n) for n in self.attr_names]\n                attrstr = ', '.join('%s' % v for v in vlist)\n            buf.write(attrstr)\n\n        if showcoord:\n            buf.write(' (at %s)' % self.coord)\n        buf.write('\\n')\n\n        for (child_name, child) in self.children():\n            child.show(\n                buf,\n                offset=offset + 2,\n                attrnames=attrnames,\n                nodenames=nodenames,\n                showcoord=showcoord,\n                _my_node_name=child_name)\n\n\nclass NodeVisitor(object):\n    \"\"\" A base NodeVisitor class for visiting c_ast nodes.\n        Subclass it and define your own visit_XXX methods, where\n        XXX is the class name you want to visit with these\n        methods.\n\n        For example:\n\n        class ConstantVisitor(NodeVisitor):\n            def __init__(self):\n                self.values = []\n\n            def visit_Constant(self, node):\n                self.values.append(node.value)\n\n        Creates a list of values of all the constant nodes\n        encountered below the given node. To use it:\n\n        cv = ConstantVisitor()\n        cv.visit(node)\n\n        Notes:\n\n        *   generic_visit() will be called for AST nodes for which\n            no visit_XXX method was defined.\n        *   The children of nodes for which a visit_XXX was\n            defined will not be visited - if you need this, call\n            generic_visit() on the node.\n            You can use:\n                NodeVisitor.generic_visit(self, node)\n        *   Modeled after Python's own AST visiting facilities\n            (the ast module of Python 3.0)\n    \"\"\"\n\n    _method_cache = None\n\n    def visit(self, node):\n        \"\"\" Visit a node.\n        \"\"\"\n\n        if self._method_cache is None:\n            self._method_cache = {}\n\n        visitor = self._method_cache.get(node.__class__.__name__, None)\n        if visitor is None:\n            method = 'visit_' + node.__class__.__name__\n            visitor = getattr(self, method, self.generic_visit)\n            self._method_cache[node.__class__.__name__] = visitor\n\n        return visitor(node)\n\n    def generic_visit(self, node):\n        \"\"\" Called if no explicit visitor function exists for a\n            node. Implements preorder visiting of the node.\n        \"\"\"\n        for c in node:\n            self.visit(c)\n\n'''\n", "pycparser/c_lexer.py": "#------------------------------------------------------------------------------\n# pycparser: c_lexer.py\n#\n# CLexer class: lexer for the C language\n#\n# Eli Bendersky [https://eli.thegreenplace.net/]\n# License: BSD\n#------------------------------------------------------------------------------\nimport re\n\nfrom .ply import lex\nfrom .ply.lex import TOKEN\n\n\nclass CLexer(object):\n    \"\"\" A lexer for the C language. After building it, set the\n        input text with input(), and call token() to get new\n        tokens.\n\n        The public attribute filename can be set to an initial\n        filename, but the lexer will update it upon #line\n        directives.\n    \"\"\"\n    def __init__(self, error_func, on_lbrace_func, on_rbrace_func,\n                 type_lookup_func):\n        \"\"\" Create a new Lexer.\n\n            error_func:\n                An error function. Will be called with an error\n                message, line and column as arguments, in case of\n                an error during lexing.\n\n            on_lbrace_func, on_rbrace_func:\n                Called when an LBRACE or RBRACE is encountered\n                (likely to push/pop type_lookup_func's scope)\n\n            type_lookup_func:\n                A type lookup function. Given a string, it must\n                return True IFF this string is a name of a type\n                that was defined with a typedef earlier.\n        \"\"\"\n        self.error_func = error_func\n        self.on_lbrace_func = on_lbrace_func\n        self.on_rbrace_func = on_rbrace_func\n        self.type_lookup_func = type_lookup_func\n        self.filename = ''\n\n        # Keeps track of the last token returned from self.token()\n        self.last_token = None\n\n        # Allow either \"# line\" or \"# <num>\" to support GCC's\n        # cpp output\n        #\n        self.line_pattern = re.compile(r'([ \\t]*line\\W)|([ \\t]*\\d+)')\n        self.pragma_pattern = re.compile(r'[ \\t]*pragma\\W')\n\n    def build(self, **kwargs):\n        \"\"\" Builds the lexer from the specification. Must be\n            called after the lexer object is created.\n\n            This method exists separately, because the PLY\n            manual warns against calling lex.lex inside\n            __init__\n        \"\"\"\n        self.lexer = lex.lex(object=self, **kwargs)\n\n    def reset_lineno(self):\n        \"\"\" Resets the internal line number counter of the lexer.\n        \"\"\"\n        self.lexer.lineno = 1\n\n    def input(self, text):\n        self.lexer.input(text)\n\n    def token(self):\n        self.last_token = self.lexer.token()\n        return self.last_token\n\n    def find_tok_column(self, token):\n        \"\"\" Find the column of the token in its line.\n        \"\"\"\n        last_cr = self.lexer.lexdata.rfind('\\n', 0, token.lexpos)\n        return token.lexpos - last_cr\n\n    ######################--   PRIVATE   --######################\n\n    ##\n    ## Internal auxiliary methods\n    ##\n    def _error(self, msg, token):\n        location = self._make_tok_location(token)\n        self.error_func(msg, location[0], location[1])\n        self.lexer.skip(1)\n\n    def _make_tok_location(self, token):\n        return (token.lineno, self.find_tok_column(token))\n\n    ##\n    ## Reserved keywords\n    ##\n    keywords = (\n        'AUTO', 'BREAK', 'CASE', 'CHAR', 'CONST',\n        'CONTINUE', 'DEFAULT', 'DO', 'DOUBLE', 'ELSE', 'ENUM', 'EXTERN',\n        'FLOAT', 'FOR', 'GOTO', 'IF', 'INLINE', 'INT', 'LONG',\n        'REGISTER', 'OFFSETOF',\n        'RESTRICT', 'RETURN', 'SHORT', 'SIGNED', 'SIZEOF', 'STATIC', 'STRUCT',\n        'SWITCH', 'TYPEDEF', 'UNION', 'UNSIGNED', 'VOID',\n        'VOLATILE', 'WHILE', '__INT128',\n    )\n\n    keywords_new = (\n        '_BOOL', '_COMPLEX',\n        '_NORETURN', '_THREAD_LOCAL', '_STATIC_ASSERT',\n        '_ATOMIC', '_ALIGNOF', '_ALIGNAS',\n        '_PRAGMA',\n        )\n\n    keyword_map = {}\n\n    for keyword in keywords:\n        keyword_map[keyword.lower()] = keyword\n\n    for keyword in keywords_new:\n        keyword_map[keyword[:2].upper() + keyword[2:].lower()] = keyword\n\n    ##\n    ## All the tokens recognized by the lexer\n    ##\n    tokens = keywords + keywords_new + (\n        # Identifiers\n        'ID',\n\n        # Type identifiers (identifiers previously defined as\n        # types with typedef)\n        'TYPEID',\n\n        # constants\n        'INT_CONST_DEC', 'INT_CONST_OCT', 'INT_CONST_HEX', 'INT_CONST_BIN', 'INT_CONST_CHAR',\n        'FLOAT_CONST', 'HEX_FLOAT_CONST',\n        'CHAR_CONST',\n        'WCHAR_CONST',\n        'U8CHAR_CONST',\n        'U16CHAR_CONST',\n        'U32CHAR_CONST',\n\n        # String literals\n        'STRING_LITERAL',\n        'WSTRING_LITERAL',\n        'U8STRING_LITERAL',\n        'U16STRING_LITERAL',\n        'U32STRING_LITERAL',\n\n        # Operators\n        'PLUS', 'MINUS', 'TIMES', 'DIVIDE', 'MOD',\n        'OR', 'AND', 'NOT', 'XOR', 'LSHIFT', 'RSHIFT',\n        'LOR', 'LAND', 'LNOT',\n        'LT', 'LE', 'GT', 'GE', 'EQ', 'NE',\n\n        # Assignment\n        'EQUALS', 'TIMESEQUAL', 'DIVEQUAL', 'MODEQUAL',\n        'PLUSEQUAL', 'MINUSEQUAL',\n        'LSHIFTEQUAL','RSHIFTEQUAL', 'ANDEQUAL', 'XOREQUAL',\n        'OREQUAL',\n\n        # Increment/decrement\n        'PLUSPLUS', 'MINUSMINUS',\n\n        # Structure dereference (->)\n        'ARROW',\n\n        # Conditional operator (?)\n        'CONDOP',\n\n        # Delimiters\n        'LPAREN', 'RPAREN',         # ( )\n        'LBRACKET', 'RBRACKET',     # [ ]\n        'LBRACE', 'RBRACE',         # { }\n        'COMMA', 'PERIOD',          # . ,\n        'SEMI', 'COLON',            # ; :\n\n        # Ellipsis (...)\n        'ELLIPSIS',\n\n        # pre-processor\n        'PPHASH',       # '#'\n        'PPPRAGMA',     # 'pragma'\n        'PPPRAGMASTR',\n    )\n\n    ##\n    ## Regexes for use in tokens\n    ##\n    ##\n\n    # valid C identifiers (K&R2: A.2.3), plus '$' (supported by some compilers)\n    identifier = r'[a-zA-Z_$][0-9a-zA-Z_$]*'\n\n    hex_prefix = '0[xX]'\n    hex_digits = '[0-9a-fA-F]+'\n    bin_prefix = '0[bB]'\n    bin_digits = '[01]+'\n\n    # integer constants (K&R2: A.2.5.1)\n    integer_suffix_opt = r'(([uU]ll)|([uU]LL)|(ll[uU]?)|(LL[uU]?)|([uU][lL])|([lL][uU]?)|[uU])?'\n    decimal_constant = '(0'+integer_suffix_opt+')|([1-9][0-9]*'+integer_suffix_opt+')'\n    octal_constant = '0[0-7]*'+integer_suffix_opt\n    hex_constant = hex_prefix+hex_digits+integer_suffix_opt\n    bin_constant = bin_prefix+bin_digits+integer_suffix_opt\n\n    bad_octal_constant = '0[0-7]*[89]'\n\n    # character constants (K&R2: A.2.5.2)\n    # Note: a-zA-Z and '.-~^_!=&;,' are allowed as escape chars to support #line\n    # directives with Windows paths as filenames (..\\..\\dir\\file)\n    # For the same reason, decimal_escape allows all digit sequences. We want to\n    # parse all correct code, even if it means to sometimes parse incorrect\n    # code.\n    #\n    # The original regexes were taken verbatim from the C syntax definition,\n    # and were later modified to avoid worst-case exponential running time.\n    #\n    #   simple_escape = r\"\"\"([a-zA-Z._~!=&\\^\\-\\\\?'\"])\"\"\"\n    #   decimal_escape = r\"\"\"(\\d+)\"\"\"\n    #   hex_escape = r\"\"\"(x[0-9a-fA-F]+)\"\"\"\n    #   bad_escape = r\"\"\"([\\\\][^a-zA-Z._~^!=&\\^\\-\\\\?'\"x0-7])\"\"\"\n    #\n    # The following modifications were made to avoid the ambiguity that allowed backtracking:\n    # (https://github.com/eliben/pycparser/issues/61)\n    #\n    # - \\x was removed from simple_escape, unless it was not followed by a hex digit, to avoid ambiguity with hex_escape.\n    # - hex_escape allows one or more hex characters, but requires that the next character(if any) is not hex\n    # - decimal_escape allows one or more decimal characters, but requires that the next character(if any) is not a decimal\n    # - bad_escape does not allow any decimals (8-9), to avoid conflicting with the permissive decimal_escape.\n    #\n    # Without this change, python's `re` module would recursively try parsing each ambiguous escape sequence in multiple ways.\n    # e.g. `\\123` could be parsed as `\\1`+`23`, `\\12`+`3`, and `\\123`.\n\n    simple_escape = r\"\"\"([a-wyzA-Z._~!=&\\^\\-\\\\?'\"]|x(?![0-9a-fA-F]))\"\"\"\n    decimal_escape = r\"\"\"(\\d+)(?!\\d)\"\"\"\n    hex_escape = r\"\"\"(x[0-9a-fA-F]+)(?![0-9a-fA-F])\"\"\"\n    bad_escape = r\"\"\"([\\\\][^a-zA-Z._~^!=&\\^\\-\\\\?'\"x0-9])\"\"\"\n\n    escape_sequence = r\"\"\"(\\\\(\"\"\"+simple_escape+'|'+decimal_escape+'|'+hex_escape+'))'\n\n    # This complicated regex with lookahead might be slow for strings, so because all of the valid escapes (including \\x) allowed\n    # 0 or more non-escaped characters after the first character, simple_escape+decimal_escape+hex_escape got simplified to\n\n    escape_sequence_start_in_string = r\"\"\"(\\\\[0-9a-zA-Z._~!=&\\^\\-\\\\?'\"])\"\"\"\n\n    cconst_char = r\"\"\"([^'\\\\\\n]|\"\"\"+escape_sequence+')'\n    char_const = \"'\"+cconst_char+\"'\"\n    wchar_const = 'L'+char_const\n    u8char_const = 'u8'+char_const\n    u16char_const = 'u'+char_const\n    u32char_const = 'U'+char_const\n    multicharacter_constant = \"'\"+cconst_char+\"{2,4}'\"\n    unmatched_quote = \"('\"+cconst_char+\"*\\\\n)|('\"+cconst_char+\"*$)\"\n    bad_char_const = r\"\"\"('\"\"\"+cconst_char+\"\"\"[^'\\n]+')|('')|('\"\"\"+bad_escape+r\"\"\"[^'\\n]*')\"\"\"\n\n    # string literals (K&R2: A.2.6)\n    string_char = r\"\"\"([^\"\\\\\\n]|\"\"\"+escape_sequence_start_in_string+')'\n    string_literal = '\"'+string_char+'*\"'\n    wstring_literal = 'L'+string_literal\n    u8string_literal = 'u8'+string_literal\n    u16string_literal = 'u'+string_literal\n    u32string_literal = 'U'+string_literal\n    bad_string_literal = '\"'+string_char+'*'+bad_escape+string_char+'*\"'\n\n    # floating constants (K&R2: A.2.5.3)\n    exponent_part = r\"\"\"([eE][-+]?[0-9]+)\"\"\"\n    fractional_constant = r\"\"\"([0-9]*\\.[0-9]+)|([0-9]+\\.)\"\"\"\n    floating_constant = '(((('+fractional_constant+')'+exponent_part+'?)|([0-9]+'+exponent_part+'))[FfLl]?)'\n    binary_exponent_part = r'''([pP][+-]?[0-9]+)'''\n    hex_fractional_constant = '((('+hex_digits+r\"\"\")?\\.\"\"\"+hex_digits+')|('+hex_digits+r\"\"\"\\.))\"\"\"\n    hex_floating_constant = '('+hex_prefix+'('+hex_digits+'|'+hex_fractional_constant+')'+binary_exponent_part+'[FfLl]?)'\n\n    ##\n    ## Lexer states: used for preprocessor \\n-terminated directives\n    ##\n    states = (\n        # ppline: preprocessor line directives\n        #\n        ('ppline', 'exclusive'),\n\n        # pppragma: pragma\n        #\n        ('pppragma', 'exclusive'),\n    )\n\n    def t_PPHASH(self, t):\n        r'[ \\t]*\\#'\n        if self.line_pattern.match(t.lexer.lexdata, pos=t.lexer.lexpos):\n            t.lexer.begin('ppline')\n            self.pp_line = self.pp_filename = None\n        elif self.pragma_pattern.match(t.lexer.lexdata, pos=t.lexer.lexpos):\n            t.lexer.begin('pppragma')\n        else:\n            t.type = 'PPHASH'\n            return t\n\n    ##\n    ## Rules for the ppline state\n    ##\n    @TOKEN(string_literal)\n    def t_ppline_FILENAME(self, t):\n        if self.pp_line is None:\n            self._error('filename before line number in #line', t)\n        else:\n            self.pp_filename = t.value.lstrip('\"').rstrip('\"')\n\n    @TOKEN(decimal_constant)\n    def t_ppline_LINE_NUMBER(self, t):\n        if self.pp_line is None:\n            self.pp_line = t.value\n        else:\n            # Ignore: GCC's cpp sometimes inserts a numeric flag\n            # after the file name\n            pass\n\n    def t_ppline_NEWLINE(self, t):\n        r'\\n'\n        if self.pp_line is None:\n            self._error('line number missing in #line', t)\n        else:\n            self.lexer.lineno = int(self.pp_line)\n\n            if self.pp_filename is not None:\n                self.filename = self.pp_filename\n\n        t.lexer.begin('INITIAL')\n\n    def t_ppline_PPLINE(self, t):\n        r'line'\n        pass\n\n    t_ppline_ignore = ' \\t'\n\n    def t_ppline_error(self, t):\n        self._error('invalid #line directive', t)\n\n    ##\n    ## Rules for the pppragma state\n    ##\n    def t_pppragma_NEWLINE(self, t):\n        r'\\n'\n        t.lexer.lineno += 1\n        t.lexer.begin('INITIAL')\n\n    def t_pppragma_PPPRAGMA(self, t):\n        r'pragma'\n        return t\n\n    t_pppragma_ignore = ' \\t'\n\n    def t_pppragma_STR(self, t):\n        '.+'\n        t.type = 'PPPRAGMASTR'\n        return t\n\n    def t_pppragma_error(self, t):\n        self._error('invalid #pragma directive', t)\n\n    ##\n    ## Rules for the normal state\n    ##\n    t_ignore = ' \\t'\n\n    # Newlines\n    def t_NEWLINE(self, t):\n        r'\\n+'\n        t.lexer.lineno += t.value.count(\"\\n\")\n\n    # Operators\n    t_PLUS              = r'\\+'\n    t_MINUS             = r'-'\n    t_TIMES             = r'\\*'\n    t_DIVIDE            = r'/'\n    t_MOD               = r'%'\n    t_OR                = r'\\|'\n    t_AND               = r'&'\n    t_NOT               = r'~'\n    t_XOR               = r'\\^'\n    t_LSHIFT            = r'<<'\n    t_RSHIFT            = r'>>'\n    t_LOR               = r'\\|\\|'\n    t_LAND              = r'&&'\n    t_LNOT              = r'!'\n    t_LT                = r'<'\n    t_GT                = r'>'\n    t_LE                = r'<='\n    t_GE                = r'>='\n    t_EQ                = r'=='\n    t_NE                = r'!='\n\n    # Assignment operators\n    t_EQUALS            = r'='\n    t_TIMESEQUAL        = r'\\*='\n    t_DIVEQUAL          = r'/='\n    t_MODEQUAL          = r'%='\n    t_PLUSEQUAL         = r'\\+='\n    t_MINUSEQUAL        = r'-='\n    t_LSHIFTEQUAL       = r'<<='\n    t_RSHIFTEQUAL       = r'>>='\n    t_ANDEQUAL          = r'&='\n    t_OREQUAL           = r'\\|='\n    t_XOREQUAL          = r'\\^='\n\n    # Increment/decrement\n    t_PLUSPLUS          = r'\\+\\+'\n    t_MINUSMINUS        = r'--'\n\n    # ->\n    t_ARROW             = r'->'\n\n    # ?\n    t_CONDOP            = r'\\?'\n\n    # Delimiters\n    t_LPAREN            = r'\\('\n    t_RPAREN            = r'\\)'\n    t_LBRACKET          = r'\\['\n    t_RBRACKET          = r'\\]'\n    t_COMMA             = r','\n    t_PERIOD            = r'\\.'\n    t_SEMI              = r';'\n    t_COLON             = r':'\n    t_ELLIPSIS          = r'\\.\\.\\.'\n\n    # Scope delimiters\n    # To see why on_lbrace_func is needed, consider:\n    #   typedef char TT;\n    #   void foo(int TT) { TT = 10; }\n    #   TT x = 5;\n    # Outside the function, TT is a typedef, but inside (starting and ending\n    # with the braces) it's a parameter.  The trouble begins with yacc's\n    # lookahead token.  If we open a new scope in brace_open, then TT has\n    # already been read and incorrectly interpreted as TYPEID.  So, we need\n    # to open and close scopes from within the lexer.\n    # Similar for the TT immediately outside the end of the function.\n    #\n    @TOKEN(r'\\{')\n    def t_LBRACE(self, t):\n        self.on_lbrace_func()\n        return t\n    @TOKEN(r'\\}')\n    def t_RBRACE(self, t):\n        self.on_rbrace_func()\n        return t\n\n    t_STRING_LITERAL = string_literal\n\n    # The following floating and integer constants are defined as\n    # functions to impose a strict order (otherwise, decimal\n    # is placed before the others because its regex is longer,\n    # and this is bad)\n    #\n    @TOKEN(floating_constant)\n    def t_FLOAT_CONST(self, t):\n        return t\n\n    @TOKEN(hex_floating_constant)\n    def t_HEX_FLOAT_CONST(self, t):\n        return t\n\n    @TOKEN(hex_constant)\n    def t_INT_CONST_HEX(self, t):\n        return t\n\n    @TOKEN(bin_constant)\n    def t_INT_CONST_BIN(self, t):\n        return t\n\n    @TOKEN(bad_octal_constant)\n    def t_BAD_CONST_OCT(self, t):\n        msg = \"Invalid octal constant\"\n        self._error(msg, t)\n\n    @TOKEN(octal_constant)\n    def t_INT_CONST_OCT(self, t):\n        return t\n\n    @TOKEN(decimal_constant)\n    def t_INT_CONST_DEC(self, t):\n        return t\n\n    # Must come before bad_char_const, to prevent it from\n    # catching valid char constants as invalid\n    #\n    @TOKEN(multicharacter_constant)\n    def t_INT_CONST_CHAR(self, t):\n        return t\n\n    @TOKEN(char_const)\n    def t_CHAR_CONST(self, t):\n        return t\n\n    @TOKEN(wchar_const)\n    def t_WCHAR_CONST(self, t):\n        return t\n\n    @TOKEN(u8char_const)\n    def t_U8CHAR_CONST(self, t):\n        return t\n\n    @TOKEN(u16char_const)\n    def t_U16CHAR_CONST(self, t):\n        return t\n\n    @TOKEN(u32char_const)\n    def t_U32CHAR_CONST(self, t):\n        return t\n\n    @TOKEN(unmatched_quote)\n    def t_UNMATCHED_QUOTE(self, t):\n        msg = \"Unmatched '\"\n        self._error(msg, t)\n\n    @TOKEN(bad_char_const)\n    def t_BAD_CHAR_CONST(self, t):\n        msg = \"Invalid char constant %s\" % t.value\n        self._error(msg, t)\n\n    @TOKEN(wstring_literal)\n    def t_WSTRING_LITERAL(self, t):\n        return t\n\n    @TOKEN(u8string_literal)\n    def t_U8STRING_LITERAL(self, t):\n        return t\n\n    @TOKEN(u16string_literal)\n    def t_U16STRING_LITERAL(self, t):\n        return t\n\n    @TOKEN(u32string_literal)\n    def t_U32STRING_LITERAL(self, t):\n        return t\n\n    # unmatched string literals are caught by the preprocessor\n\n    @TOKEN(bad_string_literal)\n    def t_BAD_STRING_LITERAL(self, t):\n        msg = \"String contains invalid escape code\"\n        self._error(msg, t)\n\n    @TOKEN(identifier)\n    def t_ID(self, t):\n        t.type = self.keyword_map.get(t.value, \"ID\")\n        if t.type == 'ID' and self.type_lookup_func(t.value):\n            t.type = \"TYPEID\"\n        return t\n\n    def t_error(self, t):\n        msg = 'Illegal character %s' % repr(t.value[0])\n        self._error(msg, t)\n", "pycparser/c_parser.py": "#------------------------------------------------------------------------------\n# pycparser: c_parser.py\n#\n# CParser class: Parser and AST builder for the C language\n#\n# Eli Bendersky [https://eli.thegreenplace.net/]\n# License: BSD\n#------------------------------------------------------------------------------\nfrom .ply import yacc\n\nfrom . import c_ast\nfrom .c_lexer import CLexer\nfrom .plyparser import PLYParser, ParseError, parameterized, template\nfrom .ast_transforms import fix_switch_cases, fix_atomic_specifiers\n\n\n@template\nclass CParser(PLYParser):\n    def __init__(\n            self,\n            lex_optimize=True,\n            lexer=CLexer,\n            lextab='pycparser.lextab',\n            yacc_optimize=True,\n            yacctab='pycparser.yacctab',\n            yacc_debug=False,\n            taboutputdir=''):\n        \"\"\" Create a new CParser.\n\n            Some arguments for controlling the debug/optimization\n            level of the parser are provided. The defaults are\n            tuned for release/performance mode.\n            The simple rules for using them are:\n            *) When tweaking CParser/CLexer, set these to False\n            *) When releasing a stable parser, set to True\n\n            lex_optimize:\n                Set to False when you're modifying the lexer.\n                Otherwise, changes in the lexer won't be used, if\n                some lextab.py file exists.\n                When releasing with a stable lexer, set to True\n                to save the re-generation of the lexer table on\n                each run.\n\n            lexer:\n                Set this parameter to define the lexer to use if\n                you're not using the default CLexer.\n\n            lextab:\n                Points to the lex table that's used for optimized\n                mode. Only if you're modifying the lexer and want\n                some tests to avoid re-generating the table, make\n                this point to a local lex table file (that's been\n                earlier generated with lex_optimize=True)\n\n            yacc_optimize:\n                Set to False when you're modifying the parser.\n                Otherwise, changes in the parser won't be used, if\n                some parsetab.py file exists.\n                When releasing with a stable parser, set to True\n                to save the re-generation of the parser table on\n                each run.\n\n            yacctab:\n                Points to the yacc table that's used for optimized\n                mode. Only if you're modifying the parser, make\n                this point to a local yacc table file\n\n            yacc_debug:\n                Generate a parser.out file that explains how yacc\n                built the parsing table from the grammar.\n\n            taboutputdir:\n                Set this parameter to control the location of generated\n                lextab and yacctab files.\n        \"\"\"\n        self.clex = lexer(\n            error_func=self._lex_error_func,\n            on_lbrace_func=self._lex_on_lbrace_func,\n            on_rbrace_func=self._lex_on_rbrace_func,\n            type_lookup_func=self._lex_type_lookup_func)\n\n        self.clex.build(\n            optimize=lex_optimize,\n            lextab=lextab,\n            outputdir=taboutputdir)\n        self.tokens = self.clex.tokens\n\n        rules_with_opt = [\n            'abstract_declarator',\n            'assignment_expression',\n            'declaration_list',\n            'declaration_specifiers_no_type',\n            'designation',\n            'expression',\n            'identifier_list',\n            'init_declarator_list',\n            'id_init_declarator_list',\n            'initializer_list',\n            'parameter_type_list',\n            'block_item_list',\n            'type_qualifier_list',\n            'struct_declarator_list'\n        ]\n\n        for rule in rules_with_opt:\n            self._create_opt_rule(rule)\n\n        self.cparser = yacc.yacc(\n            module=self,\n            start='translation_unit_or_empty',\n            debug=yacc_debug,\n            optimize=yacc_optimize,\n            tabmodule=yacctab,\n            outputdir=taboutputdir)\n\n        # Stack of scopes for keeping track of symbols. _scope_stack[-1] is\n        # the current (topmost) scope. Each scope is a dictionary that\n        # specifies whether a name is a type. If _scope_stack[n][name] is\n        # True, 'name' is currently a type in the scope. If it's False,\n        # 'name' is used in the scope but not as a type (for instance, if we\n        # saw: int name;\n        # If 'name' is not a key in _scope_stack[n] then 'name' was not defined\n        # in this scope at all.\n        self._scope_stack = [dict()]\n\n        # Keeps track of the last token given to yacc (the lookahead token)\n        self._last_yielded_token = None\n\n    def parse(self, text, filename='', debug=False):\n        \"\"\" Parses C code and returns an AST.\n\n            text:\n                A string containing the C source code\n\n            filename:\n                Name of the file being parsed (for meaningful\n                error messages)\n\n            debug:\n                Debug flag to YACC\n        \"\"\"\n        self.clex.filename = filename\n        self.clex.reset_lineno()\n        self._scope_stack = [dict()]\n        self._last_yielded_token = None\n        return self.cparser.parse(\n                input=text,\n                lexer=self.clex,\n                debug=debug)\n\n    ######################--   PRIVATE   --######################\n\n    def _push_scope(self):\n        self._scope_stack.append(dict())\n\n    def _pop_scope(self):\n        assert len(self._scope_stack) > 1\n        self._scope_stack.pop()\n\n    def _add_typedef_name(self, name, coord):\n        \"\"\" Add a new typedef name (ie a TYPEID) to the current scope\n        \"\"\"\n        if not self._scope_stack[-1].get(name, True):\n            self._parse_error(\n                \"Typedef %r previously declared as non-typedef \"\n                \"in this scope\" % name, coord)\n        self._scope_stack[-1][name] = True\n\n    def _add_identifier(self, name, coord):\n        \"\"\" Add a new object, function, or enum member name (ie an ID) to the\n            current scope\n        \"\"\"\n        if self._scope_stack[-1].get(name, False):\n            self._parse_error(\n                \"Non-typedef %r previously declared as typedef \"\n                \"in this scope\" % name, coord)\n        self._scope_stack[-1][name] = False\n\n    def _is_type_in_scope(self, name):\n        \"\"\" Is *name* a typedef-name in the current scope?\n        \"\"\"\n        for scope in reversed(self._scope_stack):\n            # If name is an identifier in this scope it shadows typedefs in\n            # higher scopes.\n            in_scope = scope.get(name)\n            if in_scope is not None: return in_scope\n        return False\n\n    def _lex_error_func(self, msg, line, column):\n        self._parse_error(msg, self._coord(line, column))\n\n    def _lex_on_lbrace_func(self):\n        self._push_scope()\n\n    def _lex_on_rbrace_func(self):\n        self._pop_scope()\n\n    def _lex_type_lookup_func(self, name):\n        \"\"\" Looks up types that were previously defined with\n            typedef.\n            Passed to the lexer for recognizing identifiers that\n            are types.\n        \"\"\"\n        is_type = self._is_type_in_scope(name)\n        return is_type\n\n    def _get_yacc_lookahead_token(self):\n        \"\"\" We need access to yacc's lookahead token in certain cases.\n            This is the last token yacc requested from the lexer, so we\n            ask the lexer.\n        \"\"\"\n        return self.clex.last_token\n\n    # To understand what's going on here, read sections A.8.5 and\n    # A.8.6 of K&R2 very carefully.\n    #\n    # A C type consists of a basic type declaration, with a list\n    # of modifiers. For example:\n    #\n    # int *c[5];\n    #\n    # The basic declaration here is 'int c', and the pointer and\n    # the array are the modifiers.\n    #\n    # Basic declarations are represented by TypeDecl (from module c_ast) and the\n    # modifiers are FuncDecl, PtrDecl and ArrayDecl.\n    #\n    # The standard states that whenever a new modifier is parsed, it should be\n    # added to the end of the list of modifiers. For example:\n    #\n    # K&R2 A.8.6.2: Array Declarators\n    #\n    # In a declaration T D where D has the form\n    #   D1 [constant-expression-opt]\n    # and the type of the identifier in the declaration T D1 is\n    # \"type-modifier T\", the type of the\n    # identifier of D is \"type-modifier array of T\"\n    #\n    # This is what this method does. The declarator it receives\n    # can be a list of declarators ending with TypeDecl. It\n    # tacks the modifier to the end of this list, just before\n    # the TypeDecl.\n    #\n    # Additionally, the modifier may be a list itself. This is\n    # useful for pointers, that can come as a chain from the rule\n    # p_pointer. In this case, the whole modifier list is spliced\n    # into the new location.\n    def _type_modify_decl(self, decl, modifier):\n        \"\"\" Tacks a type modifier on a declarator, and returns\n            the modified declarator.\n\n            Note: the declarator and modifier may be modified\n        \"\"\"\n        #~ print '****'\n        #~ decl.show(offset=3)\n        #~ modifier.show(offset=3)\n        #~ print '****'\n\n        modifier_head = modifier\n        modifier_tail = modifier\n\n        # The modifier may be a nested list. Reach its tail.\n        while modifier_tail.type:\n            modifier_tail = modifier_tail.type\n\n        # If the decl is a basic type, just tack the modifier onto it.\n        if isinstance(decl, c_ast.TypeDecl):\n            modifier_tail.type = decl\n            return modifier\n        else:\n            # Otherwise, the decl is a list of modifiers. Reach\n            # its tail and splice the modifier onto the tail,\n            # pointing to the underlying basic type.\n            decl_tail = decl\n\n            while not isinstance(decl_tail.type, c_ast.TypeDecl):\n                decl_tail = decl_tail.type\n\n            modifier_tail.type = decl_tail.type\n            decl_tail.type = modifier_head\n            return decl\n\n    # Due to the order in which declarators are constructed,\n    # they have to be fixed in order to look like a normal AST.\n    #\n    # When a declaration arrives from syntax construction, it has\n    # these problems:\n    # * The innermost TypeDecl has no type (because the basic\n    #   type is only known at the uppermost declaration level)\n    # * The declaration has no variable name, since that is saved\n    #   in the innermost TypeDecl\n    # * The typename of the declaration is a list of type\n    #   specifiers, and not a node. Here, basic identifier types\n    #   should be separated from more complex types like enums\n    #   and structs.\n    #\n    # This method fixes these problems.\n    def _fix_decl_name_type(self, decl, typename):\n        \"\"\" Fixes a declaration. Modifies decl.\n        \"\"\"\n        # Reach the underlying basic type\n        #\n        type = decl\n        while not isinstance(type, c_ast.TypeDecl):\n            type = type.type\n\n        decl.name = type.declname\n        type.quals = decl.quals[:]\n\n        # The typename is a list of types. If any type in this\n        # list isn't an IdentifierType, it must be the only\n        # type in the list (it's illegal to declare \"int enum ..\")\n        # If all the types are basic, they're collected in the\n        # IdentifierType holder.\n        for tn in typename:\n            if not isinstance(tn, c_ast.IdentifierType):\n                if len(typename) > 1:\n                    self._parse_error(\n                        \"Invalid multiple types specified\", tn.coord)\n                else:\n                    type.type = tn\n                    return decl\n\n        if not typename:\n            # Functions default to returning int\n            #\n            if not isinstance(decl.type, c_ast.FuncDecl):\n                self._parse_error(\n                        \"Missing type in declaration\", decl.coord)\n            type.type = c_ast.IdentifierType(\n                    ['int'],\n                    coord=decl.coord)\n        else:\n            # At this point, we know that typename is a list of IdentifierType\n            # nodes. Concatenate all the names into a single list.\n            #\n            type.type = c_ast.IdentifierType(\n                [name for id in typename for name in id.names],\n                coord=typename[0].coord)\n        return decl\n\n    def _add_declaration_specifier(self, declspec, newspec, kind, append=False):\n        \"\"\" Declaration specifiers are represented by a dictionary\n            with the entries:\n            * qual: a list of type qualifiers\n            * storage: a list of storage type qualifiers\n            * type: a list of type specifiers\n            * function: a list of function specifiers\n            * alignment: a list of alignment specifiers\n\n            This method is given a declaration specifier, and a\n            new specifier of a given kind.\n            If `append` is True, the new specifier is added to the end of\n            the specifiers list, otherwise it's added at the beginning.\n            Returns the declaration specifier, with the new\n            specifier incorporated.\n        \"\"\"\n        spec = declspec or dict(qual=[], storage=[], type=[], function=[], alignment=[])\n\n        if append:\n            spec[kind].append(newspec)\n        else:\n            spec[kind].insert(0, newspec)\n\n        return spec\n\n    def _build_declarations(self, spec, decls, typedef_namespace=False):\n        \"\"\" Builds a list of declarations all sharing the given specifiers.\n            If typedef_namespace is true, each declared name is added\n            to the \"typedef namespace\", which also includes objects,\n            functions, and enum constants.\n        \"\"\"\n        is_typedef = 'typedef' in spec['storage']\n        declarations = []\n\n        # Bit-fields are allowed to be unnamed.\n        if decls[0].get('bitsize') is not None:\n            pass\n\n        # When redeclaring typedef names as identifiers in inner scopes, a\n        # problem can occur where the identifier gets grouped into\n        # spec['type'], leaving decl as None.  This can only occur for the\n        # first declarator.\n        elif decls[0]['decl'] is None:\n            if len(spec['type']) < 2 or len(spec['type'][-1].names) != 1 or \\\n                    not self._is_type_in_scope(spec['type'][-1].names[0]):\n                coord = '?'\n                for t in spec['type']:\n                    if hasattr(t, 'coord'):\n                        coord = t.coord\n                        break\n                self._parse_error('Invalid declaration', coord)\n\n            # Make this look as if it came from \"direct_declarator:ID\"\n            decls[0]['decl'] = c_ast.TypeDecl(\n                declname=spec['type'][-1].names[0],\n                type=None,\n                quals=None,\n                align=spec['alignment'],\n                coord=spec['type'][-1].coord)\n            # Remove the \"new\" type's name from the end of spec['type']\n            del spec['type'][-1]\n\n        # A similar problem can occur where the declaration ends up looking\n        # like an abstract declarator.  Give it a name if this is the case.\n        elif not isinstance(decls[0]['decl'], (\n                c_ast.Enum, c_ast.Struct, c_ast.Union, c_ast.IdentifierType)):\n            decls_0_tail = decls[0]['decl']\n            while not isinstance(decls_0_tail, c_ast.TypeDecl):\n                decls_0_tail = decls_0_tail.type\n            if decls_0_tail.declname is None:\n                decls_0_tail.declname = spec['type'][-1].names[0]\n                del spec['type'][-1]\n\n        for decl in decls:\n            assert decl['decl'] is not None\n            if is_typedef:\n                declaration = c_ast.Typedef(\n                    name=None,\n                    quals=spec['qual'],\n                    storage=spec['storage'],\n                    type=decl['decl'],\n                    coord=decl['decl'].coord)\n            else:\n                declaration = c_ast.Decl(\n                    name=None,\n                    quals=spec['qual'],\n                    align=spec['alignment'],\n                    storage=spec['storage'],\n                    funcspec=spec['function'],\n                    type=decl['decl'],\n                    init=decl.get('init'),\n                    bitsize=decl.get('bitsize'),\n                    coord=decl['decl'].coord)\n\n            if isinstance(declaration.type, (\n                    c_ast.Enum, c_ast.Struct, c_ast.Union,\n                    c_ast.IdentifierType)):\n                fixed_decl = declaration\n            else:\n                fixed_decl = self._fix_decl_name_type(declaration, spec['type'])\n\n            # Add the type name defined by typedef to a\n            # symbol table (for usage in the lexer)\n            if typedef_namespace:\n                if is_typedef:\n                    self._add_typedef_name(fixed_decl.name, fixed_decl.coord)\n                else:\n                    self._add_identifier(fixed_decl.name, fixed_decl.coord)\n\n            fixed_decl = fix_atomic_specifiers(fixed_decl)\n            declarations.append(fixed_decl)\n\n        return declarations\n\n    def _build_function_definition(self, spec, decl, param_decls, body):\n        \"\"\" Builds a function definition.\n        \"\"\"\n        if 'typedef' in spec['storage']:\n            self._parse_error(\"Invalid typedef\", decl.coord)\n\n        declaration = self._build_declarations(\n            spec=spec,\n            decls=[dict(decl=decl, init=None)],\n            typedef_namespace=True)[0]\n\n        return c_ast.FuncDef(\n            decl=declaration,\n            param_decls=param_decls,\n            body=body,\n            coord=decl.coord)\n\n    def _select_struct_union_class(self, token):\n        \"\"\" Given a token (either STRUCT or UNION), selects the\n            appropriate AST class.\n        \"\"\"\n        if token == 'struct':\n            return c_ast.Struct\n        else:\n            return c_ast.Union\n\n    ##\n    ## Precedence and associativity of operators\n    ##\n    # If this changes, c_generator.CGenerator.precedence_map needs to change as\n    # well\n    precedence = (\n        ('left', 'LOR'),\n        ('left', 'LAND'),\n        ('left', 'OR'),\n        ('left', 'XOR'),\n        ('left', 'AND'),\n        ('left', 'EQ', 'NE'),\n        ('left', 'GT', 'GE', 'LT', 'LE'),\n        ('left', 'RSHIFT', 'LSHIFT'),\n        ('left', 'PLUS', 'MINUS'),\n        ('left', 'TIMES', 'DIVIDE', 'MOD')\n    )\n\n    ##\n    ## Grammar productions\n    ## Implementation of the BNF defined in K&R2 A.13\n    ##\n\n    # Wrapper around a translation unit, to allow for empty input.\n    # Not strictly part of the C99 Grammar, but useful in practice.\n    def p_translation_unit_or_empty(self, p):\n        \"\"\" translation_unit_or_empty   : translation_unit\n                                        | empty\n        \"\"\"\n        if p[1] is None:\n            p[0] = c_ast.FileAST([])\n        else:\n            p[0] = c_ast.FileAST(p[1])\n\n    def p_translation_unit_1(self, p):\n        \"\"\" translation_unit    : external_declaration\n        \"\"\"\n        # Note: external_declaration is already a list\n        p[0] = p[1]\n\n    def p_translation_unit_2(self, p):\n        \"\"\" translation_unit    : translation_unit external_declaration\n        \"\"\"\n        p[1].extend(p[2])\n        p[0] = p[1]\n\n    # Declarations always come as lists (because they can be\n    # several in one line), so we wrap the function definition\n    # into a list as well, to make the return value of\n    # external_declaration homogeneous.\n    def p_external_declaration_1(self, p):\n        \"\"\" external_declaration    : function_definition\n        \"\"\"\n        p[0] = [p[1]]\n\n    def p_external_declaration_2(self, p):\n        \"\"\" external_declaration    : declaration\n        \"\"\"\n        p[0] = p[1]\n\n    def p_external_declaration_3(self, p):\n        \"\"\" external_declaration    : pp_directive\n                                    | pppragma_directive\n        \"\"\"\n        p[0] = [p[1]]\n\n    def p_external_declaration_4(self, p):\n        \"\"\" external_declaration    : SEMI\n        \"\"\"\n        p[0] = []\n\n    def p_external_declaration_5(self, p):\n        \"\"\" external_declaration    : static_assert\n        \"\"\"\n        p[0] = p[1]\n\n    def p_static_assert_declaration(self, p):\n        \"\"\" static_assert           : _STATIC_ASSERT LPAREN constant_expression COMMA unified_string_literal RPAREN\n                                    | _STATIC_ASSERT LPAREN constant_expression RPAREN\n        \"\"\"\n        if len(p) == 5:\n            p[0] = [c_ast.StaticAssert(p[3], None, self._token_coord(p, 1))]\n        else:\n            p[0] = [c_ast.StaticAssert(p[3], p[5], self._token_coord(p, 1))]\n\n    def p_pp_directive(self, p):\n        \"\"\" pp_directive  : PPHASH\n        \"\"\"\n        self._parse_error('Directives not supported yet',\n                          self._token_coord(p, 1))\n\n    # This encompasses two types of C99-compatible pragmas:\n    # - The #pragma directive:\n    #       # pragma character_sequence\n    # - The _Pragma unary operator:\n    #       _Pragma ( \" string_literal \" )\n    def p_pppragma_directive(self, p):\n        \"\"\" pppragma_directive      : PPPRAGMA\n                                    | PPPRAGMA PPPRAGMASTR\n                                    | _PRAGMA LPAREN unified_string_literal RPAREN\n        \"\"\"\n        if len(p) == 5:\n            p[0] = c_ast.Pragma(p[3], self._token_coord(p, 2))\n        elif len(p) == 3:\n            p[0] = c_ast.Pragma(p[2], self._token_coord(p, 2))\n        else:\n            p[0] = c_ast.Pragma(\"\", self._token_coord(p, 1))\n\n    def p_pppragma_directive_list(self, p):\n        \"\"\" pppragma_directive_list : pppragma_directive\n                                    | pppragma_directive_list pppragma_directive\n        \"\"\"\n        p[0] = [p[1]] if len(p) == 2 else p[1] + [p[2]]\n\n    # In function definitions, the declarator can be followed by\n    # a declaration list, for old \"K&R style\" function definitios.\n    def p_function_definition_1(self, p):\n        \"\"\" function_definition : id_declarator declaration_list_opt compound_statement\n        \"\"\"\n        # no declaration specifiers - 'int' becomes the default type\n        spec = dict(\n            qual=[],\n            alignment=[],\n            storage=[],\n            type=[c_ast.IdentifierType(['int'],\n                                       coord=self._token_coord(p, 1))],\n            function=[])\n\n        p[0] = self._build_function_definition(\n            spec=spec,\n            decl=p[1],\n            param_decls=p[2],\n            body=p[3])\n\n    def p_function_definition_2(self, p):\n        \"\"\" function_definition : declaration_specifiers id_declarator declaration_list_opt compound_statement\n        \"\"\"\n        spec = p[1]\n\n        p[0] = self._build_function_definition(\n            spec=spec,\n            decl=p[2],\n            param_decls=p[3],\n            body=p[4])\n\n    # Note, according to C18 A.2.2 6.7.10 static_assert-declaration _Static_assert\n    # is a declaration, not a statement. We additionally recognise it as a statement\n    # to fix parsing of _Static_assert inside the functions.\n    #\n    def p_statement(self, p):\n        \"\"\" statement   : labeled_statement\n                        | expression_statement\n                        | compound_statement\n                        | selection_statement\n                        | iteration_statement\n                        | jump_statement\n                        | pppragma_directive\n                        | static_assert\n        \"\"\"\n        p[0] = p[1]\n\n    # A pragma is generally considered a decorator rather than an actual\n    # statement. Still, for the purposes of analyzing an abstract syntax tree of\n    # C code, pragma's should not be ignored and were previously treated as a\n    # statement. This presents a problem for constructs that take a statement\n    # such as labeled_statements, selection_statements, and\n    # iteration_statements, causing a misleading structure in the AST. For\n    # example, consider the following C code.\n    #\n    #   for (int i = 0; i < 3; i++)\n    #       #pragma omp critical\n    #       sum += 1;\n    #\n    # This code will compile and execute \"sum += 1;\" as the body of the for\n    # loop. Previous implementations of PyCParser would render the AST for this\n    # block of code as follows:\n    #\n    #   For:\n    #     DeclList:\n    #       Decl: i, [], [], []\n    #         TypeDecl: i, []\n    #           IdentifierType: ['int']\n    #         Constant: int, 0\n    #     BinaryOp: <\n    #       ID: i\n    #       Constant: int, 3\n    #     UnaryOp: p++\n    #       ID: i\n    #     Pragma: omp critical\n    #   Assignment: +=\n    #     ID: sum\n    #     Constant: int, 1\n    #\n    # This AST misleadingly takes the Pragma as the body of the loop and the\n    # assignment then becomes a sibling of the loop.\n    #\n    # To solve edge cases like these, the pragmacomp_or_statement rule groups\n    # a pragma and its following statement (which would otherwise be orphaned)\n    # using a compound block, effectively turning the above code into:\n    #\n    #   for (int i = 0; i < 3; i++) {\n    #       #pragma omp critical\n    #       sum += 1;\n    #   }\n    def p_pragmacomp_or_statement(self, p):\n        \"\"\" pragmacomp_or_statement     : pppragma_directive_list statement\n                                        | statement\n        \"\"\"\n        if len(p) == 3:\n            p[0] = c_ast.Compound(\n                block_items=p[1]+[p[2]],\n                coord=self._token_coord(p, 1))\n        else:\n            p[0] = p[1]\n\n    # In C, declarations can come several in a line:\n    #   int x, *px, romulo = 5;\n    #\n    # However, for the AST, we will split them to separate Decl\n    # nodes.\n    #\n    # This rule splits its declarations and always returns a list\n    # of Decl nodes, even if it's one element long.\n    #\n    def p_decl_body(self, p):\n        \"\"\" decl_body : declaration_specifiers init_declarator_list_opt\n                      | declaration_specifiers_no_type id_init_declarator_list_opt\n        \"\"\"\n        spec = p[1]\n\n        # p[2] (init_declarator_list_opt) is either a list or None\n        #\n        if p[2] is None:\n            # By the standard, you must have at least one declarator unless\n            # declaring a structure tag, a union tag, or the members of an\n            # enumeration.\n            #\n            ty = spec['type']\n            s_u_or_e = (c_ast.Struct, c_ast.Union, c_ast.Enum)\n            if len(ty) == 1 and isinstance(ty[0], s_u_or_e):\n                decls = [c_ast.Decl(\n                    name=None,\n                    quals=spec['qual'],\n                    align=spec['alignment'],\n                    storage=spec['storage'],\n                    funcspec=spec['function'],\n                    type=ty[0],\n                    init=None,\n                    bitsize=None,\n                    coord=ty[0].coord)]\n\n            # However, this case can also occur on redeclared identifiers in\n            # an inner scope.  The trouble is that the redeclared type's name\n            # gets grouped into declaration_specifiers; _build_declarations\n            # compensates for this.\n            #\n            else:\n                decls = self._build_declarations(\n                    spec=spec,\n                    decls=[dict(decl=None, init=None)],\n                    typedef_namespace=True)\n\n        else:\n            decls = self._build_declarations(\n                spec=spec,\n                decls=p[2],\n                typedef_namespace=True)\n\n        p[0] = decls\n\n    # The declaration has been split to a decl_body sub-rule and\n    # SEMI, because having them in a single rule created a problem\n    # for defining typedefs.\n    #\n    # If a typedef line was directly followed by a line using the\n    # type defined with the typedef, the type would not be\n    # recognized. This is because to reduce the declaration rule,\n    # the parser's lookahead asked for the token after SEMI, which\n    # was the type from the next line, and the lexer had no chance\n    # to see the updated type symbol table.\n    #\n    # Splitting solves this problem, because after seeing SEMI,\n    # the parser reduces decl_body, which actually adds the new\n    # type into the table to be seen by the lexer before the next\n    # line is reached.\n    def p_declaration(self, p):\n        \"\"\" declaration : decl_body SEMI\n        \"\"\"\n        p[0] = p[1]\n\n    # Since each declaration is a list of declarations, this\n    # rule will combine all the declarations and return a single\n    # list\n    #\n    def p_declaration_list(self, p):\n        \"\"\" declaration_list    : declaration\n                                | declaration_list declaration\n        \"\"\"\n        p[0] = p[1] if len(p) == 2 else p[1] + p[2]\n\n    # To know when declaration-specifiers end and declarators begin,\n    # we require declaration-specifiers to have at least one\n    # type-specifier, and disallow typedef-names after we've seen any\n    # type-specifier. These are both required by the spec.\n    #\n    def p_declaration_specifiers_no_type_1(self, p):\n        \"\"\" declaration_specifiers_no_type  : type_qualifier declaration_specifiers_no_type_opt\n        \"\"\"\n        p[0] = self._add_declaration_specifier(p[2], p[1], 'qual')\n\n    def p_declaration_specifiers_no_type_2(self, p):\n        \"\"\" declaration_specifiers_no_type  : storage_class_specifier declaration_specifiers_no_type_opt\n        \"\"\"\n        p[0] = self._add_declaration_specifier(p[2], p[1], 'storage')\n\n    def p_declaration_specifiers_no_type_3(self, p):\n        \"\"\" declaration_specifiers_no_type  : function_specifier declaration_specifiers_no_type_opt\n        \"\"\"\n        p[0] = self._add_declaration_specifier(p[2], p[1], 'function')\n\n    # Without this, `typedef _Atomic(T) U` will parse incorrectly because the\n    # _Atomic qualifier will match, instead of the specifier.\n    def p_declaration_specifiers_no_type_4(self, p):\n        \"\"\" declaration_specifiers_no_type  : atomic_specifier declaration_specifiers_no_type_opt\n        \"\"\"\n        p[0] = self._add_declaration_specifier(p[2], p[1], 'type')\n\n    def p_declaration_specifiers_no_type_5(self, p):\n        \"\"\" declaration_specifiers_no_type  : alignment_specifier declaration_specifiers_no_type_opt\n        \"\"\"\n        p[0] = self._add_declaration_specifier(p[2], p[1], 'alignment')\n\n    def p_declaration_specifiers_1(self, p):\n        \"\"\" declaration_specifiers  : declaration_specifiers type_qualifier\n        \"\"\"\n        p[0] = self._add_declaration_specifier(p[1], p[2], 'qual', append=True)\n\n    def p_declaration_specifiers_2(self, p):\n        \"\"\" declaration_specifiers  : declaration_specifiers storage_class_specifier\n        \"\"\"\n        p[0] = self._add_declaration_specifier(p[1], p[2], 'storage', append=True)\n\n    def p_declaration_specifiers_3(self, p):\n        \"\"\" declaration_specifiers  : declaration_specifiers function_specifier\n        \"\"\"\n        p[0] = self._add_declaration_specifier(p[1], p[2], 'function', append=True)\n\n    def p_declaration_specifiers_4(self, p):\n        \"\"\" declaration_specifiers  : declaration_specifiers type_specifier_no_typeid\n        \"\"\"\n        p[0] = self._add_declaration_specifier(p[1], p[2], 'type', append=True)\n\n    def p_declaration_specifiers_5(self, p):\n        \"\"\" declaration_specifiers  : type_specifier\n        \"\"\"\n        p[0] = self._add_declaration_specifier(None, p[1], 'type')\n\n    def p_declaration_specifiers_6(self, p):\n        \"\"\" declaration_specifiers  : declaration_specifiers_no_type type_specifier\n        \"\"\"\n        p[0] = self._add_declaration_specifier(p[1], p[2], 'type', append=True)\n\n    def p_declaration_specifiers_7(self, p):\n        \"\"\" declaration_specifiers  : declaration_specifiers alignment_specifier\n        \"\"\"\n        p[0] = self._add_declaration_specifier(p[1], p[2], 'alignment', append=True)\n\n    def p_storage_class_specifier(self, p):\n        \"\"\" storage_class_specifier : AUTO\n                                    | REGISTER\n                                    | STATIC\n                                    | EXTERN\n                                    | TYPEDEF\n                                    | _THREAD_LOCAL\n        \"\"\"\n        p[0] = p[1]\n\n    def p_function_specifier(self, p):\n        \"\"\" function_specifier  : INLINE\n                                | _NORETURN\n        \"\"\"\n        p[0] = p[1]\n\n    def p_type_specifier_no_typeid(self, p):\n        \"\"\" type_specifier_no_typeid  : VOID\n                                      | _BOOL\n                                      | CHAR\n                                      | SHORT\n                                      | INT\n                                      | LONG\n                                      | FLOAT\n                                      | DOUBLE\n                                      | _COMPLEX\n                                      | SIGNED\n                                      | UNSIGNED\n                                      | __INT128\n        \"\"\"\n        p[0] = c_ast.IdentifierType([p[1]], coord=self._token_coord(p, 1))\n\n    def p_type_specifier(self, p):\n        \"\"\" type_specifier  : typedef_name\n                            | enum_specifier\n                            | struct_or_union_specifier\n                            | type_specifier_no_typeid\n                            | atomic_specifier\n        \"\"\"\n        p[0] = p[1]\n\n    # See section 6.7.2.4 of the C11 standard.\n    def p_atomic_specifier(self, p):\n        \"\"\" atomic_specifier  : _ATOMIC LPAREN type_name RPAREN\n        \"\"\"\n        typ = p[3]\n        typ.quals.append('_Atomic')\n        p[0] = typ\n\n    def p_type_qualifier(self, p):\n        \"\"\" type_qualifier  : CONST\n                            | RESTRICT\n                            | VOLATILE\n                            | _ATOMIC\n        \"\"\"\n        p[0] = p[1]\n\n    def p_init_declarator_list(self, p):\n        \"\"\" init_declarator_list    : init_declarator\n                                    | init_declarator_list COMMA init_declarator\n        \"\"\"\n        p[0] = p[1] + [p[3]] if len(p) == 4 else [p[1]]\n\n    # Returns a {decl=<declarator> : init=<initializer>} dictionary\n    # If there's no initializer, uses None\n    #\n    def p_init_declarator(self, p):\n        \"\"\" init_declarator : declarator\n                            | declarator EQUALS initializer\n        \"\"\"\n        p[0] = dict(decl=p[1], init=(p[3] if len(p) > 2 else None))\n\n    def p_id_init_declarator_list(self, p):\n        \"\"\" id_init_declarator_list    : id_init_declarator\n                                       | id_init_declarator_list COMMA init_declarator\n        \"\"\"\n        p[0] = p[1] + [p[3]] if len(p) == 4 else [p[1]]\n\n    def p_id_init_declarator(self, p):\n        \"\"\" id_init_declarator : id_declarator\n                               | id_declarator EQUALS initializer\n        \"\"\"\n        p[0] = dict(decl=p[1], init=(p[3] if len(p) > 2 else None))\n\n    # Require at least one type specifier in a specifier-qualifier-list\n    #\n    def p_specifier_qualifier_list_1(self, p):\n        \"\"\" specifier_qualifier_list    : specifier_qualifier_list type_specifier_no_typeid\n        \"\"\"\n        p[0] = self._add_declaration_specifier(p[1], p[2], 'type', append=True)\n\n    def p_specifier_qualifier_list_2(self, p):\n        \"\"\" specifier_qualifier_list    : specifier_qualifier_list type_qualifier\n        \"\"\"\n        p[0] = self._add_declaration_specifier(p[1], p[2], 'qual', append=True)\n\n    def p_specifier_qualifier_list_3(self, p):\n        \"\"\" specifier_qualifier_list  : type_specifier\n        \"\"\"\n        p[0] = self._add_declaration_specifier(None, p[1], 'type')\n\n    def p_specifier_qualifier_list_4(self, p):\n        \"\"\" specifier_qualifier_list  : type_qualifier_list type_specifier\n        \"\"\"\n        p[0] = dict(qual=p[1], alignment=[], storage=[], type=[p[2]], function=[])\n\n    def p_specifier_qualifier_list_5(self, p):\n        \"\"\" specifier_qualifier_list  : alignment_specifier\n        \"\"\"\n        p[0] = dict(qual=[], alignment=[p[1]], storage=[], type=[], function=[])\n\n    def p_specifier_qualifier_list_6(self, p):\n        \"\"\" specifier_qualifier_list  : specifier_qualifier_list alignment_specifier\n        \"\"\"\n        p[0] = self._add_declaration_specifier(p[1], p[2], 'alignment')\n\n    # TYPEID is allowed here (and in other struct/enum related tag names), because\n    # struct/enum tags reside in their own namespace and can be named the same as types\n    #\n    def p_struct_or_union_specifier_1(self, p):\n        \"\"\" struct_or_union_specifier   : struct_or_union ID\n                                        | struct_or_union TYPEID\n        \"\"\"\n        klass = self._select_struct_union_class(p[1])\n        # None means no list of members\n        p[0] = klass(\n            name=p[2],\n            decls=None,\n            coord=self._token_coord(p, 2))\n\n    def p_struct_or_union_specifier_2(self, p):\n        \"\"\" struct_or_union_specifier : struct_or_union brace_open struct_declaration_list brace_close\n                                      | struct_or_union brace_open brace_close\n        \"\"\"\n        klass = self._select_struct_union_class(p[1])\n        if len(p) == 4:\n            # Empty sequence means an empty list of members\n            p[0] = klass(\n                name=None,\n                decls=[],\n                coord=self._token_coord(p, 2))\n        else:\n            p[0] = klass(\n                name=None,\n                decls=p[3],\n                coord=self._token_coord(p, 2))\n\n\n    def p_struct_or_union_specifier_3(self, p):\n        \"\"\" struct_or_union_specifier   : struct_or_union ID brace_open struct_declaration_list brace_close\n                                        | struct_or_union ID brace_open brace_close\n                                        | struct_or_union TYPEID brace_open struct_declaration_list brace_close\n                                        | struct_or_union TYPEID brace_open brace_close\n        \"\"\"\n        klass = self._select_struct_union_class(p[1])\n        if len(p) == 5:\n            # Empty sequence means an empty list of members\n            p[0] = klass(\n                name=p[2],\n                decls=[],\n                coord=self._token_coord(p, 2))\n        else:\n            p[0] = klass(\n                name=p[2],\n                decls=p[4],\n                coord=self._token_coord(p, 2))\n\n    def p_struct_or_union(self, p):\n        \"\"\" struct_or_union : STRUCT\n                            | UNION\n        \"\"\"\n        p[0] = p[1]\n\n    # Combine all declarations into a single list\n    #\n    def p_struct_declaration_list(self, p):\n        \"\"\" struct_declaration_list     : struct_declaration\n                                        | struct_declaration_list struct_declaration\n        \"\"\"\n        if len(p) == 2:\n            p[0] = p[1] or []\n        else:\n            p[0] = p[1] + (p[2] or [])\n\n    def p_struct_declaration_1(self, p):\n        \"\"\" struct_declaration : specifier_qualifier_list struct_declarator_list_opt SEMI\n        \"\"\"\n        spec = p[1]\n        assert 'typedef' not in spec['storage']\n\n        if p[2] is not None:\n            decls = self._build_declarations(\n                spec=spec,\n                decls=p[2])\n\n        elif len(spec['type']) == 1:\n            # Anonymous struct/union, gcc extension, C1x feature.\n            # Although the standard only allows structs/unions here, I see no\n            # reason to disallow other types since some compilers have typedefs\n            # here, and pycparser isn't about rejecting all invalid code.\n            #\n            node = spec['type'][0]\n            if isinstance(node, c_ast.Node):\n                decl_type = node\n            else:\n                decl_type = c_ast.IdentifierType(node)\n\n            decls = self._build_declarations(\n                spec=spec,\n                decls=[dict(decl=decl_type)])\n\n        else:\n            # Structure/union members can have the same names as typedefs.\n            # The trouble is that the member's name gets grouped into\n            # specifier_qualifier_list; _build_declarations compensates.\n            #\n            decls = self._build_declarations(\n                spec=spec,\n                decls=[dict(decl=None, init=None)])\n\n        p[0] = decls\n\n    def p_struct_declaration_2(self, p):\n        \"\"\" struct_declaration : SEMI\n        \"\"\"\n        p[0] = None\n\n    def p_struct_declaration_3(self, p):\n        \"\"\" struct_declaration : pppragma_directive\n        \"\"\"\n        p[0] = [p[1]]\n\n    def p_struct_declarator_list(self, p):\n        \"\"\" struct_declarator_list  : struct_declarator\n                                    | struct_declarator_list COMMA struct_declarator\n        \"\"\"\n        p[0] = p[1] + [p[3]] if len(p) == 4 else [p[1]]\n\n    # struct_declarator passes up a dict with the keys: decl (for\n    # the underlying declarator) and bitsize (for the bitsize)\n    #\n    def p_struct_declarator_1(self, p):\n        \"\"\" struct_declarator : declarator\n        \"\"\"\n        p[0] = {'decl': p[1], 'bitsize': None}\n\n    def p_struct_declarator_2(self, p):\n        \"\"\" struct_declarator   : declarator COLON constant_expression\n                                | COLON constant_expression\n        \"\"\"\n        if len(p) > 3:\n            p[0] = {'decl': p[1], 'bitsize': p[3]}\n        else:\n            p[0] = {'decl': c_ast.TypeDecl(None, None, None, None), 'bitsize': p[2]}\n\n    def p_enum_specifier_1(self, p):\n        \"\"\" enum_specifier  : ENUM ID\n                            | ENUM TYPEID\n        \"\"\"\n        p[0] = c_ast.Enum(p[2], None, self._token_coord(p, 1))\n\n    def p_enum_specifier_2(self, p):\n        \"\"\" enum_specifier  : ENUM brace_open enumerator_list brace_close\n        \"\"\"\n        p[0] = c_ast.Enum(None, p[3], self._token_coord(p, 1))\n\n    def p_enum_specifier_3(self, p):\n        \"\"\" enum_specifier  : ENUM ID brace_open enumerator_list brace_close\n                            | ENUM TYPEID brace_open enumerator_list brace_close\n        \"\"\"\n        p[0] = c_ast.Enum(p[2], p[4], self._token_coord(p, 1))\n\n    def p_enumerator_list(self, p):\n        \"\"\" enumerator_list : enumerator\n                            | enumerator_list COMMA\n                            | enumerator_list COMMA enumerator\n        \"\"\"\n        if len(p) == 2:\n            p[0] = c_ast.EnumeratorList([p[1]], p[1].coord)\n        elif len(p) == 3:\n            p[0] = p[1]\n        else:\n            p[1].enumerators.append(p[3])\n            p[0] = p[1]\n\n    def p_alignment_specifier(self, p):\n        \"\"\" alignment_specifier  : _ALIGNAS LPAREN type_name RPAREN\n                                 | _ALIGNAS LPAREN constant_expression RPAREN\n        \"\"\"\n        p[0] = c_ast.Alignas(p[3], self._token_coord(p, 1))\n\n    def p_enumerator(self, p):\n        \"\"\" enumerator  : ID\n                        | ID EQUALS constant_expression\n        \"\"\"\n        if len(p) == 2:\n            enumerator = c_ast.Enumerator(\n                        p[1], None,\n                        self._token_coord(p, 1))\n        else:\n            enumerator = c_ast.Enumerator(\n                        p[1], p[3],\n                        self._token_coord(p, 1))\n        self._add_identifier(enumerator.name, enumerator.coord)\n\n        p[0] = enumerator\n\n    def p_declarator(self, p):\n        \"\"\" declarator  : id_declarator\n                        | typeid_declarator\n        \"\"\"\n        p[0] = p[1]\n\n    @parameterized(('id', 'ID'), ('typeid', 'TYPEID'), ('typeid_noparen', 'TYPEID'))\n    def p_xxx_declarator_1(self, p):\n        \"\"\" xxx_declarator  : direct_xxx_declarator\n        \"\"\"\n        p[0] = p[1]\n\n    @parameterized(('id', 'ID'), ('typeid', 'TYPEID'), ('typeid_noparen', 'TYPEID'))\n    def p_xxx_declarator_2(self, p):\n        \"\"\" xxx_declarator  : pointer direct_xxx_declarator\n        \"\"\"\n        p[0] = self._type_modify_decl(p[2], p[1])\n\n    @parameterized(('id', 'ID'), ('typeid', 'TYPEID'), ('typeid_noparen', 'TYPEID'))\n    def p_direct_xxx_declarator_1(self, p):\n        \"\"\" direct_xxx_declarator   : yyy\n        \"\"\"\n        p[0] = c_ast.TypeDecl(\n            declname=p[1],\n            type=None,\n            quals=None,\n            align=None,\n            coord=self._token_coord(p, 1))\n\n    @parameterized(('id', 'ID'), ('typeid', 'TYPEID'))\n    def p_direct_xxx_declarator_2(self, p):\n        \"\"\" direct_xxx_declarator   : LPAREN xxx_declarator RPAREN\n        \"\"\"\n        p[0] = p[2]\n\n    @parameterized(('id', 'ID'), ('typeid', 'TYPEID'), ('typeid_noparen', 'TYPEID'))\n    def p_direct_xxx_declarator_3(self, p):\n        \"\"\" direct_xxx_declarator   : direct_xxx_declarator LBRACKET type_qualifier_list_opt assignment_expression_opt RBRACKET\n        \"\"\"\n        quals = (p[3] if len(p) > 5 else []) or []\n        # Accept dimension qualifiers\n        # Per C99 6.7.5.3 p7\n        arr = c_ast.ArrayDecl(\n            type=None,\n            dim=p[4] if len(p) > 5 else p[3],\n            dim_quals=quals,\n            coord=p[1].coord)\n\n        p[0] = self._type_modify_decl(decl=p[1], modifier=arr)\n\n    @parameterized(('id', 'ID'), ('typeid', 'TYPEID'), ('typeid_noparen', 'TYPEID'))\n    def p_direct_xxx_declarator_4(self, p):\n        \"\"\" direct_xxx_declarator   : direct_xxx_declarator LBRACKET STATIC type_qualifier_list_opt assignment_expression RBRACKET\n                                    | direct_xxx_declarator LBRACKET type_qualifier_list STATIC assignment_expression RBRACKET\n        \"\"\"\n        # Using slice notation for PLY objects doesn't work in Python 3 for the\n        # version of PLY embedded with pycparser; see PLY Google Code issue 30.\n        # Work around that here by listing the two elements separately.\n        listed_quals = [item if isinstance(item, list) else [item]\n            for item in [p[3],p[4]]]\n        dim_quals = [qual for sublist in listed_quals for qual in sublist\n            if qual is not None]\n        arr = c_ast.ArrayDecl(\n            type=None,\n            dim=p[5],\n            dim_quals=dim_quals,\n            coord=p[1].coord)\n\n        p[0] = self._type_modify_decl(decl=p[1], modifier=arr)\n\n    # Special for VLAs\n    #\n    @parameterized(('id', 'ID'), ('typeid', 'TYPEID'), ('typeid_noparen', 'TYPEID'))\n    def p_direct_xxx_declarator_5(self, p):\n        \"\"\" direct_xxx_declarator   : direct_xxx_declarator LBRACKET type_qualifier_list_opt TIMES RBRACKET\n        \"\"\"\n        arr = c_ast.ArrayDecl(\n            type=None,\n            dim=c_ast.ID(p[4], self._token_coord(p, 4)),\n            dim_quals=p[3] if p[3] is not None else [],\n            coord=p[1].coord)\n\n        p[0] = self._type_modify_decl(decl=p[1], modifier=arr)\n\n    @parameterized(('id', 'ID'), ('typeid', 'TYPEID'), ('typeid_noparen', 'TYPEID'))\n    def p_direct_xxx_declarator_6(self, p):\n        \"\"\" direct_xxx_declarator   : direct_xxx_declarator LPAREN parameter_type_list RPAREN\n                                    | direct_xxx_declarator LPAREN identifier_list_opt RPAREN\n        \"\"\"\n        func = c_ast.FuncDecl(\n            args=p[3],\n            type=None,\n            coord=p[1].coord)\n\n        # To see why _get_yacc_lookahead_token is needed, consider:\n        #   typedef char TT;\n        #   void foo(int TT) { TT = 10; }\n        # Outside the function, TT is a typedef, but inside (starting and\n        # ending with the braces) it's a parameter.  The trouble begins with\n        # yacc's lookahead token.  We don't know if we're declaring or\n        # defining a function until we see LBRACE, but if we wait for yacc to\n        # trigger a rule on that token, then TT will have already been read\n        # and incorrectly interpreted as TYPEID.  We need to add the\n        # parameters to the scope the moment the lexer sees LBRACE.\n        #\n        if self._get_yacc_lookahead_token().type == \"LBRACE\":\n            if func.args is not None:\n                for param in func.args.params:\n                    if isinstance(param, c_ast.EllipsisParam): break\n                    self._add_identifier(param.name, param.coord)\n\n        p[0] = self._type_modify_decl(decl=p[1], modifier=func)\n\n    def p_pointer(self, p):\n        \"\"\" pointer : TIMES type_qualifier_list_opt\n                    | TIMES type_qualifier_list_opt pointer\n        \"\"\"\n        coord = self._token_coord(p, 1)\n        # Pointer decls nest from inside out. This is important when different\n        # levels have different qualifiers. For example:\n        #\n        #  char * const * p;\n        #\n        # Means \"pointer to const pointer to char\"\n        #\n        # While:\n        #\n        #  char ** const p;\n        #\n        # Means \"const pointer to pointer to char\"\n        #\n        # So when we construct PtrDecl nestings, the leftmost pointer goes in\n        # as the most nested type.\n        nested_type = c_ast.PtrDecl(quals=p[2] or [], type=None, coord=coord)\n        if len(p) > 3:\n            tail_type = p[3]\n            while tail_type.type is not None:\n                tail_type = tail_type.type\n            tail_type.type = nested_type\n            p[0] = p[3]\n        else:\n            p[0] = nested_type\n\n    def p_type_qualifier_list(self, p):\n        \"\"\" type_qualifier_list : type_qualifier\n                                | type_qualifier_list type_qualifier\n        \"\"\"\n        p[0] = [p[1]] if len(p) == 2 else p[1] + [p[2]]\n\n    def p_parameter_type_list(self, p):\n        \"\"\" parameter_type_list : parameter_list\n                                | parameter_list COMMA ELLIPSIS\n        \"\"\"\n        if len(p) > 2:\n            p[1].params.append(c_ast.EllipsisParam(self._token_coord(p, 3)))\n\n        p[0] = p[1]\n\n    def p_parameter_list(self, p):\n        \"\"\" parameter_list  : parameter_declaration\n                            | parameter_list COMMA parameter_declaration\n        \"\"\"\n        if len(p) == 2: # single parameter\n            p[0] = c_ast.ParamList([p[1]], p[1].coord)\n        else:\n            p[1].params.append(p[3])\n            p[0] = p[1]\n\n    # From ISO/IEC 9899:TC2, 6.7.5.3.11:\n    # \"If, in a parameter declaration, an identifier can be treated either\n    #  as a typedef name or as a parameter name, it shall be taken as a\n    #  typedef name.\"\n    #\n    # Inside a parameter declaration, once we've reduced declaration specifiers,\n    # if we shift in an LPAREN and see a TYPEID, it could be either an abstract\n    # declarator or a declarator nested inside parens. This rule tells us to\n    # always treat it as an abstract declarator. Therefore, we only accept\n    # `id_declarator`s and `typeid_noparen_declarator`s.\n    def p_parameter_declaration_1(self, p):\n        \"\"\" parameter_declaration   : declaration_specifiers id_declarator\n                                    | declaration_specifiers typeid_noparen_declarator\n        \"\"\"\n        spec = p[1]\n        if not spec['type']:\n            spec['type'] = [c_ast.IdentifierType(['int'],\n                coord=self._token_coord(p, 1))]\n        p[0] = self._build_declarations(\n            spec=spec,\n            decls=[dict(decl=p[2])])[0]\n\n    def p_parameter_declaration_2(self, p):\n        \"\"\" parameter_declaration   : declaration_specifiers abstract_declarator_opt\n        \"\"\"\n        spec = p[1]\n        if not spec['type']:\n            spec['type'] = [c_ast.IdentifierType(['int'],\n                coord=self._token_coord(p, 1))]\n\n        # Parameters can have the same names as typedefs.  The trouble is that\n        # the parameter's name gets grouped into declaration_specifiers, making\n        # it look like an old-style declaration; compensate.\n        #\n        if len(spec['type']) > 1 and len(spec['type'][-1].names) == 1 and \\\n                self._is_type_in_scope(spec['type'][-1].names[0]):\n            decl = self._build_declarations(\n                    spec=spec,\n                    decls=[dict(decl=p[2], init=None)])[0]\n\n        # This truly is an old-style parameter declaration\n        #\n        else:\n            decl = c_ast.Typename(\n                name='',\n                quals=spec['qual'],\n                align=None,\n                type=p[2] or c_ast.TypeDecl(None, None, None, None),\n                coord=self._token_coord(p, 2))\n            typename = spec['type']\n            decl = self._fix_decl_name_type(decl, typename)\n\n        p[0] = decl\n\n    def p_identifier_list(self, p):\n        \"\"\" identifier_list : identifier\n                            | identifier_list COMMA identifier\n        \"\"\"\n        if len(p) == 2: # single parameter\n            p[0] = c_ast.ParamList([p[1]], p[1].coord)\n        else:\n            p[1].params.append(p[3])\n            p[0] = p[1]\n\n    def p_initializer_1(self, p):\n        \"\"\" initializer : assignment_expression\n        \"\"\"\n        p[0] = p[1]\n\n    def p_initializer_2(self, p):\n        \"\"\" initializer : brace_open initializer_list_opt brace_close\n                        | brace_open initializer_list COMMA brace_close\n        \"\"\"\n        if p[2] is None:\n            p[0] = c_ast.InitList([], self._token_coord(p, 1))\n        else:\n            p[0] = p[2]\n\n    def p_initializer_list(self, p):\n        \"\"\" initializer_list    : designation_opt initializer\n                                | initializer_list COMMA designation_opt initializer\n        \"\"\"\n        if len(p) == 3: # single initializer\n            init = p[2] if p[1] is None else c_ast.NamedInitializer(p[1], p[2])\n            p[0] = c_ast.InitList([init], p[2].coord)\n        else:\n            init = p[4] if p[3] is None else c_ast.NamedInitializer(p[3], p[4])\n            p[1].exprs.append(init)\n            p[0] = p[1]\n\n    def p_designation(self, p):\n        \"\"\" designation : designator_list EQUALS\n        \"\"\"\n        p[0] = p[1]\n\n    # Designators are represented as a list of nodes, in the order in which\n    # they're written in the code.\n    #\n    def p_designator_list(self, p):\n        \"\"\" designator_list : designator\n                            | designator_list designator\n        \"\"\"\n        p[0] = [p[1]] if len(p) == 2 else p[1] + [p[2]]\n\n    def p_designator(self, p):\n        \"\"\" designator  : LBRACKET constant_expression RBRACKET\n                        | PERIOD identifier\n        \"\"\"\n        p[0] = p[2]\n\n    def p_type_name(self, p):\n        \"\"\" type_name   : specifier_qualifier_list abstract_declarator_opt\n        \"\"\"\n        typename = c_ast.Typename(\n            name='',\n            quals=p[1]['qual'][:],\n            align=None,\n            type=p[2] or c_ast.TypeDecl(None, None, None, None),\n            coord=self._token_coord(p, 2))\n\n        p[0] = self._fix_decl_name_type(typename, p[1]['type'])\n\n    def p_abstract_declarator_1(self, p):\n        \"\"\" abstract_declarator     : pointer\n        \"\"\"\n        dummytype = c_ast.TypeDecl(None, None, None, None)\n        p[0] = self._type_modify_decl(\n            decl=dummytype,\n            modifier=p[1])\n\n    def p_abstract_declarator_2(self, p):\n        \"\"\" abstract_declarator     : pointer direct_abstract_declarator\n        \"\"\"\n        p[0] = self._type_modify_decl(p[2], p[1])\n\n    def p_abstract_declarator_3(self, p):\n        \"\"\" abstract_declarator     : direct_abstract_declarator\n        \"\"\"\n        p[0] = p[1]\n\n    # Creating and using direct_abstract_declarator_opt here\n    # instead of listing both direct_abstract_declarator and the\n    # lack of it in the beginning of _1 and _2 caused two\n    # shift/reduce errors.\n    #\n    def p_direct_abstract_declarator_1(self, p):\n        \"\"\" direct_abstract_declarator  : LPAREN abstract_declarator RPAREN \"\"\"\n        p[0] = p[2]\n\n    def p_direct_abstract_declarator_2(self, p):\n        \"\"\" direct_abstract_declarator  : direct_abstract_declarator LBRACKET assignment_expression_opt RBRACKET\n        \"\"\"\n        arr = c_ast.ArrayDecl(\n            type=None,\n            dim=p[3],\n            dim_quals=[],\n            coord=p[1].coord)\n\n        p[0] = self._type_modify_decl(decl=p[1], modifier=arr)\n\n    def p_direct_abstract_declarator_3(self, p):\n        \"\"\" direct_abstract_declarator  : LBRACKET type_qualifier_list_opt assignment_expression_opt RBRACKET\n        \"\"\"\n        quals = (p[2] if len(p) > 4 else []) or []\n        p[0] = c_ast.ArrayDecl(\n            type=c_ast.TypeDecl(None, None, None, None),\n            dim=p[3] if len(p) > 4 else p[2],\n            dim_quals=quals,\n            coord=self._token_coord(p, 1))\n\n    def p_direct_abstract_declarator_4(self, p):\n        \"\"\" direct_abstract_declarator  : direct_abstract_declarator LBRACKET TIMES RBRACKET\n        \"\"\"\n        arr = c_ast.ArrayDecl(\n            type=None,\n            dim=c_ast.ID(p[3], self._token_coord(p, 3)),\n            dim_quals=[],\n            coord=p[1].coord)\n\n        p[0] = self._type_modify_decl(decl=p[1], modifier=arr)\n\n    def p_direct_abstract_declarator_5(self, p):\n        \"\"\" direct_abstract_declarator  : LBRACKET TIMES RBRACKET\n        \"\"\"\n        p[0] = c_ast.ArrayDecl(\n            type=c_ast.TypeDecl(None, None, None, None),\n            dim=c_ast.ID(p[3], self._token_coord(p, 3)),\n            dim_quals=[],\n            coord=self._token_coord(p, 1))\n\n    def p_direct_abstract_declarator_6(self, p):\n        \"\"\" direct_abstract_declarator  : direct_abstract_declarator LPAREN parameter_type_list_opt RPAREN\n        \"\"\"\n        func = c_ast.FuncDecl(\n            args=p[3],\n            type=None,\n            coord=p[1].coord)\n\n        p[0] = self._type_modify_decl(decl=p[1], modifier=func)\n\n    def p_direct_abstract_declarator_7(self, p):\n        \"\"\" direct_abstract_declarator  : LPAREN parameter_type_list_opt RPAREN\n        \"\"\"\n        p[0] = c_ast.FuncDecl(\n            args=p[2],\n            type=c_ast.TypeDecl(None, None, None, None),\n            coord=self._token_coord(p, 1))\n\n    def p_direct_abstract_declarator_8(self, p):\n        \"\"\" direct_abstract_declarator  : LBRACKET STATIC type_qualifier_list_opt assignment_expression RBRACKET\n                                         | LBRACKET type_qualifier_list STATIC assignment_expression RBRACKET\n        \"\"\"\n        listed_quals = [item if isinstance(item, list) else [item]\n            for item in [p[2],p[3]]]\n        quals = [qual for sublist in listed_quals for qual in sublist\n            if qual is not None]\n        p[0] = c_ast.ArrayDecl(\n            type=c_ast.TypeDecl(None, None, None, None),\n            dim=p[4],\n            dim_quals=quals,\n            coord=self._token_coord(p, 1))\n\n    # declaration is a list, statement isn't. To make it consistent, block_item\n    # will always be a list\n    #\n    def p_block_item(self, p):\n        \"\"\" block_item  : declaration\n                        | statement\n        \"\"\"\n        p[0] = p[1] if isinstance(p[1], list) else [p[1]]\n\n    # Since we made block_item a list, this just combines lists\n    #\n    def p_block_item_list(self, p):\n        \"\"\" block_item_list : block_item\n                            | block_item_list block_item\n        \"\"\"\n        # Empty block items (plain ';') produce [None], so ignore them\n        p[0] = p[1] if (len(p) == 2 or p[2] == [None]) else p[1] + p[2]\n\n    def p_compound_statement_1(self, p):\n        \"\"\" compound_statement : brace_open block_item_list_opt brace_close \"\"\"\n        p[0] = c_ast.Compound(\n            block_items=p[2],\n            coord=self._token_coord(p, 1))\n\n    def p_labeled_statement_1(self, p):\n        \"\"\" labeled_statement : ID COLON pragmacomp_or_statement \"\"\"\n        p[0] = c_ast.Label(p[1], p[3], self._token_coord(p, 1))\n\n    def p_labeled_statement_2(self, p):\n        \"\"\" labeled_statement : CASE constant_expression COLON pragmacomp_or_statement \"\"\"\n        p[0] = c_ast.Case(p[2], [p[4]], self._token_coord(p, 1))\n\n    def p_labeled_statement_3(self, p):\n        \"\"\" labeled_statement : DEFAULT COLON pragmacomp_or_statement \"\"\"\n        p[0] = c_ast.Default([p[3]], self._token_coord(p, 1))\n\n    def p_selection_statement_1(self, p):\n        \"\"\" selection_statement : IF LPAREN expression RPAREN pragmacomp_or_statement \"\"\"\n        p[0] = c_ast.If(p[3], p[5], None, self._token_coord(p, 1))\n\n    def p_selection_statement_2(self, p):\n        \"\"\" selection_statement : IF LPAREN expression RPAREN statement ELSE pragmacomp_or_statement \"\"\"\n        p[0] = c_ast.If(p[3], p[5], p[7], self._token_coord(p, 1))\n\n    def p_selection_statement_3(self, p):\n        \"\"\" selection_statement : SWITCH LPAREN expression RPAREN pragmacomp_or_statement \"\"\"\n        p[0] = fix_switch_cases(\n                c_ast.Switch(p[3], p[5], self._token_coord(p, 1)))\n\n    def p_iteration_statement_1(self, p):\n        \"\"\" iteration_statement : WHILE LPAREN expression RPAREN pragmacomp_or_statement \"\"\"\n        p[0] = c_ast.While(p[3], p[5], self._token_coord(p, 1))\n\n    def p_iteration_statement_2(self, p):\n        \"\"\" iteration_statement : DO pragmacomp_or_statement WHILE LPAREN expression RPAREN SEMI \"\"\"\n        p[0] = c_ast.DoWhile(p[5], p[2], self._token_coord(p, 1))\n\n    def p_iteration_statement_3(self, p):\n        \"\"\" iteration_statement : FOR LPAREN expression_opt SEMI expression_opt SEMI expression_opt RPAREN pragmacomp_or_statement \"\"\"\n        p[0] = c_ast.For(p[3], p[5], p[7], p[9], self._token_coord(p, 1))\n\n    def p_iteration_statement_4(self, p):\n        \"\"\" iteration_statement : FOR LPAREN declaration expression_opt SEMI expression_opt RPAREN pragmacomp_or_statement \"\"\"\n        p[0] = c_ast.For(c_ast.DeclList(p[3], self._token_coord(p, 1)),\n                         p[4], p[6], p[8], self._token_coord(p, 1))\n\n    def p_jump_statement_1(self, p):\n        \"\"\" jump_statement  : GOTO ID SEMI \"\"\"\n        p[0] = c_ast.Goto(p[2], self._token_coord(p, 1))\n\n    def p_jump_statement_2(self, p):\n        \"\"\" jump_statement  : BREAK SEMI \"\"\"\n        p[0] = c_ast.Break(self._token_coord(p, 1))\n\n    def p_jump_statement_3(self, p):\n        \"\"\" jump_statement  : CONTINUE SEMI \"\"\"\n        p[0] = c_ast.Continue(self._token_coord(p, 1))\n\n    def p_jump_statement_4(self, p):\n        \"\"\" jump_statement  : RETURN expression SEMI\n                            | RETURN SEMI\n        \"\"\"\n        p[0] = c_ast.Return(p[2] if len(p) == 4 else None, self._token_coord(p, 1))\n\n    def p_expression_statement(self, p):\n        \"\"\" expression_statement : expression_opt SEMI \"\"\"\n        if p[1] is None:\n            p[0] = c_ast.EmptyStatement(self._token_coord(p, 2))\n        else:\n            p[0] = p[1]\n\n    def p_expression(self, p):\n        \"\"\" expression  : assignment_expression\n                        | expression COMMA assignment_expression\n        \"\"\"\n        if len(p) == 2:\n            p[0] = p[1]\n        else:\n            if not isinstance(p[1], c_ast.ExprList):\n                p[1] = c_ast.ExprList([p[1]], p[1].coord)\n\n            p[1].exprs.append(p[3])\n            p[0] = p[1]\n\n    def p_parenthesized_compound_expression(self, p):\n        \"\"\" assignment_expression : LPAREN compound_statement RPAREN \"\"\"\n        p[0] = p[2]\n\n    def p_typedef_name(self, p):\n        \"\"\" typedef_name : TYPEID \"\"\"\n        p[0] = c_ast.IdentifierType([p[1]], coord=self._token_coord(p, 1))\n\n    def p_assignment_expression(self, p):\n        \"\"\" assignment_expression   : conditional_expression\n                                    | unary_expression assignment_operator assignment_expression\n        \"\"\"\n        if len(p) == 2:\n            p[0] = p[1]\n        else:\n            p[0] = c_ast.Assignment(p[2], p[1], p[3], p[1].coord)\n\n    # K&R2 defines these as many separate rules, to encode\n    # precedence and associativity. Why work hard ? I'll just use\n    # the built in precedence/associativity specification feature\n    # of PLY. (see precedence declaration above)\n    #\n    def p_assignment_operator(self, p):\n        \"\"\" assignment_operator : EQUALS\n                                | XOREQUAL\n                                | TIMESEQUAL\n                                | DIVEQUAL\n                                | MODEQUAL\n                                | PLUSEQUAL\n                                | MINUSEQUAL\n                                | LSHIFTEQUAL\n                                | RSHIFTEQUAL\n                                | ANDEQUAL\n                                | OREQUAL\n        \"\"\"\n        p[0] = p[1]\n\n    def p_constant_expression(self, p):\n        \"\"\" constant_expression : conditional_expression \"\"\"\n        p[0] = p[1]\n\n    def p_conditional_expression(self, p):\n        \"\"\" conditional_expression  : binary_expression\n                                    | binary_expression CONDOP expression COLON conditional_expression\n        \"\"\"\n        if len(p) == 2:\n            p[0] = p[1]\n        else:\n            p[0] = c_ast.TernaryOp(p[1], p[3], p[5], p[1].coord)\n\n    def p_binary_expression(self, p):\n        \"\"\" binary_expression   : cast_expression\n                                | binary_expression TIMES binary_expression\n                                | binary_expression DIVIDE binary_expression\n                                | binary_expression MOD binary_expression\n                                | binary_expression PLUS binary_expression\n                                | binary_expression MINUS binary_expression\n                                | binary_expression RSHIFT binary_expression\n                                | binary_expression LSHIFT binary_expression\n                                | binary_expression LT binary_expression\n                                | binary_expression LE binary_expression\n                                | binary_expression GE binary_expression\n                                | binary_expression GT binary_expression\n                                | binary_expression EQ binary_expression\n                                | binary_expression NE binary_expression\n                                | binary_expression AND binary_expression\n                                | binary_expression OR binary_expression\n                                | binary_expression XOR binary_expression\n                                | binary_expression LAND binary_expression\n                                | binary_expression LOR binary_expression\n        \"\"\"\n        if len(p) == 2:\n            p[0] = p[1]\n        else:\n            p[0] = c_ast.BinaryOp(p[2], p[1], p[3], p[1].coord)\n\n    def p_cast_expression_1(self, p):\n        \"\"\" cast_expression : unary_expression \"\"\"\n        p[0] = p[1]\n\n    def p_cast_expression_2(self, p):\n        \"\"\" cast_expression : LPAREN type_name RPAREN cast_expression \"\"\"\n        p[0] = c_ast.Cast(p[2], p[4], self._token_coord(p, 1))\n\n    def p_unary_expression_1(self, p):\n        \"\"\" unary_expression    : postfix_expression \"\"\"\n        p[0] = p[1]\n\n    def p_unary_expression_2(self, p):\n        \"\"\" unary_expression    : PLUSPLUS unary_expression\n                                | MINUSMINUS unary_expression\n                                | unary_operator cast_expression\n        \"\"\"\n        p[0] = c_ast.UnaryOp(p[1], p[2], p[2].coord)\n\n    def p_unary_expression_3(self, p):\n        \"\"\" unary_expression    : SIZEOF unary_expression\n                                | SIZEOF LPAREN type_name RPAREN\n                                | _ALIGNOF LPAREN type_name RPAREN\n        \"\"\"\n        p[0] = c_ast.UnaryOp(\n            p[1],\n            p[2] if len(p) == 3 else p[3],\n            self._token_coord(p, 1))\n\n    def p_unary_operator(self, p):\n        \"\"\" unary_operator  : AND\n                            | TIMES\n                            | PLUS\n                            | MINUS\n                            | NOT\n                            | LNOT\n        \"\"\"\n        p[0] = p[1]\n\n    def p_postfix_expression_1(self, p):\n        \"\"\" postfix_expression  : primary_expression \"\"\"\n        p[0] = p[1]\n\n    def p_postfix_expression_2(self, p):\n        \"\"\" postfix_expression  : postfix_expression LBRACKET expression RBRACKET \"\"\"\n        p[0] = c_ast.ArrayRef(p[1], p[3], p[1].coord)\n\n    def p_postfix_expression_3(self, p):\n        \"\"\" postfix_expression  : postfix_expression LPAREN argument_expression_list RPAREN\n                                | postfix_expression LPAREN RPAREN\n        \"\"\"\n        p[0] = c_ast.FuncCall(p[1], p[3] if len(p) == 5 else None, p[1].coord)\n\n    def p_postfix_expression_4(self, p):\n        \"\"\" postfix_expression  : postfix_expression PERIOD ID\n                                | postfix_expression PERIOD TYPEID\n                                | postfix_expression ARROW ID\n                                | postfix_expression ARROW TYPEID\n        \"\"\"\n        field = c_ast.ID(p[3], self._token_coord(p, 3))\n        p[0] = c_ast.StructRef(p[1], p[2], field, p[1].coord)\n\n    def p_postfix_expression_5(self, p):\n        \"\"\" postfix_expression  : postfix_expression PLUSPLUS\n                                | postfix_expression MINUSMINUS\n        \"\"\"\n        p[0] = c_ast.UnaryOp('p' + p[2], p[1], p[1].coord)\n\n    def p_postfix_expression_6(self, p):\n        \"\"\" postfix_expression  : LPAREN type_name RPAREN brace_open initializer_list brace_close\n                                | LPAREN type_name RPAREN brace_open initializer_list COMMA brace_close\n        \"\"\"\n        p[0] = c_ast.CompoundLiteral(p[2], p[5])\n\n    def p_primary_expression_1(self, p):\n        \"\"\" primary_expression  : identifier \"\"\"\n        p[0] = p[1]\n\n    def p_primary_expression_2(self, p):\n        \"\"\" primary_expression  : constant \"\"\"\n        p[0] = p[1]\n\n    def p_primary_expression_3(self, p):\n        \"\"\" primary_expression  : unified_string_literal\n                                | unified_wstring_literal\n        \"\"\"\n        p[0] = p[1]\n\n    def p_primary_expression_4(self, p):\n        \"\"\" primary_expression  : LPAREN expression RPAREN \"\"\"\n        p[0] = p[2]\n\n    def p_primary_expression_5(self, p):\n        \"\"\" primary_expression  : OFFSETOF LPAREN type_name COMMA offsetof_member_designator RPAREN\n        \"\"\"\n        coord = self._token_coord(p, 1)\n        p[0] = c_ast.FuncCall(c_ast.ID(p[1], coord),\n                              c_ast.ExprList([p[3], p[5]], coord),\n                              coord)\n\n    def p_offsetof_member_designator(self, p):\n        \"\"\" offsetof_member_designator : identifier\n                                         | offsetof_member_designator PERIOD identifier\n                                         | offsetof_member_designator LBRACKET expression RBRACKET\n        \"\"\"\n        if len(p) == 2:\n            p[0] = p[1]\n        elif len(p) == 4:\n            p[0] = c_ast.StructRef(p[1], p[2], p[3], p[1].coord)\n        elif len(p) == 5:\n            p[0] = c_ast.ArrayRef(p[1], p[3], p[1].coord)\n        else:\n            raise NotImplementedError(\"Unexpected parsing state. len(p): %u\" % len(p))\n\n    def p_argument_expression_list(self, p):\n        \"\"\" argument_expression_list    : assignment_expression\n                                        | argument_expression_list COMMA assignment_expression\n        \"\"\"\n        if len(p) == 2: # single expr\n            p[0] = c_ast.ExprList([p[1]], p[1].coord)\n        else:\n            p[1].exprs.append(p[3])\n            p[0] = p[1]\n\n    def p_identifier(self, p):\n        \"\"\" identifier  : ID \"\"\"\n        p[0] = c_ast.ID(p[1], self._token_coord(p, 1))\n\n    def p_constant_1(self, p):\n        \"\"\" constant    : INT_CONST_DEC\n                        | INT_CONST_OCT\n                        | INT_CONST_HEX\n                        | INT_CONST_BIN\n                        | INT_CONST_CHAR\n        \"\"\"\n        uCount = 0\n        lCount = 0\n        for x in p[1][-3:]:\n            if x in ('l', 'L'):\n                lCount += 1\n            elif x in ('u', 'U'):\n                uCount += 1\n        t = ''\n        if uCount > 1:\n             raise ValueError('Constant cannot have more than one u/U suffix.')\n        elif lCount > 2:\n             raise ValueError('Constant cannot have more than two l/L suffix.')\n        prefix = 'unsigned ' * uCount + 'long ' * lCount\n        p[0] = c_ast.Constant(\n            prefix + 'int', p[1], self._token_coord(p, 1))\n\n    def p_constant_2(self, p):\n        \"\"\" constant    : FLOAT_CONST\n                        | HEX_FLOAT_CONST\n        \"\"\"\n        if 'x' in p[1].lower():\n            t = 'float'\n        else:\n            if p[1][-1] in ('f', 'F'):\n                t = 'float'\n            elif p[1][-1] in ('l', 'L'):\n                t = 'long double'\n            else:\n                t = 'double'\n\n        p[0] = c_ast.Constant(\n            t, p[1], self._token_coord(p, 1))\n\n    def p_constant_3(self, p):\n        \"\"\" constant    : CHAR_CONST\n                        | WCHAR_CONST\n                        | U8CHAR_CONST\n                        | U16CHAR_CONST\n                        | U32CHAR_CONST\n        \"\"\"\n        p[0] = c_ast.Constant(\n            'char', p[1], self._token_coord(p, 1))\n\n    # The \"unified\" string and wstring literal rules are for supporting\n    # concatenation of adjacent string literals.\n    # I.e. \"hello \" \"world\" is seen by the C compiler as a single string literal\n    # with the value \"hello world\"\n    #\n    def p_unified_string_literal(self, p):\n        \"\"\" unified_string_literal  : STRING_LITERAL\n                                    | unified_string_literal STRING_LITERAL\n        \"\"\"\n        if len(p) == 2: # single literal\n            p[0] = c_ast.Constant(\n                'string', p[1], self._token_coord(p, 1))\n        else:\n            p[1].value = p[1].value[:-1] + p[2][1:]\n            p[0] = p[1]\n\n    def p_unified_wstring_literal(self, p):\n        \"\"\" unified_wstring_literal : WSTRING_LITERAL\n                                    | U8STRING_LITERAL\n                                    | U16STRING_LITERAL\n                                    | U32STRING_LITERAL\n                                    | unified_wstring_literal WSTRING_LITERAL\n                                    | unified_wstring_literal U8STRING_LITERAL\n                                    | unified_wstring_literal U16STRING_LITERAL\n                                    | unified_wstring_literal U32STRING_LITERAL\n        \"\"\"\n        if len(p) == 2: # single literal\n            p[0] = c_ast.Constant(\n                'string', p[1], self._token_coord(p, 1))\n        else:\n            p[1].value = p[1].value.rstrip()[:-1] + p[2][2:]\n            p[0] = p[1]\n\n    def p_brace_open(self, p):\n        \"\"\" brace_open  :   LBRACE\n        \"\"\"\n        p[0] = p[1]\n        p.set_lineno(0, p.lineno(1))\n\n    def p_brace_close(self, p):\n        \"\"\" brace_close :   RBRACE\n        \"\"\"\n        p[0] = p[1]\n        p.set_lineno(0, p.lineno(1))\n\n    def p_empty(self, p):\n        'empty : '\n        p[0] = None\n\n    def p_error(self, p):\n        # If error recovery is added here in the future, make sure\n        # _get_yacc_lookahead_token still works!\n        #\n        if p:\n            self._parse_error(\n                'before: %s' % p.value,\n                self._coord(lineno=p.lineno,\n                            column=self.clex.find_tok_column(p)))\n        else:\n            self._parse_error('At end of input', self.clex.filename)\n", "pycparser/plyparser.py": "#-----------------------------------------------------------------\n# plyparser.py\n#\n# PLYParser class and other utilities for simplifying programming\n# parsers with PLY\n#\n# Eli Bendersky [https://eli.thegreenplace.net/]\n# License: BSD\n#-----------------------------------------------------------------\n\nimport warnings\n\nclass Coord(object):\n    \"\"\" Coordinates of a syntactic element. Consists of:\n            - File name\n            - Line number\n            - (optional) column number, for the Lexer\n    \"\"\"\n    __slots__ = ('file', 'line', 'column', '__weakref__')\n    def __init__(self, file, line, column=None):\n        self.file = file\n        self.line = line\n        self.column = column\n\n    def __str__(self):\n        str = \"%s:%s\" % (self.file, self.line)\n        if self.column: str += \":%s\" % self.column\n        return str\n\n\nclass ParseError(Exception): pass\n\n\nclass PLYParser(object):\n    def _create_opt_rule(self, rulename):\n        \"\"\" Given a rule name, creates an optional ply.yacc rule\n            for it. The name of the optional rule is\n            <rulename>_opt\n        \"\"\"\n        optname = rulename + '_opt'\n\n        def optrule(self, p):\n            p[0] = p[1]\n\n        optrule.__doc__ = '%s : empty\\n| %s' % (optname, rulename)\n        optrule.__name__ = 'p_%s' % optname\n        setattr(self.__class__, optrule.__name__, optrule)\n\n    def _coord(self, lineno, column=None):\n        return Coord(\n                file=self.clex.filename,\n                line=lineno,\n                column=column)\n\n    def _token_coord(self, p, token_idx):\n        \"\"\" Returns the coordinates for the YaccProduction object 'p' indexed\n            with 'token_idx'. The coordinate includes the 'lineno' and\n            'column'. Both follow the lex semantic, starting from 1.\n        \"\"\"\n        last_cr = p.lexer.lexer.lexdata.rfind('\\n', 0, p.lexpos(token_idx))\n        if last_cr < 0:\n            last_cr = -1\n        column = (p.lexpos(token_idx) - (last_cr))\n        return self._coord(p.lineno(token_idx), column)\n\n    def _parse_error(self, msg, coord):\n        raise ParseError(\"%s: %s\" % (coord, msg))\n\n\ndef parameterized(*params):\n    \"\"\" Decorator to create parameterized rules.\n\n    Parameterized rule methods must be named starting with 'p_' and contain\n    'xxx', and their docstrings may contain 'xxx' and 'yyy'. These will be\n    replaced by the given parameter tuples. For example, ``p_xxx_rule()`` with\n    docstring 'xxx_rule  : yyy' when decorated with\n    ``@parameterized(('id', 'ID'))`` produces ``p_id_rule()`` with the docstring\n    'id_rule  : ID'. Using multiple tuples produces multiple rules.\n    \"\"\"\n    def decorate(rule_func):\n        rule_func._params = params\n        return rule_func\n    return decorate\n\n\ndef template(cls):\n    \"\"\" Class decorator to generate rules from parameterized rule templates.\n\n    See `parameterized` for more information on parameterized rules.\n    \"\"\"\n    issued_nodoc_warning = False\n    for attr_name in dir(cls):\n        if attr_name.startswith('p_'):\n            method = getattr(cls, attr_name)\n            if hasattr(method, '_params'):\n                # Remove the template method\n                delattr(cls, attr_name)\n                # Create parameterized rules from this method; only run this if\n                # the method has a docstring. This is to address an issue when\n                # pycparser's users are installed in -OO mode which strips\n                # docstrings away.\n                # See: https://github.com/eliben/pycparser/pull/198/ and\n                #      https://github.com/eliben/pycparser/issues/197\n                # for discussion.\n                if method.__doc__ is not None:\n                    _create_param_rules(cls, method)\n                elif not issued_nodoc_warning:\n                    warnings.warn(\n                        'parsing methods must have __doc__ for pycparser to work properly',\n                        RuntimeWarning,\n                        stacklevel=2)\n                    issued_nodoc_warning = True\n    return cls\n\n\ndef _create_param_rules(cls, func):\n    \"\"\" Create ply.yacc rules based on a parameterized rule function\n\n    Generates new methods (one per each pair of parameters) based on the\n    template rule function `func`, and attaches them to `cls`. The rule\n    function's parameters must be accessible via its `_params` attribute.\n    \"\"\"\n    for xxx, yyy in func._params:\n        # Use the template method's body for each new method\n        def param_rule(self, p):\n            func(self, p)\n\n        # Substitute in the params for the grammar rule and function name\n        param_rule.__doc__ = func.__doc__.replace('xxx', xxx).replace('yyy', yyy)\n        param_rule.__name__ = func.__name__.replace('xxx', xxx)\n\n        # Attach the new method to the class\n        setattr(cls, param_rule.__name__, param_rule)\n", "pycparser/__init__.py": "#-----------------------------------------------------------------\n# pycparser: __init__.py\n#\n# This package file exports some convenience functions for\n# interacting with pycparser\n#\n# Eli Bendersky [https://eli.thegreenplace.net/]\n# License: BSD\n#-----------------------------------------------------------------\n__all__ = ['c_lexer', 'c_parser', 'c_ast']\n__version__ = '2.22'\n\nimport io\nfrom subprocess import check_output\nfrom .c_parser import CParser\n\n\ndef preprocess_file(filename, cpp_path='cpp', cpp_args=''):\n    \"\"\" Preprocess a file using cpp.\n\n        filename:\n            Name of the file you want to preprocess.\n\n        cpp_path:\n        cpp_args:\n            Refer to the documentation of parse_file for the meaning of these\n            arguments.\n\n        When successful, returns the preprocessed file's contents.\n        Errors from cpp will be printed out.\n    \"\"\"\n    path_list = [cpp_path]\n    if isinstance(cpp_args, list):\n        path_list += cpp_args\n    elif cpp_args != '':\n        path_list += [cpp_args]\n    path_list += [filename]\n\n    try:\n        # Note the use of universal_newlines to treat all newlines\n        # as \\n for Python's purpose\n        text = check_output(path_list, universal_newlines=True)\n    except OSError as e:\n        raise RuntimeError(\"Unable to invoke 'cpp'.  \" +\n            'Make sure its path was passed correctly\\n' +\n            ('Original error: %s' % e))\n\n    return text\n\n\ndef parse_file(filename, use_cpp=False, cpp_path='cpp', cpp_args='',\n               parser=None, encoding=None):\n    \"\"\" Parse a C file using pycparser.\n\n        filename:\n            Name of the file you want to parse.\n\n        use_cpp:\n            Set to True if you want to execute the C pre-processor\n            on the file prior to parsing it.\n\n        cpp_path:\n            If use_cpp is True, this is the path to 'cpp' on your\n            system. If no path is provided, it attempts to just\n            execute 'cpp', so it must be in your PATH.\n\n        cpp_args:\n            If use_cpp is True, set this to the command line arguments strings\n            to cpp. Be careful with quotes - it's best to pass a raw string\n            (r'') here. For example:\n            r'-I../utils/fake_libc_include'\n            If several arguments are required, pass a list of strings.\n\n        encoding:\n            Encoding to use for the file to parse\n\n        parser:\n            Optional parser object to be used instead of the default CParser\n\n        When successful, an AST is returned. ParseError can be\n        thrown if the file doesn't parse successfully.\n\n        Errors from cpp will be printed out.\n    \"\"\"\n    if use_cpp:\n        text = preprocess_file(filename, cpp_path, cpp_args)\n    else:\n        with io.open(filename, encoding=encoding) as f:\n            text = f.read()\n\n    if parser is None:\n        parser = CParser()\n    return parser.parse(text, filename)\n", "pycparser/c_ast.py": "#-----------------------------------------------------------------\n# ** ATTENTION **\n# This code was automatically generated from the file:\n# _c_ast.cfg\n#\n# Do not modify it directly. Modify the configuration file and\n# run the generator again.\n# ** ** *** ** **\n#\n# pycparser: c_ast.py\n#\n# AST Node classes.\n#\n# Eli Bendersky [https://eli.thegreenplace.net/]\n# License: BSD\n#-----------------------------------------------------------------\n\n\nimport sys\n\ndef _repr(obj):\n    \"\"\"\n    Get the representation of an object, with dedicated pprint-like format for lists.\n    \"\"\"\n    if isinstance(obj, list):\n        return '[' + (',\\n '.join((_repr(e).replace('\\n', '\\n ') for e in obj))) + '\\n]'\n    else:\n        return repr(obj)\n\nclass Node(object):\n    __slots__ = ()\n    \"\"\" Abstract base class for AST nodes.\n    \"\"\"\n    def __repr__(self):\n        \"\"\" Generates a python representation of the current node\n        \"\"\"\n        result = self.__class__.__name__ + '('\n\n        indent = ''\n        separator = ''\n        for name in self.__slots__[:-2]:\n            result += separator\n            result += indent\n            result += name + '=' + (_repr(getattr(self, name)).replace('\\n', '\\n  ' + (' ' * (len(name) + len(self.__class__.__name__)))))\n\n            separator = ','\n            indent = '\\n ' + (' ' * len(self.__class__.__name__))\n\n        result += indent + ')'\n\n        return result\n\n    def children(self):\n        \"\"\" A sequence of all children that are Nodes\n        \"\"\"\n        pass\n\n    def show(self, buf=sys.stdout, offset=0, attrnames=False, nodenames=False, showcoord=False, _my_node_name=None):\n        \"\"\" Pretty print the Node and all its attributes and\n            children (recursively) to a buffer.\n\n            buf:\n                Open IO buffer into which the Node is printed.\n\n            offset:\n                Initial offset (amount of leading spaces)\n\n            attrnames:\n                True if you want to see the attribute names in\n                name=value pairs. False to only see the values.\n\n            nodenames:\n                True if you want to see the actual node names\n                within their parents.\n\n            showcoord:\n                Do you want the coordinates of each Node to be\n                displayed.\n        \"\"\"\n        lead = ' ' * offset\n        if nodenames and _my_node_name is not None:\n            buf.write(lead + self.__class__.__name__+ ' <' + _my_node_name + '>: ')\n        else:\n            buf.write(lead + self.__class__.__name__+ ': ')\n\n        if self.attr_names:\n            if attrnames:\n                nvlist = [(n, getattr(self,n)) for n in self.attr_names]\n                attrstr = ', '.join('%s=%s' % nv for nv in nvlist)\n            else:\n                vlist = [getattr(self, n) for n in self.attr_names]\n                attrstr = ', '.join('%s' % v for v in vlist)\n            buf.write(attrstr)\n\n        if showcoord:\n            buf.write(' (at %s)' % self.coord)\n        buf.write('\\n')\n\n        for (child_name, child) in self.children():\n            child.show(\n                buf,\n                offset=offset + 2,\n                attrnames=attrnames,\n                nodenames=nodenames,\n                showcoord=showcoord,\n                _my_node_name=child_name)\n\n\nclass NodeVisitor(object):\n    \"\"\" A base NodeVisitor class for visiting c_ast nodes.\n        Subclass it and define your own visit_XXX methods, where\n        XXX is the class name you want to visit with these\n        methods.\n\n        For example:\n\n        class ConstantVisitor(NodeVisitor):\n            def __init__(self):\n                self.values = []\n\n            def visit_Constant(self, node):\n                self.values.append(node.value)\n\n        Creates a list of values of all the constant nodes\n        encountered below the given node. To use it:\n\n        cv = ConstantVisitor()\n        cv.visit(node)\n\n        Notes:\n\n        *   generic_visit() will be called for AST nodes for which\n            no visit_XXX method was defined.\n        *   The children of nodes for which a visit_XXX was\n            defined will not be visited - if you need this, call\n            generic_visit() on the node.\n            You can use:\n                NodeVisitor.generic_visit(self, node)\n        *   Modeled after Python's own AST visiting facilities\n            (the ast module of Python 3.0)\n    \"\"\"\n\n    _method_cache = None\n\n    def visit(self, node):\n        \"\"\" Visit a node.\n        \"\"\"\n\n        if self._method_cache is None:\n            self._method_cache = {}\n\n        visitor = self._method_cache.get(node.__class__.__name__, None)\n        if visitor is None:\n            method = 'visit_' + node.__class__.__name__\n            visitor = getattr(self, method, self.generic_visit)\n            self._method_cache[node.__class__.__name__] = visitor\n\n        return visitor(node)\n\n    def generic_visit(self, node):\n        \"\"\" Called if no explicit visitor function exists for a\n            node. Implements preorder visiting of the node.\n        \"\"\"\n        for c in node:\n            self.visit(c)\n\nclass ArrayDecl(Node):\n    __slots__ = ('type', 'dim', 'dim_quals', 'coord', '__weakref__')\n    def __init__(self, type, dim, dim_quals, coord=None):\n        self.type = type\n        self.dim = dim\n        self.dim_quals = dim_quals\n        self.coord = coord\n\n    def children(self):\n        nodelist = []\n        if self.type is not None: nodelist.append((\"type\", self.type))\n        if self.dim is not None: nodelist.append((\"dim\", self.dim))\n        return tuple(nodelist)\n\n    def __iter__(self):\n        if self.type is not None:\n            yield self.type\n        if self.dim is not None:\n            yield self.dim\n\n    attr_names = ('dim_quals', )\n\nclass ArrayRef(Node):\n    __slots__ = ('name', 'subscript', 'coord', '__weakref__')\n    def __init__(self, name, subscript, coord=None):\n        self.name = name\n        self.subscript = subscript\n        self.coord = coord\n\n    def children(self):\n        nodelist = []\n        if self.name is not None: nodelist.append((\"name\", self.name))\n        if self.subscript is not None: nodelist.append((\"subscript\", self.subscript))\n        return tuple(nodelist)\n\n    def __iter__(self):\n        if self.name is not None:\n            yield self.name\n        if self.subscript is not None:\n            yield self.subscript\n\n    attr_names = ()\n\nclass Assignment(Node):\n    __slots__ = ('op', 'lvalue', 'rvalue', 'coord', '__weakref__')\n    def __init__(self, op, lvalue, rvalue, coord=None):\n        self.op = op\n        self.lvalue = lvalue\n        self.rvalue = rvalue\n        self.coord = coord\n\n    def children(self):\n        nodelist = []\n        if self.lvalue is not None: nodelist.append((\"lvalue\", self.lvalue))\n        if self.rvalue is not None: nodelist.append((\"rvalue\", self.rvalue))\n        return tuple(nodelist)\n\n    def __iter__(self):\n        if self.lvalue is not None:\n            yield self.lvalue\n        if self.rvalue is not None:\n            yield self.rvalue\n\n    attr_names = ('op', )\n\nclass Alignas(Node):\n    __slots__ = ('alignment', 'coord', '__weakref__')\n    def __init__(self, alignment, coord=None):\n        self.alignment = alignment\n        self.coord = coord\n\n    def children(self):\n        nodelist = []\n        if self.alignment is not None: nodelist.append((\"alignment\", self.alignment))\n        return tuple(nodelist)\n\n    def __iter__(self):\n        if self.alignment is not None:\n            yield self.alignment\n\n    attr_names = ()\n\nclass BinaryOp(Node):\n    __slots__ = ('op', 'left', 'right', 'coord', '__weakref__')\n    def __init__(self, op, left, right, coord=None):\n        self.op = op\n        self.left = left\n        self.right = right\n        self.coord = coord\n\n    def children(self):\n        nodelist = []\n        if self.left is not None: nodelist.append((\"left\", self.left))\n        if self.right is not None: nodelist.append((\"right\", self.right))\n        return tuple(nodelist)\n\n    def __iter__(self):\n        if self.left is not None:\n            yield self.left\n        if self.right is not None:\n            yield self.right\n\n    attr_names = ('op', )\n\nclass Break(Node):\n    __slots__ = ('coord', '__weakref__')\n    def __init__(self, coord=None):\n        self.coord = coord\n\n    def children(self):\n        return ()\n\n    def __iter__(self):\n        return\n        yield\n\n    attr_names = ()\n\nclass Case(Node):\n    __slots__ = ('expr', 'stmts', 'coord', '__weakref__')\n    def __init__(self, expr, stmts, coord=None):\n        self.expr = expr\n        self.stmts = stmts\n        self.coord = coord\n\n    def children(self):\n        nodelist = []\n        if self.expr is not None: nodelist.append((\"expr\", self.expr))\n        for i, child in enumerate(self.stmts or []):\n            nodelist.append((\"stmts[%d]\" % i, child))\n        return tuple(nodelist)\n\n    def __iter__(self):\n        if self.expr is not None:\n            yield self.expr\n        for child in (self.stmts or []):\n            yield child\n\n    attr_names = ()\n\nclass Cast(Node):\n    __slots__ = ('to_type', 'expr', 'coord', '__weakref__')\n    def __init__(self, to_type, expr, coord=None):\n        self.to_type = to_type\n        self.expr = expr\n        self.coord = coord\n\n    def children(self):\n        nodelist = []\n        if self.to_type is not None: nodelist.append((\"to_type\", self.to_type))\n        if self.expr is not None: nodelist.append((\"expr\", self.expr))\n        return tuple(nodelist)\n\n    def __iter__(self):\n        if self.to_type is not None:\n            yield self.to_type\n        if self.expr is not None:\n            yield self.expr\n\n    attr_names = ()\n\nclass Compound(Node):\n    __slots__ = ('block_items', 'coord', '__weakref__')\n    def __init__(self, block_items, coord=None):\n        self.block_items = block_items\n        self.coord = coord\n\n    def children(self):\n        nodelist = []\n        for i, child in enumerate(self.block_items or []):\n            nodelist.append((\"block_items[%d]\" % i, child))\n        return tuple(nodelist)\n\n    def __iter__(self):\n        for child in (self.block_items or []):\n            yield child\n\n    attr_names = ()\n\nclass CompoundLiteral(Node):\n    __slots__ = ('type', 'init', 'coord', '__weakref__')\n    def __init__(self, type, init, coord=None):\n        self.type = type\n        self.init = init\n        self.coord = coord\n\n    def children(self):\n        nodelist = []\n        if self.type is not None: nodelist.append((\"type\", self.type))\n        if self.init is not None: nodelist.append((\"init\", self.init))\n        return tuple(nodelist)\n\n    def __iter__(self):\n        if self.type is not None:\n            yield self.type\n        if self.init is not None:\n            yield self.init\n\n    attr_names = ()\n\nclass Constant(Node):\n    __slots__ = ('type', 'value', 'coord', '__weakref__')\n    def __init__(self, type, value, coord=None):\n        self.type = type\n        self.value = value\n        self.coord = coord\n\n    def children(self):\n        nodelist = []\n        return tuple(nodelist)\n\n    def __iter__(self):\n        return\n        yield\n\n    attr_names = ('type', 'value', )\n\nclass Continue(Node):\n    __slots__ = ('coord', '__weakref__')\n    def __init__(self, coord=None):\n        self.coord = coord\n\n    def children(self):\n        return ()\n\n    def __iter__(self):\n        return\n        yield\n\n    attr_names = ()\n\nclass Decl(Node):\n    __slots__ = ('name', 'quals', 'align', 'storage', 'funcspec', 'type', 'init', 'bitsize', 'coord', '__weakref__')\n    def __init__(self, name, quals, align, storage, funcspec, type, init, bitsize, coord=None):\n        self.name = name\n        self.quals = quals\n        self.align = align\n        self.storage = storage\n        self.funcspec = funcspec\n        self.type = type\n        self.init = init\n        self.bitsize = bitsize\n        self.coord = coord\n\n    def children(self):\n        nodelist = []\n        if self.type is not None: nodelist.append((\"type\", self.type))\n        if self.init is not None: nodelist.append((\"init\", self.init))\n        if self.bitsize is not None: nodelist.append((\"bitsize\", self.bitsize))\n        return tuple(nodelist)\n\n    def __iter__(self):\n        if self.type is not None:\n            yield self.type\n        if self.init is not None:\n            yield self.init\n        if self.bitsize is not None:\n            yield self.bitsize\n\n    attr_names = ('name', 'quals', 'align', 'storage', 'funcspec', )\n\nclass DeclList(Node):\n    __slots__ = ('decls', 'coord', '__weakref__')\n    def __init__(self, decls, coord=None):\n        self.decls = decls\n        self.coord = coord\n\n    def children(self):\n        nodelist = []\n        for i, child in enumerate(self.decls or []):\n            nodelist.append((\"decls[%d]\" % i, child))\n        return tuple(nodelist)\n\n    def __iter__(self):\n        for child in (self.decls or []):\n            yield child\n\n    attr_names = ()\n\nclass Default(Node):\n    __slots__ = ('stmts', 'coord', '__weakref__')\n    def __init__(self, stmts, coord=None):\n        self.stmts = stmts\n        self.coord = coord\n\n    def children(self):\n        nodelist = []\n        for i, child in enumerate(self.stmts or []):\n            nodelist.append((\"stmts[%d]\" % i, child))\n        return tuple(nodelist)\n\n    def __iter__(self):\n        for child in (self.stmts or []):\n            yield child\n\n    attr_names = ()\n\nclass DoWhile(Node):\n    __slots__ = ('cond', 'stmt', 'coord', '__weakref__')\n    def __init__(self, cond, stmt, coord=None):\n        self.cond = cond\n        self.stmt = stmt\n        self.coord = coord\n\n    def children(self):\n        nodelist = []\n        if self.cond is not None: nodelist.append((\"cond\", self.cond))\n        if self.stmt is not None: nodelist.append((\"stmt\", self.stmt))\n        return tuple(nodelist)\n\n    def __iter__(self):\n        if self.cond is not None:\n            yield self.cond\n        if self.stmt is not None:\n            yield self.stmt\n\n    attr_names = ()\n\nclass EllipsisParam(Node):\n    __slots__ = ('coord', '__weakref__')\n    def __init__(self, coord=None):\n        self.coord = coord\n\n    def children(self):\n        return ()\n\n    def __iter__(self):\n        return\n        yield\n\n    attr_names = ()\n\nclass EmptyStatement(Node):\n    __slots__ = ('coord', '__weakref__')\n    def __init__(self, coord=None):\n        self.coord = coord\n\n    def children(self):\n        return ()\n\n    def __iter__(self):\n        return\n        yield\n\n    attr_names = ()\n\nclass Enum(Node):\n    __slots__ = ('name', 'values', 'coord', '__weakref__')\n    def __init__(self, name, values, coord=None):\n        self.name = name\n        self.values = values\n        self.coord = coord\n\n    def children(self):\n        nodelist = []\n        if self.values is not None: nodelist.append((\"values\", self.values))\n        return tuple(nodelist)\n\n    def __iter__(self):\n        if self.values is not None:\n            yield self.values\n\n    attr_names = ('name', )\n\nclass Enumerator(Node):\n    __slots__ = ('name', 'value', 'coord', '__weakref__')\n    def __init__(self, name, value, coord=None):\n        self.name = name\n        self.value = value\n        self.coord = coord\n\n    def children(self):\n        nodelist = []\n        if self.value is not None: nodelist.append((\"value\", self.value))\n        return tuple(nodelist)\n\n    def __iter__(self):\n        if self.value is not None:\n            yield self.value\n\n    attr_names = ('name', )\n\nclass EnumeratorList(Node):\n    __slots__ = ('enumerators', 'coord', '__weakref__')\n    def __init__(self, enumerators, coord=None):\n        self.enumerators = enumerators\n        self.coord = coord\n\n    def children(self):\n        nodelist = []\n        for i, child in enumerate(self.enumerators or []):\n            nodelist.append((\"enumerators[%d]\" % i, child))\n        return tuple(nodelist)\n\n    def __iter__(self):\n        for child in (self.enumerators or []):\n            yield child\n\n    attr_names = ()\n\nclass ExprList(Node):\n    __slots__ = ('exprs', 'coord', '__weakref__')\n    def __init__(self, exprs, coord=None):\n        self.exprs = exprs\n        self.coord = coord\n\n    def children(self):\n        nodelist = []\n        for i, child in enumerate(self.exprs or []):\n            nodelist.append((\"exprs[%d]\" % i, child))\n        return tuple(nodelist)\n\n    def __iter__(self):\n        for child in (self.exprs or []):\n            yield child\n\n    attr_names = ()\n\nclass FileAST(Node):\n    __slots__ = ('ext', 'coord', '__weakref__')\n    def __init__(self, ext, coord=None):\n        self.ext = ext\n        self.coord = coord\n\n    def children(self):\n        nodelist = []\n        for i, child in enumerate(self.ext or []):\n            nodelist.append((\"ext[%d]\" % i, child))\n        return tuple(nodelist)\n\n    def __iter__(self):\n        for child in (self.ext or []):\n            yield child\n\n    attr_names = ()\n\nclass For(Node):\n    __slots__ = ('init', 'cond', 'next', 'stmt', 'coord', '__weakref__')\n    def __init__(self, init, cond, next, stmt, coord=None):\n        self.init = init\n        self.cond = cond\n        self.next = next\n        self.stmt = stmt\n        self.coord = coord\n\n    def children(self):\n        nodelist = []\n        if self.init is not None: nodelist.append((\"init\", self.init))\n        if self.cond is not None: nodelist.append((\"cond\", self.cond))\n        if self.next is not None: nodelist.append((\"next\", self.next))\n        if self.stmt is not None: nodelist.append((\"stmt\", self.stmt))\n        return tuple(nodelist)\n\n    def __iter__(self):\n        if self.init is not None:\n            yield self.init\n        if self.cond is not None:\n            yield self.cond\n        if self.next is not None:\n            yield self.next\n        if self.stmt is not None:\n            yield self.stmt\n\n    attr_names = ()\n\nclass FuncCall(Node):\n    __slots__ = ('name', 'args', 'coord', '__weakref__')\n    def __init__(self, name, args, coord=None):\n        self.name = name\n        self.args = args\n        self.coord = coord\n\n    def children(self):\n        nodelist = []\n        if self.name is not None: nodelist.append((\"name\", self.name))\n        if self.args is not None: nodelist.append((\"args\", self.args))\n        return tuple(nodelist)\n\n    def __iter__(self):\n        if self.name is not None:\n            yield self.name\n        if self.args is not None:\n            yield self.args\n\n    attr_names = ()\n\nclass FuncDecl(Node):\n    __slots__ = ('args', 'type', 'coord', '__weakref__')\n    def __init__(self, args, type, coord=None):\n        self.args = args\n        self.type = type\n        self.coord = coord\n\n    def children(self):\n        nodelist = []\n        if self.args is not None: nodelist.append((\"args\", self.args))\n        if self.type is not None: nodelist.append((\"type\", self.type))\n        return tuple(nodelist)\n\n    def __iter__(self):\n        if self.args is not None:\n            yield self.args\n        if self.type is not None:\n            yield self.type\n\n    attr_names = ()\n\nclass FuncDef(Node):\n    __slots__ = ('decl', 'param_decls', 'body', 'coord', '__weakref__')\n    def __init__(self, decl, param_decls, body, coord=None):\n        self.decl = decl\n        self.param_decls = param_decls\n        self.body = body\n        self.coord = coord\n\n    def children(self):\n        nodelist = []\n        if self.decl is not None: nodelist.append((\"decl\", self.decl))\n        if self.body is not None: nodelist.append((\"body\", self.body))\n        for i, child in enumerate(self.param_decls or []):\n            nodelist.append((\"param_decls[%d]\" % i, child))\n        return tuple(nodelist)\n\n    def __iter__(self):\n        if self.decl is not None:\n            yield self.decl\n        if self.body is not None:\n            yield self.body\n        for child in (self.param_decls or []):\n            yield child\n\n    attr_names = ()\n\nclass Goto(Node):\n    __slots__ = ('name', 'coord', '__weakref__')\n    def __init__(self, name, coord=None):\n        self.name = name\n        self.coord = coord\n\n    def children(self):\n        nodelist = []\n        return tuple(nodelist)\n\n    def __iter__(self):\n        return\n        yield\n\n    attr_names = ('name', )\n\nclass ID(Node):\n    __slots__ = ('name', 'coord', '__weakref__')\n    def __init__(self, name, coord=None):\n        self.name = name\n        self.coord = coord\n\n    def children(self):\n        nodelist = []\n        return tuple(nodelist)\n\n    def __iter__(self):\n        return\n        yield\n\n    attr_names = ('name', )\n\nclass IdentifierType(Node):\n    __slots__ = ('names', 'coord', '__weakref__')\n    def __init__(self, names, coord=None):\n        self.names = names\n        self.coord = coord\n\n    def children(self):\n        nodelist = []\n        return tuple(nodelist)\n\n    def __iter__(self):\n        return\n        yield\n\n    attr_names = ('names', )\n\nclass If(Node):\n    __slots__ = ('cond', 'iftrue', 'iffalse', 'coord', '__weakref__')\n    def __init__(self, cond, iftrue, iffalse, coord=None):\n        self.cond = cond\n        self.iftrue = iftrue\n        self.iffalse = iffalse\n        self.coord = coord\n\n    def children(self):\n        nodelist = []\n        if self.cond is not None: nodelist.append((\"cond\", self.cond))\n        if self.iftrue is not None: nodelist.append((\"iftrue\", self.iftrue))\n        if self.iffalse is not None: nodelist.append((\"iffalse\", self.iffalse))\n        return tuple(nodelist)\n\n    def __iter__(self):\n        if self.cond is not None:\n            yield self.cond\n        if self.iftrue is not None:\n            yield self.iftrue\n        if self.iffalse is not None:\n            yield self.iffalse\n\n    attr_names = ()\n\nclass InitList(Node):\n    __slots__ = ('exprs', 'coord', '__weakref__')\n    def __init__(self, exprs, coord=None):\n        self.exprs = exprs\n        self.coord = coord\n\n    def children(self):\n        nodelist = []\n        for i, child in enumerate(self.exprs or []):\n            nodelist.append((\"exprs[%d]\" % i, child))\n        return tuple(nodelist)\n\n    def __iter__(self):\n        for child in (self.exprs or []):\n            yield child\n\n    attr_names = ()\n\nclass Label(Node):\n    __slots__ = ('name', 'stmt', 'coord', '__weakref__')\n    def __init__(self, name, stmt, coord=None):\n        self.name = name\n        self.stmt = stmt\n        self.coord = coord\n\n    def children(self):\n        nodelist = []\n        if self.stmt is not None: nodelist.append((\"stmt\", self.stmt))\n        return tuple(nodelist)\n\n    def __iter__(self):\n        if self.stmt is not None:\n            yield self.stmt\n\n    attr_names = ('name', )\n\nclass NamedInitializer(Node):\n    __slots__ = ('name', 'expr', 'coord', '__weakref__')\n    def __init__(self, name, expr, coord=None):\n        self.name = name\n        self.expr = expr\n        self.coord = coord\n\n    def children(self):\n        nodelist = []\n        if self.expr is not None: nodelist.append((\"expr\", self.expr))\n        for i, child in enumerate(self.name or []):\n            nodelist.append((\"name[%d]\" % i, child))\n        return tuple(nodelist)\n\n    def __iter__(self):\n        if self.expr is not None:\n            yield self.expr\n        for child in (self.name or []):\n            yield child\n\n    attr_names = ()\n\nclass ParamList(Node):\n    __slots__ = ('params', 'coord', '__weakref__')\n    def __init__(self, params, coord=None):\n        self.params = params\n        self.coord = coord\n\n    def children(self):\n        nodelist = []\n        for i, child in enumerate(self.params or []):\n            nodelist.append((\"params[%d]\" % i, child))\n        return tuple(nodelist)\n\n    def __iter__(self):\n        for child in (self.params or []):\n            yield child\n\n    attr_names = ()\n\nclass PtrDecl(Node):\n    __slots__ = ('quals', 'type', 'coord', '__weakref__')\n    def __init__(self, quals, type, coord=None):\n        self.quals = quals\n        self.type = type\n        self.coord = coord\n\n    def children(self):\n        nodelist = []\n        if self.type is not None: nodelist.append((\"type\", self.type))\n        return tuple(nodelist)\n\n    def __iter__(self):\n        if self.type is not None:\n            yield self.type\n\n    attr_names = ('quals', )\n\nclass Return(Node):\n    __slots__ = ('expr', 'coord', '__weakref__')\n    def __init__(self, expr, coord=None):\n        self.expr = expr\n        self.coord = coord\n\n    def children(self):\n        nodelist = []\n        if self.expr is not None: nodelist.append((\"expr\", self.expr))\n        return tuple(nodelist)\n\n    def __iter__(self):\n        if self.expr is not None:\n            yield self.expr\n\n    attr_names = ()\n\nclass StaticAssert(Node):\n    __slots__ = ('cond', 'message', 'coord', '__weakref__')\n    def __init__(self, cond, message, coord=None):\n        self.cond = cond\n        self.message = message\n        self.coord = coord\n\n    def children(self):\n        nodelist = []\n        if self.cond is not None: nodelist.append((\"cond\", self.cond))\n        if self.message is not None: nodelist.append((\"message\", self.message))\n        return tuple(nodelist)\n\n    def __iter__(self):\n        if self.cond is not None:\n            yield self.cond\n        if self.message is not None:\n            yield self.message\n\n    attr_names = ()\n\nclass Struct(Node):\n    __slots__ = ('name', 'decls', 'coord', '__weakref__')\n    def __init__(self, name, decls, coord=None):\n        self.name = name\n        self.decls = decls\n        self.coord = coord\n\n    def children(self):\n        nodelist = []\n        for i, child in enumerate(self.decls or []):\n            nodelist.append((\"decls[%d]\" % i, child))\n        return tuple(nodelist)\n\n    def __iter__(self):\n        for child in (self.decls or []):\n            yield child\n\n    attr_names = ('name', )\n\nclass StructRef(Node):\n    __slots__ = ('name', 'type', 'field', 'coord', '__weakref__')\n    def __init__(self, name, type, field, coord=None):\n        self.name = name\n        self.type = type\n        self.field = field\n        self.coord = coord\n\n    def children(self):\n        nodelist = []\n        if self.name is not None: nodelist.append((\"name\", self.name))\n        if self.field is not None: nodelist.append((\"field\", self.field))\n        return tuple(nodelist)\n\n    def __iter__(self):\n        if self.name is not None:\n            yield self.name\n        if self.field is not None:\n            yield self.field\n\n    attr_names = ('type', )\n\nclass Switch(Node):\n    __slots__ = ('cond', 'stmt', 'coord', '__weakref__')\n    def __init__(self, cond, stmt, coord=None):\n        self.cond = cond\n        self.stmt = stmt\n        self.coord = coord\n\n    def children(self):\n        nodelist = []\n        if self.cond is not None: nodelist.append((\"cond\", self.cond))\n        if self.stmt is not None: nodelist.append((\"stmt\", self.stmt))\n        return tuple(nodelist)\n\n    def __iter__(self):\n        if self.cond is not None:\n            yield self.cond\n        if self.stmt is not None:\n            yield self.stmt\n\n    attr_names = ()\n\nclass TernaryOp(Node):\n    __slots__ = ('cond', 'iftrue', 'iffalse', 'coord', '__weakref__')\n    def __init__(self, cond, iftrue, iffalse, coord=None):\n        self.cond = cond\n        self.iftrue = iftrue\n        self.iffalse = iffalse\n        self.coord = coord\n\n    def children(self):\n        nodelist = []\n        if self.cond is not None: nodelist.append((\"cond\", self.cond))\n        if self.iftrue is not None: nodelist.append((\"iftrue\", self.iftrue))\n        if self.iffalse is not None: nodelist.append((\"iffalse\", self.iffalse))\n        return tuple(nodelist)\n\n    def __iter__(self):\n        if self.cond is not None:\n            yield self.cond\n        if self.iftrue is not None:\n            yield self.iftrue\n        if self.iffalse is not None:\n            yield self.iffalse\n\n    attr_names = ()\n\nclass TypeDecl(Node):\n    __slots__ = ('declname', 'quals', 'align', 'type', 'coord', '__weakref__')\n    def __init__(self, declname, quals, align, type, coord=None):\n        self.declname = declname\n        self.quals = quals\n        self.align = align\n        self.type = type\n        self.coord = coord\n\n    def children(self):\n        nodelist = []\n        if self.type is not None: nodelist.append((\"type\", self.type))\n        return tuple(nodelist)\n\n    def __iter__(self):\n        if self.type is not None:\n            yield self.type\n\n    attr_names = ('declname', 'quals', 'align', )\n\nclass Typedef(Node):\n    __slots__ = ('name', 'quals', 'storage', 'type', 'coord', '__weakref__')\n    def __init__(self, name, quals, storage, type, coord=None):\n        self.name = name\n        self.quals = quals\n        self.storage = storage\n        self.type = type\n        self.coord = coord\n\n    def children(self):\n        nodelist = []\n        if self.type is not None: nodelist.append((\"type\", self.type))\n        return tuple(nodelist)\n\n    def __iter__(self):\n        if self.type is not None:\n            yield self.type\n\n    attr_names = ('name', 'quals', 'storage', )\n\nclass Typename(Node):\n    __slots__ = ('name', 'quals', 'align', 'type', 'coord', '__weakref__')\n    def __init__(self, name, quals, align, type, coord=None):\n        self.name = name\n        self.quals = quals\n        self.align = align\n        self.type = type\n        self.coord = coord\n\n    def children(self):\n        nodelist = []\n        if self.type is not None: nodelist.append((\"type\", self.type))\n        return tuple(nodelist)\n\n    def __iter__(self):\n        if self.type is not None:\n            yield self.type\n\n    attr_names = ('name', 'quals', 'align', )\n\nclass UnaryOp(Node):\n    __slots__ = ('op', 'expr', 'coord', '__weakref__')\n    def __init__(self, op, expr, coord=None):\n        self.op = op\n        self.expr = expr\n        self.coord = coord\n\n    def children(self):\n        nodelist = []\n        if self.expr is not None: nodelist.append((\"expr\", self.expr))\n        return tuple(nodelist)\n\n    def __iter__(self):\n        if self.expr is not None:\n            yield self.expr\n\n    attr_names = ('op', )\n\nclass Union(Node):\n    __slots__ = ('name', 'decls', 'coord', '__weakref__')\n    def __init__(self, name, decls, coord=None):\n        self.name = name\n        self.decls = decls\n        self.coord = coord\n\n    def children(self):\n        nodelist = []\n        for i, child in enumerate(self.decls or []):\n            nodelist.append((\"decls[%d]\" % i, child))\n        return tuple(nodelist)\n\n    def __iter__(self):\n        for child in (self.decls or []):\n            yield child\n\n    attr_names = ('name', )\n\nclass While(Node):\n    __slots__ = ('cond', 'stmt', 'coord', '__weakref__')\n    def __init__(self, cond, stmt, coord=None):\n        self.cond = cond\n        self.stmt = stmt\n        self.coord = coord\n\n    def children(self):\n        nodelist = []\n        if self.cond is not None: nodelist.append((\"cond\", self.cond))\n        if self.stmt is not None: nodelist.append((\"stmt\", self.stmt))\n        return tuple(nodelist)\n\n    def __iter__(self):\n        if self.cond is not None:\n            yield self.cond\n        if self.stmt is not None:\n            yield self.stmt\n\n    attr_names = ()\n\nclass Pragma(Node):\n    __slots__ = ('string', 'coord', '__weakref__')\n    def __init__(self, string, coord=None):\n        self.string = string\n        self.coord = coord\n\n    def children(self):\n        nodelist = []\n        return tuple(nodelist)\n\n    def __iter__(self):\n        return\n        yield\n\n    attr_names = ('string', )\n\n", "pycparser/ast_transforms.py": "#------------------------------------------------------------------------------\n# pycparser: ast_transforms.py\n#\n# Some utilities used by the parser to create a friendlier AST.\n#\n# Eli Bendersky [https://eli.thegreenplace.net/]\n# License: BSD\n#------------------------------------------------------------------------------\n\nfrom . import c_ast\n\n\ndef fix_switch_cases(switch_node):\n    \"\"\" The 'case' statements in a 'switch' come out of parsing with one\n        child node, so subsequent statements are just tucked to the parent\n        Compound. Additionally, consecutive (fall-through) case statements\n        come out messy. This is a peculiarity of the C grammar. The following:\n\n            switch (myvar) {\n                case 10:\n                    k = 10;\n                    p = k + 1;\n                    return 10;\n                case 20:\n                case 30:\n                    return 20;\n                default:\n                    break;\n            }\n\n        Creates this tree (pseudo-dump):\n\n            Switch\n                ID: myvar\n                Compound:\n                    Case 10:\n                        k = 10\n                    p = k + 1\n                    return 10\n                    Case 20:\n                        Case 30:\n                            return 20\n                    Default:\n                        break\n\n        The goal of this transform is to fix this mess, turning it into the\n        following:\n\n            Switch\n                ID: myvar\n                Compound:\n                    Case 10:\n                        k = 10\n                        p = k + 1\n                        return 10\n                    Case 20:\n                    Case 30:\n                        return 20\n                    Default:\n                        break\n\n        A fixed AST node is returned. The argument may be modified.\n    \"\"\"\n    assert isinstance(switch_node, c_ast.Switch)\n    if not isinstance(switch_node.stmt, c_ast.Compound):\n        return switch_node\n\n    # The new Compound child for the Switch, which will collect children in the\n    # correct order\n    new_compound = c_ast.Compound([], switch_node.stmt.coord)\n\n    # The last Case/Default node\n    last_case = None\n\n    # Goes over the children of the Compound below the Switch, adding them\n    # either directly below new_compound or below the last Case as appropriate\n    # (for `switch(cond) {}`, block_items would have been None)\n    for child in (switch_node.stmt.block_items or []):\n        if isinstance(child, (c_ast.Case, c_ast.Default)):\n            # If it's a Case/Default:\n            # 1. Add it to the Compound and mark as \"last case\"\n            # 2. If its immediate child is also a Case or Default, promote it\n            #    to a sibling.\n            new_compound.block_items.append(child)\n            _extract_nested_case(child, new_compound.block_items)\n            last_case = new_compound.block_items[-1]\n        else:\n            # Other statements are added as children to the last case, if it\n            # exists.\n            if last_case is None:\n                new_compound.block_items.append(child)\n            else:\n                last_case.stmts.append(child)\n\n    switch_node.stmt = new_compound\n    return switch_node\n\n\ndef _extract_nested_case(case_node, stmts_list):\n    \"\"\" Recursively extract consecutive Case statements that are made nested\n        by the parser and add them to the stmts_list.\n    \"\"\"\n    if isinstance(case_node.stmts[0], (c_ast.Case, c_ast.Default)):\n        stmts_list.append(case_node.stmts.pop())\n        _extract_nested_case(stmts_list[-1], stmts_list)\n\n\ndef fix_atomic_specifiers(decl):\n    \"\"\" Atomic specifiers like _Atomic(type) are unusually structured,\n        conferring a qualifier upon the contained type.\n\n        This function fixes a decl with atomic specifiers to have a sane AST\n        structure, by removing spurious Typename->TypeDecl pairs and attaching\n        the _Atomic qualifier in the right place.\n    \"\"\"\n    # There can be multiple levels of _Atomic in a decl; fix them until a\n    # fixed point is reached.\n    while True:\n        decl, found = _fix_atomic_specifiers_once(decl)\n        if not found:\n            break\n\n    # Make sure to add an _Atomic qual on the topmost decl if needed. Also\n    # restore the declname on the innermost TypeDecl (it gets placed in the\n    # wrong place during construction).\n    typ = decl\n    while not isinstance(typ, c_ast.TypeDecl):\n        try:\n            typ = typ.type\n        except AttributeError:\n            return decl\n    if '_Atomic' in typ.quals and '_Atomic' not in decl.quals:\n        decl.quals.append('_Atomic')\n    if typ.declname is None:\n        typ.declname = decl.name\n\n    return decl\n\n\ndef _fix_atomic_specifiers_once(decl):\n    \"\"\" Performs one 'fix' round of atomic specifiers.\n        Returns (modified_decl, found) where found is True iff a fix was made.\n    \"\"\"\n    parent = decl\n    grandparent = None\n    node = decl.type\n    while node is not None:\n        if isinstance(node, c_ast.Typename) and '_Atomic' in node.quals:\n            break\n        try:\n            grandparent = parent\n            parent = node\n            node = node.type\n        except AttributeError:\n            # If we've reached a node without a `type` field, it means we won't\n            # find what we're looking for at this point; give up the search\n            # and return the original decl unmodified.\n            return decl, False\n\n    assert isinstance(parent, c_ast.TypeDecl)\n    grandparent.type = node.type\n    if '_Atomic' not in node.type.quals:\n        node.type.quals.append('_Atomic')\n    return decl, True\n", "pycparser/ply/ygen.py": "# ply: ygen.py\n#\n# This is a support program that auto-generates different versions of the YACC parsing\n# function with different features removed for the purposes of performance.\n#\n# Users should edit the method LParser.parsedebug() in yacc.py.   The source code \n# for that method is then used to create the other methods.   See the comments in\n# yacc.py for further details.\n\nimport os.path\nimport shutil\n\ndef get_source_range(lines, tag):\n    srclines = enumerate(lines)\n    start_tag = '#--! %s-start' % tag\n    end_tag = '#--! %s-end' % tag\n\n    for start_index, line in srclines:\n        if line.strip().startswith(start_tag):\n            break\n\n    for end_index, line in srclines:\n        if line.strip().endswith(end_tag):\n            break\n\n    return (start_index + 1, end_index)\n\ndef filter_section(lines, tag):\n    filtered_lines = []\n    include = True\n    tag_text = '#--! %s' % tag\n    for line in lines:\n        if line.strip().startswith(tag_text):\n            include = not include\n        elif include:\n            filtered_lines.append(line)\n    return filtered_lines\n\ndef main():\n    dirname = os.path.dirname(__file__)\n    shutil.copy2(os.path.join(dirname, 'yacc.py'), os.path.join(dirname, 'yacc.py.bak'))\n    with open(os.path.join(dirname, 'yacc.py'), 'r') as f:\n        lines = f.readlines()\n\n    parse_start, parse_end = get_source_range(lines, 'parsedebug')\n    parseopt_start, parseopt_end = get_source_range(lines, 'parseopt')\n    parseopt_notrack_start, parseopt_notrack_end = get_source_range(lines, 'parseopt-notrack')\n\n    # Get the original source\n    orig_lines = lines[parse_start:parse_end]\n\n    # Filter the DEBUG sections out\n    parseopt_lines = filter_section(orig_lines, 'DEBUG')\n\n    # Filter the TRACKING sections out\n    parseopt_notrack_lines = filter_section(parseopt_lines, 'TRACKING')\n\n    # Replace the parser source sections with updated versions\n    lines[parseopt_notrack_start:parseopt_notrack_end] = parseopt_notrack_lines\n    lines[parseopt_start:parseopt_end] = parseopt_lines\n\n    lines = [line.rstrip()+'\\n' for line in lines]\n    with open(os.path.join(dirname, 'yacc.py'), 'w') as f:\n        f.writelines(lines)\n\n    print('Updated yacc.py')\n\nif __name__ == '__main__':\n    main()\n\n\n\n\n\n", "pycparser/ply/ctokens.py": "# ----------------------------------------------------------------------\n# ctokens.py\n#\n# Token specifications for symbols in ANSI C and C++.  This file is\n# meant to be used as a library in other tokenizers.\n# ----------------------------------------------------------------------\n\n# Reserved words\n\ntokens = [\n    # Literals (identifier, integer constant, float constant, string constant, char const)\n    'ID', 'TYPEID', 'INTEGER', 'FLOAT', 'STRING', 'CHARACTER',\n\n    # Operators (+,-,*,/,%,|,&,~,^,<<,>>, ||, &&, !, <, <=, >, >=, ==, !=)\n    'PLUS', 'MINUS', 'TIMES', 'DIVIDE', 'MODULO',\n    'OR', 'AND', 'NOT', 'XOR', 'LSHIFT', 'RSHIFT',\n    'LOR', 'LAND', 'LNOT',\n    'LT', 'LE', 'GT', 'GE', 'EQ', 'NE',\n    \n    # Assignment (=, *=, /=, %=, +=, -=, <<=, >>=, &=, ^=, |=)\n    'EQUALS', 'TIMESEQUAL', 'DIVEQUAL', 'MODEQUAL', 'PLUSEQUAL', 'MINUSEQUAL',\n    'LSHIFTEQUAL','RSHIFTEQUAL', 'ANDEQUAL', 'XOREQUAL', 'OREQUAL',\n\n    # Increment/decrement (++,--)\n    'INCREMENT', 'DECREMENT',\n\n    # Structure dereference (->)\n    'ARROW',\n\n    # Ternary operator (?)\n    'TERNARY',\n    \n    # Delimeters ( ) [ ] { } , . ; :\n    'LPAREN', 'RPAREN',\n    'LBRACKET', 'RBRACKET',\n    'LBRACE', 'RBRACE',\n    'COMMA', 'PERIOD', 'SEMI', 'COLON',\n\n    # Ellipsis (...)\n    'ELLIPSIS',\n]\n    \n# Operators\nt_PLUS             = r'\\+'\nt_MINUS            = r'-'\nt_TIMES            = r'\\*'\nt_DIVIDE           = r'/'\nt_MODULO           = r'%'\nt_OR               = r'\\|'\nt_AND              = r'&'\nt_NOT              = r'~'\nt_XOR              = r'\\^'\nt_LSHIFT           = r'<<'\nt_RSHIFT           = r'>>'\nt_LOR              = r'\\|\\|'\nt_LAND             = r'&&'\nt_LNOT             = r'!'\nt_LT               = r'<'\nt_GT               = r'>'\nt_LE               = r'<='\nt_GE               = r'>='\nt_EQ               = r'=='\nt_NE               = r'!='\n\n# Assignment operators\n\nt_EQUALS           = r'='\nt_TIMESEQUAL       = r'\\*='\nt_DIVEQUAL         = r'/='\nt_MODEQUAL         = r'%='\nt_PLUSEQUAL        = r'\\+='\nt_MINUSEQUAL       = r'-='\nt_LSHIFTEQUAL      = r'<<='\nt_RSHIFTEQUAL      = r'>>='\nt_ANDEQUAL         = r'&='\nt_OREQUAL          = r'\\|='\nt_XOREQUAL         = r'\\^='\n\n# Increment/decrement\nt_INCREMENT        = r'\\+\\+'\nt_DECREMENT        = r'--'\n\n# ->\nt_ARROW            = r'->'\n\n# ?\nt_TERNARY          = r'\\?'\n\n# Delimeters\nt_LPAREN           = r'\\('\nt_RPAREN           = r'\\)'\nt_LBRACKET         = r'\\['\nt_RBRACKET         = r'\\]'\nt_LBRACE           = r'\\{'\nt_RBRACE           = r'\\}'\nt_COMMA            = r','\nt_PERIOD           = r'\\.'\nt_SEMI             = r';'\nt_COLON            = r':'\nt_ELLIPSIS         = r'\\.\\.\\.'\n\n# Identifiers\nt_ID = r'[A-Za-z_][A-Za-z0-9_]*'\n\n# Integer literal\nt_INTEGER = r'\\d+([uU]|[lL]|[uU][lL]|[lL][uU])?'\n\n# Floating literal\nt_FLOAT = r'((\\d+)(\\.\\d+)(e(\\+|-)?(\\d+))? | (\\d+)e(\\+|-)?(\\d+))([lL]|[fF])?'\n\n# String literal\nt_STRING = r'\\\"([^\\\\\\n]|(\\\\.))*?\\\"'\n\n# Character constant 'c' or L'c'\nt_CHARACTER = r'(L)?\\'([^\\\\\\n]|(\\\\.))*?\\''\n\n# Comment (C-Style)\ndef t_COMMENT(t):\n    r'/\\*(.|\\n)*?\\*/'\n    t.lexer.lineno += t.value.count('\\n')\n    return t\n\n# Comment (C++-Style)\ndef t_CPPCOMMENT(t):\n    r'//.*\\n'\n    t.lexer.lineno += 1\n    return t\n\n\n    \n\n\n\n", "pycparser/ply/cpp.py": "# -----------------------------------------------------------------------------\n# cpp.py\n#\n# Author:  David Beazley (http://www.dabeaz.com)\n# Copyright (C) 2017\n# All rights reserved\n#\n# This module implements an ANSI-C style lexical preprocessor for PLY.\n# -----------------------------------------------------------------------------\nimport sys\n\n# Some Python 3 compatibility shims\nif sys.version_info.major < 3:\n    STRING_TYPES = (str, unicode)\nelse:\n    STRING_TYPES = str\n    xrange = range\n\n# -----------------------------------------------------------------------------\n# Default preprocessor lexer definitions.   These tokens are enough to get\n# a basic preprocessor working.   Other modules may import these if they want\n# -----------------------------------------------------------------------------\n\ntokens = (\n   'CPP_ID','CPP_INTEGER', 'CPP_FLOAT', 'CPP_STRING', 'CPP_CHAR', 'CPP_WS', 'CPP_COMMENT1', 'CPP_COMMENT2', 'CPP_POUND','CPP_DPOUND'\n)\n\nliterals = \"+-*/%|&~^<>=!?()[]{}.,;:\\\\\\'\\\"\"\n\n# Whitespace\ndef t_CPP_WS(t):\n    r'\\s+'\n    t.lexer.lineno += t.value.count(\"\\n\")\n    return t\n\nt_CPP_POUND = r'\\#'\nt_CPP_DPOUND = r'\\#\\#'\n\n# Identifier\nt_CPP_ID = r'[A-Za-z_][\\w_]*'\n\n# Integer literal\ndef CPP_INTEGER(t):\n    r'(((((0x)|(0X))[0-9a-fA-F]+)|(\\d+))([uU][lL]|[lL][uU]|[uU]|[lL])?)'\n    return t\n\nt_CPP_INTEGER = CPP_INTEGER\n\n# Floating literal\nt_CPP_FLOAT = r'((\\d+)(\\.\\d+)(e(\\+|-)?(\\d+))? | (\\d+)e(\\+|-)?(\\d+))([lL]|[fF])?'\n\n# String literal\ndef t_CPP_STRING(t):\n    r'\\\"([^\\\\\\n]|(\\\\(.|\\n)))*?\\\"'\n    t.lexer.lineno += t.value.count(\"\\n\")\n    return t\n\n# Character constant 'c' or L'c'\ndef t_CPP_CHAR(t):\n    r'(L)?\\'([^\\\\\\n]|(\\\\(.|\\n)))*?\\''\n    t.lexer.lineno += t.value.count(\"\\n\")\n    return t\n\n# Comment\ndef t_CPP_COMMENT1(t):\n    r'(/\\*(.|\\n)*?\\*/)'\n    ncr = t.value.count(\"\\n\")\n    t.lexer.lineno += ncr\n    # replace with one space or a number of '\\n'\n    t.type = 'CPP_WS'; t.value = '\\n' * ncr if ncr else ' '\n    return t\n\n# Line comment\ndef t_CPP_COMMENT2(t):\n    r'(//.*?(\\n|$))'\n    # replace with '/n'\n    t.type = 'CPP_WS'; t.value = '\\n'\n    return t\n\ndef t_error(t):\n    t.type = t.value[0]\n    t.value = t.value[0]\n    t.lexer.skip(1)\n    return t\n\nimport re\nimport copy\nimport time\nimport os.path\n\n# -----------------------------------------------------------------------------\n# trigraph()\n#\n# Given an input string, this function replaces all trigraph sequences.\n# The following mapping is used:\n#\n#     ??=    #\n#     ??/    \\\n#     ??'    ^\n#     ??(    [\n#     ??)    ]\n#     ??!    |\n#     ??<    {\n#     ??>    }\n#     ??-    ~\n# -----------------------------------------------------------------------------\n\n_trigraph_pat = re.compile(r'''\\?\\?[=/\\'\\(\\)\\!<>\\-]''')\n_trigraph_rep = {\n    '=':'#',\n    '/':'\\\\',\n    \"'\":'^',\n    '(':'[',\n    ')':']',\n    '!':'|',\n    '<':'{',\n    '>':'}',\n    '-':'~'\n}\n\ndef trigraph(input):\n    return _trigraph_pat.sub(lambda g: _trigraph_rep[g.group()[-1]],input)\n\n# ------------------------------------------------------------------\n# Macro object\n#\n# This object holds information about preprocessor macros\n#\n#    .name      - Macro name (string)\n#    .value     - Macro value (a list of tokens)\n#    .arglist   - List of argument names\n#    .variadic  - Boolean indicating whether or not variadic macro\n#    .vararg    - Name of the variadic parameter\n#\n# When a macro is created, the macro replacement token sequence is\n# pre-scanned and used to create patch lists that are later used\n# during macro expansion\n# ------------------------------------------------------------------\n\nclass Macro(object):\n    def __init__(self,name,value,arglist=None,variadic=False):\n        self.name = name\n        self.value = value\n        self.arglist = arglist\n        self.variadic = variadic\n        if variadic:\n            self.vararg = arglist[-1]\n        self.source = None\n\n# ------------------------------------------------------------------\n# Preprocessor object\n#\n# Object representing a preprocessor.  Contains macro definitions,\n# include directories, and other information\n# ------------------------------------------------------------------\n\nclass Preprocessor(object):\n    def __init__(self,lexer=None):\n        if lexer is None:\n            lexer = lex.lexer\n        self.lexer = lexer\n        self.macros = { }\n        self.path = []\n        self.temp_path = []\n\n        # Probe the lexer for selected tokens\n        self.lexprobe()\n\n        tm = time.localtime()\n        self.define(\"__DATE__ \\\"%s\\\"\" % time.strftime(\"%b %d %Y\",tm))\n        self.define(\"__TIME__ \\\"%s\\\"\" % time.strftime(\"%H:%M:%S\",tm))\n        self.parser = None\n\n    # -----------------------------------------------------------------------------\n    # tokenize()\n    #\n    # Utility function. Given a string of text, tokenize into a list of tokens\n    # -----------------------------------------------------------------------------\n\n    def tokenize(self,text):\n        tokens = []\n        self.lexer.input(text)\n        while True:\n            tok = self.lexer.token()\n            if not tok: break\n            tokens.append(tok)\n        return tokens\n\n    # ---------------------------------------------------------------------\n    # error()\n    #\n    # Report a preprocessor error/warning of some kind\n    # ----------------------------------------------------------------------\n\n    def error(self,file,line,msg):\n        print(\"%s:%d %s\" % (file,line,msg))\n\n    # ----------------------------------------------------------------------\n    # lexprobe()\n    #\n    # This method probes the preprocessor lexer object to discover\n    # the token types of symbols that are important to the preprocessor.\n    # If this works right, the preprocessor will simply \"work\"\n    # with any suitable lexer regardless of how tokens have been named.\n    # ----------------------------------------------------------------------\n\n    def lexprobe(self):\n\n        # Determine the token type for identifiers\n        self.lexer.input(\"identifier\")\n        tok = self.lexer.token()\n        if not tok or tok.value != \"identifier\":\n            print(\"Couldn't determine identifier type\")\n        else:\n            self.t_ID = tok.type\n\n        # Determine the token type for integers\n        self.lexer.input(\"12345\")\n        tok = self.lexer.token()\n        if not tok or int(tok.value) != 12345:\n            print(\"Couldn't determine integer type\")\n        else:\n            self.t_INTEGER = tok.type\n            self.t_INTEGER_TYPE = type(tok.value)\n\n        # Determine the token type for strings enclosed in double quotes\n        self.lexer.input(\"\\\"filename\\\"\")\n        tok = self.lexer.token()\n        if not tok or tok.value != \"\\\"filename\\\"\":\n            print(\"Couldn't determine string type\")\n        else:\n            self.t_STRING = tok.type\n\n        # Determine the token type for whitespace--if any\n        self.lexer.input(\"  \")\n        tok = self.lexer.token()\n        if not tok or tok.value != \"  \":\n            self.t_SPACE = None\n        else:\n            self.t_SPACE = tok.type\n\n        # Determine the token type for newlines\n        self.lexer.input(\"\\n\")\n        tok = self.lexer.token()\n        if not tok or tok.value != \"\\n\":\n            self.t_NEWLINE = None\n            print(\"Couldn't determine token for newlines\")\n        else:\n            self.t_NEWLINE = tok.type\n\n        self.t_WS = (self.t_SPACE, self.t_NEWLINE)\n\n        # Check for other characters used by the preprocessor\n        chars = [ '<','>','#','##','\\\\','(',')',',','.']\n        for c in chars:\n            self.lexer.input(c)\n            tok = self.lexer.token()\n            if not tok or tok.value != c:\n                print(\"Unable to lex '%s' required for preprocessor\" % c)\n\n    # ----------------------------------------------------------------------\n    # add_path()\n    #\n    # Adds a search path to the preprocessor.\n    # ----------------------------------------------------------------------\n\n    def add_path(self,path):\n        self.path.append(path)\n\n    # ----------------------------------------------------------------------\n    # group_lines()\n    #\n    # Given an input string, this function splits it into lines.  Trailing whitespace\n    # is removed.   Any line ending with \\ is grouped with the next line.  This\n    # function forms the lowest level of the preprocessor---grouping into text into\n    # a line-by-line format.\n    # ----------------------------------------------------------------------\n\n    def group_lines(self,input):\n        lex = self.lexer.clone()\n        lines = [x.rstrip() for x in input.splitlines()]\n        for i in xrange(len(lines)):\n            j = i+1\n            while lines[i].endswith('\\\\') and (j < len(lines)):\n                lines[i] = lines[i][:-1]+lines[j]\n                lines[j] = \"\"\n                j += 1\n\n        input = \"\\n\".join(lines)\n        lex.input(input)\n        lex.lineno = 1\n\n        current_line = []\n        while True:\n            tok = lex.token()\n            if not tok:\n                break\n            current_line.append(tok)\n            if tok.type in self.t_WS and '\\n' in tok.value:\n                yield current_line\n                current_line = []\n\n        if current_line:\n            yield current_line\n\n    # ----------------------------------------------------------------------\n    # tokenstrip()\n    #\n    # Remove leading/trailing whitespace tokens from a token list\n    # ----------------------------------------------------------------------\n\n    def tokenstrip(self,tokens):\n        i = 0\n        while i < len(tokens) and tokens[i].type in self.t_WS:\n            i += 1\n        del tokens[:i]\n        i = len(tokens)-1\n        while i >= 0 and tokens[i].type in self.t_WS:\n            i -= 1\n        del tokens[i+1:]\n        return tokens\n\n\n    # ----------------------------------------------------------------------\n    # collect_args()\n    #\n    # Collects comma separated arguments from a list of tokens.   The arguments\n    # must be enclosed in parenthesis.  Returns a tuple (tokencount,args,positions)\n    # where tokencount is the number of tokens consumed, args is a list of arguments,\n    # and positions is a list of integers containing the starting index of each\n    # argument.  Each argument is represented by a list of tokens.\n    #\n    # When collecting arguments, leading and trailing whitespace is removed\n    # from each argument.\n    #\n    # This function properly handles nested parenthesis and commas---these do not\n    # define new arguments.\n    # ----------------------------------------------------------------------\n\n    def collect_args(self,tokenlist):\n        args = []\n        positions = []\n        current_arg = []\n        nesting = 1\n        tokenlen = len(tokenlist)\n\n        # Search for the opening '('.\n        i = 0\n        while (i < tokenlen) and (tokenlist[i].type in self.t_WS):\n            i += 1\n\n        if (i < tokenlen) and (tokenlist[i].value == '('):\n            positions.append(i+1)\n        else:\n            self.error(self.source,tokenlist[0].lineno,\"Missing '(' in macro arguments\")\n            return 0, [], []\n\n        i += 1\n\n        while i < tokenlen:\n            t = tokenlist[i]\n            if t.value == '(':\n                current_arg.append(t)\n                nesting += 1\n            elif t.value == ')':\n                nesting -= 1\n                if nesting == 0:\n                    if current_arg:\n                        args.append(self.tokenstrip(current_arg))\n                        positions.append(i)\n                    return i+1,args,positions\n                current_arg.append(t)\n            elif t.value == ',' and nesting == 1:\n                args.append(self.tokenstrip(current_arg))\n                positions.append(i+1)\n                current_arg = []\n            else:\n                current_arg.append(t)\n            i += 1\n\n        # Missing end argument\n        self.error(self.source,tokenlist[-1].lineno,\"Missing ')' in macro arguments\")\n        return 0, [],[]\n\n    # ----------------------------------------------------------------------\n    # macro_prescan()\n    #\n    # Examine the macro value (token sequence) and identify patch points\n    # This is used to speed up macro expansion later on---we'll know\n    # right away where to apply patches to the value to form the expansion\n    # ----------------------------------------------------------------------\n\n    def macro_prescan(self,macro):\n        macro.patch     = []             # Standard macro arguments\n        macro.str_patch = []             # String conversion expansion\n        macro.var_comma_patch = []       # Variadic macro comma patch\n        i = 0\n        while i < len(macro.value):\n            if macro.value[i].type == self.t_ID and macro.value[i].value in macro.arglist:\n                argnum = macro.arglist.index(macro.value[i].value)\n                # Conversion of argument to a string\n                if i > 0 and macro.value[i-1].value == '#':\n                    macro.value[i] = copy.copy(macro.value[i])\n                    macro.value[i].type = self.t_STRING\n                    del macro.value[i-1]\n                    macro.str_patch.append((argnum,i-1))\n                    continue\n                # Concatenation\n                elif (i > 0 and macro.value[i-1].value == '##'):\n                    macro.patch.append(('c',argnum,i-1))\n                    del macro.value[i-1]\n                    continue\n                elif ((i+1) < len(macro.value) and macro.value[i+1].value == '##'):\n                    macro.patch.append(('c',argnum,i))\n                    i += 1\n                    continue\n                # Standard expansion\n                else:\n                    macro.patch.append(('e',argnum,i))\n            elif macro.value[i].value == '##':\n                if macro.variadic and (i > 0) and (macro.value[i-1].value == ',') and \\\n                        ((i+1) < len(macro.value)) and (macro.value[i+1].type == self.t_ID) and \\\n                        (macro.value[i+1].value == macro.vararg):\n                    macro.var_comma_patch.append(i-1)\n            i += 1\n        macro.patch.sort(key=lambda x: x[2],reverse=True)\n\n    # ----------------------------------------------------------------------\n    # macro_expand_args()\n    #\n    # Given a Macro and list of arguments (each a token list), this method\n    # returns an expanded version of a macro.  The return value is a token sequence\n    # representing the replacement macro tokens\n    # ----------------------------------------------------------------------\n\n    def macro_expand_args(self,macro,args):\n        # Make a copy of the macro token sequence\n        rep = [copy.copy(_x) for _x in macro.value]\n\n        # Make string expansion patches.  These do not alter the length of the replacement sequence\n\n        str_expansion = {}\n        for argnum, i in macro.str_patch:\n            if argnum not in str_expansion:\n                str_expansion[argnum] = ('\"%s\"' % \"\".join([x.value for x in args[argnum]])).replace(\"\\\\\",\"\\\\\\\\\")\n            rep[i] = copy.copy(rep[i])\n            rep[i].value = str_expansion[argnum]\n\n        # Make the variadic macro comma patch.  If the variadic macro argument is empty, we get rid\n        comma_patch = False\n        if macro.variadic and not args[-1]:\n            for i in macro.var_comma_patch:\n                rep[i] = None\n                comma_patch = True\n\n        # Make all other patches.   The order of these matters.  It is assumed that the patch list\n        # has been sorted in reverse order of patch location since replacements will cause the\n        # size of the replacement sequence to expand from the patch point.\n\n        expanded = { }\n        for ptype, argnum, i in macro.patch:\n            # Concatenation.   Argument is left unexpanded\n            if ptype == 'c':\n                rep[i:i+1] = args[argnum]\n            # Normal expansion.  Argument is macro expanded first\n            elif ptype == 'e':\n                if argnum not in expanded:\n                    expanded[argnum] = self.expand_macros(args[argnum])\n                rep[i:i+1] = expanded[argnum]\n\n        # Get rid of removed comma if necessary\n        if comma_patch:\n            rep = [_i for _i in rep if _i]\n\n        return rep\n\n\n    # ----------------------------------------------------------------------\n    # expand_macros()\n    #\n    # Given a list of tokens, this function performs macro expansion.\n    # The expanded argument is a dictionary that contains macros already\n    # expanded.  This is used to prevent infinite recursion.\n    # ----------------------------------------------------------------------\n\n    def expand_macros(self,tokens,expanded=None):\n        if expanded is None:\n            expanded = {}\n        i = 0\n        while i < len(tokens):\n            t = tokens[i]\n            if t.type == self.t_ID:\n                if t.value in self.macros and t.value not in expanded:\n                    # Yes, we found a macro match\n                    expanded[t.value] = True\n\n                    m = self.macros[t.value]\n                    if not m.arglist:\n                        # A simple macro\n                        ex = self.expand_macros([copy.copy(_x) for _x in m.value],expanded)\n                        for e in ex:\n                            e.lineno = t.lineno\n                        tokens[i:i+1] = ex\n                        i += len(ex)\n                    else:\n                        # A macro with arguments\n                        j = i + 1\n                        while j < len(tokens) and tokens[j].type in self.t_WS:\n                            j += 1\n                        if tokens[j].value == '(':\n                            tokcount,args,positions = self.collect_args(tokens[j:])\n                            if not m.variadic and len(args) !=  len(m.arglist):\n                                self.error(self.source,t.lineno,\"Macro %s requires %d arguments\" % (t.value,len(m.arglist)))\n                                i = j + tokcount\n                            elif m.variadic and len(args) < len(m.arglist)-1:\n                                if len(m.arglist) > 2:\n                                    self.error(self.source,t.lineno,\"Macro %s must have at least %d arguments\" % (t.value, len(m.arglist)-1))\n                                else:\n                                    self.error(self.source,t.lineno,\"Macro %s must have at least %d argument\" % (t.value, len(m.arglist)-1))\n                                i = j + tokcount\n                            else:\n                                if m.variadic:\n                                    if len(args) == len(m.arglist)-1:\n                                        args.append([])\n                                    else:\n                                        args[len(m.arglist)-1] = tokens[j+positions[len(m.arglist)-1]:j+tokcount-1]\n                                        del args[len(m.arglist):]\n\n                                # Get macro replacement text\n                                rep = self.macro_expand_args(m,args)\n                                rep = self.expand_macros(rep,expanded)\n                                for r in rep:\n                                    r.lineno = t.lineno\n                                tokens[i:j+tokcount] = rep\n                                i += len(rep)\n                    del expanded[t.value]\n                    continue\n                elif t.value == '__LINE__':\n                    t.type = self.t_INTEGER\n                    t.value = self.t_INTEGER_TYPE(t.lineno)\n\n            i += 1\n        return tokens\n\n    # ----------------------------------------------------------------------\n    # evalexpr()\n    #\n    # Evaluate an expression token sequence for the purposes of evaluating\n    # integral expressions.\n    # ----------------------------------------------------------------------\n\n    def evalexpr(self,tokens):\n        # tokens = tokenize(line)\n        # Search for defined macros\n        i = 0\n        while i < len(tokens):\n            if tokens[i].type == self.t_ID and tokens[i].value == 'defined':\n                j = i + 1\n                needparen = False\n                result = \"0L\"\n                while j < len(tokens):\n                    if tokens[j].type in self.t_WS:\n                        j += 1\n                        continue\n                    elif tokens[j].type == self.t_ID:\n                        if tokens[j].value in self.macros:\n                            result = \"1L\"\n                        else:\n                            result = \"0L\"\n                        if not needparen: break\n                    elif tokens[j].value == '(':\n                        needparen = True\n                    elif tokens[j].value == ')':\n                        break\n                    else:\n                        self.error(self.source,tokens[i].lineno,\"Malformed defined()\")\n                    j += 1\n                tokens[i].type = self.t_INTEGER\n                tokens[i].value = self.t_INTEGER_TYPE(result)\n                del tokens[i+1:j+1]\n            i += 1\n        tokens = self.expand_macros(tokens)\n        for i,t in enumerate(tokens):\n            if t.type == self.t_ID:\n                tokens[i] = copy.copy(t)\n                tokens[i].type = self.t_INTEGER\n                tokens[i].value = self.t_INTEGER_TYPE(\"0L\")\n            elif t.type == self.t_INTEGER:\n                tokens[i] = copy.copy(t)\n                # Strip off any trailing suffixes\n                tokens[i].value = str(tokens[i].value)\n                while tokens[i].value[-1] not in \"0123456789abcdefABCDEF\":\n                    tokens[i].value = tokens[i].value[:-1]\n\n        expr = \"\".join([str(x.value) for x in tokens])\n        expr = expr.replace(\"&&\",\" and \")\n        expr = expr.replace(\"||\",\" or \")\n        expr = expr.replace(\"!\",\" not \")\n        try:\n            result = eval(expr)\n        except Exception:\n            self.error(self.source,tokens[0].lineno,\"Couldn't evaluate expression\")\n            result = 0\n        return result\n\n    # ----------------------------------------------------------------------\n    # parsegen()\n    #\n    # Parse an input string/\n    # ----------------------------------------------------------------------\n    def parsegen(self,input,source=None):\n\n        # Replace trigraph sequences\n        t = trigraph(input)\n        lines = self.group_lines(t)\n\n        if not source:\n            source = \"\"\n\n        self.define(\"__FILE__ \\\"%s\\\"\" % source)\n\n        self.source = source\n        chunk = []\n        enable = True\n        iftrigger = False\n        ifstack = []\n\n        for x in lines:\n            for i,tok in enumerate(x):\n                if tok.type not in self.t_WS: break\n            if tok.value == '#':\n                # Preprocessor directive\n\n                # insert necessary whitespace instead of eaten tokens\n                for tok in x:\n                    if tok.type in self.t_WS and '\\n' in tok.value:\n                        chunk.append(tok)\n\n                dirtokens = self.tokenstrip(x[i+1:])\n                if dirtokens:\n                    name = dirtokens[0].value\n                    args = self.tokenstrip(dirtokens[1:])\n                else:\n                    name = \"\"\n                    args = []\n\n                if name == 'define':\n                    if enable:\n                        for tok in self.expand_macros(chunk):\n                            yield tok\n                        chunk = []\n                        self.define(args)\n                elif name == 'include':\n                    if enable:\n                        for tok in self.expand_macros(chunk):\n                            yield tok\n                        chunk = []\n                        oldfile = self.macros['__FILE__']\n                        for tok in self.include(args):\n                            yield tok\n                        self.macros['__FILE__'] = oldfile\n                        self.source = source\n                elif name == 'undef':\n                    if enable:\n                        for tok in self.expand_macros(chunk):\n                            yield tok\n                        chunk = []\n                        self.undef(args)\n                elif name == 'ifdef':\n                    ifstack.append((enable,iftrigger))\n                    if enable:\n                        if not args[0].value in self.macros:\n                            enable = False\n                            iftrigger = False\n                        else:\n                            iftrigger = True\n                elif name == 'ifndef':\n                    ifstack.append((enable,iftrigger))\n                    if enable:\n                        if args[0].value in self.macros:\n                            enable = False\n                            iftrigger = False\n                        else:\n                            iftrigger = True\n                elif name == 'if':\n                    ifstack.append((enable,iftrigger))\n                    if enable:\n                        result = self.evalexpr(args)\n                        if not result:\n                            enable = False\n                            iftrigger = False\n                        else:\n                            iftrigger = True\n                elif name == 'elif':\n                    if ifstack:\n                        if ifstack[-1][0]:     # We only pay attention if outer \"if\" allows this\n                            if enable:         # If already true, we flip enable False\n                                enable = False\n                            elif not iftrigger:   # If False, but not triggered yet, we'll check expression\n                                result = self.evalexpr(args)\n                                if result:\n                                    enable  = True\n                                    iftrigger = True\n                    else:\n                        self.error(self.source,dirtokens[0].lineno,\"Misplaced #elif\")\n\n                elif name == 'else':\n                    if ifstack:\n                        if ifstack[-1][0]:\n                            if enable:\n                                enable = False\n                            elif not iftrigger:\n                                enable = True\n                                iftrigger = True\n                    else:\n                        self.error(self.source,dirtokens[0].lineno,\"Misplaced #else\")\n\n                elif name == 'endif':\n                    if ifstack:\n                        enable,iftrigger = ifstack.pop()\n                    else:\n                        self.error(self.source,dirtokens[0].lineno,\"Misplaced #endif\")\n                else:\n                    # Unknown preprocessor directive\n                    pass\n\n            else:\n                # Normal text\n                if enable:\n                    chunk.extend(x)\n\n        for tok in self.expand_macros(chunk):\n            yield tok\n        chunk = []\n\n    # ----------------------------------------------------------------------\n    # include()\n    #\n    # Implementation of file-inclusion\n    # ----------------------------------------------------------------------\n\n    def include(self,tokens):\n        # Try to extract the filename and then process an include file\n        if not tokens:\n            return\n        if tokens:\n            if tokens[0].value != '<' and tokens[0].type != self.t_STRING:\n                tokens = self.expand_macros(tokens)\n\n            if tokens[0].value == '<':\n                # Include <...>\n                i = 1\n                while i < len(tokens):\n                    if tokens[i].value == '>':\n                        break\n                    i += 1\n                else:\n                    print(\"Malformed #include <...>\")\n                    return\n                filename = \"\".join([x.value for x in tokens[1:i]])\n                path = self.path + [\"\"] + self.temp_path\n            elif tokens[0].type == self.t_STRING:\n                filename = tokens[0].value[1:-1]\n                path = self.temp_path + [\"\"] + self.path\n            else:\n                print(\"Malformed #include statement\")\n                return\n        for p in path:\n            iname = os.path.join(p,filename)\n            try:\n                data = open(iname,\"r\").read()\n                dname = os.path.dirname(iname)\n                if dname:\n                    self.temp_path.insert(0,dname)\n                for tok in self.parsegen(data,filename):\n                    yield tok\n                if dname:\n                    del self.temp_path[0]\n                break\n            except IOError:\n                pass\n        else:\n            print(\"Couldn't find '%s'\" % filename)\n\n    # ----------------------------------------------------------------------\n    # define()\n    #\n    # Define a new macro\n    # ----------------------------------------------------------------------\n\n    def define(self,tokens):\n        if isinstance(tokens,STRING_TYPES):\n            tokens = self.tokenize(tokens)\n\n        linetok = tokens\n        try:\n            name = linetok[0]\n            if len(linetok) > 1:\n                mtype = linetok[1]\n            else:\n                mtype = None\n            if not mtype:\n                m = Macro(name.value,[])\n                self.macros[name.value] = m\n            elif mtype.type in self.t_WS:\n                # A normal macro\n                m = Macro(name.value,self.tokenstrip(linetok[2:]))\n                self.macros[name.value] = m\n            elif mtype.value == '(':\n                # A macro with arguments\n                tokcount, args, positions = self.collect_args(linetok[1:])\n                variadic = False\n                for a in args:\n                    if variadic:\n                        print(\"No more arguments may follow a variadic argument\")\n                        break\n                    astr = \"\".join([str(_i.value) for _i in a])\n                    if astr == \"...\":\n                        variadic = True\n                        a[0].type = self.t_ID\n                        a[0].value = '__VA_ARGS__'\n                        variadic = True\n                        del a[1:]\n                        continue\n                    elif astr[-3:] == \"...\" and a[0].type == self.t_ID:\n                        variadic = True\n                        del a[1:]\n                        # If, for some reason, \".\" is part of the identifier, strip off the name for the purposes\n                        # of macro expansion\n                        if a[0].value[-3:] == '...':\n                            a[0].value = a[0].value[:-3]\n                        continue\n                    if len(a) > 1 or a[0].type != self.t_ID:\n                        print(\"Invalid macro argument\")\n                        break\n                else:\n                    mvalue = self.tokenstrip(linetok[1+tokcount:])\n                    i = 0\n                    while i < len(mvalue):\n                        if i+1 < len(mvalue):\n                            if mvalue[i].type in self.t_WS and mvalue[i+1].value == '##':\n                                del mvalue[i]\n                                continue\n                            elif mvalue[i].value == '##' and mvalue[i+1].type in self.t_WS:\n                                del mvalue[i+1]\n                        i += 1\n                    m = Macro(name.value,mvalue,[x[0].value for x in args],variadic)\n                    self.macro_prescan(m)\n                    self.macros[name.value] = m\n            else:\n                print(\"Bad macro definition\")\n        except LookupError:\n            print(\"Bad macro definition\")\n\n    # ----------------------------------------------------------------------\n    # undef()\n    #\n    # Undefine a macro\n    # ----------------------------------------------------------------------\n\n    def undef(self,tokens):\n        id = tokens[0].value\n        try:\n            del self.macros[id]\n        except LookupError:\n            pass\n\n    # ----------------------------------------------------------------------\n    # parse()\n    #\n    # Parse input text.\n    # ----------------------------------------------------------------------\n    def parse(self,input,source=None,ignore={}):\n        self.ignore = ignore\n        self.parser = self.parsegen(input,source)\n\n    # ----------------------------------------------------------------------\n    # token()\n    #\n    # Method to return individual tokens\n    # ----------------------------------------------------------------------\n    def token(self):\n        try:\n            while True:\n                tok = next(self.parser)\n                if tok.type not in self.ignore: return tok\n        except StopIteration:\n            self.parser = None\n            return None\n\nif __name__ == '__main__':\n    import ply.lex as lex\n    lexer = lex.lex()\n\n    # Run a preprocessor\n    import sys\n    f = open(sys.argv[1])\n    input = f.read()\n\n    p = Preprocessor(lexer)\n    p.parse(input,sys.argv[1])\n    while True:\n        tok = p.token()\n        if not tok: break\n        print(p.source, tok)\n", "pycparser/ply/lex.py": "# -----------------------------------------------------------------------------\n# ply: lex.py\n#\n# Copyright (C) 2001-2017\n# David M. Beazley (Dabeaz LLC)\n# All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are\n# met:\n#\n# * Redistributions of source code must retain the above copyright notice,\n#   this list of conditions and the following disclaimer.\n# * Redistributions in binary form must reproduce the above copyright notice,\n#   this list of conditions and the following disclaimer in the documentation\n#   and/or other materials provided with the distribution.\n# * Neither the name of the David Beazley or Dabeaz LLC may be used to\n#   endorse or promote products derived from this software without\n#  specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n# \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n# LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n# A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n# OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n# SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n# LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n# DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n# THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# -----------------------------------------------------------------------------\n\n__version__    = '3.10'\n__tabversion__ = '3.10'\n\nimport re\nimport sys\nimport types\nimport copy\nimport os\nimport inspect\n\n# This tuple contains known string types\ntry:\n    # Python 2.6\n    StringTypes = (types.StringType, types.UnicodeType)\nexcept AttributeError:\n    # Python 3.0\n    StringTypes = (str, bytes)\n\n# This regular expression is used to match valid token names\n_is_identifier = re.compile(r'^[a-zA-Z0-9_]+$')\n\n# Exception thrown when invalid token encountered and no default error\n# handler is defined.\nclass LexError(Exception):\n    def __init__(self, message, s):\n        self.args = (message,)\n        self.text = s\n\n\n# Token class.  This class is used to represent the tokens produced.\nclass LexToken(object):\n    def __str__(self):\n        return 'LexToken(%s,%r,%d,%d)' % (self.type, self.value, self.lineno, self.lexpos)\n\n    def __repr__(self):\n        return str(self)\n\n\n# This object is a stand-in for a logging object created by the\n# logging module.\n\nclass PlyLogger(object):\n    def __init__(self, f):\n        self.f = f\n\n    def critical(self, msg, *args, **kwargs):\n        self.f.write((msg % args) + '\\n')\n\n    def warning(self, msg, *args, **kwargs):\n        self.f.write('WARNING: ' + (msg % args) + '\\n')\n\n    def error(self, msg, *args, **kwargs):\n        self.f.write('ERROR: ' + (msg % args) + '\\n')\n\n    info = critical\n    debug = critical\n\n\n# Null logger is used when no output is generated. Does nothing.\nclass NullLogger(object):\n    def __getattribute__(self, name):\n        return self\n\n    def __call__(self, *args, **kwargs):\n        return self\n\n\n# -----------------------------------------------------------------------------\n#                        === Lexing Engine ===\n#\n# The following Lexer class implements the lexer runtime.   There are only\n# a few public methods and attributes:\n#\n#    input()          -  Store a new string in the lexer\n#    token()          -  Get the next token\n#    clone()          -  Clone the lexer\n#\n#    lineno           -  Current line number\n#    lexpos           -  Current position in the input string\n# -----------------------------------------------------------------------------\n\nclass Lexer:\n    def __init__(self):\n        self.lexre = None             # Master regular expression. This is a list of\n                                      # tuples (re, findex) where re is a compiled\n                                      # regular expression and findex is a list\n                                      # mapping regex group numbers to rules\n        self.lexretext = None         # Current regular expression strings\n        self.lexstatere = {}          # Dictionary mapping lexer states to master regexs\n        self.lexstateretext = {}      # Dictionary mapping lexer states to regex strings\n        self.lexstaterenames = {}     # Dictionary mapping lexer states to symbol names\n        self.lexstate = 'INITIAL'     # Current lexer state\n        self.lexstatestack = []       # Stack of lexer states\n        self.lexstateinfo = None      # State information\n        self.lexstateignore = {}      # Dictionary of ignored characters for each state\n        self.lexstateerrorf = {}      # Dictionary of error functions for each state\n        self.lexstateeoff = {}        # Dictionary of eof functions for each state\n        self.lexreflags = 0           # Optional re compile flags\n        self.lexdata = None           # Actual input data (as a string)\n        self.lexpos = 0               # Current position in input text\n        self.lexlen = 0               # Length of the input text\n        self.lexerrorf = None         # Error rule (if any)\n        self.lexeoff = None           # EOF rule (if any)\n        self.lextokens = None         # List of valid tokens\n        self.lexignore = ''           # Ignored characters\n        self.lexliterals = ''         # Literal characters that can be passed through\n        self.lexmodule = None         # Module\n        self.lineno = 1               # Current line number\n        self.lexoptimize = False      # Optimized mode\n\n    def clone(self, object=None):\n        c = copy.copy(self)\n\n        # If the object parameter has been supplied, it means we are attaching the\n        # lexer to a new object.  In this case, we have to rebind all methods in\n        # the lexstatere and lexstateerrorf tables.\n\n        if object:\n            newtab = {}\n            for key, ritem in self.lexstatere.items():\n                newre = []\n                for cre, findex in ritem:\n                    newfindex = []\n                    for f in findex:\n                        if not f or not f[0]:\n                            newfindex.append(f)\n                            continue\n                        newfindex.append((getattr(object, f[0].__name__), f[1]))\n                newre.append((cre, newfindex))\n                newtab[key] = newre\n            c.lexstatere = newtab\n            c.lexstateerrorf = {}\n            for key, ef in self.lexstateerrorf.items():\n                c.lexstateerrorf[key] = getattr(object, ef.__name__)\n            c.lexmodule = object\n        return c\n\n    # ------------------------------------------------------------\n    # writetab() - Write lexer information to a table file\n    # ------------------------------------------------------------\n    def writetab(self, lextab, outputdir=''):\n        if isinstance(lextab, types.ModuleType):\n            raise IOError(\"Won't overwrite existing lextab module\")\n        basetabmodule = lextab.split('.')[-1]\n        filename = os.path.join(outputdir, basetabmodule) + '.py'\n        with open(filename, 'w') as tf:\n            tf.write('# %s.py. This file automatically created by PLY (version %s). Don\\'t edit!\\n' % (basetabmodule, __version__))\n            tf.write('_tabversion   = %s\\n' % repr(__tabversion__))\n            tf.write('_lextokens    = set(%s)\\n' % repr(tuple(sorted(self.lextokens))))\n            tf.write('_lexreflags   = %s\\n' % repr(self.lexreflags))\n            tf.write('_lexliterals  = %s\\n' % repr(self.lexliterals))\n            tf.write('_lexstateinfo = %s\\n' % repr(self.lexstateinfo))\n\n            # Rewrite the lexstatere table, replacing function objects with function names\n            tabre = {}\n            for statename, lre in self.lexstatere.items():\n                titem = []\n                for (pat, func), retext, renames in zip(lre, self.lexstateretext[statename], self.lexstaterenames[statename]):\n                    titem.append((retext, _funcs_to_names(func, renames)))\n                tabre[statename] = titem\n\n            tf.write('_lexstatere   = %s\\n' % repr(tabre))\n            tf.write('_lexstateignore = %s\\n' % repr(self.lexstateignore))\n\n            taberr = {}\n            for statename, ef in self.lexstateerrorf.items():\n                taberr[statename] = ef.__name__ if ef else None\n            tf.write('_lexstateerrorf = %s\\n' % repr(taberr))\n\n            tabeof = {}\n            for statename, ef in self.lexstateeoff.items():\n                tabeof[statename] = ef.__name__ if ef else None\n            tf.write('_lexstateeoff = %s\\n' % repr(tabeof))\n\n    # ------------------------------------------------------------\n    # readtab() - Read lexer information from a tab file\n    # ------------------------------------------------------------\n    def readtab(self, tabfile, fdict):\n        if isinstance(tabfile, types.ModuleType):\n            lextab = tabfile\n        else:\n            exec('import %s' % tabfile)\n            lextab = sys.modules[tabfile]\n\n        if getattr(lextab, '_tabversion', '0.0') != __tabversion__:\n            raise ImportError('Inconsistent PLY version')\n\n        self.lextokens      = lextab._lextokens\n        self.lexreflags     = lextab._lexreflags\n        self.lexliterals    = lextab._lexliterals\n        self.lextokens_all  = self.lextokens | set(self.lexliterals)\n        self.lexstateinfo   = lextab._lexstateinfo\n        self.lexstateignore = lextab._lexstateignore\n        self.lexstatere     = {}\n        self.lexstateretext = {}\n        for statename, lre in lextab._lexstatere.items():\n            titem = []\n            txtitem = []\n            for pat, func_name in lre:\n                titem.append((re.compile(pat, lextab._lexreflags), _names_to_funcs(func_name, fdict)))\n\n            self.lexstatere[statename] = titem\n            self.lexstateretext[statename] = txtitem\n\n        self.lexstateerrorf = {}\n        for statename, ef in lextab._lexstateerrorf.items():\n            self.lexstateerrorf[statename] = fdict[ef]\n\n        self.lexstateeoff = {}\n        for statename, ef in lextab._lexstateeoff.items():\n            self.lexstateeoff[statename] = fdict[ef]\n\n        self.begin('INITIAL')\n\n    # ------------------------------------------------------------\n    # input() - Push a new string into the lexer\n    # ------------------------------------------------------------\n    def input(self, s):\n        # Pull off the first character to see if s looks like a string\n        c = s[:1]\n        if not isinstance(c, StringTypes):\n            raise ValueError('Expected a string')\n        self.lexdata = s\n        self.lexpos = 0\n        self.lexlen = len(s)\n\n    # ------------------------------------------------------------\n    # begin() - Changes the lexing state\n    # ------------------------------------------------------------\n    def begin(self, state):\n        if state not in self.lexstatere:\n            raise ValueError('Undefined state')\n        self.lexre = self.lexstatere[state]\n        self.lexretext = self.lexstateretext[state]\n        self.lexignore = self.lexstateignore.get(state, '')\n        self.lexerrorf = self.lexstateerrorf.get(state, None)\n        self.lexeoff = self.lexstateeoff.get(state, None)\n        self.lexstate = state\n\n    # ------------------------------------------------------------\n    # push_state() - Changes the lexing state and saves old on stack\n    # ------------------------------------------------------------\n    def push_state(self, state):\n        self.lexstatestack.append(self.lexstate)\n        self.begin(state)\n\n    # ------------------------------------------------------------\n    # pop_state() - Restores the previous state\n    # ------------------------------------------------------------\n    def pop_state(self):\n        self.begin(self.lexstatestack.pop())\n\n    # ------------------------------------------------------------\n    # current_state() - Returns the current lexing state\n    # ------------------------------------------------------------\n    def current_state(self):\n        return self.lexstate\n\n    # ------------------------------------------------------------\n    # skip() - Skip ahead n characters\n    # ------------------------------------------------------------\n    def skip(self, n):\n        self.lexpos += n\n\n    # ------------------------------------------------------------\n    # opttoken() - Return the next token from the Lexer\n    #\n    # Note: This function has been carefully implemented to be as fast\n    # as possible.  Don't make changes unless you really know what\n    # you are doing\n    # ------------------------------------------------------------\n    def token(self):\n        # Make local copies of frequently referenced attributes\n        lexpos    = self.lexpos\n        lexlen    = self.lexlen\n        lexignore = self.lexignore\n        lexdata   = self.lexdata\n\n        while lexpos < lexlen:\n            # This code provides some short-circuit code for whitespace, tabs, and other ignored characters\n            if lexdata[lexpos] in lexignore:\n                lexpos += 1\n                continue\n\n            # Look for a regular expression match\n            for lexre, lexindexfunc in self.lexre:\n                m = lexre.match(lexdata, lexpos)\n                if not m:\n                    continue\n\n                # Create a token for return\n                tok = LexToken()\n                tok.value = m.group()\n                tok.lineno = self.lineno\n                tok.lexpos = lexpos\n\n                i = m.lastindex\n                func, tok.type = lexindexfunc[i]\n\n                if not func:\n                    # If no token type was set, it's an ignored token\n                    if tok.type:\n                        self.lexpos = m.end()\n                        return tok\n                    else:\n                        lexpos = m.end()\n                        break\n\n                lexpos = m.end()\n\n                # If token is processed by a function, call it\n\n                tok.lexer = self      # Set additional attributes useful in token rules\n                self.lexmatch = m\n                self.lexpos = lexpos\n\n                newtok = func(tok)\n\n                # Every function must return a token, if nothing, we just move to next token\n                if not newtok:\n                    lexpos    = self.lexpos         # This is here in case user has updated lexpos.\n                    lexignore = self.lexignore      # This is here in case there was a state change\n                    break\n\n                # Verify type of the token.  If not in the token map, raise an error\n                if not self.lexoptimize:\n                    if newtok.type not in self.lextokens_all:\n                        raise LexError(\"%s:%d: Rule '%s' returned an unknown token type '%s'\" % (\n                            func.__code__.co_filename, func.__code__.co_firstlineno,\n                            func.__name__, newtok.type), lexdata[lexpos:])\n\n                return newtok\n            else:\n                # No match, see if in literals\n                if lexdata[lexpos] in self.lexliterals:\n                    tok = LexToken()\n                    tok.value = lexdata[lexpos]\n                    tok.lineno = self.lineno\n                    tok.type = tok.value\n                    tok.lexpos = lexpos\n                    self.lexpos = lexpos + 1\n                    return tok\n\n                # No match. Call t_error() if defined.\n                if self.lexerrorf:\n                    tok = LexToken()\n                    tok.value = self.lexdata[lexpos:]\n                    tok.lineno = self.lineno\n                    tok.type = 'error'\n                    tok.lexer = self\n                    tok.lexpos = lexpos\n                    self.lexpos = lexpos\n                    newtok = self.lexerrorf(tok)\n                    if lexpos == self.lexpos:\n                        # Error method didn't change text position at all. This is an error.\n                        raise LexError(\"Scanning error. Illegal character '%s'\" % (lexdata[lexpos]), lexdata[lexpos:])\n                    lexpos = self.lexpos\n                    if not newtok:\n                        continue\n                    return newtok\n\n                self.lexpos = lexpos\n                raise LexError(\"Illegal character '%s' at index %d\" % (lexdata[lexpos], lexpos), lexdata[lexpos:])\n\n        if self.lexeoff:\n            tok = LexToken()\n            tok.type = 'eof'\n            tok.value = ''\n            tok.lineno = self.lineno\n            tok.lexpos = lexpos\n            tok.lexer = self\n            self.lexpos = lexpos\n            newtok = self.lexeoff(tok)\n            return newtok\n\n        self.lexpos = lexpos + 1\n        if self.lexdata is None:\n            raise RuntimeError('No input string given with input()')\n        return None\n\n    # Iterator interface\n    def __iter__(self):\n        return self\n\n    def next(self):\n        t = self.token()\n        if t is None:\n            raise StopIteration\n        return t\n\n    __next__ = next\n\n# -----------------------------------------------------------------------------\n#                           ==== Lex Builder ===\n#\n# The functions and classes below are used to collect lexing information\n# and build a Lexer object from it.\n# -----------------------------------------------------------------------------\n\n# -----------------------------------------------------------------------------\n# _get_regex(func)\n#\n# Returns the regular expression assigned to a function either as a doc string\n# or as a .regex attribute attached by the @TOKEN decorator.\n# -----------------------------------------------------------------------------\ndef _get_regex(func):\n    return getattr(func, 'regex', func.__doc__)\n\n# -----------------------------------------------------------------------------\n# get_caller_module_dict()\n#\n# This function returns a dictionary containing all of the symbols defined within\n# a caller further down the call stack.  This is used to get the environment\n# associated with the yacc() call if none was provided.\n# -----------------------------------------------------------------------------\ndef get_caller_module_dict(levels):\n    f = sys._getframe(levels)\n    ldict = f.f_globals.copy()\n    if f.f_globals != f.f_locals:\n        ldict.update(f.f_locals)\n    return ldict\n\n# -----------------------------------------------------------------------------\n# _funcs_to_names()\n#\n# Given a list of regular expression functions, this converts it to a list\n# suitable for output to a table file\n# -----------------------------------------------------------------------------\ndef _funcs_to_names(funclist, namelist):\n    result = []\n    for f, name in zip(funclist, namelist):\n        if f and f[0]:\n            result.append((name, f[1]))\n        else:\n            result.append(f)\n    return result\n\n# -----------------------------------------------------------------------------\n# _names_to_funcs()\n#\n# Given a list of regular expression function names, this converts it back to\n# functions.\n# -----------------------------------------------------------------------------\ndef _names_to_funcs(namelist, fdict):\n    result = []\n    for n in namelist:\n        if n and n[0]:\n            result.append((fdict[n[0]], n[1]))\n        else:\n            result.append(n)\n    return result\n\n# -----------------------------------------------------------------------------\n# _form_master_re()\n#\n# This function takes a list of all of the regex components and attempts to\n# form the master regular expression.  Given limitations in the Python re\n# module, it may be necessary to break the master regex into separate expressions.\n# -----------------------------------------------------------------------------\ndef _form_master_re(relist, reflags, ldict, toknames):\n    if not relist:\n        return []\n    regex = '|'.join(relist)\n    try:\n        lexre = re.compile(regex, reflags)\n\n        # Build the index to function map for the matching engine\n        lexindexfunc = [None] * (max(lexre.groupindex.values()) + 1)\n        lexindexnames = lexindexfunc[:]\n\n        for f, i in lexre.groupindex.items():\n            handle = ldict.get(f, None)\n            if type(handle) in (types.FunctionType, types.MethodType):\n                lexindexfunc[i] = (handle, toknames[f])\n                lexindexnames[i] = f\n            elif handle is not None:\n                lexindexnames[i] = f\n                if f.find('ignore_') > 0:\n                    lexindexfunc[i] = (None, None)\n                else:\n                    lexindexfunc[i] = (None, toknames[f])\n\n        return [(lexre, lexindexfunc)], [regex], [lexindexnames]\n    except Exception:\n        m = int(len(relist)/2)\n        if m == 0:\n            m = 1\n        llist, lre, lnames = _form_master_re(relist[:m], reflags, ldict, toknames)\n        rlist, rre, rnames = _form_master_re(relist[m:], reflags, ldict, toknames)\n        return (llist+rlist), (lre+rre), (lnames+rnames)\n\n# -----------------------------------------------------------------------------\n# def _statetoken(s,names)\n#\n# Given a declaration name s of the form \"t_\" and a dictionary whose keys are\n# state names, this function returns a tuple (states,tokenname) where states\n# is a tuple of state names and tokenname is the name of the token.  For example,\n# calling this with s = \"t_foo_bar_SPAM\" might return (('foo','bar'),'SPAM')\n# -----------------------------------------------------------------------------\ndef _statetoken(s, names):\n    nonstate = 1\n    parts = s.split('_')\n    for i, part in enumerate(parts[1:], 1):\n        if part not in names and part != 'ANY':\n            break\n\n    if i > 1:\n        states = tuple(parts[1:i])\n    else:\n        states = ('INITIAL',)\n\n    if 'ANY' in states:\n        states = tuple(names)\n\n    tokenname = '_'.join(parts[i:])\n    return (states, tokenname)\n\n\n# -----------------------------------------------------------------------------\n# LexerReflect()\n#\n# This class represents information needed to build a lexer as extracted from a\n# user's input file.\n# -----------------------------------------------------------------------------\nclass LexerReflect(object):\n    def __init__(self, ldict, log=None, reflags=0):\n        self.ldict      = ldict\n        self.error_func = None\n        self.tokens     = []\n        self.reflags    = reflags\n        self.stateinfo  = {'INITIAL': 'inclusive'}\n        self.modules    = set()\n        self.error      = False\n        self.log        = PlyLogger(sys.stderr) if log is None else log\n\n    # Get all of the basic information\n    def get_all(self):\n        self.get_tokens()\n        self.get_literals()\n        self.get_states()\n        self.get_rules()\n\n    # Validate all of the information\n    def validate_all(self):\n        self.validate_tokens()\n        self.validate_literals()\n        self.validate_rules()\n        return self.error\n\n    # Get the tokens map\n    def get_tokens(self):\n        tokens = self.ldict.get('tokens', None)\n        if not tokens:\n            self.log.error('No token list is defined')\n            self.error = True\n            return\n\n        if not isinstance(tokens, (list, tuple)):\n            self.log.error('tokens must be a list or tuple')\n            self.error = True\n            return\n\n        if not tokens:\n            self.log.error('tokens is empty')\n            self.error = True\n            return\n\n        self.tokens = tokens\n\n    # Validate the tokens\n    def validate_tokens(self):\n        terminals = {}\n        for n in self.tokens:\n            if not _is_identifier.match(n):\n                self.log.error(\"Bad token name '%s'\", n)\n                self.error = True\n            if n in terminals:\n                self.log.warning(\"Token '%s' multiply defined\", n)\n            terminals[n] = 1\n\n    # Get the literals specifier\n    def get_literals(self):\n        self.literals = self.ldict.get('literals', '')\n        if not self.literals:\n            self.literals = ''\n\n    # Validate literals\n    def validate_literals(self):\n        try:\n            for c in self.literals:\n                if not isinstance(c, StringTypes) or len(c) > 1:\n                    self.log.error('Invalid literal %s. Must be a single character', repr(c))\n                    self.error = True\n\n        except TypeError:\n            self.log.error('Invalid literals specification. literals must be a sequence of characters')\n            self.error = True\n\n    def get_states(self):\n        self.states = self.ldict.get('states', None)\n        # Build statemap\n        if self.states:\n            if not isinstance(self.states, (tuple, list)):\n                self.log.error('states must be defined as a tuple or list')\n                self.error = True\n            else:\n                for s in self.states:\n                    if not isinstance(s, tuple) or len(s) != 2:\n                        self.log.error(\"Invalid state specifier %s. Must be a tuple (statename,'exclusive|inclusive')\", repr(s))\n                        self.error = True\n                        continue\n                    name, statetype = s\n                    if not isinstance(name, StringTypes):\n                        self.log.error('State name %s must be a string', repr(name))\n                        self.error = True\n                        continue\n                    if not (statetype == 'inclusive' or statetype == 'exclusive'):\n                        self.log.error(\"State type for state %s must be 'inclusive' or 'exclusive'\", name)\n                        self.error = True\n                        continue\n                    if name in self.stateinfo:\n                        self.log.error(\"State '%s' already defined\", name)\n                        self.error = True\n                        continue\n                    self.stateinfo[name] = statetype\n\n    # Get all of the symbols with a t_ prefix and sort them into various\n    # categories (functions, strings, error functions, and ignore characters)\n\n    def get_rules(self):\n        tsymbols = [f for f in self.ldict if f[:2] == 't_']\n\n        # Now build up a list of functions and a list of strings\n        self.toknames = {}        # Mapping of symbols to token names\n        self.funcsym  = {}        # Symbols defined as functions\n        self.strsym   = {}        # Symbols defined as strings\n        self.ignore   = {}        # Ignore strings by state\n        self.errorf   = {}        # Error functions by state\n        self.eoff     = {}        # EOF functions by state\n\n        for s in self.stateinfo:\n            self.funcsym[s] = []\n            self.strsym[s] = []\n\n        if len(tsymbols) == 0:\n            self.log.error('No rules of the form t_rulename are defined')\n            self.error = True\n            return\n\n        for f in tsymbols:\n            t = self.ldict[f]\n            states, tokname = _statetoken(f, self.stateinfo)\n            self.toknames[f] = tokname\n\n            if hasattr(t, '__call__'):\n                if tokname == 'error':\n                    for s in states:\n                        self.errorf[s] = t\n                elif tokname == 'eof':\n                    for s in states:\n                        self.eoff[s] = t\n                elif tokname == 'ignore':\n                    line = t.__code__.co_firstlineno\n                    file = t.__code__.co_filename\n                    self.log.error(\"%s:%d: Rule '%s' must be defined as a string\", file, line, t.__name__)\n                    self.error = True\n                else:\n                    for s in states:\n                        self.funcsym[s].append((f, t))\n            elif isinstance(t, StringTypes):\n                if tokname == 'ignore':\n                    for s in states:\n                        self.ignore[s] = t\n                    if '\\\\' in t:\n                        self.log.warning(\"%s contains a literal backslash '\\\\'\", f)\n\n                elif tokname == 'error':\n                    self.log.error(\"Rule '%s' must be defined as a function\", f)\n                    self.error = True\n                else:\n                    for s in states:\n                        self.strsym[s].append((f, t))\n            else:\n                self.log.error('%s not defined as a function or string', f)\n                self.error = True\n\n        # Sort the functions by line number\n        for f in self.funcsym.values():\n            f.sort(key=lambda x: x[1].__code__.co_firstlineno)\n\n        # Sort the strings by regular expression length\n        for s in self.strsym.values():\n            s.sort(key=lambda x: len(x[1]), reverse=True)\n\n    # Validate all of the t_rules collected\n    def validate_rules(self):\n        for state in self.stateinfo:\n            # Validate all rules defined by functions\n\n            for fname, f in self.funcsym[state]:\n                line = f.__code__.co_firstlineno\n                file = f.__code__.co_filename\n                module = inspect.getmodule(f)\n                self.modules.add(module)\n\n                tokname = self.toknames[fname]\n                if isinstance(f, types.MethodType):\n                    reqargs = 2\n                else:\n                    reqargs = 1\n                nargs = f.__code__.co_argcount\n                if nargs > reqargs:\n                    self.log.error(\"%s:%d: Rule '%s' has too many arguments\", file, line, f.__name__)\n                    self.error = True\n                    continue\n\n                if nargs < reqargs:\n                    self.log.error(\"%s:%d: Rule '%s' requires an argument\", file, line, f.__name__)\n                    self.error = True\n                    continue\n\n                if not _get_regex(f):\n                    self.log.error(\"%s:%d: No regular expression defined for rule '%s'\", file, line, f.__name__)\n                    self.error = True\n                    continue\n\n                try:\n                    c = re.compile('(?P<%s>%s)' % (fname, _get_regex(f)), self.reflags)\n                    if c.match(''):\n                        self.log.error(\"%s:%d: Regular expression for rule '%s' matches empty string\", file, line, f.__name__)\n                        self.error = True\n                except re.error as e:\n                    self.log.error(\"%s:%d: Invalid regular expression for rule '%s'. %s\", file, line, f.__name__, e)\n                    if '#' in _get_regex(f):\n                        self.log.error(\"%s:%d. Make sure '#' in rule '%s' is escaped with '\\\\#'\", file, line, f.__name__)\n                    self.error = True\n\n            # Validate all rules defined by strings\n            for name, r in self.strsym[state]:\n                tokname = self.toknames[name]\n                if tokname == 'error':\n                    self.log.error(\"Rule '%s' must be defined as a function\", name)\n                    self.error = True\n                    continue\n\n                if tokname not in self.tokens and tokname.find('ignore_') < 0:\n                    self.log.error(\"Rule '%s' defined for an unspecified token %s\", name, tokname)\n                    self.error = True\n                    continue\n\n                try:\n                    c = re.compile('(?P<%s>%s)' % (name, r), self.reflags)\n                    if (c.match('')):\n                        self.log.error(\"Regular expression for rule '%s' matches empty string\", name)\n                        self.error = True\n                except re.error as e:\n                    self.log.error(\"Invalid regular expression for rule '%s'. %s\", name, e)\n                    if '#' in r:\n                        self.log.error(\"Make sure '#' in rule '%s' is escaped with '\\\\#'\", name)\n                    self.error = True\n\n            if not self.funcsym[state] and not self.strsym[state]:\n                self.log.error(\"No rules defined for state '%s'\", state)\n                self.error = True\n\n            # Validate the error function\n            efunc = self.errorf.get(state, None)\n            if efunc:\n                f = efunc\n                line = f.__code__.co_firstlineno\n                file = f.__code__.co_filename\n                module = inspect.getmodule(f)\n                self.modules.add(module)\n\n                if isinstance(f, types.MethodType):\n                    reqargs = 2\n                else:\n                    reqargs = 1\n                nargs = f.__code__.co_argcount\n                if nargs > reqargs:\n                    self.log.error(\"%s:%d: Rule '%s' has too many arguments\", file, line, f.__name__)\n                    self.error = True\n\n                if nargs < reqargs:\n                    self.log.error(\"%s:%d: Rule '%s' requires an argument\", file, line, f.__name__)\n                    self.error = True\n\n        for module in self.modules:\n            self.validate_module(module)\n\n    # -----------------------------------------------------------------------------\n    # validate_module()\n    #\n    # This checks to see if there are duplicated t_rulename() functions or strings\n    # in the parser input file.  This is done using a simple regular expression\n    # match on each line in the source code of the given module.\n    # -----------------------------------------------------------------------------\n\n    def validate_module(self, module):\n        try:\n            lines, linen = inspect.getsourcelines(module)\n        except IOError:\n            return\n\n        fre = re.compile(r'\\s*def\\s+(t_[a-zA-Z_0-9]*)\\(')\n        sre = re.compile(r'\\s*(t_[a-zA-Z_0-9]*)\\s*=')\n\n        counthash = {}\n        linen += 1\n        for line in lines:\n            m = fre.match(line)\n            if not m:\n                m = sre.match(line)\n            if m:\n                name = m.group(1)\n                prev = counthash.get(name)\n                if not prev:\n                    counthash[name] = linen\n                else:\n                    filename = inspect.getsourcefile(module)\n                    self.log.error('%s:%d: Rule %s redefined. Previously defined on line %d', filename, linen, name, prev)\n                    self.error = True\n            linen += 1\n\n# -----------------------------------------------------------------------------\n# lex(module)\n#\n# Build all of the regular expression rules from definitions in the supplied module\n# -----------------------------------------------------------------------------\ndef lex(module=None, object=None, debug=False, optimize=False, lextab='lextab',\n        reflags=int(re.VERBOSE), nowarn=False, outputdir=None, debuglog=None, errorlog=None):\n\n    if lextab is None:\n        lextab = 'lextab'\n\n    global lexer\n\n    ldict = None\n    stateinfo  = {'INITIAL': 'inclusive'}\n    lexobj = Lexer()\n    lexobj.lexoptimize = optimize\n    global token, input\n\n    if errorlog is None:\n        errorlog = PlyLogger(sys.stderr)\n\n    if debug:\n        if debuglog is None:\n            debuglog = PlyLogger(sys.stderr)\n\n    # Get the module dictionary used for the lexer\n    if object:\n        module = object\n\n    # Get the module dictionary used for the parser\n    if module:\n        _items = [(k, getattr(module, k)) for k in dir(module)]\n        ldict = dict(_items)\n        # If no __file__ attribute is available, try to obtain it from the __module__ instead\n        if '__file__' not in ldict:\n            ldict['__file__'] = sys.modules[ldict['__module__']].__file__\n    else:\n        ldict = get_caller_module_dict(2)\n\n    # Determine if the module is package of a package or not.\n    # If so, fix the tabmodule setting so that tables load correctly\n    pkg = ldict.get('__package__')\n    if pkg and isinstance(lextab, str):\n        if '.' not in lextab:\n            lextab = pkg + '.' + lextab\n\n    # Collect parser information from the dictionary\n    linfo = LexerReflect(ldict, log=errorlog, reflags=reflags)\n    linfo.get_all()\n    if not optimize:\n        if linfo.validate_all():\n            raise SyntaxError(\"Can't build lexer\")\n\n    if optimize and lextab:\n        try:\n            lexobj.readtab(lextab, ldict)\n            token = lexobj.token\n            input = lexobj.input\n            lexer = lexobj\n            return lexobj\n\n        except ImportError:\n            pass\n\n    # Dump some basic debugging information\n    if debug:\n        debuglog.info('lex: tokens   = %r', linfo.tokens)\n        debuglog.info('lex: literals = %r', linfo.literals)\n        debuglog.info('lex: states   = %r', linfo.stateinfo)\n\n    # Build a dictionary of valid token names\n    lexobj.lextokens = set()\n    for n in linfo.tokens:\n        lexobj.lextokens.add(n)\n\n    # Get literals specification\n    if isinstance(linfo.literals, (list, tuple)):\n        lexobj.lexliterals = type(linfo.literals[0])().join(linfo.literals)\n    else:\n        lexobj.lexliterals = linfo.literals\n\n    lexobj.lextokens_all = lexobj.lextokens | set(lexobj.lexliterals)\n\n    # Get the stateinfo dictionary\n    stateinfo = linfo.stateinfo\n\n    regexs = {}\n    # Build the master regular expressions\n    for state in stateinfo:\n        regex_list = []\n\n        # Add rules defined by functions first\n        for fname, f in linfo.funcsym[state]:\n            line = f.__code__.co_firstlineno\n            file = f.__code__.co_filename\n            regex_list.append('(?P<%s>%s)' % (fname, _get_regex(f)))\n            if debug:\n                debuglog.info(\"lex: Adding rule %s -> '%s' (state '%s')\", fname, _get_regex(f), state)\n\n        # Now add all of the simple rules\n        for name, r in linfo.strsym[state]:\n            regex_list.append('(?P<%s>%s)' % (name, r))\n            if debug:\n                debuglog.info(\"lex: Adding rule %s -> '%s' (state '%s')\", name, r, state)\n\n        regexs[state] = regex_list\n\n    # Build the master regular expressions\n\n    if debug:\n        debuglog.info('lex: ==== MASTER REGEXS FOLLOW ====')\n\n    for state in regexs:\n        lexre, re_text, re_names = _form_master_re(regexs[state], reflags, ldict, linfo.toknames)\n        lexobj.lexstatere[state] = lexre\n        lexobj.lexstateretext[state] = re_text\n        lexobj.lexstaterenames[state] = re_names\n        if debug:\n            for i, text in enumerate(re_text):\n                debuglog.info(\"lex: state '%s' : regex[%d] = '%s'\", state, i, text)\n\n    # For inclusive states, we need to add the regular expressions from the INITIAL state\n    for state, stype in stateinfo.items():\n        if state != 'INITIAL' and stype == 'inclusive':\n            lexobj.lexstatere[state].extend(lexobj.lexstatere['INITIAL'])\n            lexobj.lexstateretext[state].extend(lexobj.lexstateretext['INITIAL'])\n            lexobj.lexstaterenames[state].extend(lexobj.lexstaterenames['INITIAL'])\n\n    lexobj.lexstateinfo = stateinfo\n    lexobj.lexre = lexobj.lexstatere['INITIAL']\n    lexobj.lexretext = lexobj.lexstateretext['INITIAL']\n    lexobj.lexreflags = reflags\n\n    # Set up ignore variables\n    lexobj.lexstateignore = linfo.ignore\n    lexobj.lexignore = lexobj.lexstateignore.get('INITIAL', '')\n\n    # Set up error functions\n    lexobj.lexstateerrorf = linfo.errorf\n    lexobj.lexerrorf = linfo.errorf.get('INITIAL', None)\n    if not lexobj.lexerrorf:\n        errorlog.warning('No t_error rule is defined')\n\n    # Set up eof functions\n    lexobj.lexstateeoff = linfo.eoff\n    lexobj.lexeoff = linfo.eoff.get('INITIAL', None)\n\n    # Check state information for ignore and error rules\n    for s, stype in stateinfo.items():\n        if stype == 'exclusive':\n            if s not in linfo.errorf:\n                errorlog.warning(\"No error rule is defined for exclusive state '%s'\", s)\n            if s not in linfo.ignore and lexobj.lexignore:\n                errorlog.warning(\"No ignore rule is defined for exclusive state '%s'\", s)\n        elif stype == 'inclusive':\n            if s not in linfo.errorf:\n                linfo.errorf[s] = linfo.errorf.get('INITIAL', None)\n            if s not in linfo.ignore:\n                linfo.ignore[s] = linfo.ignore.get('INITIAL', '')\n\n    # Create global versions of the token() and input() functions\n    token = lexobj.token\n    input = lexobj.input\n    lexer = lexobj\n\n    # If in optimize mode, we write the lextab\n    if lextab and optimize:\n        if outputdir is None:\n            # If no output directory is set, the location of the output files\n            # is determined according to the following rules:\n            #     - If lextab specifies a package, files go into that package directory\n            #     - Otherwise, files go in the same directory as the specifying module\n            if isinstance(lextab, types.ModuleType):\n                srcfile = lextab.__file__\n            else:\n                if '.' not in lextab:\n                    srcfile = ldict['__file__']\n                else:\n                    parts = lextab.split('.')\n                    pkgname = '.'.join(parts[:-1])\n                    exec('import %s' % pkgname)\n                    srcfile = getattr(sys.modules[pkgname], '__file__', '')\n            outputdir = os.path.dirname(srcfile)\n        try:\n            lexobj.writetab(lextab, outputdir)\n        except IOError as e:\n            errorlog.warning(\"Couldn't write lextab module %r. %s\" % (lextab, e))\n\n    return lexobj\n\n# -----------------------------------------------------------------------------\n# runmain()\n#\n# This runs the lexer as a main program\n# -----------------------------------------------------------------------------\n\ndef runmain(lexer=None, data=None):\n    if not data:\n        try:\n            filename = sys.argv[1]\n            f = open(filename)\n            data = f.read()\n            f.close()\n        except IndexError:\n            sys.stdout.write('Reading from standard input (type EOF to end):\\n')\n            data = sys.stdin.read()\n\n    if lexer:\n        _input = lexer.input\n    else:\n        _input = input\n    _input(data)\n    if lexer:\n        _token = lexer.token\n    else:\n        _token = token\n\n    while True:\n        tok = _token()\n        if not tok:\n            break\n        sys.stdout.write('(%s,%r,%d,%d)\\n' % (tok.type, tok.value, tok.lineno, tok.lexpos))\n\n# -----------------------------------------------------------------------------\n# @TOKEN(regex)\n#\n# This decorator function can be used to set the regex expression on a function\n# when its docstring might need to be set in an alternative way\n# -----------------------------------------------------------------------------\n\ndef TOKEN(r):\n    def set_regex(f):\n        if hasattr(r, '__call__'):\n            f.regex = _get_regex(r)\n        else:\n            f.regex = r\n        return f\n    return set_regex\n\n# Alternative spelling of the TOKEN decorator\nToken = TOKEN\n", "pycparser/ply/__init__.py": "# PLY package\n# Author: David Beazley (dave@dabeaz.com)\n\n__version__ = '3.9'\n__all__ = ['lex','yacc']\n", "pycparser/ply/yacc.py": "# -----------------------------------------------------------------------------\n# ply: yacc.py\n#\n# Copyright (C) 2001-2017\n# David M. Beazley (Dabeaz LLC)\n# All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are\n# met:\n#\n# * Redistributions of source code must retain the above copyright notice,\n#   this list of conditions and the following disclaimer.\n# * Redistributions in binary form must reproduce the above copyright notice,\n#   this list of conditions and the following disclaimer in the documentation\n#   and/or other materials provided with the distribution.\n# * Neither the name of the David Beazley or Dabeaz LLC may be used to\n#   endorse or promote products derived from this software without\n#  specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n# \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n# LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n# A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n# OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n# SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n# LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n# DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n# THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# -----------------------------------------------------------------------------\n#\n# This implements an LR parser that is constructed from grammar rules defined\n# as Python functions. The grammer is specified by supplying the BNF inside\n# Python documentation strings.  The inspiration for this technique was borrowed\n# from John Aycock's Spark parsing system.  PLY might be viewed as cross between\n# Spark and the GNU bison utility.\n#\n# The current implementation is only somewhat object-oriented. The\n# LR parser itself is defined in terms of an object (which allows multiple\n# parsers to co-exist).  However, most of the variables used during table\n# construction are defined in terms of global variables.  Users shouldn't\n# notice unless they are trying to define multiple parsers at the same\n# time using threads (in which case they should have their head examined).\n#\n# This implementation supports both SLR and LALR(1) parsing.  LALR(1)\n# support was originally implemented by Elias Ioup (ezioup@alumni.uchicago.edu),\n# using the algorithm found in Aho, Sethi, and Ullman \"Compilers: Principles,\n# Techniques, and Tools\" (The Dragon Book).  LALR(1) has since been replaced\n# by the more efficient DeRemer and Pennello algorithm.\n#\n# :::::::: WARNING :::::::\n#\n# Construction of LR parsing tables is fairly complicated and expensive.\n# To make this module run fast, a *LOT* of work has been put into\n# optimization---often at the expensive of readability and what might\n# consider to be good Python \"coding style.\"   Modify the code at your\n# own risk!\n# ----------------------------------------------------------------------------\n\nimport re\nimport types\nimport sys\nimport os.path\nimport inspect\nimport base64\nimport warnings\n\n__version__    = '3.10'\n__tabversion__ = '3.10'\n\n#-----------------------------------------------------------------------------\n#                     === User configurable parameters ===\n#\n# Change these to modify the default behavior of yacc (if you wish)\n#-----------------------------------------------------------------------------\n\nyaccdebug   = True             # Debugging mode.  If set, yacc generates a\n                               # a 'parser.out' file in the current directory\n\ndebug_file  = 'parser.out'     # Default name of the debugging file\ntab_module  = 'parsetab'       # Default name of the table module\ndefault_lr  = 'LALR'           # Default LR table generation method\n\nerror_count = 3                # Number of symbols that must be shifted to leave recovery mode\n\nyaccdevel   = False            # Set to True if developing yacc.  This turns off optimized\n                               # implementations of certain functions.\n\nresultlimit = 40               # Size limit of results when running in debug mode.\n\npickle_protocol = 0            # Protocol to use when writing pickle files\n\n# String type-checking compatibility\nif sys.version_info[0] < 3:\n    string_types = basestring\nelse:\n    string_types = str\n\nMAXINT = sys.maxsize\n\n# This object is a stand-in for a logging object created by the\n# logging module.   PLY will use this by default to create things\n# such as the parser.out file.  If a user wants more detailed\n# information, they can create their own logging object and pass\n# it into PLY.\n\nclass PlyLogger(object):\n    def __init__(self, f):\n        self.f = f\n\n    def debug(self, msg, *args, **kwargs):\n        self.f.write((msg % args) + '\\n')\n\n    info = debug\n\n    def warning(self, msg, *args, **kwargs):\n        self.f.write('WARNING: ' + (msg % args) + '\\n')\n\n    def error(self, msg, *args, **kwargs):\n        self.f.write('ERROR: ' + (msg % args) + '\\n')\n\n    critical = debug\n\n# Null logger is used when no output is generated. Does nothing.\nclass NullLogger(object):\n    def __getattribute__(self, name):\n        return self\n\n    def __call__(self, *args, **kwargs):\n        return self\n\n# Exception raised for yacc-related errors\nclass YaccError(Exception):\n    pass\n\n# Format the result message that the parser produces when running in debug mode.\ndef format_result(r):\n    repr_str = repr(r)\n    if '\\n' in repr_str:\n        repr_str = repr(repr_str)\n    if len(repr_str) > resultlimit:\n        repr_str = repr_str[:resultlimit] + ' ...'\n    result = '<%s @ 0x%x> (%s)' % (type(r).__name__, id(r), repr_str)\n    return result\n\n# Format stack entries when the parser is running in debug mode\ndef format_stack_entry(r):\n    repr_str = repr(r)\n    if '\\n' in repr_str:\n        repr_str = repr(repr_str)\n    if len(repr_str) < 16:\n        return repr_str\n    else:\n        return '<%s @ 0x%x>' % (type(r).__name__, id(r))\n\n# Panic mode error recovery support.   This feature is being reworked--much of the\n# code here is to offer a deprecation/backwards compatible transition\n\n_errok = None\n_token = None\n_restart = None\n_warnmsg = '''PLY: Don't use global functions errok(), token(), and restart() in p_error().\nInstead, invoke the methods on the associated parser instance:\n\n    def p_error(p):\n        ...\n        # Use parser.errok(), parser.token(), parser.restart()\n        ...\n\n    parser = yacc.yacc()\n'''\n\ndef errok():\n    warnings.warn(_warnmsg)\n    return _errok()\n\ndef restart():\n    warnings.warn(_warnmsg)\n    return _restart()\n\ndef token():\n    warnings.warn(_warnmsg)\n    return _token()\n\n# Utility function to call the p_error() function with some deprecation hacks\ndef call_errorfunc(errorfunc, token, parser):\n    global _errok, _token, _restart\n    _errok = parser.errok\n    _token = parser.token\n    _restart = parser.restart\n    r = errorfunc(token)\n    try:\n        del _errok, _token, _restart\n    except NameError:\n        pass\n    return r\n\n#-----------------------------------------------------------------------------\n#                        ===  LR Parsing Engine ===\n#\n# The following classes are used for the LR parser itself.  These are not\n# used during table construction and are independent of the actual LR\n# table generation algorithm\n#-----------------------------------------------------------------------------\n\n# This class is used to hold non-terminal grammar symbols during parsing.\n# It normally has the following attributes set:\n#        .type       = Grammar symbol type\n#        .value      = Symbol value\n#        .lineno     = Starting line number\n#        .endlineno  = Ending line number (optional, set automatically)\n#        .lexpos     = Starting lex position\n#        .endlexpos  = Ending lex position (optional, set automatically)\n\nclass YaccSymbol:\n    def __str__(self):\n        return self.type\n\n    def __repr__(self):\n        return str(self)\n\n# This class is a wrapper around the objects actually passed to each\n# grammar rule.   Index lookup and assignment actually assign the\n# .value attribute of the underlying YaccSymbol object.\n# The lineno() method returns the line number of a given\n# item (or 0 if not defined).   The linespan() method returns\n# a tuple of (startline,endline) representing the range of lines\n# for a symbol.  The lexspan() method returns a tuple (lexpos,endlexpos)\n# representing the range of positional information for a symbol.\n\nclass YaccProduction:\n    def __init__(self, s, stack=None):\n        self.slice = s\n        self.stack = stack\n        self.lexer = None\n        self.parser = None\n\n    def __getitem__(self, n):\n        if isinstance(n, slice):\n            return [s.value for s in self.slice[n]]\n        elif n >= 0:\n            return self.slice[n].value\n        else:\n            return self.stack[n].value\n\n    def __setitem__(self, n, v):\n        self.slice[n].value = v\n\n    def __getslice__(self, i, j):\n        return [s.value for s in self.slice[i:j]]\n\n    def __len__(self):\n        return len(self.slice)\n\n    def lineno(self, n):\n        return getattr(self.slice[n], 'lineno', 0)\n\n    def set_lineno(self, n, lineno):\n        self.slice[n].lineno = lineno\n\n    def linespan(self, n):\n        startline = getattr(self.slice[n], 'lineno', 0)\n        endline = getattr(self.slice[n], 'endlineno', startline)\n        return startline, endline\n\n    def lexpos(self, n):\n        return getattr(self.slice[n], 'lexpos', 0)\n\n    def lexspan(self, n):\n        startpos = getattr(self.slice[n], 'lexpos', 0)\n        endpos = getattr(self.slice[n], 'endlexpos', startpos)\n        return startpos, endpos\n\n    def error(self):\n        raise SyntaxError\n\n# -----------------------------------------------------------------------------\n#                               == LRParser ==\n#\n# The LR Parsing engine.\n# -----------------------------------------------------------------------------\n\nclass LRParser:\n    def __init__(self, lrtab, errorf):\n        self.productions = lrtab.lr_productions\n        self.action = lrtab.lr_action\n        self.goto = lrtab.lr_goto\n        self.errorfunc = errorf\n        self.set_defaulted_states()\n        self.errorok = True\n\n    def errok(self):\n        self.errorok = True\n\n    def restart(self):\n        del self.statestack[:]\n        del self.symstack[:]\n        sym = YaccSymbol()\n        sym.type = '$end'\n        self.symstack.append(sym)\n        self.statestack.append(0)\n\n    # Defaulted state support.\n    # This method identifies parser states where there is only one possible reduction action.\n    # For such states, the parser can make a choose to make a rule reduction without consuming\n    # the next look-ahead token.  This delayed invocation of the tokenizer can be useful in\n    # certain kinds of advanced parsing situations where the lexer and parser interact with\n    # each other or change states (i.e., manipulation of scope, lexer states, etc.).\n    #\n    # See:  https://www.gnu.org/software/bison/manual/html_node/Default-Reductions.html#Default-Reductions\n    def set_defaulted_states(self):\n        self.defaulted_states = {}\n        for state, actions in self.action.items():\n            rules = list(actions.values())\n            if len(rules) == 1 and rules[0] < 0:\n                self.defaulted_states[state] = rules[0]\n\n    def disable_defaulted_states(self):\n        self.defaulted_states = {}\n\n    def parse(self, input=None, lexer=None, debug=False, tracking=False, tokenfunc=None):\n        if debug or yaccdevel:\n            if isinstance(debug, int):\n                debug = PlyLogger(sys.stderr)\n            return self.parsedebug(input, lexer, debug, tracking, tokenfunc)\n        elif tracking:\n            return self.parseopt(input, lexer, debug, tracking, tokenfunc)\n        else:\n            return self.parseopt_notrack(input, lexer, debug, tracking, tokenfunc)\n\n\n    # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n    # parsedebug().\n    #\n    # This is the debugging enabled version of parse().  All changes made to the\n    # parsing engine should be made here.   Optimized versions of this function\n    # are automatically created by the ply/ygen.py script.  This script cuts out\n    # sections enclosed in markers such as this:\n    #\n    #      #--! DEBUG\n    #      statements\n    #      #--! DEBUG\n    #\n    # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n\n    def parsedebug(self, input=None, lexer=None, debug=False, tracking=False, tokenfunc=None):\n        #--! parsedebug-start\n        lookahead = None                         # Current lookahead symbol\n        lookaheadstack = []                      # Stack of lookahead symbols\n        actions = self.action                    # Local reference to action table (to avoid lookup on self.)\n        goto    = self.goto                      # Local reference to goto table (to avoid lookup on self.)\n        prod    = self.productions               # Local reference to production list (to avoid lookup on self.)\n        defaulted_states = self.defaulted_states # Local reference to defaulted states\n        pslice  = YaccProduction(None)           # Production object passed to grammar rules\n        errorcount = 0                           # Used during error recovery\n\n        #--! DEBUG\n        debug.info('PLY: PARSE DEBUG START')\n        #--! DEBUG\n\n        # If no lexer was given, we will try to use the lex module\n        if not lexer:\n            from . import lex\n            lexer = lex.lexer\n\n        # Set up the lexer and parser objects on pslice\n        pslice.lexer = lexer\n        pslice.parser = self\n\n        # If input was supplied, pass to lexer\n        if input is not None:\n            lexer.input(input)\n\n        if tokenfunc is None:\n            # Tokenize function\n            get_token = lexer.token\n        else:\n            get_token = tokenfunc\n\n        # Set the parser() token method (sometimes used in error recovery)\n        self.token = get_token\n\n        # Set up the state and symbol stacks\n\n        statestack = []                # Stack of parsing states\n        self.statestack = statestack\n        symstack   = []                # Stack of grammar symbols\n        self.symstack = symstack\n\n        pslice.stack = symstack         # Put in the production\n        errtoken   = None               # Err token\n\n        # The start state is assumed to be (0,$end)\n\n        statestack.append(0)\n        sym = YaccSymbol()\n        sym.type = '$end'\n        symstack.append(sym)\n        state = 0\n        while True:\n            # Get the next symbol on the input.  If a lookahead symbol\n            # is already set, we just use that. Otherwise, we'll pull\n            # the next token off of the lookaheadstack or from the lexer\n\n            #--! DEBUG\n            debug.debug('')\n            debug.debug('State  : %s', state)\n            #--! DEBUG\n\n            if state not in defaulted_states:\n                if not lookahead:\n                    if not lookaheadstack:\n                        lookahead = get_token()     # Get the next token\n                    else:\n                        lookahead = lookaheadstack.pop()\n                    if not lookahead:\n                        lookahead = YaccSymbol()\n                        lookahead.type = '$end'\n\n                # Check the action table\n                ltype = lookahead.type\n                t = actions[state].get(ltype)\n            else:\n                t = defaulted_states[state]\n                #--! DEBUG\n                debug.debug('Defaulted state %s: Reduce using %d', state, -t)\n                #--! DEBUG\n\n            #--! DEBUG\n            debug.debug('Stack  : %s',\n                        ('%s . %s' % (' '.join([xx.type for xx in symstack][1:]), str(lookahead))).lstrip())\n            #--! DEBUG\n\n            if t is not None:\n                if t > 0:\n                    # shift a symbol on the stack\n                    statestack.append(t)\n                    state = t\n\n                    #--! DEBUG\n                    debug.debug('Action : Shift and goto state %s', t)\n                    #--! DEBUG\n\n                    symstack.append(lookahead)\n                    lookahead = None\n\n                    # Decrease error count on successful shift\n                    if errorcount:\n                        errorcount -= 1\n                    continue\n\n                if t < 0:\n                    # reduce a symbol on the stack, emit a production\n                    p = prod[-t]\n                    pname = p.name\n                    plen  = p.len\n\n                    # Get production function\n                    sym = YaccSymbol()\n                    sym.type = pname       # Production name\n                    sym.value = None\n\n                    #--! DEBUG\n                    if plen:\n                        debug.info('Action : Reduce rule [%s] with %s and goto state %d', p.str,\n                                   '['+','.join([format_stack_entry(_v.value) for _v in symstack[-plen:]])+']',\n                                   goto[statestack[-1-plen]][pname])\n                    else:\n                        debug.info('Action : Reduce rule [%s] with %s and goto state %d', p.str, [],\n                                   goto[statestack[-1]][pname])\n\n                    #--! DEBUG\n\n                    if plen:\n                        targ = symstack[-plen-1:]\n                        targ[0] = sym\n\n                        #--! TRACKING\n                        if tracking:\n                            t1 = targ[1]\n                            sym.lineno = t1.lineno\n                            sym.lexpos = t1.lexpos\n                            t1 = targ[-1]\n                            sym.endlineno = getattr(t1, 'endlineno', t1.lineno)\n                            sym.endlexpos = getattr(t1, 'endlexpos', t1.lexpos)\n                        #--! TRACKING\n\n                        # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n                        # The code enclosed in this section is duplicated\n                        # below as a performance optimization.  Make sure\n                        # changes get made in both locations.\n\n                        pslice.slice = targ\n\n                        try:\n                            # Call the grammar rule with our special slice object\n                            del symstack[-plen:]\n                            self.state = state\n                            p.callable(pslice)\n                            del statestack[-plen:]\n                            #--! DEBUG\n                            debug.info('Result : %s', format_result(pslice[0]))\n                            #--! DEBUG\n                            symstack.append(sym)\n                            state = goto[statestack[-1]][pname]\n                            statestack.append(state)\n                        except SyntaxError:\n                            # If an error was set. Enter error recovery state\n                            lookaheadstack.append(lookahead)    # Save the current lookahead token\n                            symstack.extend(targ[1:-1])         # Put the production slice back on the stack\n                            statestack.pop()                    # Pop back one state (before the reduce)\n                            state = statestack[-1]\n                            sym.type = 'error'\n                            sym.value = 'error'\n                            lookahead = sym\n                            errorcount = error_count\n                            self.errorok = False\n\n                        continue\n                        # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n\n                    else:\n\n                        #--! TRACKING\n                        if tracking:\n                            sym.lineno = lexer.lineno\n                            sym.lexpos = lexer.lexpos\n                        #--! TRACKING\n\n                        targ = [sym]\n\n                        # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n                        # The code enclosed in this section is duplicated\n                        # above as a performance optimization.  Make sure\n                        # changes get made in both locations.\n\n                        pslice.slice = targ\n\n                        try:\n                            # Call the grammar rule with our special slice object\n                            self.state = state\n                            p.callable(pslice)\n                            #--! DEBUG\n                            debug.info('Result : %s', format_result(pslice[0]))\n                            #--! DEBUG\n                            symstack.append(sym)\n                            state = goto[statestack[-1]][pname]\n                            statestack.append(state)\n                        except SyntaxError:\n                            # If an error was set. Enter error recovery state\n                            lookaheadstack.append(lookahead)    # Save the current lookahead token\n                            statestack.pop()                    # Pop back one state (before the reduce)\n                            state = statestack[-1]\n                            sym.type = 'error'\n                            sym.value = 'error'\n                            lookahead = sym\n                            errorcount = error_count\n                            self.errorok = False\n\n                        continue\n                        # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n\n                if t == 0:\n                    n = symstack[-1]\n                    result = getattr(n, 'value', None)\n                    #--! DEBUG\n                    debug.info('Done   : Returning %s', format_result(result))\n                    debug.info('PLY: PARSE DEBUG END')\n                    #--! DEBUG\n                    return result\n\n            if t is None:\n\n                #--! DEBUG\n                debug.error('Error  : %s',\n                            ('%s . %s' % (' '.join([xx.type for xx in symstack][1:]), str(lookahead))).lstrip())\n                #--! DEBUG\n\n                # We have some kind of parsing error here.  To handle\n                # this, we are going to push the current token onto\n                # the tokenstack and replace it with an 'error' token.\n                # If there are any synchronization rules, they may\n                # catch it.\n                #\n                # In addition to pushing the error token, we call call\n                # the user defined p_error() function if this is the\n                # first syntax error.  This function is only called if\n                # errorcount == 0.\n                if errorcount == 0 or self.errorok:\n                    errorcount = error_count\n                    self.errorok = False\n                    errtoken = lookahead\n                    if errtoken.type == '$end':\n                        errtoken = None               # End of file!\n                    if self.errorfunc:\n                        if errtoken and not hasattr(errtoken, 'lexer'):\n                            errtoken.lexer = lexer\n                        self.state = state\n                        tok = call_errorfunc(self.errorfunc, errtoken, self)\n                        if self.errorok:\n                            # User must have done some kind of panic\n                            # mode recovery on their own.  The\n                            # returned token is the next lookahead\n                            lookahead = tok\n                            errtoken = None\n                            continue\n                    else:\n                        if errtoken:\n                            if hasattr(errtoken, 'lineno'):\n                                lineno = lookahead.lineno\n                            else:\n                                lineno = 0\n                            if lineno:\n                                sys.stderr.write('yacc: Syntax error at line %d, token=%s\\n' % (lineno, errtoken.type))\n                            else:\n                                sys.stderr.write('yacc: Syntax error, token=%s' % errtoken.type)\n                        else:\n                            sys.stderr.write('yacc: Parse error in input. EOF\\n')\n                            return\n\n                else:\n                    errorcount = error_count\n\n                # case 1:  the statestack only has 1 entry on it.  If we're in this state, the\n                # entire parse has been rolled back and we're completely hosed.   The token is\n                # discarded and we just keep going.\n\n                if len(statestack) <= 1 and lookahead.type != '$end':\n                    lookahead = None\n                    errtoken = None\n                    state = 0\n                    # Nuke the pushback stack\n                    del lookaheadstack[:]\n                    continue\n\n                # case 2: the statestack has a couple of entries on it, but we're\n                # at the end of the file. nuke the top entry and generate an error token\n\n                # Start nuking entries on the stack\n                if lookahead.type == '$end':\n                    # Whoa. We're really hosed here. Bail out\n                    return\n\n                if lookahead.type != 'error':\n                    sym = symstack[-1]\n                    if sym.type == 'error':\n                        # Hmmm. Error is on top of stack, we'll just nuke input\n                        # symbol and continue\n                        #--! TRACKING\n                        if tracking:\n                            sym.endlineno = getattr(lookahead, 'lineno', sym.lineno)\n                            sym.endlexpos = getattr(lookahead, 'lexpos', sym.lexpos)\n                        #--! TRACKING\n                        lookahead = None\n                        continue\n\n                    # Create the error symbol for the first time and make it the new lookahead symbol\n                    t = YaccSymbol()\n                    t.type = 'error'\n\n                    if hasattr(lookahead, 'lineno'):\n                        t.lineno = t.endlineno = lookahead.lineno\n                    if hasattr(lookahead, 'lexpos'):\n                        t.lexpos = t.endlexpos = lookahead.lexpos\n                    t.value = lookahead\n                    lookaheadstack.append(lookahead)\n                    lookahead = t\n                else:\n                    sym = symstack.pop()\n                    #--! TRACKING\n                    if tracking:\n                        lookahead.lineno = sym.lineno\n                        lookahead.lexpos = sym.lexpos\n                    #--! TRACKING\n                    statestack.pop()\n                    state = statestack[-1]\n\n                continue\n\n            # Call an error function here\n            raise RuntimeError('yacc: internal parser error!!!\\n')\n\n        #--! parsedebug-end\n\n    # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n    # parseopt().\n    #\n    # Optimized version of parse() method.  DO NOT EDIT THIS CODE DIRECTLY!\n    # This code is automatically generated by the ply/ygen.py script. Make\n    # changes to the parsedebug() method instead.\n    # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n\n    def parseopt(self, input=None, lexer=None, debug=False, tracking=False, tokenfunc=None):\n        #--! parseopt-start\n        lookahead = None                         # Current lookahead symbol\n        lookaheadstack = []                      # Stack of lookahead symbols\n        actions = self.action                    # Local reference to action table (to avoid lookup on self.)\n        goto    = self.goto                      # Local reference to goto table (to avoid lookup on self.)\n        prod    = self.productions               # Local reference to production list (to avoid lookup on self.)\n        defaulted_states = self.defaulted_states # Local reference to defaulted states\n        pslice  = YaccProduction(None)           # Production object passed to grammar rules\n        errorcount = 0                           # Used during error recovery\n\n\n        # If no lexer was given, we will try to use the lex module\n        if not lexer:\n            from . import lex\n            lexer = lex.lexer\n\n        # Set up the lexer and parser objects on pslice\n        pslice.lexer = lexer\n        pslice.parser = self\n\n        # If input was supplied, pass to lexer\n        if input is not None:\n            lexer.input(input)\n\n        if tokenfunc is None:\n            # Tokenize function\n            get_token = lexer.token\n        else:\n            get_token = tokenfunc\n\n        # Set the parser() token method (sometimes used in error recovery)\n        self.token = get_token\n\n        # Set up the state and symbol stacks\n\n        statestack = []                # Stack of parsing states\n        self.statestack = statestack\n        symstack   = []                # Stack of grammar symbols\n        self.symstack = symstack\n\n        pslice.stack = symstack         # Put in the production\n        errtoken   = None               # Err token\n\n        # The start state is assumed to be (0,$end)\n\n        statestack.append(0)\n        sym = YaccSymbol()\n        sym.type = '$end'\n        symstack.append(sym)\n        state = 0\n        while True:\n            # Get the next symbol on the input.  If a lookahead symbol\n            # is already set, we just use that. Otherwise, we'll pull\n            # the next token off of the lookaheadstack or from the lexer\n\n\n            if state not in defaulted_states:\n                if not lookahead:\n                    if not lookaheadstack:\n                        lookahead = get_token()     # Get the next token\n                    else:\n                        lookahead = lookaheadstack.pop()\n                    if not lookahead:\n                        lookahead = YaccSymbol()\n                        lookahead.type = '$end'\n\n                # Check the action table\n                ltype = lookahead.type\n                t = actions[state].get(ltype)\n            else:\n                t = defaulted_states[state]\n\n\n            if t is not None:\n                if t > 0:\n                    # shift a symbol on the stack\n                    statestack.append(t)\n                    state = t\n\n\n                    symstack.append(lookahead)\n                    lookahead = None\n\n                    # Decrease error count on successful shift\n                    if errorcount:\n                        errorcount -= 1\n                    continue\n\n                if t < 0:\n                    # reduce a symbol on the stack, emit a production\n                    p = prod[-t]\n                    pname = p.name\n                    plen  = p.len\n\n                    # Get production function\n                    sym = YaccSymbol()\n                    sym.type = pname       # Production name\n                    sym.value = None\n\n\n                    if plen:\n                        targ = symstack[-plen-1:]\n                        targ[0] = sym\n\n                        #--! TRACKING\n                        if tracking:\n                            t1 = targ[1]\n                            sym.lineno = t1.lineno\n                            sym.lexpos = t1.lexpos\n                            t1 = targ[-1]\n                            sym.endlineno = getattr(t1, 'endlineno', t1.lineno)\n                            sym.endlexpos = getattr(t1, 'endlexpos', t1.lexpos)\n                        #--! TRACKING\n\n                        # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n                        # The code enclosed in this section is duplicated\n                        # below as a performance optimization.  Make sure\n                        # changes get made in both locations.\n\n                        pslice.slice = targ\n\n                        try:\n                            # Call the grammar rule with our special slice object\n                            del symstack[-plen:]\n                            self.state = state\n                            p.callable(pslice)\n                            del statestack[-plen:]\n                            symstack.append(sym)\n                            state = goto[statestack[-1]][pname]\n                            statestack.append(state)\n                        except SyntaxError:\n                            # If an error was set. Enter error recovery state\n                            lookaheadstack.append(lookahead)    # Save the current lookahead token\n                            symstack.extend(targ[1:-1])         # Put the production slice back on the stack\n                            statestack.pop()                    # Pop back one state (before the reduce)\n                            state = statestack[-1]\n                            sym.type = 'error'\n                            sym.value = 'error'\n                            lookahead = sym\n                            errorcount = error_count\n                            self.errorok = False\n\n                        continue\n                        # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n\n                    else:\n\n                        #--! TRACKING\n                        if tracking:\n                            sym.lineno = lexer.lineno\n                            sym.lexpos = lexer.lexpos\n                        #--! TRACKING\n\n                        targ = [sym]\n\n                        # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n                        # The code enclosed in this section is duplicated\n                        # above as a performance optimization.  Make sure\n                        # changes get made in both locations.\n\n                        pslice.slice = targ\n\n                        try:\n                            # Call the grammar rule with our special slice object\n                            self.state = state\n                            p.callable(pslice)\n                            symstack.append(sym)\n                            state = goto[statestack[-1]][pname]\n                            statestack.append(state)\n                        except SyntaxError:\n                            # If an error was set. Enter error recovery state\n                            lookaheadstack.append(lookahead)    # Save the current lookahead token\n                            statestack.pop()                    # Pop back one state (before the reduce)\n                            state = statestack[-1]\n                            sym.type = 'error'\n                            sym.value = 'error'\n                            lookahead = sym\n                            errorcount = error_count\n                            self.errorok = False\n\n                        continue\n                        # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n\n                if t == 0:\n                    n = symstack[-1]\n                    result = getattr(n, 'value', None)\n                    return result\n\n            if t is None:\n\n\n                # We have some kind of parsing error here.  To handle\n                # this, we are going to push the current token onto\n                # the tokenstack and replace it with an 'error' token.\n                # If there are any synchronization rules, they may\n                # catch it.\n                #\n                # In addition to pushing the error token, we call call\n                # the user defined p_error() function if this is the\n                # first syntax error.  This function is only called if\n                # errorcount == 0.\n                if errorcount == 0 or self.errorok:\n                    errorcount = error_count\n                    self.errorok = False\n                    errtoken = lookahead\n                    if errtoken.type == '$end':\n                        errtoken = None               # End of file!\n                    if self.errorfunc:\n                        if errtoken and not hasattr(errtoken, 'lexer'):\n                            errtoken.lexer = lexer\n                        self.state = state\n                        tok = call_errorfunc(self.errorfunc, errtoken, self)\n                        if self.errorok:\n                            # User must have done some kind of panic\n                            # mode recovery on their own.  The\n                            # returned token is the next lookahead\n                            lookahead = tok\n                            errtoken = None\n                            continue\n                    else:\n                        if errtoken:\n                            if hasattr(errtoken, 'lineno'):\n                                lineno = lookahead.lineno\n                            else:\n                                lineno = 0\n                            if lineno:\n                                sys.stderr.write('yacc: Syntax error at line %d, token=%s\\n' % (lineno, errtoken.type))\n                            else:\n                                sys.stderr.write('yacc: Syntax error, token=%s' % errtoken.type)\n                        else:\n                            sys.stderr.write('yacc: Parse error in input. EOF\\n')\n                            return\n\n                else:\n                    errorcount = error_count\n\n                # case 1:  the statestack only has 1 entry on it.  If we're in this state, the\n                # entire parse has been rolled back and we're completely hosed.   The token is\n                # discarded and we just keep going.\n\n                if len(statestack) <= 1 and lookahead.type != '$end':\n                    lookahead = None\n                    errtoken = None\n                    state = 0\n                    # Nuke the pushback stack\n                    del lookaheadstack[:]\n                    continue\n\n                # case 2: the statestack has a couple of entries on it, but we're\n                # at the end of the file. nuke the top entry and generate an error token\n\n                # Start nuking entries on the stack\n                if lookahead.type == '$end':\n                    # Whoa. We're really hosed here. Bail out\n                    return\n\n                if lookahead.type != 'error':\n                    sym = symstack[-1]\n                    if sym.type == 'error':\n                        # Hmmm. Error is on top of stack, we'll just nuke input\n                        # symbol and continue\n                        #--! TRACKING\n                        if tracking:\n                            sym.endlineno = getattr(lookahead, 'lineno', sym.lineno)\n                            sym.endlexpos = getattr(lookahead, 'lexpos', sym.lexpos)\n                        #--! TRACKING\n                        lookahead = None\n                        continue\n\n                    # Create the error symbol for the first time and make it the new lookahead symbol\n                    t = YaccSymbol()\n                    t.type = 'error'\n\n                    if hasattr(lookahead, 'lineno'):\n                        t.lineno = t.endlineno = lookahead.lineno\n                    if hasattr(lookahead, 'lexpos'):\n                        t.lexpos = t.endlexpos = lookahead.lexpos\n                    t.value = lookahead\n                    lookaheadstack.append(lookahead)\n                    lookahead = t\n                else:\n                    sym = symstack.pop()\n                    #--! TRACKING\n                    if tracking:\n                        lookahead.lineno = sym.lineno\n                        lookahead.lexpos = sym.lexpos\n                    #--! TRACKING\n                    statestack.pop()\n                    state = statestack[-1]\n\n                continue\n\n            # Call an error function here\n            raise RuntimeError('yacc: internal parser error!!!\\n')\n\n        #--! parseopt-end\n\n    # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n    # parseopt_notrack().\n    #\n    # Optimized version of parseopt() with line number tracking removed.\n    # DO NOT EDIT THIS CODE DIRECTLY. This code is automatically generated\n    # by the ply/ygen.py script. Make changes to the parsedebug() method instead.\n    # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n\n    def parseopt_notrack(self, input=None, lexer=None, debug=False, tracking=False, tokenfunc=None):\n        #--! parseopt-notrack-start\n        lookahead = None                         # Current lookahead symbol\n        lookaheadstack = []                      # Stack of lookahead symbols\n        actions = self.action                    # Local reference to action table (to avoid lookup on self.)\n        goto    = self.goto                      # Local reference to goto table (to avoid lookup on self.)\n        prod    = self.productions               # Local reference to production list (to avoid lookup on self.)\n        defaulted_states = self.defaulted_states # Local reference to defaulted states\n        pslice  = YaccProduction(None)           # Production object passed to grammar rules\n        errorcount = 0                           # Used during error recovery\n\n\n        # If no lexer was given, we will try to use the lex module\n        if not lexer:\n            from . import lex\n            lexer = lex.lexer\n\n        # Set up the lexer and parser objects on pslice\n        pslice.lexer = lexer\n        pslice.parser = self\n\n        # If input was supplied, pass to lexer\n        if input is not None:\n            lexer.input(input)\n\n        if tokenfunc is None:\n            # Tokenize function\n            get_token = lexer.token\n        else:\n            get_token = tokenfunc\n\n        # Set the parser() token method (sometimes used in error recovery)\n        self.token = get_token\n\n        # Set up the state and symbol stacks\n\n        statestack = []                # Stack of parsing states\n        self.statestack = statestack\n        symstack   = []                # Stack of grammar symbols\n        self.symstack = symstack\n\n        pslice.stack = symstack         # Put in the production\n        errtoken   = None               # Err token\n\n        # The start state is assumed to be (0,$end)\n\n        statestack.append(0)\n        sym = YaccSymbol()\n        sym.type = '$end'\n        symstack.append(sym)\n        state = 0\n        while True:\n            # Get the next symbol on the input.  If a lookahead symbol\n            # is already set, we just use that. Otherwise, we'll pull\n            # the next token off of the lookaheadstack or from the lexer\n\n\n            if state not in defaulted_states:\n                if not lookahead:\n                    if not lookaheadstack:\n                        lookahead = get_token()     # Get the next token\n                    else:\n                        lookahead = lookaheadstack.pop()\n                    if not lookahead:\n                        lookahead = YaccSymbol()\n                        lookahead.type = '$end'\n\n                # Check the action table\n                ltype = lookahead.type\n                t = actions[state].get(ltype)\n            else:\n                t = defaulted_states[state]\n\n\n            if t is not None:\n                if t > 0:\n                    # shift a symbol on the stack\n                    statestack.append(t)\n                    state = t\n\n\n                    symstack.append(lookahead)\n                    lookahead = None\n\n                    # Decrease error count on successful shift\n                    if errorcount:\n                        errorcount -= 1\n                    continue\n\n                if t < 0:\n                    # reduce a symbol on the stack, emit a production\n                    p = prod[-t]\n                    pname = p.name\n                    plen  = p.len\n\n                    # Get production function\n                    sym = YaccSymbol()\n                    sym.type = pname       # Production name\n                    sym.value = None\n\n\n                    if plen:\n                        targ = symstack[-plen-1:]\n                        targ[0] = sym\n\n\n                        # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n                        # The code enclosed in this section is duplicated\n                        # below as a performance optimization.  Make sure\n                        # changes get made in both locations.\n\n                        pslice.slice = targ\n\n                        try:\n                            # Call the grammar rule with our special slice object\n                            del symstack[-plen:]\n                            self.state = state\n                            p.callable(pslice)\n                            del statestack[-plen:]\n                            symstack.append(sym)\n                            state = goto[statestack[-1]][pname]\n                            statestack.append(state)\n                        except SyntaxError:\n                            # If an error was set. Enter error recovery state\n                            lookaheadstack.append(lookahead)    # Save the current lookahead token\n                            symstack.extend(targ[1:-1])         # Put the production slice back on the stack\n                            statestack.pop()                    # Pop back one state (before the reduce)\n                            state = statestack[-1]\n                            sym.type = 'error'\n                            sym.value = 'error'\n                            lookahead = sym\n                            errorcount = error_count\n                            self.errorok = False\n\n                        continue\n                        # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n\n                    else:\n\n\n                        targ = [sym]\n\n                        # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n                        # The code enclosed in this section is duplicated\n                        # above as a performance optimization.  Make sure\n                        # changes get made in both locations.\n\n                        pslice.slice = targ\n\n                        try:\n                            # Call the grammar rule with our special slice object\n                            self.state = state\n                            p.callable(pslice)\n                            symstack.append(sym)\n                            state = goto[statestack[-1]][pname]\n                            statestack.append(state)\n                        except SyntaxError:\n                            # If an error was set. Enter error recovery state\n                            lookaheadstack.append(lookahead)    # Save the current lookahead token\n                            statestack.pop()                    # Pop back one state (before the reduce)\n                            state = statestack[-1]\n                            sym.type = 'error'\n                            sym.value = 'error'\n                            lookahead = sym\n                            errorcount = error_count\n                            self.errorok = False\n\n                        continue\n                        # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n\n                if t == 0:\n                    n = symstack[-1]\n                    result = getattr(n, 'value', None)\n                    return result\n\n            if t is None:\n\n\n                # We have some kind of parsing error here.  To handle\n                # this, we are going to push the current token onto\n                # the tokenstack and replace it with an 'error' token.\n                # If there are any synchronization rules, they may\n                # catch it.\n                #\n                # In addition to pushing the error token, we call call\n                # the user defined p_error() function if this is the\n                # first syntax error.  This function is only called if\n                # errorcount == 0.\n                if errorcount == 0 or self.errorok:\n                    errorcount = error_count\n                    self.errorok = False\n                    errtoken = lookahead\n                    if errtoken.type == '$end':\n                        errtoken = None               # End of file!\n                    if self.errorfunc:\n                        if errtoken and not hasattr(errtoken, 'lexer'):\n                            errtoken.lexer = lexer\n                        self.state = state\n                        tok = call_errorfunc(self.errorfunc, errtoken, self)\n                        if self.errorok:\n                            # User must have done some kind of panic\n                            # mode recovery on their own.  The\n                            # returned token is the next lookahead\n                            lookahead = tok\n                            errtoken = None\n                            continue\n                    else:\n                        if errtoken:\n                            if hasattr(errtoken, 'lineno'):\n                                lineno = lookahead.lineno\n                            else:\n                                lineno = 0\n                            if lineno:\n                                sys.stderr.write('yacc: Syntax error at line %d, token=%s\\n' % (lineno, errtoken.type))\n                            else:\n                                sys.stderr.write('yacc: Syntax error, token=%s' % errtoken.type)\n                        else:\n                            sys.stderr.write('yacc: Parse error in input. EOF\\n')\n                            return\n\n                else:\n                    errorcount = error_count\n\n                # case 1:  the statestack only has 1 entry on it.  If we're in this state, the\n                # entire parse has been rolled back and we're completely hosed.   The token is\n                # discarded and we just keep going.\n\n                if len(statestack) <= 1 and lookahead.type != '$end':\n                    lookahead = None\n                    errtoken = None\n                    state = 0\n                    # Nuke the pushback stack\n                    del lookaheadstack[:]\n                    continue\n\n                # case 2: the statestack has a couple of entries on it, but we're\n                # at the end of the file. nuke the top entry and generate an error token\n\n                # Start nuking entries on the stack\n                if lookahead.type == '$end':\n                    # Whoa. We're really hosed here. Bail out\n                    return\n\n                if lookahead.type != 'error':\n                    sym = symstack[-1]\n                    if sym.type == 'error':\n                        # Hmmm. Error is on top of stack, we'll just nuke input\n                        # symbol and continue\n                        lookahead = None\n                        continue\n\n                    # Create the error symbol for the first time and make it the new lookahead symbol\n                    t = YaccSymbol()\n                    t.type = 'error'\n\n                    if hasattr(lookahead, 'lineno'):\n                        t.lineno = t.endlineno = lookahead.lineno\n                    if hasattr(lookahead, 'lexpos'):\n                        t.lexpos = t.endlexpos = lookahead.lexpos\n                    t.value = lookahead\n                    lookaheadstack.append(lookahead)\n                    lookahead = t\n                else:\n                    sym = symstack.pop()\n                    statestack.pop()\n                    state = statestack[-1]\n\n                continue\n\n            # Call an error function here\n            raise RuntimeError('yacc: internal parser error!!!\\n')\n\n        #--! parseopt-notrack-end\n\n# -----------------------------------------------------------------------------\n#                          === Grammar Representation ===\n#\n# The following functions, classes, and variables are used to represent and\n# manipulate the rules that make up a grammar.\n# -----------------------------------------------------------------------------\n\n# regex matching identifiers\n_is_identifier = re.compile(r'^[a-zA-Z0-9_-]+$')\n\n# -----------------------------------------------------------------------------\n# class Production:\n#\n# This class stores the raw information about a single production or grammar rule.\n# A grammar rule refers to a specification such as this:\n#\n#       expr : expr PLUS term\n#\n# Here are the basic attributes defined on all productions\n#\n#       name     - Name of the production.  For example 'expr'\n#       prod     - A list of symbols on the right side ['expr','PLUS','term']\n#       prec     - Production precedence level\n#       number   - Production number.\n#       func     - Function that executes on reduce\n#       file     - File where production function is defined\n#       lineno   - Line number where production function is defined\n#\n# The following attributes are defined or optional.\n#\n#       len       - Length of the production (number of symbols on right hand side)\n#       usyms     - Set of unique symbols found in the production\n# -----------------------------------------------------------------------------\n\nclass Production(object):\n    reduced = 0\n    def __init__(self, number, name, prod, precedence=('right', 0), func=None, file='', line=0):\n        self.name     = name\n        self.prod     = tuple(prod)\n        self.number   = number\n        self.func     = func\n        self.callable = None\n        self.file     = file\n        self.line     = line\n        self.prec     = precedence\n\n        # Internal settings used during table construction\n\n        self.len  = len(self.prod)   # Length of the production\n\n        # Create a list of unique production symbols used in the production\n        self.usyms = []\n        for s in self.prod:\n            if s not in self.usyms:\n                self.usyms.append(s)\n\n        # List of all LR items for the production\n        self.lr_items = []\n        self.lr_next = None\n\n        # Create a string representation\n        if self.prod:\n            self.str = '%s -> %s' % (self.name, ' '.join(self.prod))\n        else:\n            self.str = '%s -> <empty>' % self.name\n\n    def __str__(self):\n        return self.str\n\n    def __repr__(self):\n        return 'Production(' + str(self) + ')'\n\n    def __len__(self):\n        return len(self.prod)\n\n    def __nonzero__(self):\n        return 1\n\n    def __getitem__(self, index):\n        return self.prod[index]\n\n    # Return the nth lr_item from the production (or None if at the end)\n    def lr_item(self, n):\n        if n > len(self.prod):\n            return None\n        p = LRItem(self, n)\n        # Precompute the list of productions immediately following.\n        try:\n            p.lr_after = Prodnames[p.prod[n+1]]\n        except (IndexError, KeyError):\n            p.lr_after = []\n        try:\n            p.lr_before = p.prod[n-1]\n        except IndexError:\n            p.lr_before = None\n        return p\n\n    # Bind the production function name to a callable\n    def bind(self, pdict):\n        if self.func:\n            self.callable = pdict[self.func]\n\n# This class serves as a minimal standin for Production objects when\n# reading table data from files.   It only contains information\n# actually used by the LR parsing engine, plus some additional\n# debugging information.\nclass MiniProduction(object):\n    def __init__(self, str, name, len, func, file, line):\n        self.name     = name\n        self.len      = len\n        self.func     = func\n        self.callable = None\n        self.file     = file\n        self.line     = line\n        self.str      = str\n\n    def __str__(self):\n        return self.str\n\n    def __repr__(self):\n        return 'MiniProduction(%s)' % self.str\n\n    # Bind the production function name to a callable\n    def bind(self, pdict):\n        if self.func:\n            self.callable = pdict[self.func]\n\n\n# -----------------------------------------------------------------------------\n# class LRItem\n#\n# This class represents a specific stage of parsing a production rule.  For\n# example:\n#\n#       expr : expr . PLUS term\n#\n# In the above, the \".\" represents the current location of the parse.  Here\n# basic attributes:\n#\n#       name       - Name of the production.  For example 'expr'\n#       prod       - A list of symbols on the right side ['expr','.', 'PLUS','term']\n#       number     - Production number.\n#\n#       lr_next      Next LR item. Example, if we are ' expr -> expr . PLUS term'\n#                    then lr_next refers to 'expr -> expr PLUS . term'\n#       lr_index   - LR item index (location of the \".\") in the prod list.\n#       lookaheads - LALR lookahead symbols for this item\n#       len        - Length of the production (number of symbols on right hand side)\n#       lr_after    - List of all productions that immediately follow\n#       lr_before   - Grammar symbol immediately before\n# -----------------------------------------------------------------------------\n\nclass LRItem(object):\n    def __init__(self, p, n):\n        self.name       = p.name\n        self.prod       = list(p.prod)\n        self.number     = p.number\n        self.lr_index   = n\n        self.lookaheads = {}\n        self.prod.insert(n, '.')\n        self.prod       = tuple(self.prod)\n        self.len        = len(self.prod)\n        self.usyms      = p.usyms\n\n    def __str__(self):\n        if self.prod:\n            s = '%s -> %s' % (self.name, ' '.join(self.prod))\n        else:\n            s = '%s -> <empty>' % self.name\n        return s\n\n    def __repr__(self):\n        return 'LRItem(' + str(self) + ')'\n\n# -----------------------------------------------------------------------------\n# rightmost_terminal()\n#\n# Return the rightmost terminal from a list of symbols.  Used in add_production()\n# -----------------------------------------------------------------------------\ndef rightmost_terminal(symbols, terminals):\n    i = len(symbols) - 1\n    while i >= 0:\n        if symbols[i] in terminals:\n            return symbols[i]\n        i -= 1\n    return None\n\n# -----------------------------------------------------------------------------\n#                           === GRAMMAR CLASS ===\n#\n# The following class represents the contents of the specified grammar along\n# with various computed properties such as first sets, follow sets, LR items, etc.\n# This data is used for critical parts of the table generation process later.\n# -----------------------------------------------------------------------------\n\nclass GrammarError(YaccError):\n    pass\n\nclass Grammar(object):\n    def __init__(self, terminals):\n        self.Productions  = [None]  # A list of all of the productions.  The first\n                                    # entry is always reserved for the purpose of\n                                    # building an augmented grammar\n\n        self.Prodnames    = {}      # A dictionary mapping the names of nonterminals to a list of all\n                                    # productions of that nonterminal.\n\n        self.Prodmap      = {}      # A dictionary that is only used to detect duplicate\n                                    # productions.\n\n        self.Terminals    = {}      # A dictionary mapping the names of terminal symbols to a\n                                    # list of the rules where they are used.\n\n        for term in terminals:\n            self.Terminals[term] = []\n\n        self.Terminals['error'] = []\n\n        self.Nonterminals = {}      # A dictionary mapping names of nonterminals to a list\n                                    # of rule numbers where they are used.\n\n        self.First        = {}      # A dictionary of precomputed FIRST(x) symbols\n\n        self.Follow       = {}      # A dictionary of precomputed FOLLOW(x) symbols\n\n        self.Precedence   = {}      # Precedence rules for each terminal. Contains tuples of the\n                                    # form ('right',level) or ('nonassoc', level) or ('left',level)\n\n        self.UsedPrecedence = set() # Precedence rules that were actually used by the grammer.\n                                    # This is only used to provide error checking and to generate\n                                    # a warning about unused precedence rules.\n\n        self.Start = None           # Starting symbol for the grammar\n\n\n    def __len__(self):\n        return len(self.Productions)\n\n    def __getitem__(self, index):\n        return self.Productions[index]\n\n    # -----------------------------------------------------------------------------\n    # set_precedence()\n    #\n    # Sets the precedence for a given terminal. assoc is the associativity such as\n    # 'left','right', or 'nonassoc'.  level is a numeric level.\n    #\n    # -----------------------------------------------------------------------------\n\n    def set_precedence(self, term, assoc, level):\n        assert self.Productions == [None], 'Must call set_precedence() before add_production()'\n        if term in self.Precedence:\n            raise GrammarError('Precedence already specified for terminal %r' % term)\n        if assoc not in ['left', 'right', 'nonassoc']:\n            raise GrammarError(\"Associativity must be one of 'left','right', or 'nonassoc'\")\n        self.Precedence[term] = (assoc, level)\n\n    # -----------------------------------------------------------------------------\n    # add_production()\n    #\n    # Given an action function, this function assembles a production rule and\n    # computes its precedence level.\n    #\n    # The production rule is supplied as a list of symbols.   For example,\n    # a rule such as 'expr : expr PLUS term' has a production name of 'expr' and\n    # symbols ['expr','PLUS','term'].\n    #\n    # Precedence is determined by the precedence of the right-most non-terminal\n    # or the precedence of a terminal specified by %prec.\n    #\n    # A variety of error checks are performed to make sure production symbols\n    # are valid and that %prec is used correctly.\n    # -----------------------------------------------------------------------------\n\n    def add_production(self, prodname, syms, func=None, file='', line=0):\n\n        if prodname in self.Terminals:\n            raise GrammarError('%s:%d: Illegal rule name %r. Already defined as a token' % (file, line, prodname))\n        if prodname == 'error':\n            raise GrammarError('%s:%d: Illegal rule name %r. error is a reserved word' % (file, line, prodname))\n        if not _is_identifier.match(prodname):\n            raise GrammarError('%s:%d: Illegal rule name %r' % (file, line, prodname))\n\n        # Look for literal tokens\n        for n, s in enumerate(syms):\n            if s[0] in \"'\\\"\":\n                try:\n                    c = eval(s)\n                    if (len(c) > 1):\n                        raise GrammarError('%s:%d: Literal token %s in rule %r may only be a single character' %\n                                           (file, line, s, prodname))\n                    if c not in self.Terminals:\n                        self.Terminals[c] = []\n                    syms[n] = c\n                    continue\n                except SyntaxError:\n                    pass\n            if not _is_identifier.match(s) and s != '%prec':\n                raise GrammarError('%s:%d: Illegal name %r in rule %r' % (file, line, s, prodname))\n\n        # Determine the precedence level\n        if '%prec' in syms:\n            if syms[-1] == '%prec':\n                raise GrammarError('%s:%d: Syntax error. Nothing follows %%prec' % (file, line))\n            if syms[-2] != '%prec':\n                raise GrammarError('%s:%d: Syntax error. %%prec can only appear at the end of a grammar rule' %\n                                   (file, line))\n            precname = syms[-1]\n            prodprec = self.Precedence.get(precname)\n            if not prodprec:\n                raise GrammarError('%s:%d: Nothing known about the precedence of %r' % (file, line, precname))\n            else:\n                self.UsedPrecedence.add(precname)\n            del syms[-2:]     # Drop %prec from the rule\n        else:\n            # If no %prec, precedence is determined by the rightmost terminal symbol\n            precname = rightmost_terminal(syms, self.Terminals)\n            prodprec = self.Precedence.get(precname, ('right', 0))\n\n        # See if the rule is already in the rulemap\n        map = '%s -> %s' % (prodname, syms)\n        if map in self.Prodmap:\n            m = self.Prodmap[map]\n            raise GrammarError('%s:%d: Duplicate rule %s. ' % (file, line, m) +\n                               'Previous definition at %s:%d' % (m.file, m.line))\n\n        # From this point on, everything is valid.  Create a new Production instance\n        pnumber  = len(self.Productions)\n        if prodname not in self.Nonterminals:\n            self.Nonterminals[prodname] = []\n\n        # Add the production number to Terminals and Nonterminals\n        for t in syms:\n            if t in self.Terminals:\n                self.Terminals[t].append(pnumber)\n            else:\n                if t not in self.Nonterminals:\n                    self.Nonterminals[t] = []\n                self.Nonterminals[t].append(pnumber)\n\n        # Create a production and add it to the list of productions\n        p = Production(pnumber, prodname, syms, prodprec, func, file, line)\n        self.Productions.append(p)\n        self.Prodmap[map] = p\n\n        # Add to the global productions list\n        try:\n            self.Prodnames[prodname].append(p)\n        except KeyError:\n            self.Prodnames[prodname] = [p]\n\n    # -----------------------------------------------------------------------------\n    # set_start()\n    #\n    # Sets the starting symbol and creates the augmented grammar.  Production\n    # rule 0 is S' -> start where start is the start symbol.\n    # -----------------------------------------------------------------------------\n\n    def set_start(self, start=None):\n        if not start:\n            start = self.Productions[1].name\n        if start not in self.Nonterminals:\n            raise GrammarError('start symbol %s undefined' % start)\n        self.Productions[0] = Production(0, \"S'\", [start])\n        self.Nonterminals[start].append(0)\n        self.Start = start\n\n    # -----------------------------------------------------------------------------\n    # find_unreachable()\n    #\n    # Find all of the nonterminal symbols that can't be reached from the starting\n    # symbol.  Returns a list of nonterminals that can't be reached.\n    # -----------------------------------------------------------------------------\n\n    def find_unreachable(self):\n\n        # Mark all symbols that are reachable from a symbol s\n        def mark_reachable_from(s):\n            if s in reachable:\n                return\n            reachable.add(s)\n            for p in self.Prodnames.get(s, []):\n                for r in p.prod:\n                    mark_reachable_from(r)\n\n        reachable = set()\n        mark_reachable_from(self.Productions[0].prod[0])\n        return [s for s in self.Nonterminals if s not in reachable]\n\n    # -----------------------------------------------------------------------------\n    # infinite_cycles()\n    #\n    # This function looks at the various parsing rules and tries to detect\n    # infinite recursion cycles (grammar rules where there is no possible way\n    # to derive a string of only terminals).\n    # -----------------------------------------------------------------------------\n\n    def infinite_cycles(self):\n        terminates = {}\n\n        # Terminals:\n        for t in self.Terminals:\n            terminates[t] = True\n\n        terminates['$end'] = True\n\n        # Nonterminals:\n\n        # Initialize to false:\n        for n in self.Nonterminals:\n            terminates[n] = False\n\n        # Then propagate termination until no change:\n        while True:\n            some_change = False\n            for (n, pl) in self.Prodnames.items():\n                # Nonterminal n terminates iff any of its productions terminates.\n                for p in pl:\n                    # Production p terminates iff all of its rhs symbols terminate.\n                    for s in p.prod:\n                        if not terminates[s]:\n                            # The symbol s does not terminate,\n                            # so production p does not terminate.\n                            p_terminates = False\n                            break\n                    else:\n                        # didn't break from the loop,\n                        # so every symbol s terminates\n                        # so production p terminates.\n                        p_terminates = True\n\n                    if p_terminates:\n                        # symbol n terminates!\n                        if not terminates[n]:\n                            terminates[n] = True\n                            some_change = True\n                        # Don't need to consider any more productions for this n.\n                        break\n\n            if not some_change:\n                break\n\n        infinite = []\n        for (s, term) in terminates.items():\n            if not term:\n                if s not in self.Prodnames and s not in self.Terminals and s != 'error':\n                    # s is used-but-not-defined, and we've already warned of that,\n                    # so it would be overkill to say that it's also non-terminating.\n                    pass\n                else:\n                    infinite.append(s)\n\n        return infinite\n\n    # -----------------------------------------------------------------------------\n    # undefined_symbols()\n    #\n    # Find all symbols that were used the grammar, but not defined as tokens or\n    # grammar rules.  Returns a list of tuples (sym, prod) where sym in the symbol\n    # and prod is the production where the symbol was used.\n    # -----------------------------------------------------------------------------\n    def undefined_symbols(self):\n        result = []\n        for p in self.Productions:\n            if not p:\n                continue\n\n            for s in p.prod:\n                if s not in self.Prodnames and s not in self.Terminals and s != 'error':\n                    result.append((s, p))\n        return result\n\n    # -----------------------------------------------------------------------------\n    # unused_terminals()\n    #\n    # Find all terminals that were defined, but not used by the grammar.  Returns\n    # a list of all symbols.\n    # -----------------------------------------------------------------------------\n    def unused_terminals(self):\n        unused_tok = []\n        for s, v in self.Terminals.items():\n            if s != 'error' and not v:\n                unused_tok.append(s)\n\n        return unused_tok\n\n    # ------------------------------------------------------------------------------\n    # unused_rules()\n    #\n    # Find all grammar rules that were defined,  but not used (maybe not reachable)\n    # Returns a list of productions.\n    # ------------------------------------------------------------------------------\n\n    def unused_rules(self):\n        unused_prod = []\n        for s, v in self.Nonterminals.items():\n            if not v:\n                p = self.Prodnames[s][0]\n                unused_prod.append(p)\n        return unused_prod\n\n    # -----------------------------------------------------------------------------\n    # unused_precedence()\n    #\n    # Returns a list of tuples (term,precedence) corresponding to precedence\n    # rules that were never used by the grammar.  term is the name of the terminal\n    # on which precedence was applied and precedence is a string such as 'left' or\n    # 'right' corresponding to the type of precedence.\n    # -----------------------------------------------------------------------------\n\n    def unused_precedence(self):\n        unused = []\n        for termname in self.Precedence:\n            if not (termname in self.Terminals or termname in self.UsedPrecedence):\n                unused.append((termname, self.Precedence[termname][0]))\n\n        return unused\n\n    # -------------------------------------------------------------------------\n    # _first()\n    #\n    # Compute the value of FIRST1(beta) where beta is a tuple of symbols.\n    #\n    # During execution of compute_first1, the result may be incomplete.\n    # Afterward (e.g., when called from compute_follow()), it will be complete.\n    # -------------------------------------------------------------------------\n    def _first(self, beta):\n\n        # We are computing First(x1,x2,x3,...,xn)\n        result = []\n        for x in beta:\n            x_produces_empty = False\n\n            # Add all the non-<empty> symbols of First[x] to the result.\n            for f in self.First[x]:\n                if f == '<empty>':\n                    x_produces_empty = True\n                else:\n                    if f not in result:\n                        result.append(f)\n\n            if x_produces_empty:\n                # We have to consider the next x in beta,\n                # i.e. stay in the loop.\n                pass\n            else:\n                # We don't have to consider any further symbols in beta.\n                break\n        else:\n            # There was no 'break' from the loop,\n            # so x_produces_empty was true for all x in beta,\n            # so beta produces empty as well.\n            result.append('<empty>')\n\n        return result\n\n    # -------------------------------------------------------------------------\n    # compute_first()\n    #\n    # Compute the value of FIRST1(X) for all symbols\n    # -------------------------------------------------------------------------\n    def compute_first(self):\n        if self.First:\n            return self.First\n\n        # Terminals:\n        for t in self.Terminals:\n            self.First[t] = [t]\n\n        self.First['$end'] = ['$end']\n\n        # Nonterminals:\n\n        # Initialize to the empty set:\n        for n in self.Nonterminals:\n            self.First[n] = []\n\n        # Then propagate symbols until no change:\n        while True:\n            some_change = False\n            for n in self.Nonterminals:\n                for p in self.Prodnames[n]:\n                    for f in self._first(p.prod):\n                        if f not in self.First[n]:\n                            self.First[n].append(f)\n                            some_change = True\n            if not some_change:\n                break\n\n        return self.First\n\n    # ---------------------------------------------------------------------\n    # compute_follow()\n    #\n    # Computes all of the follow sets for every non-terminal symbol.  The\n    # follow set is the set of all symbols that might follow a given\n    # non-terminal.  See the Dragon book, 2nd Ed. p. 189.\n    # ---------------------------------------------------------------------\n    def compute_follow(self, start=None):\n        # If already computed, return the result\n        if self.Follow:\n            return self.Follow\n\n        # If first sets not computed yet, do that first.\n        if not self.First:\n            self.compute_first()\n\n        # Add '$end' to the follow list of the start symbol\n        for k in self.Nonterminals:\n            self.Follow[k] = []\n\n        if not start:\n            start = self.Productions[1].name\n\n        self.Follow[start] = ['$end']\n\n        while True:\n            didadd = False\n            for p in self.Productions[1:]:\n                # Here is the production set\n                for i, B in enumerate(p.prod):\n                    if B in self.Nonterminals:\n                        # Okay. We got a non-terminal in a production\n                        fst = self._first(p.prod[i+1:])\n                        hasempty = False\n                        for f in fst:\n                            if f != '<empty>' and f not in self.Follow[B]:\n                                self.Follow[B].append(f)\n                                didadd = True\n                            if f == '<empty>':\n                                hasempty = True\n                        if hasempty or i == (len(p.prod)-1):\n                            # Add elements of follow(a) to follow(b)\n                            for f in self.Follow[p.name]:\n                                if f not in self.Follow[B]:\n                                    self.Follow[B].append(f)\n                                    didadd = True\n            if not didadd:\n                break\n        return self.Follow\n\n\n    # -----------------------------------------------------------------------------\n    # build_lritems()\n    #\n    # This function walks the list of productions and builds a complete set of the\n    # LR items.  The LR items are stored in two ways:  First, they are uniquely\n    # numbered and placed in the list _lritems.  Second, a linked list of LR items\n    # is built for each production.  For example:\n    #\n    #   E -> E PLUS E\n    #\n    # Creates the list\n    #\n    #  [E -> . E PLUS E, E -> E . PLUS E, E -> E PLUS . E, E -> E PLUS E . ]\n    # -----------------------------------------------------------------------------\n\n    def build_lritems(self):\n        for p in self.Productions:\n            lastlri = p\n            i = 0\n            lr_items = []\n            while True:\n                if i > len(p):\n                    lri = None\n                else:\n                    lri = LRItem(p, i)\n                    # Precompute the list of productions immediately following\n                    try:\n                        lri.lr_after = self.Prodnames[lri.prod[i+1]]\n                    except (IndexError, KeyError):\n                        lri.lr_after = []\n                    try:\n                        lri.lr_before = lri.prod[i-1]\n                    except IndexError:\n                        lri.lr_before = None\n\n                lastlri.lr_next = lri\n                if not lri:\n                    break\n                lr_items.append(lri)\n                lastlri = lri\n                i += 1\n            p.lr_items = lr_items\n\n# -----------------------------------------------------------------------------\n#                            == Class LRTable ==\n#\n# This basic class represents a basic table of LR parsing information.\n# Methods for generating the tables are not defined here.  They are defined\n# in the derived class LRGeneratedTable.\n# -----------------------------------------------------------------------------\n\nclass VersionError(YaccError):\n    pass\n\nclass LRTable(object):\n    def __init__(self):\n        self.lr_action = None\n        self.lr_goto = None\n        self.lr_productions = None\n        self.lr_method = None\n\n    def read_table(self, module):\n        if isinstance(module, types.ModuleType):\n            parsetab = module\n        else:\n            exec('import %s' % module)\n            parsetab = sys.modules[module]\n\n        if parsetab._tabversion != __tabversion__:\n            raise VersionError('yacc table file version is out of date')\n\n        self.lr_action = parsetab._lr_action\n        self.lr_goto = parsetab._lr_goto\n\n        self.lr_productions = []\n        for p in parsetab._lr_productions:\n            self.lr_productions.append(MiniProduction(*p))\n\n        self.lr_method = parsetab._lr_method\n        return parsetab._lr_signature\n\n    def read_pickle(self, filename):\n        try:\n            import cPickle as pickle\n        except ImportError:\n            import pickle\n\n        if not os.path.exists(filename):\n          raise ImportError\n\n        in_f = open(filename, 'rb')\n\n        tabversion = pickle.load(in_f)\n        if tabversion != __tabversion__:\n            raise VersionError('yacc table file version is out of date')\n        self.lr_method = pickle.load(in_f)\n        signature      = pickle.load(in_f)\n        self.lr_action = pickle.load(in_f)\n        self.lr_goto   = pickle.load(in_f)\n        productions    = pickle.load(in_f)\n\n        self.lr_productions = []\n        for p in productions:\n            self.lr_productions.append(MiniProduction(*p))\n\n        in_f.close()\n        return signature\n\n    # Bind all production function names to callable objects in pdict\n    def bind_callables(self, pdict):\n        for p in self.lr_productions:\n            p.bind(pdict)\n\n\n# -----------------------------------------------------------------------------\n#                           === LR Generator ===\n#\n# The following classes and functions are used to generate LR parsing tables on\n# a grammar.\n# -----------------------------------------------------------------------------\n\n# -----------------------------------------------------------------------------\n# digraph()\n# traverse()\n#\n# The following two functions are used to compute set valued functions\n# of the form:\n#\n#     F(x) = F'(x) U U{F(y) | x R y}\n#\n# This is used to compute the values of Read() sets as well as FOLLOW sets\n# in LALR(1) generation.\n#\n# Inputs:  X    - An input set\n#          R    - A relation\n#          FP   - Set-valued function\n# ------------------------------------------------------------------------------\n\ndef digraph(X, R, FP):\n    N = {}\n    for x in X:\n        N[x] = 0\n    stack = []\n    F = {}\n    for x in X:\n        if N[x] == 0:\n            traverse(x, N, stack, F, X, R, FP)\n    return F\n\ndef traverse(x, N, stack, F, X, R, FP):\n    stack.append(x)\n    d = len(stack)\n    N[x] = d\n    F[x] = FP(x)             # F(X) <- F'(x)\n\n    rel = R(x)               # Get y's related to x\n    for y in rel:\n        if N[y] == 0:\n            traverse(y, N, stack, F, X, R, FP)\n        N[x] = min(N[x], N[y])\n        for a in F.get(y, []):\n            if a not in F[x]:\n                F[x].append(a)\n    if N[x] == d:\n        N[stack[-1]] = MAXINT\n        F[stack[-1]] = F[x]\n        element = stack.pop()\n        while element != x:\n            N[stack[-1]] = MAXINT\n            F[stack[-1]] = F[x]\n            element = stack.pop()\n\nclass LALRError(YaccError):\n    pass\n\n# -----------------------------------------------------------------------------\n#                             == LRGeneratedTable ==\n#\n# This class implements the LR table generation algorithm.  There are no\n# public methods except for write()\n# -----------------------------------------------------------------------------\n\nclass LRGeneratedTable(LRTable):\n    def __init__(self, grammar, method='LALR', log=None):\n        if method not in ['SLR', 'LALR']:\n            raise LALRError('Unsupported method %s' % method)\n\n        self.grammar = grammar\n        self.lr_method = method\n\n        # Set up the logger\n        if not log:\n            log = NullLogger()\n        self.log = log\n\n        # Internal attributes\n        self.lr_action     = {}        # Action table\n        self.lr_goto       = {}        # Goto table\n        self.lr_productions  = grammar.Productions    # Copy of grammar Production array\n        self.lr_goto_cache = {}        # Cache of computed gotos\n        self.lr0_cidhash   = {}        # Cache of closures\n\n        self._add_count    = 0         # Internal counter used to detect cycles\n\n        # Diagonistic information filled in by the table generator\n        self.sr_conflict   = 0\n        self.rr_conflict   = 0\n        self.conflicts     = []        # List of conflicts\n\n        self.sr_conflicts  = []\n        self.rr_conflicts  = []\n\n        # Build the tables\n        self.grammar.build_lritems()\n        self.grammar.compute_first()\n        self.grammar.compute_follow()\n        self.lr_parse_table()\n\n    # Compute the LR(0) closure operation on I, where I is a set of LR(0) items.\n\n    def lr0_closure(self, I):\n        self._add_count += 1\n\n        # Add everything in I to J\n        J = I[:]\n        didadd = True\n        while didadd:\n            didadd = False\n            for j in J:\n                for x in j.lr_after:\n                    if getattr(x, 'lr0_added', 0) == self._add_count:\n                        continue\n                    # Add B --> .G to J\n                    J.append(x.lr_next)\n                    x.lr0_added = self._add_count\n                    didadd = True\n\n        return J\n\n    # Compute the LR(0) goto function goto(I,X) where I is a set\n    # of LR(0) items and X is a grammar symbol.   This function is written\n    # in a way that guarantees uniqueness of the generated goto sets\n    # (i.e. the same goto set will never be returned as two different Python\n    # objects).  With uniqueness, we can later do fast set comparisons using\n    # id(obj) instead of element-wise comparison.\n\n    def lr0_goto(self, I, x):\n        # First we look for a previously cached entry\n        g = self.lr_goto_cache.get((id(I), x))\n        if g:\n            return g\n\n        # Now we generate the goto set in a way that guarantees uniqueness\n        # of the result\n\n        s = self.lr_goto_cache.get(x)\n        if not s:\n            s = {}\n            self.lr_goto_cache[x] = s\n\n        gs = []\n        for p in I:\n            n = p.lr_next\n            if n and n.lr_before == x:\n                s1 = s.get(id(n))\n                if not s1:\n                    s1 = {}\n                    s[id(n)] = s1\n                gs.append(n)\n                s = s1\n        g = s.get('$end')\n        if not g:\n            if gs:\n                g = self.lr0_closure(gs)\n                s['$end'] = g\n            else:\n                s['$end'] = gs\n        self.lr_goto_cache[(id(I), x)] = g\n        return g\n\n    # Compute the LR(0) sets of item function\n    def lr0_items(self):\n        C = [self.lr0_closure([self.grammar.Productions[0].lr_next])]\n        i = 0\n        for I in C:\n            self.lr0_cidhash[id(I)] = i\n            i += 1\n\n        # Loop over the items in C and each grammar symbols\n        i = 0\n        while i < len(C):\n            I = C[i]\n            i += 1\n\n            # Collect all of the symbols that could possibly be in the goto(I,X) sets\n            asyms = {}\n            for ii in I:\n                for s in ii.usyms:\n                    asyms[s] = None\n\n            for x in asyms:\n                g = self.lr0_goto(I, x)\n                if not g or id(g) in self.lr0_cidhash:\n                    continue\n                self.lr0_cidhash[id(g)] = len(C)\n                C.append(g)\n\n        return C\n\n    # -----------------------------------------------------------------------------\n    #                       ==== LALR(1) Parsing ====\n    #\n    # LALR(1) parsing is almost exactly the same as SLR except that instead of\n    # relying upon Follow() sets when performing reductions, a more selective\n    # lookahead set that incorporates the state of the LR(0) machine is utilized.\n    # Thus, we mainly just have to focus on calculating the lookahead sets.\n    #\n    # The method used here is due to DeRemer and Pennelo (1982).\n    #\n    # DeRemer, F. L., and T. J. Pennelo: \"Efficient Computation of LALR(1)\n    #     Lookahead Sets\", ACM Transactions on Programming Languages and Systems,\n    #     Vol. 4, No. 4, Oct. 1982, pp. 615-649\n    #\n    # Further details can also be found in:\n    #\n    #  J. Tremblay and P. Sorenson, \"The Theory and Practice of Compiler Writing\",\n    #      McGraw-Hill Book Company, (1985).\n    #\n    # -----------------------------------------------------------------------------\n\n    # -----------------------------------------------------------------------------\n    # compute_nullable_nonterminals()\n    #\n    # Creates a dictionary containing all of the non-terminals that might produce\n    # an empty production.\n    # -----------------------------------------------------------------------------\n\n    def compute_nullable_nonterminals(self):\n        nullable = set()\n        num_nullable = 0\n        while True:\n            for p in self.grammar.Productions[1:]:\n                if p.len == 0:\n                    nullable.add(p.name)\n                    continue\n                for t in p.prod:\n                    if t not in nullable:\n                        break\n                else:\n                    nullable.add(p.name)\n            if len(nullable) == num_nullable:\n                break\n            num_nullable = len(nullable)\n        return nullable\n\n    # -----------------------------------------------------------------------------\n    # find_nonterminal_trans(C)\n    #\n    # Given a set of LR(0) items, this functions finds all of the non-terminal\n    # transitions.    These are transitions in which a dot appears immediately before\n    # a non-terminal.   Returns a list of tuples of the form (state,N) where state\n    # is the state number and N is the nonterminal symbol.\n    #\n    # The input C is the set of LR(0) items.\n    # -----------------------------------------------------------------------------\n\n    def find_nonterminal_transitions(self, C):\n        trans = []\n        for stateno, state in enumerate(C):\n            for p in state:\n                if p.lr_index < p.len - 1:\n                    t = (stateno, p.prod[p.lr_index+1])\n                    if t[1] in self.grammar.Nonterminals:\n                        if t not in trans:\n                            trans.append(t)\n        return trans\n\n    # -----------------------------------------------------------------------------\n    # dr_relation()\n    #\n    # Computes the DR(p,A) relationships for non-terminal transitions.  The input\n    # is a tuple (state,N) where state is a number and N is a nonterminal symbol.\n    #\n    # Returns a list of terminals.\n    # -----------------------------------------------------------------------------\n\n    def dr_relation(self, C, trans, nullable):\n        dr_set = {}\n        state, N = trans\n        terms = []\n\n        g = self.lr0_goto(C[state], N)\n        for p in g:\n            if p.lr_index < p.len - 1:\n                a = p.prod[p.lr_index+1]\n                if a in self.grammar.Terminals:\n                    if a not in terms:\n                        terms.append(a)\n\n        # This extra bit is to handle the start state\n        if state == 0 and N == self.grammar.Productions[0].prod[0]:\n            terms.append('$end')\n\n        return terms\n\n    # -----------------------------------------------------------------------------\n    # reads_relation()\n    #\n    # Computes the READS() relation (p,A) READS (t,C).\n    # -----------------------------------------------------------------------------\n\n    def reads_relation(self, C, trans, empty):\n        # Look for empty transitions\n        rel = []\n        state, N = trans\n\n        g = self.lr0_goto(C[state], N)\n        j = self.lr0_cidhash.get(id(g), -1)\n        for p in g:\n            if p.lr_index < p.len - 1:\n                a = p.prod[p.lr_index + 1]\n                if a in empty:\n                    rel.append((j, a))\n\n        return rel\n\n    # -----------------------------------------------------------------------------\n    # compute_lookback_includes()\n    #\n    # Determines the lookback and includes relations\n    #\n    # LOOKBACK:\n    #\n    # This relation is determined by running the LR(0) state machine forward.\n    # For example, starting with a production \"N : . A B C\", we run it forward\n    # to obtain \"N : A B C .\"   We then build a relationship between this final\n    # state and the starting state.   These relationships are stored in a dictionary\n    # lookdict.\n    #\n    # INCLUDES:\n    #\n    # Computes the INCLUDE() relation (p,A) INCLUDES (p',B).\n    #\n    # This relation is used to determine non-terminal transitions that occur\n    # inside of other non-terminal transition states.   (p,A) INCLUDES (p', B)\n    # if the following holds:\n    #\n    #       B -> LAT, where T -> epsilon and p' -L-> p\n    #\n    # L is essentially a prefix (which may be empty), T is a suffix that must be\n    # able to derive an empty string.  State p' must lead to state p with the string L.\n    #\n    # -----------------------------------------------------------------------------\n\n    def compute_lookback_includes(self, C, trans, nullable):\n        lookdict = {}          # Dictionary of lookback relations\n        includedict = {}       # Dictionary of include relations\n\n        # Make a dictionary of non-terminal transitions\n        dtrans = {}\n        for t in trans:\n            dtrans[t] = 1\n\n        # Loop over all transitions and compute lookbacks and includes\n        for state, N in trans:\n            lookb = []\n            includes = []\n            for p in C[state]:\n                if p.name != N:\n                    continue\n\n                # Okay, we have a name match.  We now follow the production all the way\n                # through the state machine until we get the . on the right hand side\n\n                lr_index = p.lr_index\n                j = state\n                while lr_index < p.len - 1:\n                    lr_index = lr_index + 1\n                    t = p.prod[lr_index]\n\n                    # Check to see if this symbol and state are a non-terminal transition\n                    if (j, t) in dtrans:\n                        # Yes.  Okay, there is some chance that this is an includes relation\n                        # the only way to know for certain is whether the rest of the\n                        # production derives empty\n\n                        li = lr_index + 1\n                        while li < p.len:\n                            if p.prod[li] in self.grammar.Terminals:\n                                break      # No forget it\n                            if p.prod[li] not in nullable:\n                                break\n                            li = li + 1\n                        else:\n                            # Appears to be a relation between (j,t) and (state,N)\n                            includes.append((j, t))\n\n                    g = self.lr0_goto(C[j], t)               # Go to next set\n                    j = self.lr0_cidhash.get(id(g), -1)      # Go to next state\n\n                # When we get here, j is the final state, now we have to locate the production\n                for r in C[j]:\n                    if r.name != p.name:\n                        continue\n                    if r.len != p.len:\n                        continue\n                    i = 0\n                    # This look is comparing a production \". A B C\" with \"A B C .\"\n                    while i < r.lr_index:\n                        if r.prod[i] != p.prod[i+1]:\n                            break\n                        i = i + 1\n                    else:\n                        lookb.append((j, r))\n            for i in includes:\n                if i not in includedict:\n                    includedict[i] = []\n                includedict[i].append((state, N))\n            lookdict[(state, N)] = lookb\n\n        return lookdict, includedict\n\n    # -----------------------------------------------------------------------------\n    # compute_read_sets()\n    #\n    # Given a set of LR(0) items, this function computes the read sets.\n    #\n    # Inputs:  C        =  Set of LR(0) items\n    #          ntrans   = Set of nonterminal transitions\n    #          nullable = Set of empty transitions\n    #\n    # Returns a set containing the read sets\n    # -----------------------------------------------------------------------------\n\n    def compute_read_sets(self, C, ntrans, nullable):\n        FP = lambda x: self.dr_relation(C, x, nullable)\n        R =  lambda x: self.reads_relation(C, x, nullable)\n        F = digraph(ntrans, R, FP)\n        return F\n\n    # -----------------------------------------------------------------------------\n    # compute_follow_sets()\n    #\n    # Given a set of LR(0) items, a set of non-terminal transitions, a readset,\n    # and an include set, this function computes the follow sets\n    #\n    # Follow(p,A) = Read(p,A) U U {Follow(p',B) | (p,A) INCLUDES (p',B)}\n    #\n    # Inputs:\n    #            ntrans     = Set of nonterminal transitions\n    #            readsets   = Readset (previously computed)\n    #            inclsets   = Include sets (previously computed)\n    #\n    # Returns a set containing the follow sets\n    # -----------------------------------------------------------------------------\n\n    def compute_follow_sets(self, ntrans, readsets, inclsets):\n        FP = lambda x: readsets[x]\n        R  = lambda x: inclsets.get(x, [])\n        F = digraph(ntrans, R, FP)\n        return F\n\n    # -----------------------------------------------------------------------------\n    # add_lookaheads()\n    #\n    # Attaches the lookahead symbols to grammar rules.\n    #\n    # Inputs:    lookbacks         -  Set of lookback relations\n    #            followset         -  Computed follow set\n    #\n    # This function directly attaches the lookaheads to productions contained\n    # in the lookbacks set\n    # -----------------------------------------------------------------------------\n\n    def add_lookaheads(self, lookbacks, followset):\n        for trans, lb in lookbacks.items():\n            # Loop over productions in lookback\n            for state, p in lb:\n                if state not in p.lookaheads:\n                    p.lookaheads[state] = []\n                f = followset.get(trans, [])\n                for a in f:\n                    if a not in p.lookaheads[state]:\n                        p.lookaheads[state].append(a)\n\n    # -----------------------------------------------------------------------------\n    # add_lalr_lookaheads()\n    #\n    # This function does all of the work of adding lookahead information for use\n    # with LALR parsing\n    # -----------------------------------------------------------------------------\n\n    def add_lalr_lookaheads(self, C):\n        # Determine all of the nullable nonterminals\n        nullable = self.compute_nullable_nonterminals()\n\n        # Find all non-terminal transitions\n        trans = self.find_nonterminal_transitions(C)\n\n        # Compute read sets\n        readsets = self.compute_read_sets(C, trans, nullable)\n\n        # Compute lookback/includes relations\n        lookd, included = self.compute_lookback_includes(C, trans, nullable)\n\n        # Compute LALR FOLLOW sets\n        followsets = self.compute_follow_sets(trans, readsets, included)\n\n        # Add all of the lookaheads\n        self.add_lookaheads(lookd, followsets)\n\n    # -----------------------------------------------------------------------------\n    # lr_parse_table()\n    #\n    # This function constructs the parse tables for SLR or LALR\n    # -----------------------------------------------------------------------------\n    def lr_parse_table(self):\n        Productions = self.grammar.Productions\n        Precedence  = self.grammar.Precedence\n        goto   = self.lr_goto         # Goto array\n        action = self.lr_action       # Action array\n        log    = self.log             # Logger for output\n\n        actionp = {}                  # Action production array (temporary)\n\n        log.info('Parsing method: %s', self.lr_method)\n\n        # Step 1: Construct C = { I0, I1, ... IN}, collection of LR(0) items\n        # This determines the number of states\n\n        C = self.lr0_items()\n\n        if self.lr_method == 'LALR':\n            self.add_lalr_lookaheads(C)\n\n        # Build the parser table, state by state\n        st = 0\n        for I in C:\n            # Loop over each production in I\n            actlist = []              # List of actions\n            st_action  = {}\n            st_actionp = {}\n            st_goto    = {}\n            log.info('')\n            log.info('state %d', st)\n            log.info('')\n            for p in I:\n                log.info('    (%d) %s', p.number, p)\n            log.info('')\n\n            for p in I:\n                    if p.len == p.lr_index + 1:\n                        if p.name == \"S'\":\n                            # Start symbol. Accept!\n                            st_action['$end'] = 0\n                            st_actionp['$end'] = p\n                        else:\n                            # We are at the end of a production.  Reduce!\n                            if self.lr_method == 'LALR':\n                                laheads = p.lookaheads[st]\n                            else:\n                                laheads = self.grammar.Follow[p.name]\n                            for a in laheads:\n                                actlist.append((a, p, 'reduce using rule %d (%s)' % (p.number, p)))\n                                r = st_action.get(a)\n                                if r is not None:\n                                    # Whoa. Have a shift/reduce or reduce/reduce conflict\n                                    if r > 0:\n                                        # Need to decide on shift or reduce here\n                                        # By default we favor shifting. Need to add\n                                        # some precedence rules here.\n\n                                        # Shift precedence comes from the token\n                                        sprec, slevel = Precedence.get(a, ('right', 0))\n\n                                        # Reduce precedence comes from rule being reduced (p)\n                                        rprec, rlevel = Productions[p.number].prec\n\n                                        if (slevel < rlevel) or ((slevel == rlevel) and (rprec == 'left')):\n                                            # We really need to reduce here.\n                                            st_action[a] = -p.number\n                                            st_actionp[a] = p\n                                            if not slevel and not rlevel:\n                                                log.info('  ! shift/reduce conflict for %s resolved as reduce', a)\n                                                self.sr_conflicts.append((st, a, 'reduce'))\n                                            Productions[p.number].reduced += 1\n                                        elif (slevel == rlevel) and (rprec == 'nonassoc'):\n                                            st_action[a] = None\n                                        else:\n                                            # Hmmm. Guess we'll keep the shift\n                                            if not rlevel:\n                                                log.info('  ! shift/reduce conflict for %s resolved as shift', a)\n                                                self.sr_conflicts.append((st, a, 'shift'))\n                                    elif r < 0:\n                                        # Reduce/reduce conflict.   In this case, we favor the rule\n                                        # that was defined first in the grammar file\n                                        oldp = Productions[-r]\n                                        pp = Productions[p.number]\n                                        if oldp.line > pp.line:\n                                            st_action[a] = -p.number\n                                            st_actionp[a] = p\n                                            chosenp, rejectp = pp, oldp\n                                            Productions[p.number].reduced += 1\n                                            Productions[oldp.number].reduced -= 1\n                                        else:\n                                            chosenp, rejectp = oldp, pp\n                                        self.rr_conflicts.append((st, chosenp, rejectp))\n                                        log.info('  ! reduce/reduce conflict for %s resolved using rule %d (%s)',\n                                                 a, st_actionp[a].number, st_actionp[a])\n                                    else:\n                                        raise LALRError('Unknown conflict in state %d' % st)\n                                else:\n                                    st_action[a] = -p.number\n                                    st_actionp[a] = p\n                                    Productions[p.number].reduced += 1\n                    else:\n                        i = p.lr_index\n                        a = p.prod[i+1]       # Get symbol right after the \".\"\n                        if a in self.grammar.Terminals:\n                            g = self.lr0_goto(I, a)\n                            j = self.lr0_cidhash.get(id(g), -1)\n                            if j >= 0:\n                                # We are in a shift state\n                                actlist.append((a, p, 'shift and go to state %d' % j))\n                                r = st_action.get(a)\n                                if r is not None:\n                                    # Whoa have a shift/reduce or shift/shift conflict\n                                    if r > 0:\n                                        if r != j:\n                                            raise LALRError('Shift/shift conflict in state %d' % st)\n                                    elif r < 0:\n                                        # Do a precedence check.\n                                        #   -  if precedence of reduce rule is higher, we reduce.\n                                        #   -  if precedence of reduce is same and left assoc, we reduce.\n                                        #   -  otherwise we shift\n\n                                        # Shift precedence comes from the token\n                                        sprec, slevel = Precedence.get(a, ('right', 0))\n\n                                        # Reduce precedence comes from the rule that could have been reduced\n                                        rprec, rlevel = Productions[st_actionp[a].number].prec\n\n                                        if (slevel > rlevel) or ((slevel == rlevel) and (rprec == 'right')):\n                                            # We decide to shift here... highest precedence to shift\n                                            Productions[st_actionp[a].number].reduced -= 1\n                                            st_action[a] = j\n                                            st_actionp[a] = p\n                                            if not rlevel:\n                                                log.info('  ! shift/reduce conflict for %s resolved as shift', a)\n                                                self.sr_conflicts.append((st, a, 'shift'))\n                                        elif (slevel == rlevel) and (rprec == 'nonassoc'):\n                                            st_action[a] = None\n                                        else:\n                                            # Hmmm. Guess we'll keep the reduce\n                                            if not slevel and not rlevel:\n                                                log.info('  ! shift/reduce conflict for %s resolved as reduce', a)\n                                                self.sr_conflicts.append((st, a, 'reduce'))\n\n                                    else:\n                                        raise LALRError('Unknown conflict in state %d' % st)\n                                else:\n                                    st_action[a] = j\n                                    st_actionp[a] = p\n\n            # Print the actions associated with each terminal\n            _actprint = {}\n            for a, p, m in actlist:\n                if a in st_action:\n                    if p is st_actionp[a]:\n                        log.info('    %-15s %s', a, m)\n                        _actprint[(a, m)] = 1\n            log.info('')\n            # Print the actions that were not used. (debugging)\n            not_used = 0\n            for a, p, m in actlist:\n                if a in st_action:\n                    if p is not st_actionp[a]:\n                        if not (a, m) in _actprint:\n                            log.debug('  ! %-15s [ %s ]', a, m)\n                            not_used = 1\n                            _actprint[(a, m)] = 1\n            if not_used:\n                log.debug('')\n\n            # Construct the goto table for this state\n\n            nkeys = {}\n            for ii in I:\n                for s in ii.usyms:\n                    if s in self.grammar.Nonterminals:\n                        nkeys[s] = None\n            for n in nkeys:\n                g = self.lr0_goto(I, n)\n                j = self.lr0_cidhash.get(id(g), -1)\n                if j >= 0:\n                    st_goto[n] = j\n                    log.info('    %-30s shift and go to state %d', n, j)\n\n            action[st] = st_action\n            actionp[st] = st_actionp\n            goto[st] = st_goto\n            st += 1\n\n    # -----------------------------------------------------------------------------\n    # write()\n    #\n    # This function writes the LR parsing tables to a file\n    # -----------------------------------------------------------------------------\n\n    def write_table(self, tabmodule, outputdir='', signature=''):\n        if isinstance(tabmodule, types.ModuleType):\n            raise IOError(\"Won't overwrite existing tabmodule\")\n\n        basemodulename = tabmodule.split('.')[-1]\n        filename = os.path.join(outputdir, basemodulename) + '.py'\n        try:\n            f = open(filename, 'w')\n\n            f.write('''\n# %s\n# This file is automatically generated. Do not edit.\n_tabversion = %r\n\n_lr_method = %r\n\n_lr_signature = %r\n    ''' % (os.path.basename(filename), __tabversion__, self.lr_method, signature))\n\n            # Change smaller to 0 to go back to original tables\n            smaller = 1\n\n            # Factor out names to try and make smaller\n            if smaller:\n                items = {}\n\n                for s, nd in self.lr_action.items():\n                    for name, v in nd.items():\n                        i = items.get(name)\n                        if not i:\n                            i = ([], [])\n                            items[name] = i\n                        i[0].append(s)\n                        i[1].append(v)\n\n                f.write('\\n_lr_action_items = {')\n                for k, v in items.items():\n                    f.write('%r:([' % k)\n                    for i in v[0]:\n                        f.write('%r,' % i)\n                    f.write('],[')\n                    for i in v[1]:\n                        f.write('%r,' % i)\n\n                    f.write(']),')\n                f.write('}\\n')\n\n                f.write('''\n_lr_action = {}\nfor _k, _v in _lr_action_items.items():\n   for _x,_y in zip(_v[0],_v[1]):\n      if not _x in _lr_action:  _lr_action[_x] = {}\n      _lr_action[_x][_k] = _y\ndel _lr_action_items\n''')\n\n            else:\n                f.write('\\n_lr_action = { ')\n                for k, v in self.lr_action.items():\n                    f.write('(%r,%r):%r,' % (k[0], k[1], v))\n                f.write('}\\n')\n\n            if smaller:\n                # Factor out names to try and make smaller\n                items = {}\n\n                for s, nd in self.lr_goto.items():\n                    for name, v in nd.items():\n                        i = items.get(name)\n                        if not i:\n                            i = ([], [])\n                            items[name] = i\n                        i[0].append(s)\n                        i[1].append(v)\n\n                f.write('\\n_lr_goto_items = {')\n                for k, v in items.items():\n                    f.write('%r:([' % k)\n                    for i in v[0]:\n                        f.write('%r,' % i)\n                    f.write('],[')\n                    for i in v[1]:\n                        f.write('%r,' % i)\n\n                    f.write(']),')\n                f.write('}\\n')\n\n                f.write('''\n_lr_goto = {}\nfor _k, _v in _lr_goto_items.items():\n   for _x, _y in zip(_v[0], _v[1]):\n       if not _x in _lr_goto: _lr_goto[_x] = {}\n       _lr_goto[_x][_k] = _y\ndel _lr_goto_items\n''')\n            else:\n                f.write('\\n_lr_goto = { ')\n                for k, v in self.lr_goto.items():\n                    f.write('(%r,%r):%r,' % (k[0], k[1], v))\n                f.write('}\\n')\n\n            # Write production table\n            f.write('_lr_productions = [\\n')\n            for p in self.lr_productions:\n                if p.func:\n                    f.write('  (%r,%r,%d,%r,%r,%d),\\n' % (p.str, p.name, p.len,\n                                                          p.func, os.path.basename(p.file), p.line))\n                else:\n                    f.write('  (%r,%r,%d,None,None,None),\\n' % (str(p), p.name, p.len))\n            f.write(']\\n')\n            f.close()\n\n        except IOError as e:\n            raise\n\n\n    # -----------------------------------------------------------------------------\n    # pickle_table()\n    #\n    # This function pickles the LR parsing tables to a supplied file object\n    # -----------------------------------------------------------------------------\n\n    def pickle_table(self, filename, signature=''):\n        try:\n            import cPickle as pickle\n        except ImportError:\n            import pickle\n        with open(filename, 'wb') as outf:\n            pickle.dump(__tabversion__, outf, pickle_protocol)\n            pickle.dump(self.lr_method, outf, pickle_protocol)\n            pickle.dump(signature, outf, pickle_protocol)\n            pickle.dump(self.lr_action, outf, pickle_protocol)\n            pickle.dump(self.lr_goto, outf, pickle_protocol)\n\n            outp = []\n            for p in self.lr_productions:\n                if p.func:\n                    outp.append((p.str, p.name, p.len, p.func, os.path.basename(p.file), p.line))\n                else:\n                    outp.append((str(p), p.name, p.len, None, None, None))\n            pickle.dump(outp, outf, pickle_protocol)\n\n# -----------------------------------------------------------------------------\n#                            === INTROSPECTION ===\n#\n# The following functions and classes are used to implement the PLY\n# introspection features followed by the yacc() function itself.\n# -----------------------------------------------------------------------------\n\n# -----------------------------------------------------------------------------\n# get_caller_module_dict()\n#\n# This function returns a dictionary containing all of the symbols defined within\n# a caller further down the call stack.  This is used to get the environment\n# associated with the yacc() call if none was provided.\n# -----------------------------------------------------------------------------\n\ndef get_caller_module_dict(levels):\n    f = sys._getframe(levels)\n    ldict = f.f_globals.copy()\n    if f.f_globals != f.f_locals:\n        ldict.update(f.f_locals)\n    return ldict\n\n# -----------------------------------------------------------------------------\n# parse_grammar()\n#\n# This takes a raw grammar rule string and parses it into production data\n# -----------------------------------------------------------------------------\ndef parse_grammar(doc, file, line):\n    grammar = []\n    # Split the doc string into lines\n    pstrings = doc.splitlines()\n    lastp = None\n    dline = line\n    for ps in pstrings:\n        dline += 1\n        p = ps.split()\n        if not p:\n            continue\n        try:\n            if p[0] == '|':\n                # This is a continuation of a previous rule\n                if not lastp:\n                    raise SyntaxError(\"%s:%d: Misplaced '|'\" % (file, dline))\n                prodname = lastp\n                syms = p[1:]\n            else:\n                prodname = p[0]\n                lastp = prodname\n                syms   = p[2:]\n                assign = p[1]\n                if assign != ':' and assign != '::=':\n                    raise SyntaxError(\"%s:%d: Syntax error. Expected ':'\" % (file, dline))\n\n            grammar.append((file, dline, prodname, syms))\n        except SyntaxError:\n            raise\n        except Exception:\n            raise SyntaxError('%s:%d: Syntax error in rule %r' % (file, dline, ps.strip()))\n\n    return grammar\n\n# -----------------------------------------------------------------------------\n# ParserReflect()\n#\n# This class represents information extracted for building a parser including\n# start symbol, error function, tokens, precedence list, action functions,\n# etc.\n# -----------------------------------------------------------------------------\nclass ParserReflect(object):\n    def __init__(self, pdict, log=None):\n        self.pdict      = pdict\n        self.start      = None\n        self.error_func = None\n        self.tokens     = None\n        self.modules    = set()\n        self.grammar    = []\n        self.error      = False\n\n        if log is None:\n            self.log = PlyLogger(sys.stderr)\n        else:\n            self.log = log\n\n    # Get all of the basic information\n    def get_all(self):\n        self.get_start()\n        self.get_error_func()\n        self.get_tokens()\n        self.get_precedence()\n        self.get_pfunctions()\n\n    # Validate all of the information\n    def validate_all(self):\n        self.validate_start()\n        self.validate_error_func()\n        self.validate_tokens()\n        self.validate_precedence()\n        self.validate_pfunctions()\n        self.validate_modules()\n        return self.error\n\n    # Compute a signature over the grammar\n    def signature(self):\n        parts = []\n        try:\n            if self.start:\n                parts.append(self.start)\n            if self.prec:\n                parts.append(''.join([''.join(p) for p in self.prec]))\n            if self.tokens:\n                parts.append(' '.join(self.tokens))\n            for f in self.pfuncs:\n                if f[3]:\n                    parts.append(f[3])\n        except (TypeError, ValueError):\n            pass\n        return ''.join(parts)\n\n    # -----------------------------------------------------------------------------\n    # validate_modules()\n    #\n    # This method checks to see if there are duplicated p_rulename() functions\n    # in the parser module file.  Without this function, it is really easy for\n    # users to make mistakes by cutting and pasting code fragments (and it's a real\n    # bugger to try and figure out why the resulting parser doesn't work).  Therefore,\n    # we just do a little regular expression pattern matching of def statements\n    # to try and detect duplicates.\n    # -----------------------------------------------------------------------------\n\n    def validate_modules(self):\n        # Match def p_funcname(\n        fre = re.compile(r'\\s*def\\s+(p_[a-zA-Z_0-9]*)\\(')\n\n        for module in self.modules:\n            try:\n                lines, linen = inspect.getsourcelines(module)\n            except IOError:\n                continue\n\n            counthash = {}\n            for linen, line in enumerate(lines):\n                linen += 1\n                m = fre.match(line)\n                if m:\n                    name = m.group(1)\n                    prev = counthash.get(name)\n                    if not prev:\n                        counthash[name] = linen\n                    else:\n                        filename = inspect.getsourcefile(module)\n                        self.log.warning('%s:%d: Function %s redefined. Previously defined on line %d',\n                                         filename, linen, name, prev)\n\n    # Get the start symbol\n    def get_start(self):\n        self.start = self.pdict.get('start')\n\n    # Validate the start symbol\n    def validate_start(self):\n        if self.start is not None:\n            if not isinstance(self.start, string_types):\n                self.log.error(\"'start' must be a string\")\n\n    # Look for error handler\n    def get_error_func(self):\n        self.error_func = self.pdict.get('p_error')\n\n    # Validate the error function\n    def validate_error_func(self):\n        if self.error_func:\n            if isinstance(self.error_func, types.FunctionType):\n                ismethod = 0\n            elif isinstance(self.error_func, types.MethodType):\n                ismethod = 1\n            else:\n                self.log.error(\"'p_error' defined, but is not a function or method\")\n                self.error = True\n                return\n\n            eline = self.error_func.__code__.co_firstlineno\n            efile = self.error_func.__code__.co_filename\n            module = inspect.getmodule(self.error_func)\n            self.modules.add(module)\n\n            argcount = self.error_func.__code__.co_argcount - ismethod\n            if argcount != 1:\n                self.log.error('%s:%d: p_error() requires 1 argument', efile, eline)\n                self.error = True\n\n    # Get the tokens map\n    def get_tokens(self):\n        tokens = self.pdict.get('tokens')\n        if not tokens:\n            self.log.error('No token list is defined')\n            self.error = True\n            return\n\n        if not isinstance(tokens, (list, tuple)):\n            self.log.error('tokens must be a list or tuple')\n            self.error = True\n            return\n\n        if not tokens:\n            self.log.error('tokens is empty')\n            self.error = True\n            return\n\n        self.tokens = tokens\n\n    # Validate the tokens\n    def validate_tokens(self):\n        # Validate the tokens.\n        if 'error' in self.tokens:\n            self.log.error(\"Illegal token name 'error'. Is a reserved word\")\n            self.error = True\n            return\n\n        terminals = set()\n        for n in self.tokens:\n            if n in terminals:\n                self.log.warning('Token %r multiply defined', n)\n            terminals.add(n)\n\n    # Get the precedence map (if any)\n    def get_precedence(self):\n        self.prec = self.pdict.get('precedence')\n\n    # Validate and parse the precedence map\n    def validate_precedence(self):\n        preclist = []\n        if self.prec:\n            if not isinstance(self.prec, (list, tuple)):\n                self.log.error('precedence must be a list or tuple')\n                self.error = True\n                return\n            for level, p in enumerate(self.prec):\n                if not isinstance(p, (list, tuple)):\n                    self.log.error('Bad precedence table')\n                    self.error = True\n                    return\n\n                if len(p) < 2:\n                    self.log.error('Malformed precedence entry %s. Must be (assoc, term, ..., term)', p)\n                    self.error = True\n                    return\n                assoc = p[0]\n                if not isinstance(assoc, string_types):\n                    self.log.error('precedence associativity must be a string')\n                    self.error = True\n                    return\n                for term in p[1:]:\n                    if not isinstance(term, string_types):\n                        self.log.error('precedence items must be strings')\n                        self.error = True\n                        return\n                    preclist.append((term, assoc, level+1))\n        self.preclist = preclist\n\n    # Get all p_functions from the grammar\n    def get_pfunctions(self):\n        p_functions = []\n        for name, item in self.pdict.items():\n            if not name.startswith('p_') or name == 'p_error':\n                continue\n            if isinstance(item, (types.FunctionType, types.MethodType)):\n                line = getattr(item, 'co_firstlineno', item.__code__.co_firstlineno)\n                module = inspect.getmodule(item)\n                p_functions.append((line, module, name, item.__doc__))\n\n        # Sort all of the actions by line number; make sure to stringify\n        # modules to make them sortable, since `line` may not uniquely sort all\n        # p functions\n        p_functions.sort(key=lambda p_function: (\n            p_function[0],\n            str(p_function[1]),\n            p_function[2],\n            p_function[3]))\n        self.pfuncs = p_functions\n\n    # Validate all of the p_functions\n    def validate_pfunctions(self):\n        grammar = []\n        # Check for non-empty symbols\n        if len(self.pfuncs) == 0:\n            self.log.error('no rules of the form p_rulename are defined')\n            self.error = True\n            return\n\n        for line, module, name, doc in self.pfuncs:\n            file = inspect.getsourcefile(module)\n            func = self.pdict[name]\n            if isinstance(func, types.MethodType):\n                reqargs = 2\n            else:\n                reqargs = 1\n            if func.__code__.co_argcount > reqargs:\n                self.log.error('%s:%d: Rule %r has too many arguments', file, line, func.__name__)\n                self.error = True\n            elif func.__code__.co_argcount < reqargs:\n                self.log.error('%s:%d: Rule %r requires an argument', file, line, func.__name__)\n                self.error = True\n            elif not func.__doc__:\n                self.log.warning('%s:%d: No documentation string specified in function %r (ignored)',\n                                 file, line, func.__name__)\n            else:\n                try:\n                    parsed_g = parse_grammar(doc, file, line)\n                    for g in parsed_g:\n                        grammar.append((name, g))\n                except SyntaxError as e:\n                    self.log.error(str(e))\n                    self.error = True\n\n                # Looks like a valid grammar rule\n                # Mark the file in which defined.\n                self.modules.add(module)\n\n        # Secondary validation step that looks for p_ definitions that are not functions\n        # or functions that look like they might be grammar rules.\n\n        for n, v in self.pdict.items():\n            if n.startswith('p_') and isinstance(v, (types.FunctionType, types.MethodType)):\n                continue\n            if n.startswith('t_'):\n                continue\n            if n.startswith('p_') and n != 'p_error':\n                self.log.warning('%r not defined as a function', n)\n            if ((isinstance(v, types.FunctionType) and v.__code__.co_argcount == 1) or\n                   (isinstance(v, types.MethodType) and v.__func__.__code__.co_argcount == 2)):\n                if v.__doc__:\n                    try:\n                        doc = v.__doc__.split(' ')\n                        if doc[1] == ':':\n                            self.log.warning('%s:%d: Possible grammar rule %r defined without p_ prefix',\n                                             v.__code__.co_filename, v.__code__.co_firstlineno, n)\n                    except IndexError:\n                        pass\n\n        self.grammar = grammar\n\n# -----------------------------------------------------------------------------\n# yacc(module)\n#\n# Build a parser\n# -----------------------------------------------------------------------------\n\ndef yacc(method='LALR', debug=yaccdebug, module=None, tabmodule=tab_module, start=None,\n         check_recursion=True, optimize=False, write_tables=True, debugfile=debug_file,\n         outputdir=None, debuglog=None, errorlog=None, picklefile=None):\n\n    if tabmodule is None:\n        tabmodule = tab_module\n\n    # Reference to the parsing method of the last built parser\n    global parse\n\n    # If pickling is enabled, table files are not created\n    if picklefile:\n        write_tables = 0\n\n    if errorlog is None:\n        errorlog = PlyLogger(sys.stderr)\n\n    # Get the module dictionary used for the parser\n    if module:\n        _items = [(k, getattr(module, k)) for k in dir(module)]\n        pdict = dict(_items)\n        # If no __file__ attribute is available, try to obtain it from the __module__ instead\n        if '__file__' not in pdict:\n            pdict['__file__'] = sys.modules[pdict['__module__']].__file__\n    else:\n        pdict = get_caller_module_dict(2)\n\n    if outputdir is None:\n        # If no output directory is set, the location of the output files\n        # is determined according to the following rules:\n        #     - If tabmodule specifies a package, files go into that package directory\n        #     - Otherwise, files go in the same directory as the specifying module\n        if isinstance(tabmodule, types.ModuleType):\n            srcfile = tabmodule.__file__\n        else:\n            if '.' not in tabmodule:\n                srcfile = pdict['__file__']\n            else:\n                parts = tabmodule.split('.')\n                pkgname = '.'.join(parts[:-1])\n                exec('import %s' % pkgname)\n                srcfile = getattr(sys.modules[pkgname], '__file__', '')\n        outputdir = os.path.dirname(srcfile)\n\n    # Determine if the module is package of a package or not.\n    # If so, fix the tabmodule setting so that tables load correctly\n    pkg = pdict.get('__package__')\n    if pkg and isinstance(tabmodule, str):\n        if '.' not in tabmodule:\n            tabmodule = pkg + '.' + tabmodule\n\n\n\n    # Set start symbol if it's specified directly using an argument\n    if start is not None:\n        pdict['start'] = start\n\n    # Collect parser information from the dictionary\n    pinfo = ParserReflect(pdict, log=errorlog)\n    pinfo.get_all()\n\n    if pinfo.error:\n        raise YaccError('Unable to build parser')\n\n    # Check signature against table files (if any)\n    signature = pinfo.signature()\n\n    # Read the tables\n    try:\n        lr = LRTable()\n        if picklefile:\n            read_signature = lr.read_pickle(picklefile)\n        else:\n            read_signature = lr.read_table(tabmodule)\n        if optimize or (read_signature == signature):\n            try:\n                lr.bind_callables(pinfo.pdict)\n                parser = LRParser(lr, pinfo.error_func)\n                parse = parser.parse\n                return parser\n            except Exception as e:\n                errorlog.warning('There was a problem loading the table file: %r', e)\n    except VersionError as e:\n        errorlog.warning(str(e))\n    except ImportError:\n        pass\n\n    if debuglog is None:\n        if debug:\n            try:\n                debuglog = PlyLogger(open(os.path.join(outputdir, debugfile), 'w'))\n            except IOError as e:\n                errorlog.warning(\"Couldn't open %r. %s\" % (debugfile, e))\n                debuglog = NullLogger()\n        else:\n            debuglog = NullLogger()\n\n    debuglog.info('Created by PLY version %s (http://www.dabeaz.com/ply)', __version__)\n\n    errors = False\n\n    # Validate the parser information\n    if pinfo.validate_all():\n        raise YaccError('Unable to build parser')\n\n    if not pinfo.error_func:\n        errorlog.warning('no p_error() function is defined')\n\n    # Create a grammar object\n    grammar = Grammar(pinfo.tokens)\n\n    # Set precedence level for terminals\n    for term, assoc, level in pinfo.preclist:\n        try:\n            grammar.set_precedence(term, assoc, level)\n        except GrammarError as e:\n            errorlog.warning('%s', e)\n\n    # Add productions to the grammar\n    for funcname, gram in pinfo.grammar:\n        file, line, prodname, syms = gram\n        try:\n            grammar.add_production(prodname, syms, funcname, file, line)\n        except GrammarError as e:\n            errorlog.error('%s', e)\n            errors = True\n\n    # Set the grammar start symbols\n    try:\n        if start is None:\n            grammar.set_start(pinfo.start)\n        else:\n            grammar.set_start(start)\n    except GrammarError as e:\n        errorlog.error(str(e))\n        errors = True\n\n    if errors:\n        raise YaccError('Unable to build parser')\n\n    # Verify the grammar structure\n    undefined_symbols = grammar.undefined_symbols()\n    for sym, prod in undefined_symbols:\n        errorlog.error('%s:%d: Symbol %r used, but not defined as a token or a rule', prod.file, prod.line, sym)\n        errors = True\n\n    unused_terminals = grammar.unused_terminals()\n    if unused_terminals:\n        debuglog.info('')\n        debuglog.info('Unused terminals:')\n        debuglog.info('')\n        for term in unused_terminals:\n            errorlog.warning('Token %r defined, but not used', term)\n            debuglog.info('    %s', term)\n\n    # Print out all productions to the debug log\n    if debug:\n        debuglog.info('')\n        debuglog.info('Grammar')\n        debuglog.info('')\n        for n, p in enumerate(grammar.Productions):\n            debuglog.info('Rule %-5d %s', n, p)\n\n    # Find unused non-terminals\n    unused_rules = grammar.unused_rules()\n    for prod in unused_rules:\n        errorlog.warning('%s:%d: Rule %r defined, but not used', prod.file, prod.line, prod.name)\n\n    if len(unused_terminals) == 1:\n        errorlog.warning('There is 1 unused token')\n    if len(unused_terminals) > 1:\n        errorlog.warning('There are %d unused tokens', len(unused_terminals))\n\n    if len(unused_rules) == 1:\n        errorlog.warning('There is 1 unused rule')\n    if len(unused_rules) > 1:\n        errorlog.warning('There are %d unused rules', len(unused_rules))\n\n    if debug:\n        debuglog.info('')\n        debuglog.info('Terminals, with rules where they appear')\n        debuglog.info('')\n        terms = list(grammar.Terminals)\n        terms.sort()\n        for term in terms:\n            debuglog.info('%-20s : %s', term, ' '.join([str(s) for s in grammar.Terminals[term]]))\n\n        debuglog.info('')\n        debuglog.info('Nonterminals, with rules where they appear')\n        debuglog.info('')\n        nonterms = list(grammar.Nonterminals)\n        nonterms.sort()\n        for nonterm in nonterms:\n            debuglog.info('%-20s : %s', nonterm, ' '.join([str(s) for s in grammar.Nonterminals[nonterm]]))\n        debuglog.info('')\n\n    if check_recursion:\n        unreachable = grammar.find_unreachable()\n        for u in unreachable:\n            errorlog.warning('Symbol %r is unreachable', u)\n\n        infinite = grammar.infinite_cycles()\n        for inf in infinite:\n            errorlog.error('Infinite recursion detected for symbol %r', inf)\n            errors = True\n\n    unused_prec = grammar.unused_precedence()\n    for term, assoc in unused_prec:\n        errorlog.error('Precedence rule %r defined for unknown symbol %r', assoc, term)\n        errors = True\n\n    if errors:\n        raise YaccError('Unable to build parser')\n\n    # Run the LRGeneratedTable on the grammar\n    if debug:\n        errorlog.debug('Generating %s tables', method)\n\n    lr = LRGeneratedTable(grammar, method, debuglog)\n\n    if debug:\n        num_sr = len(lr.sr_conflicts)\n\n        # Report shift/reduce and reduce/reduce conflicts\n        if num_sr == 1:\n            errorlog.warning('1 shift/reduce conflict')\n        elif num_sr > 1:\n            errorlog.warning('%d shift/reduce conflicts', num_sr)\n\n        num_rr = len(lr.rr_conflicts)\n        if num_rr == 1:\n            errorlog.warning('1 reduce/reduce conflict')\n        elif num_rr > 1:\n            errorlog.warning('%d reduce/reduce conflicts', num_rr)\n\n    # Write out conflicts to the output file\n    if debug and (lr.sr_conflicts or lr.rr_conflicts):\n        debuglog.warning('')\n        debuglog.warning('Conflicts:')\n        debuglog.warning('')\n\n        for state, tok, resolution in lr.sr_conflicts:\n            debuglog.warning('shift/reduce conflict for %s in state %d resolved as %s',  tok, state, resolution)\n\n        already_reported = set()\n        for state, rule, rejected in lr.rr_conflicts:\n            if (state, id(rule), id(rejected)) in already_reported:\n                continue\n            debuglog.warning('reduce/reduce conflict in state %d resolved using rule (%s)', state, rule)\n            debuglog.warning('rejected rule (%s) in state %d', rejected, state)\n            errorlog.warning('reduce/reduce conflict in state %d resolved using rule (%s)', state, rule)\n            errorlog.warning('rejected rule (%s) in state %d', rejected, state)\n            already_reported.add((state, id(rule), id(rejected)))\n\n        warned_never = []\n        for state, rule, rejected in lr.rr_conflicts:\n            if not rejected.reduced and (rejected not in warned_never):\n                debuglog.warning('Rule (%s) is never reduced', rejected)\n                errorlog.warning('Rule (%s) is never reduced', rejected)\n                warned_never.append(rejected)\n\n    # Write the table file if requested\n    if write_tables:\n        try:\n            lr.write_table(tabmodule, outputdir, signature)\n        except IOError as e:\n            errorlog.warning(\"Couldn't create %r. %s\" % (tabmodule, e))\n\n    # Write a pickled version of the tables\n    if picklefile:\n        try:\n            lr.pickle_table(picklefile, signature)\n        except IOError as e:\n            errorlog.warning(\"Couldn't create %r. %s\" % (picklefile, e))\n\n    # Build the parser\n    lr.bind_callables(pinfo.pdict)\n    parser = LRParser(lr, pinfo.error_func)\n\n    parse = parser.parse\n    return parser\n", "examples/explore_ast.py": "#-----------------------------------------------------------------\n# pycparser: explore_ast.py\n#\n# This example demonstrates how to \"explore\" the AST created by\n# pycparser to understand its structure. The AST is a n-nary tree\n# of nodes, each node having several children, each with a name.\n# Just read the code, and let the comments guide you. The lines\n# beginning with #~ can be uncommented to print out useful\n# information from the AST.\n# It helps to have the pycparser/_c_ast.cfg file in front of you.\n#\n# Eli Bendersky [https://eli.thegreenplace.net/]\n# License: BSD\n#-----------------------------------------------------------------\nimport sys\n\n# This is not required if you've installed pycparser into\n# your site-packages/ with setup.py\n#\nsys.path.extend(['.', '..'])\n\nfrom pycparser import c_parser\n\n# This is some C source to parse. Note that pycparser must begin\n# at the top level of the C file, i.e. with either declarations\n# or function definitions (this is called \"external declarations\"\n# in C grammar lingo)\n#\n# Also, a C parser must have all the types declared in order to\n# build the correct AST. It doesn't matter what they're declared\n# to, so I've inserted the dummy typedef in the code to let the\n# parser know Hash and Node are types. You don't need to do it\n# when parsing real, correct C code.\n\ntext = r\"\"\"\n    typedef int Node, Hash;\n\n    void HashPrint(Hash* hash, void (*PrintFunc)(char*, char*))\n    {\n        unsigned int i;\n\n        if (hash == NULL || hash->heads == NULL)\n            return;\n\n        for (i = 0; i < hash->table_size; ++i)\n        {\n            Node* temp = hash->heads[i];\n\n            while (temp != NULL)\n            {\n                PrintFunc(temp->entry->key, temp->entry->value);\n                temp = temp->next;\n            }\n        }\n    }\n\"\"\"\n\n# Create the parser and ask to parse the text. parse() will throw\n# a ParseError if there's an error in the code\n#\nparser = c_parser.CParser()\nast = parser.parse(text, filename='<none>')\n\n# Uncomment the following line to see the AST in a nice, human\n# readable way. show() is the most useful tool in exploring ASTs\n# created by pycparser. See the c_ast.py file for the options you\n# can pass it.\n\n#ast.show(showcoord=True)\n\n# OK, we've seen that the top node is FileAST. This is always the\n# top node of the AST. Its children are \"external declarations\",\n# and are stored in a list called ext[] (see _c_ast.cfg for the\n# names and types of Nodes and their children).\n# As you see from the printout, our AST has two Typedef children\n# and one FuncDef child.\n# Let's explore FuncDef more closely. As I've mentioned, the list\n# ext[] holds the children of FileAST. Since the function\n# definition is the third child, it's ext[2]. Uncomment the\n# following line to show it:\n\n#ast.ext[2].show()\n\n# A FuncDef consists of a declaration, a list of parameter\n# declarations (for K&R style function definitions), and a body.\n# First, let's examine the declaration.\n\nfunction_decl = ast.ext[2].decl\n\n# function_decl, like any other declaration, is a Decl. Its type child\n# is a FuncDecl, which has a return type and arguments stored in a\n# ParamList node\n\n#function_decl.type.show()\n#function_decl.type.args.show()\n\n# The following displays the name and type of each argument:\n\n#for param_decl in function_decl.type.args.params:\n    #print('Arg name: %s' % param_decl.name)\n    #print('Type:')\n    #param_decl.type.show(offset=6)\n\n# The body is of FuncDef is a Compound, which is a placeholder for a block\n# surrounded by {} (You should be reading _c_ast.cfg parallel to this\n# explanation and seeing these things with your own eyes).\n# Let's see the block's declarations:\n\nfunction_body = ast.ext[2].body\n\n# The following displays the declarations and statements in the function\n# body\n\n#for decl in function_body.block_items:\n    #decl.show()\n\n# We can see a single variable declaration, i, declared to be a simple type\n# declaration of type 'unsigned int', followed by statements.\n\n# block_items is a list, so the third element is the For statement:\n\nfor_stmt = function_body.block_items[2]\n#for_stmt.show()\n\n# As you can see in _c_ast.cfg, For's children are 'init, cond,\n# next' for the respective parts of the 'for' loop specifier,\n# and stmt, which is either a single stmt or a Compound if there's\n# a block.\n#\n# Let's dig deeper, to the while statement inside the for loop:\n\nwhile_stmt = for_stmt.stmt.block_items[1]\n#while_stmt.show()\n\n# While is simpler, it only has a condition node and a stmt node.\n# The condition:\n\nwhile_cond = while_stmt.cond\n#while_cond.show()\n\n# Note that it's a BinaryOp node - the basic constituent of\n# expressions in our AST. BinaryOp is the expression tree, with\n# left and right nodes as children. It also has the op attribute,\n# which is just the string representation of the operator.\n\n#print(while_cond.op)\n#while_cond.left.show()\n#while_cond.right.show()\n\n\n# That's it for the example. I hope you now see how easy it is to explore the\n# AST created by pycparser. Although on the surface it is quite complex and has\n# a lot of node types, this is the inherent complexity of the C language every\n# parser/compiler designer has to cope with.\n# Using the tools provided by the c_ast package it's easy to explore the\n# structure of AST nodes and write code that processes them.\n# Specifically, see the cdecl.py example for a non-trivial demonstration of what\n# you can do by recursively going through the AST.\n", "examples/func_calls.py": "#-----------------------------------------------------------------\n# pycparser: func_calls.py\n#\n# Using pycparser for printing out all the calls of some function\n# in a C file.\n#\n# Eli Bendersky [https://eli.thegreenplace.net/]\n# License: BSD\n#-----------------------------------------------------------------\nimport sys\n\n# This is not required if you've installed pycparser into\n# your site-packages/ with setup.py\nsys.path.extend(['.', '..'])\n\nfrom pycparser import c_ast, parse_file\n\n# A visitor with some state information (the funcname it's looking for)\nclass FuncCallVisitor(c_ast.NodeVisitor):\n    def __init__(self, funcname):\n        self.funcname = funcname\n\n    def visit_FuncCall(self, node):\n        if node.name.name == self.funcname:\n            print('%s called at %s' % (self.funcname, node.name.coord))\n        # Visit args in case they contain more func calls.\n        if node.args:\n            self.visit(node.args)\n\n\ndef show_func_calls(filename, funcname):\n    ast = parse_file(filename, use_cpp=True)\n    v = FuncCallVisitor(funcname)\n    v.visit(ast)\n\n\nif __name__ == \"__main__\":\n    if len(sys.argv) > 2:\n        filename = sys.argv[1]\n        func = sys.argv[2]\n    else:\n        filename = 'examples/c_files/basic.c'\n        func = 'foo'\n\n    show_func_calls(filename, func)\n", "examples/construct_ast_from_scratch.py": "# -----------------------------------------------------------------\n# pycparser: construct_ast_from_scratch.py\n#\n# Tiny example of writing an AST from scratch to C code.\n#\n# Andre Ribeiro [https://github.com/Andree37]\n# License: BSD\n# -----------------------------------------------------------------\nimport sys\n\n# This is not required if you've installed pycparser into\n# your site-packages/ with setup.py\nsys.path.extend(['.', '..'])\n\nfrom pycparser import c_ast, c_generator\n\n\n# target C code:\n# int main() {\n#     return 0;\n# }\n\n\ndef empty_main_function_ast():\n    constant_zero = c_ast.Constant(type='int', value='0')\n    return_node = c_ast.Return(expr=constant_zero)\n    compound_node = c_ast.Compound(block_items=[return_node])\n    type_decl_node = c_ast.TypeDecl(declname='main', quals=[],\n                                    type=c_ast.IdentifierType(names=['int']),\n                                    align=[])\n    func_decl_node = c_ast.FuncDecl(args=c_ast.ParamList([]),\n                                    type=type_decl_node)\n    func_def_node = c_ast.Decl(name='main', quals=[], storage=[], funcspec=[],\n                               type=func_decl_node, init=None,\n                               bitsize=None, align=[])\n    main_func_node = c_ast.FuncDef(decl=func_def_node, param_decls=None,\n                                   body=compound_node)\n\n    return main_func_node\n\n\ndef generate_c_code(my_ast):\n    generator = c_generator.CGenerator()\n    return generator.visit(my_ast)\n\n\nif __name__ == '__main__':\n    main_function_ast = empty_main_function_ast()\n    print(\"|----------------------------------------|\")\n    main_function_ast.show(offset=2)\n    print(\"|----------------------------------------|\")\n    main_c_code = generate_c_code(main_function_ast)\n    print(\"C code: \\n%s\" % main_c_code)\n", "examples/func_defs.py": "#-----------------------------------------------------------------\n# pycparser: func_defs.py\n#\n# Using pycparser for printing out all the functions defined in a\n# C file.\n#\n# This is a simple example of traversing the AST generated by\n# pycparser. Call it from the root directory of pycparser.\n#\n# Eli Bendersky [https://eli.thegreenplace.net/]\n# License: BSD\n#-----------------------------------------------------------------\nimport sys\n\n# This is not required if you've installed pycparser into\n# your site-packages/ with setup.py\nsys.path.extend(['.', '..'])\n\nfrom pycparser import c_ast, parse_file\n\n\n# A simple visitor for FuncDef nodes that prints the names and\n# locations of function definitions.\nclass FuncDefVisitor(c_ast.NodeVisitor):\n    def visit_FuncDef(self, node):\n        print('%s at %s' % (node.decl.name, node.decl.coord))\n\n\ndef show_func_defs(filename):\n    # Note that cpp is used. Provide a path to your own cpp or\n    # make sure one exists in PATH.\n    ast = parse_file(filename, use_cpp=True,\n                     cpp_args=r'-Iutils/fake_libc_include')\n\n    v = FuncDefVisitor()\n    v.visit(ast)\n\n\nif __name__ == \"__main__\":\n    if len(sys.argv) > 1:\n        filename  = sys.argv[1]\n    else:\n        filename = 'examples/c_files/memmgr.c'\n\n    show_func_defs(filename)\n", "examples/cdecl.py": "#-----------------------------------------------------------------\n# pycparser: cdecl.py\n#\n# Example of the CDECL tool using pycparser. CDECL \"explains\" C type\n# declarations in plain English.\n#\n# The AST generated by pycparser from the given declaration is traversed\n# recursively to build the explanation. Note that the declaration must be a\n# valid external declaration in C. As shown below, typedef can be optionally\n# expanded.\n#\n# For example:\n#\n#   c_decl = 'typedef int Node; const Node* (*ar)[10];'\n#\n#   explain_c_declaration(c_decl)\n#   => ar is a pointer to array[10] of pointer to const Node\n#\n# struct and typedef can be optionally expanded:\n#\n#   explain_c_declaration(c_decl, expand_typedef=True)\n#   => ar is a pointer to array[10] of pointer to const int\n#\n#   c_decl = 'struct P {int x; int y;} p;'\n#\n#   explain_c_declaration(c_decl)\n#   => p is a struct P\n#\n#   explain_c_declaration(c_decl, expand_struct=True)\n#   => p is a struct P containing {x is a int, y is a int}\n#\n# Eli Bendersky [https://eli.thegreenplace.net/]\n# License: BSD\n#-----------------------------------------------------------------\nimport copy\nimport sys\n\n# This is not required if you've installed pycparser into\n# your site-packages/ with setup.py\n#\nsys.path.extend(['.', '..'])\n\nfrom pycparser import c_parser, c_ast\n\n\ndef explain_c_declaration(c_decl, expand_struct=False, expand_typedef=False):\n    \"\"\" Parses the declaration in c_decl and returns a text\n        explanation as a string.\n\n        The last external node of the string is used, to allow earlier typedefs\n        for used types.\n\n        expand_struct=True will spell out struct definitions recursively.\n        expand_typedef=True will expand typedef'd types.\n    \"\"\"\n    parser = c_parser.CParser()\n\n    try:\n        node = parser.parse(c_decl, filename='<stdin>')\n    except c_parser.ParseError:\n        e = sys.exc_info()[1]\n        return \"Parse error:\" + str(e)\n\n    if (not isinstance(node, c_ast.FileAST) or\n        not isinstance(node.ext[-1], c_ast.Decl)\n        ):\n        return \"Not a valid declaration\"\n\n    try:\n        expanded = expand_struct_typedef(node.ext[-1], node,\n                                         expand_struct=expand_struct,\n                                         expand_typedef=expand_typedef)\n    except Exception as e:\n        return \"Not a valid declaration: \" + str(e)\n\n    return _explain_decl_node(expanded)\n\n\ndef _explain_decl_node(decl_node):\n    \"\"\" Receives a c_ast.Decl note and returns its explanation in\n        English.\n    \"\"\"\n    storage = ' '.join(decl_node.storage) + ' ' if decl_node.storage else ''\n\n    return (decl_node.name +\n            \" is a \" +\n            storage +\n            _explain_type(decl_node.type))\n\n\ndef _explain_type(decl):\n    \"\"\" Recursively explains a type decl node\n    \"\"\"\n    typ = type(decl)\n\n    if typ == c_ast.TypeDecl:\n        quals = ' '.join(decl.quals) + ' ' if decl.quals else ''\n        return quals + _explain_type(decl.type)\n    elif typ == c_ast.Typename or typ == c_ast.Decl:\n        return _explain_type(decl.type)\n    elif typ == c_ast.IdentifierType:\n        return ' '.join(decl.names)\n    elif typ == c_ast.PtrDecl:\n        quals = ' '.join(decl.quals) + ' ' if decl.quals else ''\n        return quals + 'pointer to ' + _explain_type(decl.type)\n    elif typ == c_ast.ArrayDecl:\n        arr = 'array'\n        if decl.dim: arr += '[%s]' % decl.dim.value\n\n        return arr + \" of \" + _explain_type(decl.type)\n\n    elif typ == c_ast.FuncDecl:\n        if decl.args:\n            params = [_explain_type(param) for param in decl.args.params]\n            args = ', '.join(params)\n        else:\n            args = ''\n\n        return ('function(%s) returning ' % (args) +\n                _explain_type(decl.type))\n\n    elif typ == c_ast.Struct:\n        decls = [_explain_decl_node(mem_decl) for mem_decl in decl.decls]\n        members = ', '.join(decls)\n\n        return ('struct%s ' % (' ' + decl.name if decl.name else '') +\n                ('containing {%s}' % members if members else ''))\n\n\ndef expand_struct_typedef(cdecl, file_ast,\n                          expand_struct=False,\n                          expand_typedef=False):\n    \"\"\"Expand struct & typedef and return a new expanded node.\"\"\"\n    decl_copy = copy.deepcopy(cdecl)\n    _expand_in_place(decl_copy, file_ast, expand_struct, expand_typedef)\n    return decl_copy\n\n\ndef _expand_in_place(decl, file_ast, expand_struct=False, expand_typedef=False):\n    \"\"\"Recursively expand struct & typedef in place, throw RuntimeError if\n       undeclared struct or typedef are used\n    \"\"\"\n    typ = type(decl)\n\n    if typ in (c_ast.Decl, c_ast.TypeDecl, c_ast.PtrDecl, c_ast.ArrayDecl):\n        decl.type = _expand_in_place(decl.type, file_ast, expand_struct,\n                                     expand_typedef)\n\n    elif typ == c_ast.Struct:\n        if not decl.decls:\n            struct = _find_struct(decl.name, file_ast)\n            if not struct:\n                raise RuntimeError('using undeclared struct %s' % decl.name)\n            decl.decls = struct.decls\n\n        for i, mem_decl in enumerate(decl.decls):\n            decl.decls[i] = _expand_in_place(mem_decl, file_ast, expand_struct,\n                                             expand_typedef)\n        if not expand_struct:\n            decl.decls = []\n\n    elif (typ == c_ast.IdentifierType and\n          decl.names[0] not in ('int', 'char')):\n        typedef = _find_typedef(decl.names[0], file_ast)\n        if not typedef:\n            raise RuntimeError('using undeclared type %s' % decl.names[0])\n\n        if expand_typedef:\n            return typedef.type\n\n    return decl\n\n\ndef _find_struct(name, file_ast):\n    \"\"\"Receives a struct name and return declared struct object in file_ast\n    \"\"\"\n    for node in file_ast.ext:\n        if (type(node) == c_ast.Decl and\n           type(node.type) == c_ast.Struct and\n           node.type.name == name):\n            return node.type\n\n\ndef _find_typedef(name, file_ast):\n    \"\"\"Receives a type name and return typedef object in file_ast\n    \"\"\"\n    for node in file_ast.ext:\n        if type(node) == c_ast.Typedef and node.name == name:\n            return node\n\n\nif __name__ == \"__main__\":\n    if len(sys.argv) > 1:\n        c_decl  = sys.argv[1]\n    else:\n        c_decl = \"char *(*(**foo[][8])())[];\"\n\n    print(\"Explaining the declaration: \" + c_decl + \"\\n\")\n    print(explain_c_declaration(c_decl) + \"\\n\")\n", "examples/c_json.py": "#------------------------------------------------------------------------------\n# pycparser: c_json.py\n#\n# by Michael White (@mypalmike)\n#\n# This example includes functions to serialize and deserialize an ast\n# to and from json format. Serializing involves walking the ast and converting\n# each node from a python Node object into a python dict. Deserializing\n# involves the opposite conversion, walking the tree formed by the\n# dict and converting each dict into the specific Node object it represents.\n# The dict itself is serialized and deserialized using the python json module.\n#\n# The dict representation is a fairly direct transformation of the object\n# attributes. Each node in the dict gets one metadata field referring to the\n# specific node class name, _nodetype. Each local attribute (i.e. not linking\n# to child nodes) has a string value or array of string values. Each child\n# attribute is either another dict or an array of dicts, exactly as in the\n# Node object representation. The \"coord\" attribute, representing the\n# node's location within the source code, is serialized/deserialized from\n# a Coord object into a string of the format \"filename:line[:column]\".\n#\n# Example TypeDecl node, with IdentifierType child node, represented as a dict:\n#     \"type\": {\n#         \"_nodetype\": \"TypeDecl\",\n#         \"coord\": \"c_files/funky.c:8\",\n#         \"declname\": \"o\",\n#         \"quals\": [],\n#         \"type\": {\n#             \"_nodetype\": \"IdentifierType\",\n#             \"coord\": \"c_files/funky.c:8\",\n#             \"names\": [\n#                 \"char\"\n#             ]\n#         }\n#     }\n#------------------------------------------------------------------------------\nimport json\nimport sys\nimport re\n\n# This is not required if you've installed pycparser into\n# your site-packages/ with setup.py\n#\nsys.path.extend(['.', '..'])\n\nfrom pycparser import parse_file, c_ast\nfrom pycparser.plyparser import Coord\n\n\nRE_CHILD_ARRAY = re.compile(r'(.*)\\[(.*)\\]')\nRE_INTERNAL_ATTR = re.compile('__.*__')\n\n\nclass CJsonError(Exception):\n    pass\n\n\ndef memodict(fn):\n    \"\"\" Fast memoization decorator for a function taking a single argument \"\"\"\n    class memodict(dict):\n        def __missing__(self, key):\n            ret = self[key] = fn(key)\n            return ret\n    return memodict().__getitem__\n\n\n@memodict\ndef child_attrs_of(klass):\n    \"\"\"\n    Given a Node class, get a set of child attrs.\n    Memoized to avoid highly repetitive string manipulation\n\n    \"\"\"\n    non_child_attrs = set(klass.attr_names)\n    all_attrs = set([i for i in klass.__slots__ if not RE_INTERNAL_ATTR.match(i)])\n    return all_attrs - non_child_attrs\n\n\ndef to_dict(node):\n    \"\"\" Recursively convert an ast into dict representation. \"\"\"\n    klass = node.__class__\n\n    result = {}\n\n    # Metadata\n    result['_nodetype'] = klass.__name__\n\n    # Local node attributes\n    for attr in klass.attr_names:\n        result[attr] = getattr(node, attr)\n\n    # Coord object\n    if node.coord:\n        result['coord'] = str(node.coord)\n    else:\n        result['coord'] = None\n\n    # Child attributes\n    for child_name, child in node.children():\n        # Child strings are either simple (e.g. 'value') or arrays (e.g. 'block_items[1]')\n        match = RE_CHILD_ARRAY.match(child_name)\n        if match:\n            array_name, array_index = match.groups()\n            array_index = int(array_index)\n            # arrays come in order, so we verify and append.\n            result[array_name] = result.get(array_name, [])\n            if array_index != len(result[array_name]):\n                raise CJsonError('Internal ast error. Array {} out of order. '\n                    'Expected index {}, got {}'.format(\n                    array_name, len(result[array_name]), array_index))\n            result[array_name].append(to_dict(child))\n        else:\n            result[child_name] = to_dict(child)\n\n    # Any child attributes that were missing need \"None\" values in the json.\n    for child_attr in child_attrs_of(klass):\n        if child_attr not in result:\n            result[child_attr] = None\n\n    return result\n\n\ndef to_json(node, **kwargs):\n    \"\"\" Convert ast node to json string \"\"\"\n    return json.dumps(to_dict(node), **kwargs)\n\n\ndef file_to_dict(filename):\n    \"\"\" Load C file into dict representation of ast \"\"\"\n    ast = parse_file(filename, use_cpp=True)\n    return to_dict(ast)\n\n\ndef file_to_json(filename, **kwargs):\n    \"\"\" Load C file into json string representation of ast \"\"\"\n    ast = parse_file(filename, use_cpp=True)\n    return to_json(ast, **kwargs)\n\n\ndef _parse_coord(coord_str):\n    \"\"\" Parse coord string (file:line[:column]) into Coord object. \"\"\"\n    if coord_str is None:\n        return None\n\n    vals = coord_str.split(':')\n    vals.extend([None] * 3)\n    filename, line, column = vals[:3]\n    return Coord(filename, line, column)\n\n\ndef _convert_to_obj(value):\n    \"\"\"\n    Convert an object in the dict representation into an object.\n    Note: Mutually recursive with from_dict.\n\n    \"\"\"\n    value_type = type(value)\n    if value_type == dict:\n        return from_dict(value)\n    elif value_type == list:\n        return [_convert_to_obj(item) for item in value]\n    else:\n        # String\n        return value\n\n\ndef from_dict(node_dict):\n    \"\"\" Recursively build an ast from dict representation \"\"\"\n    class_name = node_dict.pop('_nodetype')\n\n    klass = getattr(c_ast, class_name)\n\n    # Create a new dict containing the key-value pairs which we can pass\n    # to node constructors.\n    objs = {}\n    for key, value in node_dict.items():\n        if key == 'coord':\n            objs[key] = _parse_coord(value)\n        else:\n            objs[key] = _convert_to_obj(value)\n\n    # Use keyword parameters, which works thanks to beautifully consistent\n    # ast Node initializers.\n    return klass(**objs)\n\n\ndef from_json(ast_json):\n    \"\"\" Build an ast from json string representation \"\"\"\n    return from_dict(json.loads(ast_json))\n\n\n#------------------------------------------------------------------------------\nif __name__ == \"__main__\":\n    if len(sys.argv) > 1:\n        # Some test code...\n        # Do trip from C -> ast -> dict -> ast -> json, then print.\n        ast_dict = file_to_dict(sys.argv[1])\n        ast = from_dict(ast_dict)\n        print(to_json(ast, sort_keys=True, indent=4))\n    else:\n        print(\"Please provide a filename as argument\")\n", "examples/dump_ast.py": "#-----------------------------------------------------------------\n# pycparser: dump_ast.py\n#\n# Basic example of parsing a file and dumping its parsed AST.\n#\n# Eli Bendersky [https://eli.thegreenplace.net/]\n# License: BSD\n#-----------------------------------------------------------------\nimport argparse\nimport sys\n\n# This is not required if you've installed pycparser into\n# your site-packages/ with setup.py\nsys.path.extend(['.', '..'])\n\nfrom pycparser import parse_file\n\nif __name__ == \"__main__\":\n    argparser = argparse.ArgumentParser('Dump AST')\n    argparser.add_argument('filename',\n                           default='examples/c_files/basic.c',\n                           nargs='?',\n                           help='name of file to parse')\n    argparser.add_argument('--coord', help='show coordinates in the dump',\n                           action='store_true')\n    args = argparser.parse_args()\n\n    ast = parse_file(args.filename, use_cpp=False)\n    ast.show(showcoord=args.coord)\n", "examples/serialize_ast.py": "#-----------------------------------------------------------------\n# pycparser: serialize_ast.py\n#\n# Simple example of serializing AST\n#\n# Hart Chu [https://github.com/CtheSky]\n# Eli Bendersky [https://eli.thegreenplace.net/]\n# License: BSD\n#-----------------------------------------------------------------\nimport pickle\nimport sys\n\nsys.path.extend(['.', '..'])\nfrom pycparser import c_parser\n\ntext = r\"\"\"\nvoid func(void)\n{\n  x = 1;\n}\n\"\"\"\n\nif __name__ == '__main__':\n    parser = c_parser.CParser()\n    ast = parser.parse(text)\n    dump_filename = 'ast.pickle'\n\n    with open(dump_filename, 'wb') as f:\n        pickle.dump(ast, f, protocol=pickle.HIGHEST_PROTOCOL)\n\n    # Deserialize.\n    with open(dump_filename, 'rb') as f:\n        ast = pickle.load(f)\n        ast.show()\n", "examples/c-to-c.py": "#------------------------------------------------------------------------------\n# pycparser: c-to-c.py\n#\n# Example of using pycparser.c_generator, serving as a simplistic translator\n# from C to AST and back to C.\n#\n# Eli Bendersky [https://eli.thegreenplace.net/]\n# License: BSD\n#------------------------------------------------------------------------------\nimport sys\n\n# This is not required if you've installed pycparser into\n# your site-packages/ with setup.py\nsys.path.extend(['.', '..'])\n\nfrom pycparser import parse_file, c_generator\n\n\ndef translate_to_c(filename):\n    \"\"\" Simply use the c_generator module to emit a parsed AST.\n    \"\"\"\n    ast = parse_file(filename, use_cpp=True)\n    generator = c_generator.CGenerator()\n    print(generator.visit(ast))\n\n\nif __name__ == \"__main__\":\n    if len(sys.argv) > 1:\n        translate_to_c(sys.argv[1])\n    else:\n        print(\"Please provide a filename as argument\")\n", "examples/using_gcc_E_libc.py": "#-------------------------------------------------------------------------------\n# pycparser: using_gcc_E_libc.py\n#\n# Similar to the using_cpp_libc.py example, but uses 'gcc -E' instead\n# of 'cpp'. The same can be achieved with Clang instead of gcc. If you have\n# Clang installed, simply replace 'gcc' with 'clang' here.\n#\n# Eli Bendersky [https://eli.thegreenplace.net/]\n# License: BSD\n#-------------------------------------------------------------------------------\nimport sys\n\n# This is not required if you've installed pycparser into\n# your site-packages/ with setup.py\n#\nsys.path.extend(['.', '..'])\n\nfrom pycparser import parse_file\n\n\nif __name__ == \"__main__\":\n    if len(sys.argv) > 1:\n        filename  = sys.argv[1]\n    else:\n        filename = 'examples/c_files/year.c'\n\n    ast = parse_file(filename, use_cpp=True,\n            cpp_path='gcc',\n            cpp_args=['-E', r'-Iutils/fake_libc_include'])\n    ast.show()\n", "examples/func_defs_add_param.py": "#-----------------------------------------------------------------\n# pycparser: func_defs_add_param.py\n#\n# Example of rewriting AST nodes to add parameters to function\n# definitions. Adds an \"int _hidden\" to every function.\n#\n# Eli Bendersky [https://eli.thegreenplace.net/]\n# License: BSD\n#-----------------------------------------------------------------\nimport sys\nsys.path.extend(['.', '..'])\n\nfrom pycparser import c_parser, c_ast, c_generator\n\ntext = r\"\"\"\nvoid foo(int a, int b) {\n}\n\nvoid bar() {\n}\n\"\"\"\n\n\nclass ParamAdder(c_ast.NodeVisitor):\n    def visit_FuncDecl(self, node):\n        ty = c_ast.TypeDecl(declname='_hidden',\n                            quals=[],\n                            align=[],\n                            type=c_ast.IdentifierType(['int']))\n        newdecl = c_ast.Decl(\n                    name='_hidden',\n                    quals=[],\n                    align=[],\n                    storage=[],\n                    funcspec=[],\n                    type=ty,\n                    init=None,\n                    bitsize=None,\n                    coord=node.coord)\n        if node.args:\n            node.args.params.append(newdecl)\n        else:\n            node.args = c_ast.ParamList(params=[newdecl])\n\n\nif __name__ == '__main__':\n    parser = c_parser.CParser()\n    ast = parser.parse(text)\n    print(\"AST before change:\")\n    ast.show(offset=2)\n\n    v = ParamAdder()\n    v.visit(ast)\n\n    print(\"\\nAST after change:\")\n    ast.show(offset=2)\n\n    print(\"\\nCode after change:\")\n    generator = c_generator.CGenerator()\n    print(generator.visit(ast))\n", "examples/rewrite_ast.py": "#-----------------------------------------------------------------\n# pycparser: rewrite_ast.py\n#\n# Tiny example of rewriting a AST node\n#\n# Eli Bendersky [https://eli.thegreenplace.net/]\n# License: BSD\n#-----------------------------------------------------------------\nimport sys\n\nsys.path.extend(['.', '..'])\nfrom pycparser import c_parser\n\ntext = r\"\"\"\nvoid func(void)\n{\n  x = 1;\n}\n\"\"\"\n\nif __name__ == '__main__':\n    parser = c_parser.CParser()\n    ast = parser.parse(text)\n    print(\"Before:\")\n    ast.show(offset=2)\n\n    assign = ast.ext[0].body.block_items[0]\n    assign.lvalue.name = \"y\"\n    assign.rvalue.value = 2\n\n    print(\"After:\")\n    ast.show(offset=2)\n", "examples/using_cpp_libc.py": "#-----------------------------------------------------------------\n# pycparser: using_cpp_libc.py\n#\n# Shows how to use the provided 'cpp' (on Windows, substitute for\n# the 'real' cpp if you're on Linux/Unix) and \"fake\" libc includes\n# to parse a file that includes standard C headers.\n#\n# Eli Bendersky [https://eli.thegreenplace.net/]\n# License: BSD\n#-----------------------------------------------------------------\nimport sys\n\n# This is not required if you've installed pycparser into\n# your site-packages/ with setup.py\n#\nsys.path.extend(['.', '..'])\n\nfrom pycparser import parse_file\n\n\nif __name__ == \"__main__\":\n    if len(sys.argv) > 1:\n        filename  = sys.argv[1]\n    else:\n        filename = 'examples/c_files/year.c'\n\n    ast = parse_file(filename, use_cpp=True,\n            cpp_path='cpp',\n            cpp_args=r'-Iutils/fake_libc_include')\n    ast.show()\n"}